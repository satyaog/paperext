{
  "paper": "2402.01788.txt",
  "words": 6049,
  "extractions": {
    "title": {
      "value": "LitLLM: A Toolkit for Scientific Literature Review",
      "justification": "The title specifically names the toolkit and succinctly describes its purpose.",
      "quote": "LitLLM: A Toolkit for Scientific Literature Review"
    },
    "description": "This paper introduces LitLLM, an open-source toolkit designed to expedite the process of conducting literature reviews by leveraging Retrieval Augmented Generation (RAG) techniques combined with large language models (LLMs) for improving factual accuracy and reducing hallucinations.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper provides an empirical evaluation of the toolkit by allowing researchers to test the system and validate its efficacy in generating literature reviews.",
      "quote": "As a preliminary study, we provided access to our user interface to 5 different researchers who worked through the demo to write literature reviews and validate the systemâ€™s efficacy."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper uses NLP techniques and state-of-the-art LLMs to improve literature review generation.",
        "quote": "LLMs have demonstrated significant capabilities in storing factual knowledge and achieving state-of-the-art results when fine-tuned on downstream Natural Language Processing (NLP) tasks (Lewis et al., 2020)."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Automated Literature Review",
          "justification": "The primary focus of the paper is on automating the literature review process using AI and NLP techniques.",
          "quote": "Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Flamingo",
          "justification": "The paper discusses the potential use of Flamingo for image captioning and its interaction with other models.",
          "quote": "As DALL-E and Flamingo are not publicly available, we use Stable Diffusion and BLIP in the remaining work."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_inference_only": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Stable Diffusion",
          "justification": "The paper utilizes Stable Diffusion for generating images from text descriptions as an alternative to DALL-E.",
          "quote": "As DALL-E and Flamingo are not publicly available, we use Stable Diffusion and BLIP in the remaining work."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_inference_only": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BLIP",
          "justification": "The paper employs BLIP for both text-to-image and image-to-text generation tasks in the proposed toolkit.",
          "quote": "As DALL-E and Flamingo are not publicly available, we use Stable Diffusion and BLIP in the remaining work."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_inference_only": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Wikipedia-based Image Text",
          "justification": "The WIT dataset is referenced as a rich resource for multimodal learning in the context of the paper.",
          "quote": "The work of [4] introduces the Wikipedia-based Image Text (WIT) Dataset, a large-scale dataset for multimodal, multilingual learning."
        },
        "aliases": [
          "WIT) Dataset"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Gradio",
          "justification": "The paper mentions using Gradio to build and showcase the LitLLM system, providing an interface for users.",
          "quote": "We build our system using Gradio (Abid et al., 2019), which provides a nice interface to quickly and efficiently build system demos."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Huggingface",
          "justification": "The paper uses Huggingface space to host the LitLLM tool, making it accessible for users.",
          "quote": "Our open-source toolkit is accessible at https://github.com/ shubhamagarwal92/LitLLM and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) with the video demo at https://youtu.be/E2ggOZBAFw0"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "OpenAI API",
          "justification": "The paper leverages the OpenAI API for utilizing LLMs like GPT-3.5-turbo and GPT-4 in the toolkit.",
          "quote": "In this work, we use OpenAI API8 to generate results for LLM using GPT-3.5-turbo and GPT-4 model."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Semantic Scholar API",
          "justification": "The paper retrieves relevant academic papers using the Semantic Scholar API to aid the literature review process.",
          "quote": "In our toolkit, we retrieve relevant papers using the Semantic Scholar API."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1095,
    "prompt_tokens": 10149,
    "total_tokens": 11244
  }
}