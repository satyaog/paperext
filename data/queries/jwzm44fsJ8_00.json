{
  "paper": "jwzm44fsJ8.txt",
  "words": 9767,
  "extractions": {
    "title": {
      "value": "Multilingual Code Retrieval Without Paired Data: New Datasets and Benchmarks",
      "justification": "The title accurately encapsulates the primary focus and contributions of the paper, which involve multilingual code retrieval without relying on paired datasets, along with the introduction of new datasets and benchmarks.",
      "quote": "We seek to overcome limitations to code retrieval quality posed by the scarcity of data containing pairs of code snippets and natural language queries in languages other than English. To do so, we introduce two new datasets."
    },
    "description": "The paper introduces new datasets and benchmarks for multilingual code retrieval tasks, addressing the challenge of the scarcity of paired data for non-English languages. The authors propose a method to map between multiple natural languages and programming languages without requiring directly paired datasets by leveraging existing translation datasets and introducing a new evaluation benchmark called M2CRB. The paper includes extensive evaluations of the proposed approach and comparison with existing code retrieval and generation benchmarks.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves extensive evaluations and empirical analysis of different models and training approaches for multilingual code retrieval.",
      "quote": "Extensive evaluations on both our new tasks as well as on existing code-to-code search benchmarks confirm our hypothesis: models are able to generalize to unseen language pairs they indirectly observed during training."
    },
    "primary_research_field": {
      "name": {
        "value": "Multilingual Code Retrieval",
        "justification": "The paper focuses on the retrieval of code snippets using queries in multiple natural languages without relying on paired data.",
        "quote": "We address the limitations posed above and enable training of models to map multi-source-natural-language queries to multi-target-programming-language code."
      },
      "aliases": [
        "MCR"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The research encompasses multiple natural languages for code retrieval tasks, which falls under the domain of Natural Language Processing.",
          "quote": "The dataset is curated via an automated filtering pipeline from source files within G IT H UB followed by human verification to ensure accurate language classification."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Code Search",
          "justification": "The paper extends existing code-to-code and text-to-code benchmarks, specifically focusing on code search.",
          "quote": "Extensive evaluations on both our new tasks as well as on existing code-to-code search benchmarks confirm our hypothesis."
        },
        "aliases": [
          "CS"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "AlphaCode",
          "justification": "AlphaCode is one of the models mentioned as making remarkable progress in code generation from natural language queries.",
          "quote": "In the generative setting for instance, code models such as AlphaCode (Li et al., 2022b) obtained human-level performance when generating code from competitive programming problem statements posed in plain English."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "AlphaCode is referenced for comparison purposes and is not a contribution of this paper.",
          "quote": "In the generative setting for instance, code models such as AlphaCode (Li et al., 2022b) obtained human-level performance when generating code from competitive programming problem statements posed in plain English."
        },
        "is_executed": {
          "value": 0,
          "justification": "AlphaCode is not executed within the paper.",
          "quote": "code models such as AlphaCode (Li et al., 2022b) obtained human-level performance when generating code from competitive programming problem statements posed in plain English"
        },
        "is_compared": {
          "value": 0,
          "justification": "AlphaCode is mentioned for context and is not numerically compared with other models in this paper.",
          "quote": "code models such as AlphaCode (Li et al., 2022b) obtained human-level performance when generating code from competitive programming problem statements posed in plain English."
        },
        "referenced_paper_title": {
          "value": "Competition-Level Code Generation with AlphaCode",
          "justification": "Provides the full title for the referenced model to enable readers to find the original source.",
          "quote": "code models such as AlphaCode (Li et al., 2022b) obtained human-level performance when generating code from competitive programming problem statements posed in plain English."
        }
      },
      {
        "name": {
          "value": "CodeBERT",
          "justification": "CodeBERT is one of the models evaluated and compared in the paper for code search tasks.",
          "quote": "Moreover, approaches such as CodeBERT (Feng et al., 2020) highlighted that representations extracted from BERT-like models (Devlin et al., 2018; Liu et al., 2019) trained on code succeed on code search upon fine-tuning on specific language pairs."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "CodeBERT is not introduced in this paper but is used as a baseline for comparison.",
          "quote": "Moreover, approaches such as CodeBERT (Feng et al., 2020) highlighted that representations extracted from BERT-like models (Devlin et al., 2018; Liu et al., 2019) trained on code succeed on code search upon fine-tuning on specific language pairs."
        },
        "is_executed": {
          "value": 1,
          "justification": "CodeBERT is executed in the experiments conducted in the paper.",
          "quote": "Moreover, approaches such as CodeBERT (Feng et al., 2020) highlighted that representations extracted from BERT-like models (Devlin et al., 2018; Liu et al., 2019) trained on code succeed on code search upon fine-tuning on specific language pairs."
        },
        "is_compared": {
          "value": 1,
          "justification": "CodeBERT is numerically compared with other models in the experiments.",
          "quote": "We then evaluate models both on our new dataset and on other two existing benchmarks to address questions such as: Are models able to generalize to language combinations only indirectly paired during training?"
        },
        "referenced_paper_title": {
          "value": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
          "justification": "Provides the full title for the referenced model to enable readers to find the original source.",
          "quote": "Moreover, approaches such as CodeBERT (Feng et al., 2020) highlighted that representations extracted from BERT-like models (Devlin et al., 2018; Liu et al., 2019) trained on code succeed on code search upon fine-tuning on specific language pairs."
        }
      },
      {
        "name": {
          "value": "CodeGen",
          "justification": "CodeGen is one of the models mentioned as making remarkable progress in code generation from natural language queries.",
          "quote": "In the generative setting for instance, code models such as CodeGen (Nijkamp et al., 2022a), PALM (Chowdhery et al., 2022), StarCoder (Li et al., 2023), WizardCoder (Luo et al., 2023), and LLAMA (Touvron et al., 2023), to name but a few."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "CodeGen is not introduced in this paper but is used as a reference.",
          "quote": "code models such as CodeGen (Nijkamp et al., 2022a), PALM (Chowdhery et al., 2022), StarCoder (Li et al., 2023), WizardCoder (Luo et al., 2023), and LLAMA (Touvron et al., 2023), to name but a few"
        },
        "is_executed": {
          "value": 1,
          "justification": "CodeGen is executed in the experiments conducted in the paper.",
          "quote": "Namely, we fine-tune popular models such as CodeT5 (Wang et al., 2021), CodeBERT (Feng et al., 2020), and CodeGen (Nijkamp et al., 2022a), and further consider dual-encoder models."
        },
        "is_compared": {
          "value": 1,
          "justification": "CodeGen is numerically compared with other models in the experiments.",
          "quote": "Evaluations are split into three parts where the first two evaluate to what extent models manage to generalize to unseen language combinations."
        },
        "referenced_paper_title": {
          "value": "A Conversational Paradigm for Program Synthesis",
          "justification": "Provides the full title for the referenced model to enable readers to find the original source.",
          "quote": "code models such as CodeGen (Nijkamp et al., 2022a)"
        }
      },
      {
        "name": {
          "value": "CodeRetriever",
          "justification": "CodeRetriever is mentioned as one close to the fine-tuning approach used in the paper, leveraging contrastive techniques for learning.",
          "quote": "Similarly, CodeRetriever (Li et al., 2022a) combines contrastive objectives using an unsupervised approach to determine similar function pairs."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "CodeRetriever is not introduced in this paper but is mentioned as related work.",
          "quote": "Similarly, CodeRetriever (Li et al., 2022a) combines contrastive objectives using an unsupervised approach to determine similar function pairs."
        },
        "is_executed": {
          "value": 0,
          "justification": "CodeRetriever is not executed within the paper.",
          "quote": "Similarly, CodeRetriever (Li et al., 2022a) combines contrastive objectives using an unsupervised approach to determine similar function pairs."
        },
        "is_compared": {
          "value": 0,
          "justification": "CodeRetriever is mentioned as related work and is not numerically compared with other models in this paper.",
          "quote": "Similarly, CodeRetriever (Li et al., 2022a) combines contrastive objectives using an unsupervised approach to determine similar function pairs."
        },
        "referenced_paper_title": {
          "value": "CodeRetriever: Unimodal and Bimodal Contrastive Learning",
          "justification": "Provides the full title for the referenced model to enable readers to find the original source.",
          "quote": "Similarly, CodeRetriever (Li et al., 2022a) combines contrastive objectives using an unsupervised approach to determine similar function pairs."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "M2CRB",
          "justification": "M2CRB is directly introduced and described in detail as one of the primary contributions of the paper.",
          "quote": "First, we make a new evaluation benchmark available, dubbed M2CRB, containing pairs of text and code, for multiple natural and programming language pairs."
        },
        "aliases": [
          "M2CRB"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "As this is a newly contributed dataset, there is no referenced paper title.",
          "quote": "First, we make a new evaluation benchmark available, dubbed M2CRB, containing pairs of text and code, for multiple natural and programming language pairs."
        }
      },
      {
        "name": {
          "value": "CODESEARCHNET",
          "justification": "CodeSearchNet is extensively used as a baseline for comparison and for augmenting training datasets in the paper.",
          "quote": "Search approaches such as both CPT-CODE and CodeBERT for instance are evaluated on the CodeSearchNet benchmark (Husain et al., 2019) where, given a query in English, the model retrieves a code snippet deemed relevant among 1000 candidates."
        },
        "aliases": [
          "CSN"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search",
          "justification": "Provides the full title for the referenced dataset to enable readers to find the original source.",
          "quote": "Search approaches such as both CPT-CODE and CodeBERT for instance are evaluated on the CodeSearchNet benchmark (Husain et al., 2019) where, given a query in English, the model retrieves a code snippet deemed relevant among 1000 candidates."
        }
      },
      {
        "name": {
          "value": "TATOEBA",
          "justification": "TATOEBA is used for incorporating anchor/source pairs in the dataset preparation for training the models.",
          "quote": "As such, we used the French/German subset of WMT-19 as well as the Spanish/Portuguese and Spanish/Galician partitions of TATOEBA (Tiedemann, 2020)."
        },
        "aliases": [
          ""
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Tatoeba Translation Challengeâ€“Realistic Data Sets for Low Resource and Multilingual MT",
          "justification": "Provides the full title for the referenced dataset to enable readers to find the original source.",
          "quote": "As such, we used the French/German subset of WMT-19 as well as the Spanish/Portuguese and Spanish/Galician partitions of TATOEBA (Tiedemann, 2020)."
        }
      },
      {
        "name": {
          "value": "WMT-19",
          "justification": "WMT-19 is used for including non-anchor source/source combinations in the dataset preparation for training the models.",
          "quote": "Namely, a subset of WMT-19 (Wikimedia-Foundation, 2019) was considered given by its English/German and English/Finnish partitions."
        },
        "aliases": [
          ""
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ACL 2019 Fourth Conference on Machine Translation (WMT19), Shared Task: Machine Translation of News",
          "justification": "Provides the full title for the referenced dataset to enable readers to find the original source.",
          "quote": "Namely, a subset of WMT-19 (Wikimedia-Foundation, 2019) was considered given by its English/German and English/Finnish partitions."
        }
      },
      {
        "name": {
          "value": "CoNaLa",
          "justification": "CoNaLa is mentioned as a dataset extended to include multiple natural languages and programming languages for code retrieval tasks.",
          "quote": "Acquiring aligned code and natural language is key to solve tasks such as code retrieval and summarization. Towards this goal, Yin et al. (2018) proposed CoNaLa, mined from STACKOVERFLOW containing English paired with Python and Java. MCoNaLa (Wang et al., 2022) extended it to provide English, Spanish, Japanese, and Russian text to Python pairs."
        },
        "aliases": [
          ""
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow",
          "justification": "Provides the full title for the referenced dataset to enable readers to find the original source.",
          "quote": "Yin et al. (2018) proposed CoNaLa, mined from STACKOVERFLOW containing English paired with Python and Java."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Tree-Sitter",
          "justification": "Tree-Sitter is explicitly mentioned as used for AST parsing during dataset creation.",
          "quote": "AST parsing (we used Tree-Sitter for that purpose)."
        },
        "aliases": [
          ""
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Tree-Sitter: An Efficient and Maintainable Parser Library",
          "justification": "Provides the full title for the referenced library to enable readers to find the original source.",
          "quote": "AST parsing (we used Tree-Sitter for that purpose)."
        }
      },
      {
        "name": {
          "value": "langid.py",
          "justification": "langid.py is mentioned as one of the open-source language classification libraries used for determining docstring languages.",
          "quote": "we ensemble three different off-the-shelf open-source language classifiers from text, langid.py, cld3, xlm-roberta-base"
        },
        "aliases": [
          ""
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "langid.py: An Off-the-Shelf Language Identification Tool",
          "justification": "Provides the full title for the referenced library to enable readers to find the original source.",
          "quote": "we ensemble three different off-the-shelf open-source language classifiers from text, langid.py, cld3, xlm-roberta-base"
        }
      },
      {
        "name": {
          "value": "cld3",
          "justification": "cld3 is mentioned as one of the open-source language classification libraries used for determining docstring languages.",
          "quote": "we ensemble three different off-the-shelf open-source language classifiers from text, langid.py, cld3, xlm-roberta-base"
        },
        "aliases": [
          ""
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Compact Language Detector 3",
          "justification": "Provides the full title for the referenced library to enable readers to find the original source.",
          "quote": "we ensemble three different off-the-shelf open-source language classifiers from text, langid.py, cld3, xlm-roberta-base"
        }
      },
      {
        "name": {
          "value": "xlm-roberta-base",
          "justification": "xlm-roberta-base is mentioned as one of the open-source language classification libraries used for determining docstring languages.",
          "quote": "we ensemble three different off-the-shelf open-source language classifiers from text, langid.py, cld3, xlm-roberta-base"
        },
        "aliases": [
          ""
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Unsupervised Cross-lingual Representation Learning at Scale",
          "justification": "Provides the full title for the referenced library to enable readers to find the original source.",
          "quote": "we ensemble three different off-the-shelf open-source language classifiers from text, langid.py, cld3, xlm-roberta-base"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3448,
    "prompt_tokens": 18779,
    "total_tokens": 22227
  }
}