{
  "paper": "2312.11805.txt",
  "words": 32751,
  "extractions": {
    "title": {
      "value": "Gemini: A Family of Highly Capable Multimodal Models",
      "justification": "The title of the research paper is explicitly mentioned.",
      "quote": "Gemini: A Family of Highly Capable Multimodal Models"
    },
    "description": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical results from the evaluation of the Gemini models on various benchmarks.",
      "quote": "Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined."
    },
    "primary_research_field": {
      "name": {
        "value": "Multimodal Learning",
        "justification": "The paper focuses on multimodal models with advanced capabilities in language understanding and cross-modal reasoning.",
        "quote": "We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Language Modeling",
          "justification": "The paper discusses Gemini's advancements in language modeling.",
          "quote": "The Gemini family advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al., 2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020; OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang et al., 2023), and video understanding (Alayrac et al., 2022; Chen et al., 2023)."
        },
        "aliases": [
          "language modeling",
          "large-scale language modeling"
        ]
      },
      {
        "name": {
          "value": "Image Understanding",
          "justification": "The paper focuses on Gemini's capabilities in image understanding and cross-modal reasoning.",
          "quote": "We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases."
        },
        "aliases": [
          "image understanding",
          "cross-modal reasoning"
        ]
      },
      {
        "name": {
          "value": "Audio Processing",
          "justification": "The paper discusses Gemini's advancements in audio processing.",
          "quote": "The Gemini family advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al., 2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020; OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang et al., 2023), and video understanding (Alayrac et al., 2022; Chen et al., 2023)."
        },
        "aliases": [
          "audio processing",
          "audio understanding"
        ]
      },
      {
        "name": {
          "value": "Video Understanding",
          "justification": "The paper discusses Gemini's advancements in video understanding.",
          "quote": "The Gemini family advances state-of-the-art in large-scale language modeling (Anil et al., 2023; Brown et al., 2020; Chowdhery et al., 2023; Hoffmann et al., 2022; OpenAI, 2023a; Radford et al., 2019; Rae et al., 2021), image understanding (Alayrac et al., 2022; Chen et al., 2022; Dosovitskiy et al., 2020; OpenAI, 2023b; Reed et al., 2022; Yu et al., 2022a), audio processing (Radford et al., 2023; Zhang et al., 2023), and video understanding (Alayrac et al., 2022; Chen et al., 2023)."
        },
        "aliases": [
          "video understanding"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Gemini Ultra",
          "justification": "Gemini Ultra is explicitly named as one of the sizes in the Gemini family.",
          "quote": "The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases."
        },
        "aliases": [
          "Ultra",
          "Gemini Ultra"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Gemini Ultra is a new model contributed by Google.",
          "quote": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses the execution of Gemini Ultra on various tasks and benchmarks.",
          "quote": "Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper evaluates Gemini Ultra model on 32 benchmarks and compares its performance to other models.",
          "quote": "Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks — notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined."
        },
        "referenced_paper_title": {
          "value": "Gemini: A Family of Highly Capable Multimodal Models",
          "justification": "This is the paper that introduces the Gemini Ultra model.",
          "quote": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding."
        }
      },
      {
        "name": {
          "value": "Gemini Pro",
          "justification": "Gemini Pro is explicitly named as one of the sizes in the Gemini family.",
          "quote": "The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases."
        },
        "aliases": [
          "Pro",
          "Gemini Pro"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Gemini Pro is a new model contributed by Google.",
          "quote": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses the execution and evaluation of Gemini Pro on a range of tasks.",
          "quote": "For the Pro model, the inherent scalability of our infrastructure and learning algorithms enable us to complete pre-training in a matter of weeks, leveraging a fraction of the Ultra’s resources."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper discusses the performance of Gemini Pro in comparison to other models.",
          "quote": "Gemini 1.0, our first version, comes in three sizes: Ultra for highly-complex tasks, Pro for enhanced performance and deployability at scale, and Nano for on-device applications."
        },
        "referenced_paper_title": {
          "value": "Gemini: A Family of Highly Capable Multimodal Models",
          "justification": "This is the paper that introduces the Gemini Pro model.",
          "quote": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding."
        }
      },
      {
        "name": {
          "value": "Gemini Nano",
          "justification": "Gemini Nano is explicitly named as one of the sizes in the Gemini family.",
          "quote": "The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases."
        },
        "aliases": [
          "Nano",
          "Gemini Nano",
          "Nano-1",
          "Nano-2",
          "Gemini Nano 1",
          "Gemini Nano 2"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Gemini Nano series is a new model contributed by Google.",
          "quote": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses the training, quantization and deployment of Gemini Nano on devices.",
          "quote": "The Nano series of models leverage additional advancements in distillation and training algorithms to produce the best-in-class small language models for a wide variety of tasks, such as summarization and reading comprehension, which power our next generation on-device experiences."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper discusses the performance of Gemini Nano series in comparison to larger Gemini models.",
          "quote": "In tandem, we advance the frontier of efficiency with Gemini Nano, a series of small models targeting on-device deployment. These models excel in on-device tasks, such as summarization, reading comprehension, text completion tasks, and exhibit impressive capabilities in reasoning, STEM, coding, multimodal, and multilingual tasks relative to their sizes."
        },
        "referenced_paper_title": {
          "value": "Gemini: A Family of Highly Capable Multimodal Models",
          "justification": "This is the paper that introduces the Gemini Nano series of models.",
          "quote": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding."
        }
      },
      {
        "name": {
          "value": "AlphaCode 2",
          "justification": "AlphaCode 2 is explicitly named in the paper as a new agent powered by Gemini model.",
          "quote": "The reasoning capabilities of large language models show promise toward building generalist agents that can tackle more complex multi-step problems. The AlphaCode team built AlphaCode 2 (Leblond et al, 2023), a new Gemini-model-powered agent, that combines Gemini models’ reasoning capabilities with search and tool-use to excel at solving competitive programming problems."
        },
        "aliases": [
          "AlphaCode 2"
        ],
        "is_contributed": {
          "value": true,
          "justification": "AlphaCode 2 is built upon Gemini Pro model and is contributed by Google.",
          "quote": "The reasoning capabilities of large language models show promise toward building generalist agents that can tackle more complex multi-step problems. The AlphaCode team built AlphaCode 2 (Leblond et al, 2023), a new Gemini-model-powered agent, that combines Gemini models’ reasoning capabilities with search and tool-use to excel at solving competitive programming problems."
        },
        "is_executed": {
          "value": true,
          "justification": "AlphaCode 2 is executed to solve competitive programming problems and evaluated on Codeforces platform.",
          "quote": "AlphaCode 2 is evaluated on Codeforces,4 the same platform as AlphaCode, on 12 contests from division 1 and 2, for a total of 77 problems. AlphaCode 2 solved 43% of these competition problems, a 1.7x improvement over the prior record-setting AlphaCode system which solved 25%."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of AlphaCode 2 with its predecessor.",
          "quote": "AlphaCode 2 ranks within the top 15% of entrants on the Codeforces competitive programming platform, a large improvement over its state-of-the-art predecessor in the top 50% (Li et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "AlphaCode 2 Technical Report",
          "justification": "This is the paper that introduces the AlphaCode 2 agent.",
          "quote": "The AlphaCode team built AlphaCode 2 (Leblond et al, 2023), a new Gemini-model-powered agent, that combines Gemini models’ reasoning capabilities with search and tool-use to excel at solving competitive programming problems."
        }
      },
      {
        "name": {
          "value": "Gemini Apps models",
          "justification": "The paper refers to these models as \\\"Gemini Apps models.\\\"",
          "quote": "Chat-focused variants, referred to as Gemini Apps models, are optimized for Gemini and Gemini Advanced, our conversational AI service formerly known as Bard."
        },
        "aliases": [
          "Gemini Apps models",
          "Chat-focused variants"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Gemini Apps models are variants of the Gemini models optimized for conversational AI.",
          "quote": "Chat-focused variants, referred to as Gemini Apps models, are optimized for Gemini and Gemini Advanced, our conversational AI service formerly known as Bard."
        },
        "is_executed": {
          "value": true,
          "justification": "Gemini Apps models are used to power the Gemini and Gemini Advanced conversational AI service.",
          "quote": "Chat-focused variants, referred to as Gemini Apps models, are optimized for Gemini and Gemini Advanced, our conversational AI service formerly known as Bard."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of Gemini Apps models with a baseline model on several tasks and languages.",
          "quote": "Table 16 shows the performance of Gemini (with Pro) on 5 languages compared to Bard with an older post-training recipe and based on PaLM 2."
        },
        "referenced_paper_title": {
          "value": "Gemini: A Family of Highly Capable Multimodal Models",
          "justification": "This is the paper that introduces the Gemini Apps models.",
          "quote": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding."
        }
      },
      {
        "name": {
          "value": "Gemini API models",
          "justification": "The paper refers to these models as \\\"Gemini API models.\\\"",
          "quote": "Developer-focused variants, referred to as Gemini API models, are optimized for a range of products and are accessible through Google AI Studio and Cloud Vertex AI."
        },
        "aliases": [
          "Gemini API models",
          "Developer-focused variants"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Gemini API models are variants of the Gemini models optimized for developer use cases and accessible through APIs.",
          "quote": "Developer-focused variants, referred to as Gemini API models, are optimized for a range of products and are accessible through Google AI Studio and Cloud Vertex AI."
        },
        "is_executed": {
          "value": true,
          "justification": "Gemini API models are made accessible through Google AI Studio and Cloud Vertex AI for developers to use.",
          "quote": "Developer-focused variants, referred to as Gemini API models, are optimized for a range of products and are accessible through Google AI Studio and Cloud Vertex AI."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of Gemini API models is evaluated and compared to pre-trained models in Table 17.",
          "quote": "We evaluate the impact of post-training via SFT on Gemini API models’ multimodal vision performance by tracking the performance of both pre-trained models and post-trained Gemini API Vision models on a series of standard benchmarks. These post-trained results have already been given in Table 7, in Table 17 we further report the difference in performance between pre-trained and post-trained Gemini API models."
        },
        "referenced_paper_title": {
          "value": "Gemini: A Family of Highly Capable Multimodal Models",
          "justification": "This is the paper that introduces the Gemini API models.",
          "quote": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 60182
  }
}