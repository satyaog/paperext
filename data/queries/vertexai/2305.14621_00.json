{
  "paper": "2305.14621.txt",
  "words": 2925,
  "extractions": {
    "title": {
      "value": "Realistically distributing object placements in synthetic training data improves the performance of vision-based object detection models",
      "justification": "The paper's title reflects the core focus on realistic object placement in synthetic data for enhanced object detection model performance. ",
      "quote": "Realistically distributing object placements in synthetic training data improves the performance of vision-based object detection models"
    },
    "description": "This research paper investigates the impact of object placement distribution in synthetic training data for improving the performance of vision-based object detection models.",
    "type": {
      "value": "empirical",
      "justification": "This research paper is empirical as it involves experiments to validate the hypothesis that realistic object placement in synthetic data improves the performance of vision-based object detection models.",
      "quote": "Our experiment, training a 3D vehicle detection model in CARLA and testing on KITTI, demonstrates a substantial improvement resulting from improving the object placement distribution."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on training an object detection model, which is a computer vision task.",
        "quote": "Realistically distributing object placements in synthetic training data improves the performance of vision-based object detection models"
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Object Detection",
          "justification": "The paper specifically focuses on the task of object detection within the broader field of computer vision. ",
          "quote": "Realistically distributing object placements in synthetic training data improves the performance of vision-based object detection models"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "PGD",
          "justification": "The paper uses PGD for object detection. ",
          "quote": "We use a PGD [21] model for object detection."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper use an existing object detection model called PGD and do not introduce any novel model architectures. ",
          "quote": "We use a PGD [21] model for object detection."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper trains PGD on the baseline and INITIALIZE datasets. ",
          "quote": "We use the publicly available source code for PGD [1], and will release our training datasets and specific configurations for reproducibility purposes."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not compare different architectures or variations of PGD. It primarily compares different training datasets.",
          "quote": "We use a PGD [21] model for object detection. This choice is mostly orthogonal to the claims of the paper and we do not expect it to have a significant bearing on the results."
        },
        "referenced_paper_title": {
          "value": "Probabilistic and Geometric Depth: Detecting objects in perspective",
          "justification": "The paper references PGD when mentioning its use for the experiments.",
          "quote": "To demonstrate the importance of vehicle placement on realism in synthetically generated training data, we train a monocular 3D detection model named PGD [21], using the source code provided by its original authors [1]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "KITTI",
          "justification": "The paper uses the KITTI Vision Benchmark Suite as the test set. ",
          "quote": "We generate a KITTI-like dataset for 3D object detection from a forward-facing camera angle using CARLA.\\nWe manually designate regions of interest in Town 1, Town 2, Town 3, Town 4, and Town 10 to cover different road geometries."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Are we ready for autonomous driving? the kitti vision benchmark suite",
          "justification": "The paper references KITTI in the context of the dataset. ",
          "quote": "Our experiment, training a 3D vehicle detection model in CARLA and testing on KITTI, demonstrates a substantial improvement resulting from improving the object placement distribution."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 6212
  }
}