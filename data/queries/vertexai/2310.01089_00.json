{
  "paper": "2310.01089.txt",
  "words": 11731,
  "extractions": {
    "title": {
      "value": "GRAPHTEXT: GRAPH REASONING IN TEXT SPACE",
      "justification": "The title of the paper is clearly stated as \\\"GRAPHTEXT: GRAPH REASONING IN TEXT SPACE\\\".",
      "quote": "GRAPHTEXT: GRAPH REASONING IN TEXT SPACE"
    },
    "description": "The paper introduces GraphText, a framework designed to bridge the gap between Large Language Models (LLMs) and graph machine learning. It proposes a novel approach to translate graph data into natural language, enabling LLMs to perform graph reasoning as text generation tasks. The key idea is to represent graph information as a graph-syntax tree, which encapsulates both node attributes and inter-node relationships. Traversal of this tree produces a textual graph prompt that LLMs can process. This method eliminates the need for training graph-specific models, enabling training-free graph reasoning using pre-trained LLMs like ChatGPT. The paper demonstrates the effectiveness of GraphText in node classification tasks, showcasing comparable or even superior performance to supervised GNNs, especially in few-shot learning scenarios. Furthermore, it emphasizes the interactive potential of GraphText, allowing humans and LLMs to collaborate in understanding and reasoning about graphs through natural language.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a new framework and conducts empirical experiments, making it an empirical research paper.",
      "quote": "We conduct extensive experiments to demonstrate the effectiveness of GRAPHTEXT."
    },
    "primary_research_field": {
      "name": {
        "value": "graph machine learning",
        "justification": "The paper focuses on bridging the gap between LLMs and graph machine learning, indicating its primary research field.",
        "quote": "However, despite their impressive achievements, LLMs have not made significant advancements in the realm of graph machine learning."
      },
      "aliases": [
        "graph machine learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "node classification",
          "justification": "The paper specifically focuses on node classification as a key task in graph machine learning.",
          "quote": "Out of the three fundamental problems of graph ML (graph classification, node classification, and link prediction), we take node classification as an example to introduce our idea."
        },
        "aliases": [
          "node classification"
        ]
      },
      {
        "name": {
          "value": "in-context learning",
          "justification": "The paper highlights the use of in-context learning as a key advantage of their proposed framework.",
          "quote": "Notably, GRAPHTEXT offers multiple advantages. It introduces training-free graph reasoning: even without training on graph data, GRAPHTEXT with ChatGPT can achieve on par with, or even surpassing, the performance of supervised-trained graph neural networks through in-context learning (ICL)."
        },
        "aliases": [
          "in-context learning",
          "ICL"
        ]
      },
      {
        "name": {
          "value": "instruction tuning",
          "justification": "Instruction tuning is presented as a valuable application of the GraphText framework.",
          "quote": "Concluding our exploration, Section 4.4 illustrates how GRAPHTEXT can seamlessly function as a versatile framework, catering to both in-context learning and instruction tuning across on both general graph and text-attributed graphs."
        },
        "aliases": [
          "instruction tuning"
        ]
      },
      {
        "name": {
          "value": "few-shot learning",
          "justification": "The authors highlight the efficacy of GraphText, particularly in few-shot learning settings for node classification tasks.",
          "quote": "The paper demonstrates the effectiveness of GraphText in node classification tasks, showcasing comparable or even superior performance to supervised GNNs, especially in few-shot learning scenarios."
        },
        "aliases": [
          "few-shot learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ChatGPT",
          "justification": "The paper explicitly names \\\"ChatGPT\\\" as the LLM used with GraphText.",
          "quote": "Remarkably, even without training on graph data, GRAPHTEXT with ChatGPT can deliver performance on par with, or even surpassing, supervised graph neural networks through in-context learning."
        },
        "aliases": [
          "ChatGPT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't contribute to the development of ChatGPT.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses ChatGPT to execute the GraphText framework for graph reasoning tasks.",
          "quote": "In this paper, we propose GRAPHTEXT, a framework that enables graph reasoning in text space. ... GRAPHTEXT enables training-free graph reasoning where a GRAPHTEXT-LLM can deliver performance on par with, or even surpassing, supervised graph neural networks through in-context learning."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of GraphText using ChatGPT with supervised GNNs like GCN, GAT, GCNII, and GATv2.",
          "quote": "Remarkably, even without training on graph data, GRAPHTEXT with ChatGPT can deliver performance on par with, or even surpassing, supervised graph neural networks through in-context learning."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "The paper explicitly names \\\"GPT-4\\\" as one of the LLMs used.",
          "quote": "We leverage GRAPHTEXT with ChatGPT and GPT-4 to perform graph reasoning on the provided example."
        },
        "aliases": [
          "GPT-4"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't contribute to the development of GPT-4.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses GPT-4 to execute the GraphText framework for graph reasoning tasks.",
          "quote": "We leverage GRAPHTEXT with ChatGPT and GPT-4 to perform graph reasoning on the provided example."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of GraphText using GPT-4 alongside ChatGPT.",
          "quote": "We leverage GRAPHTEXT with ChatGPT and GPT-4 to perform graph reasoning on the provided example."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Llama-2",
          "justification": "The paper clearly identifies 'Llama-2' as the open-source LLM being used in the instruction tuning experiments.",
          "quote": "Thus, we further explored the potential of instruction tuning on currently available open-source LLMs, such as Llama-2 (Touvron et al., 2023)."
        },
        "aliases": [
          "Llama-2",
          "LLaMA2-7b"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute to the development of Llama-2.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The authors use Llama-2-7B to conduct experiments on text-attributed graphs, demonstrating its capability in instruction tuning scenarios.",
          "quote": "We utilize AdamW (Loshchilov & Hutter, 2019) in conjunction with DeepSpeed (Rasley et al., 2020) to train the huggingface LLaMA2-7b model, with FP16 activated."
        },
        "is_compared": {
          "value": true,
          "justification": "The authors compare the performance of GraphText using Llama-2, specifically the 7B variant.",
          "quote": "Thus, we further explored the potential of instruction tuning on currently available open-source LLMs, such as Llama-2 (Touvron et al., 2023)."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open foundation and fine-tuned chat models",
          "justification": "The paper references the paper \\\"Llama 2: Open foundation and fine-tuned chat models\\\" by Touvron et al. in relation to Llama-2.",
          "quote": "Thus, we further explored the potential of instruction tuning on currently available open-source LLMs, such as Llama-2 (Touvron et al., 2023)."
        }
      },
      {
        "name": {
          "value": "GCN",
          "justification": "GCN (Graph Convolutional Network) is clearly identified as a baseline model.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        },
        "aliases": [
          "GCN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't contribute to the development of GCN.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "It's not mentioned in the text that the authors used GCN to produce results.",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "GCN is used as a baseline model in the paper.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        },
        "referenced_paper_title": {
          "value": "Semi-supervised classification with graph convolutional networks",
          "justification": "The paper references \\\"Semi-supervised classification with graph convolutional networks\\\" by Kipf & Welling, 2017, in relation to GCN.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        }
      },
      {
        "name": {
          "value": "GAT",
          "justification": "GAT (Graph Attention Network) is explicitly named.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        },
        "aliases": [
          "GAT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't contribute to the development of GAT.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "It's not mentioned in the paper if the authors executed GAT.",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "GAT is used as a baseline model for comparison.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        },
        "referenced_paper_title": {
          "value": "Graph attention networks",
          "justification": "The paper \\\"Graph attention networks\\\" by Velickovic et al., 2018 is mentioned in relation to GAT.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        }
      },
      {
        "name": {
          "value": "GCNII",
          "justification": "GCNII is explicitly named as a baseline model.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        },
        "aliases": [
          "GCNII"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute to the development of GCNII.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "The paper doesn't mention if the authors executed GCNII themselves.",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "GCNII is used as a baseline model.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        },
        "referenced_paper_title": {
          "value": "Simple and deep graph convolutional networks",
          "justification": "The paper \\\"Simple and deep graph convolutional networks\\\" by Chen et al., 2020 is mentioned as a reference for GCNII.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        }
      },
      {
        "name": {
          "value": "GATv2",
          "justification": "GATv2 is explicitly named as a baseline model.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        },
        "aliases": [
          "GATv2"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't contribute to the development of GATv2.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "The paper doesn't explicitly mention if the authors executed GATv2 themselves.",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "The authors use GATv2 as a baseline model to compare against their proposed approach.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        },
        "referenced_paper_title": {
          "value": "How attentive are graph attention networks?",
          "justification": "The paper \\\"How attentive are graph attention networks?\\\" by Brody et al., 2022, is mentioned as the reference for GATv2.",
          "quote": "We selected standard GNNs, including GCN (Kipf & Welling, 2017) and GAT (Velickovic et al., 2018), along with their more recent variants GCNII (Chen et al., 2020) and GATv2 (Brody et al., 2022), as our baselines."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 27763
  }
}