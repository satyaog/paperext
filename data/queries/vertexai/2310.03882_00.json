{
  "paper": "2310.03882.txt",
  "words": 11073,
  "extractions": {
    "title": {
      "value": "Small batch deep reinforcement learning",
      "justification": "The title of the paper is 'Small batch deep reinforcement learning'.",
      "quote": "Small batch deep reinforcement learning"
    },
    "description": "This research paper explores the impact of batch size on the performance of deep reinforcement learning algorithms. The authors conduct experiments with various value-based agents, such as DQN, Rainbow, QR-DQN, and IQN, on the Atari Learning Environment. The paper does not introduce new models or datasets, but provides insights into the optimization dynamics of existing algorithms.",
    "type": {
      "value": "empirical",
      "justification": "The paper is empirical as it conducts experiments and analyzes the results.",
      "quote": "In this work we conduct a broad empirical study of batch size in online value-based deep reinforcement learning."
    },
    "primary_research_field": {
      "name": {
        "value": "deep reinforcement learning",
        "justification": "The paper focuses on deep reinforcement learning, as indicated by the title and abstract.",
        "quote": "Small batch deep reinforcement learning"
      },
      "aliases": [
        "deep reinforcement learning",
        "deep RL"
      ]
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "DQN",
          "justification": "The model is referred to as DQN in the paper.",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents, which were initially benchmarked on the ALE suite: DQN [Mnih et al., 2015]"
        },
        "aliases": [
          "DQN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute DQN but uses it as a baseline.",
          "quote": "Since the introduction of DQN [Mnih et al., 2015], one of the core components of most modern deep RL algorithms is the use of a finite replay memory where experienced transitions are stored."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper describes and conducts experiments with DQN.",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents, which were initially benchmarked on the ALE suite: DQN [Mnih et al., 2015]"
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares DQN with other value-based agents like Rainbow, QR-DQN, and IQN.",
          "quote": "In this work we conduct a broad empirical study of batch size in online value-based deep reinforcement learning. We uncover the surprising finding that reducing the batch size seems to provide substantial performance benefits and computational savings. We showcase this finding in a variety of agents and training regimes (section 3), and conduct in-depth analyses of the possible causes (section 4). The impact of our findings and analyses go beyond the choice of the batch size hyper-parameter, and help us develop a better understanding of the learning dynamics in online deep RL."
        },
        "referenced_paper_title": {
          "value": "Human-level control through deep reinforcement learning",
          "justification": "The paper references the original DQN paper by Mnih et al. (2015).",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents, which were initially benchmarked on the ALE suite: DQN [Mnih et al., 2015]"
        }
      },
      {
        "name": {
          "value": "Rainbow",
          "justification": "The model is named 'Rainbow' in the paper.",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents, which were initially benchmarked on the ALE suite: DQN [Mnih et al., 2015]"
        },
        "aliases": [
          "Rainbow"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't introduce Rainbow but utilizes it as a baseline agent.",
          "quote": "Since the introduction of DQN [Mnih et al., 2015], one of the core components of most modern deep RL algorithms is the use of a finite replay memory where experienced transitions are stored."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper conducts experiments using Rainbow to study the effects of batch size.",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents, which were initially benchmarked on the ALE suite: DQN [Mnih et al., 2015]"
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Rainbow with other value-based agents, including DQN, QR-DQN, and IQN.",
          "quote": "In this work we conduct a broad empirical study of batch size in online value-based deep reinforcement learning. We uncover the surprising finding that reducing the batch size seems to provide substantial performance benefits and computational savings. We showcase this finding in a variety of agents and training regimes (section 3), and conduct in-depth analyses of the possible causes (section 4). The impact of our findings and analyses go beyond the choice of the batch size hyper-parameter, and help us develop a better understanding of the learning dynamics in online deep RL."
        },
        "referenced_paper_title": {
          "value": "Rainbow: Combining Improvements in Deep Reinforcement learning",
          "justification": "The paper cites the original Rainbow paper by Hessel et al. (2018).",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents, which were initially benchmarked on the ALE suite: DQN [Mnih et al., 2015]"
        }
      },
      {
        "name": {
          "value": "QR-DQN",
          "justification": "The paper explicitly refers to the model as \\\"QR-DQN\\\".",
          "quote": "For reasons which will be clarified below, most of our evaluations and analyses were conducted with the QR-DQN agent."
        },
        "aliases": [
          "QR-DQN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors don't present QR-DQN as a novel contribution.",
          "quote": "For reasons which will be clarified below, most of our evaluations and analyses were conducted with the QR-DQN agent."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes experiments and results for QR-DQN with varying batch sizes.",
          "quote": "For reasons which will be clarified below, most of our evaluations and analyses were conducted with the QR-DQN agent."
        },
        "is_compared": {
          "value": true,
          "justification": "QR-DQN is compared against other value-based methods (DQN, Rainbow, IQN).",
          "quote": "In this work we conduct a broad empirical study of batch size in online value-based deep reinforcement learning."
        },
        "referenced_paper_title": {
          "value": "Distributional reinforcement learning with quantile regression",
          "justification": "The research paper cites the original QR-DQN publication.",
          "quote": "Different ways of parameterizing return distributions were proposed in the form of the IQN [Dabney et al., 2018b] and QR-DQN [Dabney et al., 2018a] algorithms."
        }
      },
      {
        "name": {
          "value": "IQN",
          "justification": "The paper explicitly refers to the model as \\\"IQN\\\".",
          "quote": "Different ways of parameterizing return distributions were proposed in the form of the IQN [Dabney et al., 2018b] and QR-DQN [Dabney et al., 2018a] algorithms."
        },
        "aliases": [
          "IQN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't present IQN as a novel contribution but uses it in comparison.",
          "quote": "Different ways of parameterizing return distributions were proposed in the form of the IQN [Dabney et al., 2018b] and QR-DQN [Dabney et al., 2018a] algorithms."
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments were conducted using IQN to examine the influence of batch size.",
          "quote": "Different ways of parameterizing return distributions were proposed in the form of the IQN [Dabney et al., 2018b] and QR-DQN [Dabney et al., 2018a] algorithms."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of IQN is compared with other value-based agents like DQN, Rainbow, and QR-DQN.",
          "quote": "In this work we conduct a broad empirical study of batch size in online value-based deep reinforcement learning. We uncover the surprising finding that reducing the batch size seems to provide substantial performance benefits and computational savings. We showcase this finding in a variety of agents and training regimes (section 3), and conduct in-depth analyses of the possible causes (section 4). The impact of our findings and analyses go beyond the choice of the batch size hyper-parameter, and help us develop a better understanding of the learning dynamics in online deep RL."
        },
        "referenced_paper_title": {
          "value": "Implicit quantile networks for distributional reinforcement learning",
          "justification": "The paper cites the original IQN publication.",
          "quote": "Different ways of parameterizing return distributions were proposed in the form of the IQN [Dabney et al., 2018b] and QR-DQN [Dabney et al., 2018a] algorithms."
        }
      },
      {
        "name": {
          "value": "C51",
          "justification": "C51 is referenced by its name.",
          "quote": "The Rainbow agent combined C51 with other advances such as multi-step learning and prioritized replay sampling [Hessel et al., 2018]."
        },
        "aliases": [
          "C51"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not introduce C51.",
          "quote": "The Rainbow agent combined C51 with other advances such as multi-step learning and prioritized replay sampling [Hessel et al., 2018]."
        },
        "is_executed": {
          "value": true,
          "justification": "Although not directly executed, C51's principles are used within the Rainbow agent.",
          "quote": "The Rainbow agent combined C51 with other advances such as multi-step learning and prioritized replay sampling [Hessel et al., 2018]."
        },
        "is_compared": {
          "value": true,
          "justification": "While not directly compared, C51's influence is present as a component of the Rainbow agent.",
          "quote": "The Rainbow agent combined C51 with other advances such as multi-step learning and prioritized replay sampling [Hessel et al., 2018]."
        },
        "referenced_paper_title": {
          "value": "A distributional perspective on reinforcement learning",
          "justification": "The paper cites the paper that introduced C51.",
          "quote": "The Rainbow agent combined C51 with other advances such as multi-step learning and prioritized replay sampling [Hessel et al., 2018]."
        }
      },
      {
        "name": {
          "value": "DER",
          "justification": "The paper names the model \\\"DER\\\", referring to it also as \\\"Data-efficient Rainbow\\\".",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]"
        },
        "aliases": [
          "DER",
          "Data-efficient Rainbow"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors use DER, but it is not a new model introduced in the paper.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors run experiments using DER to test the impact of batch size.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]"
        },
        "is_compared": {
          "value": true,
          "justification": "DER is compared in a benchmark against DrQ(ϵ) and SPR.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]; and SPR, which incorporates self-supervised learning to improve sample efficiency [Schwarzer et al., 2020]."
        },
        "referenced_paper_title": {
          "value": "When to use parametric models in reinforcement learning?",
          "justification": "The paper references the source of DER.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]"
        }
      },
      {
        "name": {
          "value": "DrQ(ϵ)",
          "justification": "The paper refers to the model as \\\"DrQ(ϵ)\\\".",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]"
        },
        "aliases": [
          "DrQ(ϵ)"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors employ DrQ(ϵ), which is an existing model, not a contribution of the paper.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]"
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments to study the effects of batch size are conducted using the DrQ(ϵ) model.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]"
        },
        "is_compared": {
          "value": true,
          "justification": "DrQ(ϵ) is benchmarked against other agents, DER and SPR, in the paper.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]; and SPR, which incorporates self-supervised learning to improve sample efficiency [Schwarzer et al., 2020]."
        },
        "referenced_paper_title": {
          "value": "Deep reinforcement learning at the edge of the statistical precipice",
          "justification": "The paper links to the publication where DrQ(ϵ) originates.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]"
        }
      },
      {
        "name": {
          "value": "SPR",
          "justification": "The model is referred to as \\\"SPR\\\".",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]; and SPR, which incorporates self-supervised learning to improve sample efficiency [Schwarzer et al., 2020]."
        },
        "aliases": [
          "SPR"
        ],
        "is_contributed": {
          "value": false,
          "justification": "SPR is an existing model used in the paper, not a novel contribution.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]; and SPR, which incorporates self-supervised learning to improve sample efficiency [Schwarzer et al., 2020]."
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments are conducted with SPR to assess the influence of batch size.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]; and SPR, which incorporates self-supervised learning to improve sample efficiency [Schwarzer et al., 2020]."
        },
        "is_compared": {
          "value": true,
          "justification": "SPR is compared against other methods (DER, DrQ(ϵ)) in benchmark experiments.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]; and SPR, which incorporates self-supervised learning to improve sample efficiency [Schwarzer et al., 2020]."
        },
        "referenced_paper_title": {
          "value": "Data-efficient reinforcement learning with self-predictive representations",
          "justification": "A citation to the original SPR publication is provided.",
          "quote": "We evaluate the effect of reduced batch size on three of the most widely used agents for this regime: Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning [van Hasselt et al., 2019]; DrQ(ϵ), which is a variant of DQN that uses data augmentation [Agarwal et al., 2021]; and SPR, which incorporates self-supervised learning to improve sample efficiency [Schwarzer et al., 2020]."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 27719
  }
}