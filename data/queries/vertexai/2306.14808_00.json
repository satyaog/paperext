{
  "paper": "2306.14808.txt",
  "words": 13970,
  "extractions": {
    "title": {
      "value": "Maximum State Entropy Exploration using Predecessor and Successor Representations",
      "justification": "The title of the paper is clearly stated.",
      "quote": "Maximum State Entropy Exploration using Predecessor and Successor Representations"
    },
    "description": "The paper proposes ηψ-Learning, a reinforcement learning algorithm that maximizes the entropy of the state visitation distribution of a single finite-length trajectory for efficient exploration. The algorithm leverages predecessor and successor representations to predict past and future state visitation frequencies, respectively. Experiments on various environments, including ChainMDP, RiverSwim, GridWorld, TwoRooms, FourRooms, Reacher, and Pusher, demonstrate the effectiveness of ηψ-Learning in achieving optimal state coverage and exploration compared to baseline methods like MaxEnt and VariBAD.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a novel algorithm and evaluates its performance empirically, making it an empirical research paper.",
      "quote": "Our experiments demonstrate the efficacy of ηψ-Learning to strategically explore the environment and maximize the state coverage with limited samples."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on exploration in the context of reinforcement learning.",
        "quote": "The domain of exploration in Reinforcement Learning (RL) focuses on discovering an agent’s environment via intrinsic motivation to accelerate learning optimal policies."
      },
      "aliases": [
        "Reinforcement Learning",
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Exploration",
          "justification": "The paper specifically addresses the problem of efficient exploration in reinforcement learning.",
          "quote": "The domain of exploration in Reinforcement Learning (RL) focuses on discovering an agent’s environment via intrinsic motivation to accelerate learning optimal policies."
        },
        "aliases": [
          "Exploration"
        ]
      },
      {
        "name": {
          "value": "Meta Reinforcement Learning",
          "justification": "The paper mentions the potential benefits of efficient exploration in subfields like meta-RL, continual RL, and unsupervised RL.",
          "quote": "We believe many subareas of RL can benefit from such efficient exploration behaviors. Some applications include Meta RL (Finn et al., 2017; Zintgraf et al., 2019; Rakelly et al., 2019; Liu et al., 2021), Continual RL (Khetarpal et al., 2020; Lehnert et al., 2017), and Unsupervised RL (Laskin et al., 2021)."
        },
        "aliases": [
          "Meta RL",
          "Continual RL",
          "Unsupervised RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ηψ-Learning",
          "justification": "The paper names the proposed method as ηψ-Learning.",
          "quote": "In this work, we propose ηψ-Learning, an algorithm to compute an exploration strategy that methodically explores within a single finite-length trajectory—as illustrated in Figure 1(b)."
        },
        "aliases": [
          "ηψ-Learning"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper introduces ηψ-Learning as a novel method for efficient exploration.",
          "quote": "In this work, we propose ηψ-Learning, an algorithm to compute an exploration strategy that methodically explores within a single finite-length trajectory—as illustrated in Figure 1(b)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides experimental results of ηψ-Learning on various environments.",
          "quote": "Our experiments demonstrate the efficacy of ηψ-Learning to strategically explore the environment and maximize the state coverage with limited samples."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares ηψ-Learning with several baseline methods, including MaxEnt and VariBAD.",
          "quote": "In this work, we propose ηψ-Learning, an algorithm to compute an exploration strategy that methodically explores within a single finite-length trajectory—as illustrated in Figure 1(b)."
        },
        "referenced_paper_title": {
          "value": "null",
          "justification": "No referenced paper title is provided for ηψ-Learning.",
          "quote": "No quote found."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 23802
  }
}