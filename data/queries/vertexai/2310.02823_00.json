{
  "paper": "2310.02823.txt",
  "words": 10014,
  "extractions": {
    "title": {
      "value": "Learning to Scale Logits for Temperature-Conditional GFlowNets",
      "justification": "The title of the paper is explicitly stated.",
      "quote": "Learning to Scale Logits for Temperature-Conditional GFlowNets"
    },
    "description": "The paper introduces Logit-GFN, a novel architecture design for temperature-conditional Generative Flow Networks (GFlowNets). The authors demonstrate its effectiveness in various biochemical tasks, including QM9, sEH, TFBind8, and RNA-binding. The key idea is to scale the logits of the policy directly using a learned function of the temperature, improving training stability and generalization. The authors also introduce an online discovery algorithm that leverages Logit-GFN to effectively discover diverse and high-reward candidate objects in scientific discovery tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a novel model architecture and provides extensive empirical results to validate its effectiveness, classifying it as empirical research.",
      "quote": "In our experimental results, the Logit-GFN architecture significantly enhances training stability, characterized by a smooth and rapid loss convergence."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on improving Generative Flow Networks (GFlowNets), which falls under the domain of Deep Learning.",
        "quote": "The paper introduces Logit-GFN, a novel architecture design for temperature-conditional Generative Flow Networks (GFlowNets)."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Biochemical Discovery",
          "justification": "The paper specifically applies and evaluates the proposed Logit-GFN architecture in the context of biochemical tasks, making it a key application area within Deep Learning.",
          "quote": "We present experimental results on 4 biochemical tasks: QM9, sEH, TFBind8, and RNA-binding."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Logit-scaling GFlowNets",
          "justification": "The paper refers to the model as \"Logit-scaling GFlowNets\" and uses the acronym \"Logit-GFN\".",
          "quote": "We propose Logit-scaling GFlowNets (Logit-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets."
        },
        "aliases": [
          "Logit-GFN",
          "Logit-scaling GFlowNets"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper introduces Logit-GFN as a novel architectural design for temperature-conditional GFlowNets.",
          "quote": "We propose Logit-scaling GFlowNets (Logit-GFN), a novel architectural design that greatly accelerates the training of temperature-conditional GFlowNets."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides experimental results of Logit-GFN on four biochemical tasks.",
          "quote": "In our experimental results, the Logit-GFN architecture significantly enhances training stability, characterized by a smooth and rapid loss convergence."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Logit-GFN with other temperature-conditional GFlowNet architectures, such as Layer-GFN, and unconditional GFlowNets.",
          "quote": "In this paper, we first suggest a new generic architecture design of temperature-conditional GFlowNets, called Logit-scaling GFlowNets (Logit-GFN), to obtain a simple yet stable training framework."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No reference paper is mentioned for the Logit-GFN model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "layer-conditioned GFlowNet",
          "justification": "The paper introduces the term \"layer-conditioned GFlowNet\" and uses the acronym \"Layer-GFN\".",
          "quote": "We denote this approach as layer-conditioned GFlowNet (Layer-GFN) that integrates temperature embeddings directly into the model parameterized by θ."
        },
        "aliases": [
          "Layer-GFN",
          "layer-conditioned GFlowNet"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Layer-GFN is not a novel contribution of this paper but is used as a baseline.",
          "quote": "A conventional approach for constructing a conditional model involves concatenating the conditioning values directly into model layers (Song et al., 2020; Ho et al., 2020; Zhang et al., 2022c). We denote this approach as layer-conditioned GFlowNet (Layer-GFN) that integrates temperature embeddings directly into the model parameterized by θ."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides experimental results comparing Layer-GFN with Logit-GFN and other models.",
          "quote": "In our experimental results, the Logit-GFN architecture significantly enhances training stability, characterized by a smooth and rapid loss convergence."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Layer-GFN with Logit-GFN and unconditional GFlowNets.",
          "quote": "In this paper, we first suggest a new generic architecture design of temperature-conditional GFlowNets, called Logit-scaling GFlowNets (Logit-GFN), to obtain a simple yet stable training framework."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper cites several references for layer-conditioned models, including Song et al. (2020), Ho et al. (2020), and Zhang et al. (2022c).",
          "quote": "A conventional approach for constructing a conditional model involves concatenating the conditioning values directly into model layers (Song et al., 2020; Ho et al., 2020; Zhang et al., 2022c)."
        }
      },
      {
        "name": {
          "value": "Generative Flow Networks",
          "justification": "The paper refers to the model as \"Generative Flow Networks\" or \"GFlowNets\".",
          "quote": "Generative Flow Networks (GFlowNets) (Bengio et al., 2021) offer a training framework for learning generative policies that sequentially construct compositional objects to be sampled according to a given unnormalized density or reward function."
        },
        "aliases": [
          "GFlowNets",
          "Generative Flow Networks"
        ],
        "is_contributed": {
          "value": false,
          "justification": "GFlowNets are not a novel contribution of this paper but are the foundational framework for the proposed Logit-GFN.",
          "quote": "Generative Flow Networks (GFlowNets) (Bengio et al., 2021) offer a training framework for learning generative policies that sequentially construct compositional objects to be sampled according to a given unnormalized density or reward function."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses GFlowNets as a baseline and for comparison in various experiments.",
          "quote": "To evaluate the proposed methods, we first compare them with the established unconditional GFlowNet."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper extensively compares GFlowNets, both conditional and unconditional, with other methods like MARS, A2C, Soft Q-Learning, and PPO.",
          "quote": "To evaluate the proposed methods, we first compare them with the established unconditional GFlowNet."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper cites Bengio et al. (2021) as the foundational work on GFlowNets.",
          "quote": "Generative Flow Networks (GFlowNets) (Bengio et al., 2021) offer a training framework for learning generative policies that sequentially construct compositional objects to be sampled according to a given unnormalized density or reward function."
        }
      },
      {
        "name": {
          "value": "Topoformer",
          "justification": "The paper refers to the model as \"Topoformer\".",
          "quote": "Temperature conditioning, applied in combinatorial scheduling problems (Zhang et al., 2022c), uses a variable temperature factor to modulate the scheduling objective’s smoothness. In the Topoformer architecture, Gagrani et al. (2022) implement this through matrix multiplication with the temperature parameter in the initial linear layer."
        },
        "aliases": [
          "Topoformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Topoformer is not a contribution of this paper and is cited as existing work.",
          "quote": "Temperature conditioning, applied in combinatorial scheduling problems (Zhang et al., 2022c), uses a variable temperature factor to modulate the scheduling objective’s smoothness. In the Topoformer architecture, Gagrani et al. (2022) implement this through matrix multiplication with the temperature parameter in the initial linear layer."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper doesn\\'t mention any experiments using the Topoformer model.",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "The paper mentions Topoformer in the context of temperature-conditional GFlowNets but doesn\\'t directly compare its performance.",
          "quote": "Temperature conditioning, applied in combinatorial scheduling problems (Zhang et al., 2022c), uses a variable temperature factor to modulate the scheduling objective’s smoothness. In the Topoformer architecture, Gagrani et al. (2022) implement this through matrix multiplication with the temperature parameter in the initial linear layer."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper cites Gagrani et al. (2022) in the context of Topoformer for scheduling problems.",
          "quote": "Temperature conditioning, applied in combinatorial scheduling problems (Zhang et al., 2022c), uses a variable temperature factor to modulate the scheduling objective’s smoothness. In the Topoformer architecture, Gagrani et al. (2022) implement this through matrix multiplication with the temperature parameter in the initial linear layer."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 18902
  }
}