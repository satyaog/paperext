{
  "paper": "2302.09852.txt",
  "words": 15592,
  "extractions": {
    "title": {
      "value": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection",
      "justification": "This is the title of the paper.",
      "quote": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection"
    },
    "description": "The authors propose an unsupervised layer-wise score aggregation method for textual out-of-distribution detection. They argue that using the last layer of the encoder for OOD detection is not always optimal, and better results can be achieved by aggregating scores from all layers. They introduce a data-driven procedure to combine layer-wise anomaly scores and evaluate their method on a new benchmark called MILTOOD-C, which includes classification tasks with a significant number of classes and datasets in multiple languages.",
    "type": {
      "value": "empirical",
      "justification": "The authors conduct experiments to evaluate their proposed method, making it empirical research.",
      "quote": "We conduct extensive experiments on our newly proposed benchmark"
    },
    "primary_research_field": {
      "name": {
        "value": "OOD detection",
        "justification": "The primary focus of the paper is on OOD detection in text classification.",
        "quote": "With the increasing deployment of ML tools and systems, the issue of their safety and robustness is becoming more and more critical. Out-of-distribution robustness and detection have emerged as an important research direction"
      },
      "aliases": [
        "OOD detection",
        "Out-of-distribution robustness",
        "Out-of-distribution detection"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "text classification",
          "justification": "The research is conducted in the context of text classification, a subfield of NLP.",
          "quote": "Although OOD detection has attracted much attention in computer vision (Huang et al. 2022; Wang et al. 2022c; Fang et al. 2022), few studies focused on textual data."
        },
        "aliases": [
          "text classification",
          "natural language processing",
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Transformer",
          "justification": "The authors use several variations of the Transformer model.",
          "quote": "To ensure that our results are consistent not only across tasks and shifts, but also across model architectures, we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al. 2018) (base, large and multilingual versions), DISTILBERT (Sanh et al. 2019) and RoBERTa (Liu et al. 2019) (base and large versions) fine-tuned on each task."
        },
        "aliases": [
          "Transformer",
          "BERT",
          "DISTILBERT",
          "RoBERTa"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors do not contribute a new model.",
          "quote": "None"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors fine-tune different Transformer models on various text classification tasks.",
          "quote": "To ensure that our results are consistent not only across tasks and shifts, but also across model architectures, we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al. 2018) (base, large and multilingual versions), DISTILBERT (Sanh et al. 2019) and RoBERTa (Liu et al. 2019) (base and large versions) fine-tuned on each task."
        },
        "is_compared": {
          "value": true,
          "justification": "The authors compare different Transformer-based models (BERT, DISTILBERT, RoBERTa) for their OOD detection performance.",
          "quote": "To ensure that our results are consistent not only across tasks and shifts, but also across model architectures, we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al. 2018) (base, large and multilingual versions), DISTILBERT (Sanh et al. 2019) and RoBERTa (Liu et al. 2019) (base and large versions) fine-tuned on each task."
        },
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "The authors reference the original Transformer paper.",
          "quote": "To ensure that our results are consistent not only across tasks and shifts, but also across model architectures, we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al. 2018) (base, large and multilingual versions), DISTILBERT (Sanh et al. 2019) and RoBERTa (Liu et al. 2019) (base and large versions) fine-tuned on each task."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 41998
  }
}