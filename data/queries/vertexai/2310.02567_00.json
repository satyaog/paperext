{
  "paper": "2310.02567.txt",
  "words": 9458,
  "extractions": {
    "title": {
      "value": "Improving Automatic VQA Evaluation Using Large Language Models",
      "justification": "The title of the paper is \\\"Improving Automatic VQA Evaluation Using Large Language Models\\\".",
      "quote": "Improving Automatic VQA Evaluation Using Large Language Models"
    },
    "description": "8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation. VQA Accuracy has been effective so far in the IID evaluation setting. However, our community is undergoing a shift towards open-ended generative models and OOD evaluation. In this new paradigm, the existing VQA Accuracy metric is overly stringent and underestimates the performance of VQA systems. Thus, there is a need to develop more robust automatic VQA metrics that serve as a proxy for human judgment. In this work, we propose to leverage the in-context learning capabilities of instruction-tuned large language models (LLMs) to build a better VQA metric. We formulate VQA evaluation as an answer-rating task where the LLM is instructed to score the accuracy of a candidate answer given a set of reference answers. We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks.\\nWe hope wide adoption of our metric will contribute to better estimating the research progress on the VQA task. We plan to release the evaluation code and collected human judgments.\\n\\n1\\nIntroduction\\n\\nVisual question answering (VQA) (Antol et al. 2015) has become an essential benchmark for assessing the progress of multimodal vision-language systems. 8 years after the task was proposed, accuracy remains the primary metric for automatically evaluating model performance. VQA Accuracy is based on exact string matching between a candidate answer predicted by the model and a set of reference answers annotated by humans. As pointed out in Agrawal et al. (2023), this metric has been effective so far because the VQA evaluation primarily followed the the independent and identically distributed (IID) paradigm, where the training and testing data distributions are quite similar. Thus, models could learn to adapt to the test answer distribution. However,\\nrecently, our community has been shifting its focus towards out-of-distribution (OOD) evaluation, either via zero-shot transfer to unseen VQA tasks or via finetuning on one VQA dataset and evaluating on another (Agrawal et al. 2023).\\nIn these settings, the answers generated by VQA models might not match any of the reference answers, while still Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\\n\\nVQA Accuracy\\n\\nMETEOR\\n\\n0.00\\n\\n0.52\\n\\nBERTScore\\n\\nHuman Score\\n\\n0.59 Question: What color is the plane?\\nReference answers: red white blue,\\nwhite, red, blue, white, red, and blue, white red blue black, …\\nCandidate answer: white and red\\n\\nLAVE\\n\\n1.00\\n\\nRationale: The candidate answer is correct because it includes both\\n\\'white\\' and \\'red\\', which are mentioned in the reference answers.\\n\\nVQA Accuracy\\n\\nMETEOR\\n\\n0.00\\n\\n0.00\\n\\nBERTScore\\n\\nHuman Score\\n\\n0.09 Question: When will the train move?\\nReference answers: after the passengers have boarded Candidate answer: soon\\n\\n1.00\\n\\nLAVE\\n\\n0.50\\n\\n0.50\\n\\nRationale: The candidate answer is ambiguous because \\'soon\\' does not provide a specific timeframe for when the train will move.\\n\\nFigure 1: Existing VQA metrics and other strong baselines tend to miss out on correct answers generated by VQA models. Our proposed metric, LAVE, is more aligned with human judgment and provides a rationale for its rating, making it also more interpretable.\\n\\nonly one reference answer per question is provided. More recently, Hu et al. (2022) devise a soft VQA Accuracy metric as part of their data filtering pipeline. We implement this metric as one of our baselines for comparison (Sec. 5.1).\\nMetrics for GenQA VQA and QA both involve answering questions related to a given context, either visual or textual. Generative QA evaluation faces similar challenges as VQA, relying primarily on metrics such as exact-match\\n(EM) and F1 Score. Similarly to Luo et al. (2021), Si, Zhao,\\nand Boyd-Graber (2021) also propose expanding reference answers with equivalent ones mined from knowledge bases.\\nLee et al. (2021) introduce a metric that weights answer tokens via keyphrase prediction. Chen et al. (2019) found that a straightforward application of BERTScore fails to provide stronger correlation with human judgements. Instead, other works (Risch et al. 2021; Bulian et al. 2022) train a semantic answer similarity metric based on BERT, showing improved correlation with human judgment. In contrast, we explore the capabilities of instruction-tuned LLMs in comparing candidate and reference answers.\\nUsing LLMs as evaluators Recently, several works (Fu et al. 2023; Liu et al. 2023; Kamalloo et al. 2023; Li et al. 2023a; Zheng et al. 2023; Rajani et al. 2023) have explored the possibility of using LLMs (Flan-T5 (Chung et al. 2022), OPT (Zhang et al. 2022), GPT-X (Brown et al. 2020; OpenAI 2023)) to evaluate text generation for different tasks (e.g., summarization, dialogue generation,\\nmachine translation, QA, ...). Closer to our work, Zhou et al.\\n(2023) propose using ChatGPT to automatically evaluate model outputs on a Likert scale. However, the quality of their metric remains uncertain as they do not provide any evidence of its alignment with human judgment. In this work, we aim to rigorously assess the effectiveness of LLMs in evaluating VQA by measuring their correlation with human judgment in diverse settings.\\n\\n3\\nAnalysis of VQA Accuracy Failure Modes\\n\\nThe motivation for developing a new VQA metric arises from the limitations of VQA Accuracy in handling openended model responses, which are not suitable for exact\\n\\n\\nFigure 2: Example of a VQA Accuracy failure mode from the multiple answers category (Tab. 1). Q: What are the sticks for? A: balance, pushing, skating, ...\\n\\n\\nTable 1: Failure modes of the strict VQA Accuracy where correct responses are marked as incorrect. Model generated answers are marked in blue and reference answers in orange.\\nCategory\\n\\nDefinition\\n\\nExamples\\n\\n%\\n\\nMultiple answers\\n\\nSubjective, answers might focus on different aspects of the scene/activity.\\n\\nQ: What are the sticks for? A: balance, pushing, skating, ...\\n(Fig. 2)\\n\\n34.25\\n\\nOver- or underspecifying and verbosity\\n\\nThe candidate answer contains more/less details than the references or is more/less verbose.\\n\\nQ: Where is the hydrant? A: on the right, right; Q: What color are the flowers? A: pink, pink and orange and red\\n\\n27.75\\n\\nSynonym\\n\\nIncludes “almost-synonym“ relation.\\n\\nQ: What is the setting of this picture? A: field, plains, grassland;\\nQ: What is the sign telling you to do or not do? A: no entry, do not enter\\n\\n21.0\\n\\nBroad/bad question or generic response\\n\\nQuestion is near impossible to answer or highly subjective; model avoids answering by being overly generic.\\n\\nQ: How many sheep are there? A: many; Q: What is the current condition of these animals? (image simply shows a baby elephant)\\n\\n18.0\\n\\nIncorrect\\n\\nHuman judgment is incorrect.\\n\\n-\\n\\n8.25\\n\\nSame stem\\n\\nReference and candidate share the same stem (plural vs. singular or gerund); different formatting or whitespace.\\n\\nQ: What are the people doing? A: playing video games/game;\\nQ: What shape is the building? A: rectangular, rectangle; Q:\\nWhat colors are in the surfer’s shirt? A: blue and white, white and blue\\n\\n5.75\\n\\nHypernym\\n\\n“Subcategory-of” relation.\\n\\nQ: What are the people doing? A: playing wii, playing video games; Q: What is in the blender? A: vegetables, carrots\\n\\n5.0\\n\\nUnknown issue\\n\\nModel responds with “unknown”.\\n\\n-\\n\\n3.75\\n\\nAmbiguous object\\n\\nPhrase could refer to multiple objects in the image.\\n\\nQ: What kind of sign is this? A: billboard, street sign (image shows multiple signs/billboards)\\n\\n2.0\\n\\nstring matching. To understand the specific failure modes a new metric should address, we conducted a small study where we manually categorized 400 VQA examples. We looked at examples where VQA Accuracy is below 0.5\\n(at most 1 out of 10 reference answers matches with the model’s response), but at least 4 out of 5 humans rated the model’s response as correct. In other words: when are actually correct responses marked as incorrect the way current VQA systems are evaluated? We annotated 100 examples for each of four model-dataset pairs: (BLIP-2,\\nVQAv2), (BLIP-2, VG-QA), (BLIP-2, OK-VQA), and\\n(PromptCap, VQAv2). We focus on BLIP-2 and PromptCap since their generation is most open-ended.\\nOur initial set of failure modes is inspired from Luo et al.\\n(2021), and manual inspection resulted in several additional categories. For clarity and conciseness, we decided to merge certain categories. Tab. 1 shows the consolidated nine categories with definitions, examples and frequencies.\\nWe identified four prevalent failure modes: (1) multiple answers, (2) over- or under-specification and verbosity,\\n(3) synonyms and (4) broad/bad question or generic response. We observe that certain question types naturally lead to various possible correct answers. For instance,\\nmany where-questions (e.g., “Where is the clock?”) can be answered using either absolute positioning or relative positioning to other objects. Other open-ended questions,\\nsuch as asking what a person is doing or feeling, can be interpreted in multiple ways (e.g., “Why is she posing for picture?”). Luo et al. (2021) introduced the category\\nambiguous object when a phrase in the question could point to several objects (e.g., “What color is the shirt?” when there are several shirts). However, our inspection showed only a few occurrences of it and we speculate it often also falls into the multiple answers category.\\nIn summary, our analysis revealed that the open-ended nature of visual question answering can lead to multiple complex failure modes in VQA Accuracy.\\n\\n4\\nMethod\\n\\nWe present LAVE, an LLM-based evaluation framework to automatically assess the quality of answers generated by VQA models. Each VQA example comprises an image i,\\na question about the image q, and a set of reference answers R provided by human annotators. Given i and q, a VQA model f generates a candidate answer c, i.e., c = f (i, q).\\nOur goal is to automatically evaluate the quality of the generated answer c by comparing it with the references R. To enhance the evaluation process, we can additionally leverage the contextual information from the question q and the image i. We build a textual prompt using R, c, and optionally q and i (as an image description). This prompt is then fed to an LLM to generate a quality rating. The following sections describe the key design decisions underlying our approach.\\n\\n4.1\\nChoosing a Large Language Model\\n\\nIt is crucial to choose an appropriate LLM as LAVE’s performance directly hinges on its capabilities. We pose VQA evaluation as a close-ended answer-verification task and\\n\\n\\nTask description You are given a question, a set of gold-standard reference answers written by experts, and a candidate answer. Please rate the accuracy of the candidate answer for the question considering the reference answers.\\nUse a scale of 1-3, with 1 indicating an incorrect or irrelevant answer, 2 indicating an ambiguous or incomplete answer, and 3 indicating a correct answer. Give the rationale before rating.\\nTHIS IS VERY IMPORTANT: A binary question should only be answered with ",
    "type": {
      "value": "empirical",
      "justification": "This research paper presents empirical findings based on experiments conducted to evaluate the proposed LLM-based VQA metric, LAVE.",
      "quote": "We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The authors focus on evaluating Visual Question Answering (VQA) models within the field of Computer Vision.",
        "quote": "Visual question answering (VQA) (Antol et al. 2015) has become an essential benchmark for assessing the progress of multimodal vision-language systems."
      },
      "aliases": []
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "LAVE",
          "justification": "The paper refers to the proposed metric as LAVE, which stands for LLM-Assisted VQA Evaluation.",
          "quote": "We propose a novel automatic VQA evaluation metric, LAVE (LLM-Assisted VQA Evaluation), which leverages the in-context learning capabilities of instruction-tuned LLMs."
        },
        "aliases": [
          "LAVE",
          "LLM-Assisted VQA Evaluation"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper proposes LAVE, an LLM-based evaluation framework to automatically assess the quality of answers generated by VQA models.",
          "quote": "We present LAVE, an LLM-based evaluation framework to automatically assess the quality of answers generated by VQA models."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper evaluates LAVE on answers generated by several VQA models across multiple VQA benchmarks.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the proposed metric, LAVE, with several baseline VQA evaluation metrics.",
          "quote": "Our results demonstrate that LAVE correlates better with human judgment compared to existing metrics in diverse settings (Fig. 1)."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No reference paper is mentioned for LAVE as it is a novel contribution of this work.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Flan-T5",
          "justification": "The paper explicitly names \\\"Flan-T5\\\" as one of the LLMs considered.",
          "quote": "Considering all these factors, we first select the Flan-T5 (Chung et al. 2022)\\nmodel family for our metric."
        },
        "aliases": [
          "Flan-T5"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses the existing Flan-T5 model and does not contribute to its development.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses Flan-T5 to evaluate the performance of LAVE.",
          "quote": "We leverage the HuggingFace Transformers’ (Wolf et al. 2020)\\nimplementation of Flan-T5 and LLaMA (for Vicuna), and use GPT-3.5-Turbo through OpenAI’s API1 ."
        },
        "is_compared": {
          "value": true,
          "justification": "The authors consider Flan-T5 as one of the Large Language Models (LLMs) for their proposed VQA evaluation metric, LAVE.",
          "quote": "Considering all these factors, we first select the Flan-T5 (Chung et al. 2022)\\nmodel family for our metric."
        },
        "referenced_paper_title": {
          "value": "Scaling instruction-finetuned language models",
          "justification": "The authors reference the paper \\\"Scaling instruction-finetuned language models\\\" by Chung et al. (2022) when mentioning Flan-T5.",
          "quote": "Considering all these factors, we first select the Flan-T5 (Chung et al. 2022)\\nmodel family for our metric."
        }
      },
      {
        "name": {
          "value": "Vicuna-v1.3",
          "justification": "The paper explicitly names \\\"Vicuna-v1.3\\\" as one of the LLMs considered.",
          "quote": "To demonstrate LAVE’s robustness across different LLMs, we also consider Vicuna-v1.3 (Chiang et al. 2023)"
        },
        "aliases": [
          "Vicuna-v1.3",
          "Vicuna"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses the existing Vicuna model and does not contribute to its development.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses Vicuna-v1.3 for evaluating LAVE's performance.",
          "quote": "We leverage the HuggingFace Transformers’ (Wolf et al. 2020)\\nimplementation of Flan-T5 and LLaMA (for Vicuna), and use GPT-3.5-Turbo through OpenAI’s API1 ."
        },
        "is_compared": {
          "value": true,
          "justification": "The authors include Vicuna-v1.3 as one of the Large Language Models (LLMs) for comparison in their proposed VQA evaluation metric, LAVE.",
          "quote": "To demonstrate LAVE’s robustness across different LLMs, we also consider Vicuna-v1.3 (Chiang et al. 2023)\\nand GPT-3.5-Turbo (aka ChatGPT (OpenAI 2022))."
        },
        "referenced_paper_title": {
          "value": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
          "justification": "The authors reference the paper \\\"Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality\\\" by Chiang et al. (2023) when mentioning Vicuna-v1.3.",
          "quote": "To demonstrate LAVE’s robustness across different LLMs, we also consider Vicuna-v1.3 (Chiang et al. 2023)"
        }
      },
      {
        "name": {
          "value": "GPT-3.5-Turbo",
          "justification": "The paper explicitly names \\\"GPT-3.5-Turbo\\\" (also known as ChatGPT) as one of the LLMs considered.",
          "quote": "To demonstrate LAVE’s robustness across different LLMs, we also consider Vicuna-v1.3 (Chiang et al. 2023)\\nand GPT-3.5-Turbo (aka ChatGPT (OpenAI 2022))."
        },
        "aliases": [
          "GPT-3.5-Turbo",
          "ChatGPT",
          "GPT-3.5"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses the existing GPT-3.5-Turbo model and does not contribute to its development.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses GPT-3.5-Turbo for evaluating the performance of LAVE.",
          "quote": "We leverage the HuggingFace Transformers’ (Wolf et al. 2020)\\nimplementation of Flan-T5 and LLaMA (for Vicuna), and use GPT-3.5-Turbo through OpenAI’s API1 ."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper includes GPT-3.5-Turbo as one of the Large Language Models (LLMs) for comparison in the proposed VQA evaluation metric, LAVE.",
          "quote": "To demonstrate LAVE’s robustness across different LLMs, we also consider Vicuna-v1.3 (Chiang et al. 2023)\\nand GPT-3.5-Turbo (aka ChatGPT (OpenAI 2022))."
        },
        "referenced_paper_title": {
          "value": "Introducing ChatGPT",
          "justification": "The paper references related work by OpenAI (2022) when mentioning GPT-3.5-Turbo.",
          "quote": "To demonstrate LAVE’s robustness across different LLMs, we also consider Vicuna-v1.3 (Chiang et al. 2023)\\nand GPT-3.5-Turbo (aka ChatGPT (OpenAI 2022))."
        }
      },
      {
        "name": {
          "value": "BLIP-2 Flan-T5XXL",
          "justification": "The paper explicitly names \\\"BLIP-2 Flan-T5XXL\\\" as one of the VQA models used.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks. In particular, we consider two representative state-of-the-art VQA models: BLIP-2 Flan-T5XXL (Li et al. 2023b) and PromptCap GPT-3 (Hu et al."
        },
        "aliases": [
          "BLIP-2",
          "BLIP-2 Flan-T5-XXL"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses the existing BLIP-2 Flan-T5-XXL model and does not contribute to its development.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The authors use BLIP-2 Flan-T5-XXL to generate answers on different VQA datasets.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks. In particular, we consider two representative state-of-the-art VQA models: BLIP-2 Flan-T5XXL (Li et al. 2023b) and PromptCap GPT-3 (Hu et al."
        },
        "is_compared": {
          "value": true,
          "justification": "BLIP-2 Flan-T5-XXL is one of the models used for generating answers that are then evaluated. Its performance is implicitly compared to other models like PromptCap.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks. In particular, we consider two representative state-of-the-art VQA models: BLIP-2 Flan-T5XXL (Li et al. 2023b) and PromptCap GPT-3 (Hu et al.\\n2022)."
        },
        "referenced_paper_title": {
          "value": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
          "justification": "The authors reference the paper \\\"Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models\\\" by Li et al. (2023b) when mentioning BLIP-2 Flan-T5XXL.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks. In particular, we consider two representative state-of-the-art VQA models: BLIP-2 Flan-T5XXL (Li et al. 2023b) and PromptCap GPT-3 (Hu et al."
        }
      },
      {
        "name": {
          "value": "PromptCap GPT-3",
          "justification": "The paper explicitly identifies \\\"PromptCap GPT-3\\\" as one of the VQA models employed in the study.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks. In particular, we consider two representative state-of-the-art VQA models: BLIP-2 Flan-T5XXL (Li et al. 2023b) and PromptCap GPT-3 (Hu et al."
        },
        "aliases": [
          "PromptCap",
          "PromptCap GPT-3"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper utilizes the existing PromptCap GPT-3 model and doesn't introduce any modifications or enhancements to its architecture or training process.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "PromptCap GPT-3 is utilized for generating answers on various VQA datasets as part of the evaluation process.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks. In particular, we consider two representative state-of-the-art VQA models: BLIP-2 Flan-T5XXL (Li et al. 2023b) and PromptCap GPT-3 (Hu et al."
        },
        "is_compared": {
          "value": true,
          "justification": "PromptCap GPT-3 is one of the models used for answer generation and its performance is implicitly compared with other models like BLIP-2.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks. In particular, we consider two representative state-of-the-art VQA models: BLIP-2 Flan-T5XXL (Li et al. 2023b) and PromptCap GPT-3 (Hu et al."
        },
        "referenced_paper_title": {
          "value": "PromptCap: Prompt-Guided Task-Aware Image Captioning",
          "justification": "The reference \\\"PromptCap: Prompt-Guided Task-Aware Image Captioning\\\" by Hu et al. (2022) is provided in the context of using the PromptCap GPT-3 model, suggesting this paper as its source.",
          "quote": "We evaluate LAVE on answers generated by several VQA models across multiple VQA benchmarks. In particular, we consider two representative state-of-the-art VQA models: BLIP-2 Flan-T5XXL (Li et al. 2023b) and PromptCap GPT-3 (Hu et al.\\n2022)."
        }
      },
      {
        "name": {
          "value": "BLIPVQA",
          "justification": "While the paper refers to the model as \\\"BLIP\\\" when discussing fine-tuning, it uses the name \\\"BLIPVQA\\\" to denote the specific instance fine-tuned on VQAv2.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ),\\nwhich represents the finetuning-OOD paradigm."
        },
        "aliases": [
          "BLIP",
          "BLIPVQA"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses a pre-existing BLIP model fine-tuned on VQAv2 and doesn't introduce any novel architectural changes or training procedures for BLIP itself.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The authors utilize BLIP, fine-tuned on VQAv2, to generate answers for different VQA datasets.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ),\\nwhich represents the finetuning-OOD paradigm."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of BLIP, fine-tuned on VQAv2, against other VQA models such as BLIP-2 and PromptCap.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ),\\nwhich represents the finetuning-OOD paradigm."
        },
        "referenced_paper_title": {
          "value": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
          "justification": "The paper cites \\\"Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation\\\" by Li et al. (2022) when initially introducing the BLIP model.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ),\\nwhich represents the finetuning-OOD paradigm."
        }
      },
      {
        "name": {
          "value": "BLIPVG",
          "justification": "The name \\\"BLIPVG\\\" is used to denote the specific instance of the BLIP model after being fine-tuned on the VG-QA dataset.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ),\\nwhich represents the finetuning-OOD paradigm."
        },
        "aliases": [
          "BLIPVG"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses a pre-existing BLIP model fine-tuned on VG-QA. It doesn't introduce any novelties regarding BLIP's architecture or training methodologies.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "BLIP, fine-tuned on VG-QA (BLIPVG), is used to generate answers on various VQA datasets for evaluation.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ),\\nwhich represents the finetuning-OOD paradigm."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of BLIP, fine-tuned on VG-QA (BLIPVG), against other VQA models such as BLIP-2 and PromptCap.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ),\\nwhich represents the finetuning-OOD paradigm."
        },
        "referenced_paper_title": {
          "value": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
          "justification": "Although no separate paper is referenced for BLIPVG, it can be inferred that it's based on the BLIP model introduced by Li et al. (2022).",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA ) and VG-QA (BLIPVG ),\\nwhich represents the finetuning-OOD paradigm."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 23354
  }
}