{
  "paper": "2305.11938.txt",
  "words": 17798,
  "extractions": {
    "title": {
      "value": "X TREME -U P: A User-Centric Scarce-Data Benchmark for Under-Represented Languages",
      "justification": "The title is mentioned in the paper.",
      "quote": "X TREME -U P: A User-Centric Scarce-Data Benchmark for Under-Represented Languages"
    },
    "description": "The paper introduces X-TREME-UP, a benchmark for evaluating multilingual models on user-centric tasks in a few-shot setting, focusing on under-resourced languages. The benchmark covers tasks like ASR, OCR, MT, QA, and NER, with a focus on realistic data scarcity.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes the creation of a benchmark and evaluates existing models on it.",
      "quote": "Motivated by this, we propose X TREME -U P, a benchmark defined by: its focus on the scarcedata scenario rather than zero-shot; its focus on user-centric tasks—tasks with broad adoption by speakers of high-resource languages; and its focus on under-represented languages where this scarce-data scenario tends to be most realistic."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on Natural Language Processing for under-resourced languages.",
        "quote": "The development of natural language processing\\n(NLP) technology that serves most of world’s languages is hindered by the stark lack of data for most languages (Joshi et al., 2020)."
      },
      "aliases": []
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "mT5",
          "justification": "mT5 is mentioned as one of the baseline models.",
          "quote": "mT5-base (Xue et al., 2021) and a subword-based multilingual encoder-decoder model"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not introduce any new models.",
          "quote": "Given that our focus in this paper is on the dataset and task setup rather than system building, we do not focus on offering novel modeling types nor do we exhaustively evaluate all possible models; rather we view these results as estimating a starting point from some well-known modeling approaches and seeding contributions from the broader research community."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses these models for baseline experiments.",
          "quote": "We provide results on a handful of baseline systems that have already been developed by the research community."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper evaluates mT5 and ByT5 models on the benchmark.",
          "quote": "We provide results on a handful of baseline systems that have already been developed by the research community."
        },
        "referenced_paper_title": {
          "value": "mT5: A massively multilingual pre-trained text-to-text transformer.",
          "justification": "The paper cites the relevant mT5 paper.",
          "quote": "mT5-base (Xue et al., 2021) and a subword-based multilingual encoder-decoder model"
        }
      },
      {
        "name": {
          "value": "ByT5",
          "justification": "ByT5 is mentioned as one of the baseline models.",
          "quote": "ByT5-base (Xue et al., 2022), a byte-based multilingual encoder-decoder model."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not introduce any new models.",
          "quote": "Given that our focus in this paper is on the dataset and task setup rather than system building, we do not focus on offering novel modeling types nor do we exhaustively evaluate all possible models; rather we view these results as estimating a starting point from some well-known modeling approaches and seeding contributions from the broader research community."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses these models for baseline experiments.",
          "quote": "We provide results on a handful of baseline systems that have already been developed by the research community."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper evaluates ByT5 models on the benchmark.",
          "quote": "We provide results on a handful of baseline systems that have already been developed by the research community."
        },
        "referenced_paper_title": {
          "value": "ByT5: Towards a token-free future with pre-trained byte-to-byte models.",
          "justification": "The paper cites the relevant ByT5 paper.",
          "quote": "ByT5-base (Xue et al., 2022), a byte-based multilingual encoder-decoder model."
        }
      },
      {
        "name": {
          "value": "Flan-PaLM",
          "justification": "Flan-PaLM is mentioned as the in-context learning baseline.",
          "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not introduce any new models.",
          "quote": "Given that our focus in this paper is on the dataset and task setup rather than system building, we do not focus on offering novel modeling types nor do we exhaustively evaluate all possible models; rather we view these results as estimating a starting point from some well-known modeling approaches and seeding contributions from the broader research community."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses Flan-PaLM for baseline experiments.",
          "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Flan-PaLM's performance with fine-tuning approaches.",
          "quote": "In-context learning underperforms fine-tuning on limited data."
        },
        "referenced_paper_title": {
          "value": "Scaling instruction-finetuned language models.",
          "justification": "The paper cites the relevant Flan-PaLM paper.",
          "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
        }
      },
      {
        "name": {
          "value": "PaLM",
          "justification": "PaLM is mentioned as the base model for Flan-PaLM.",
          "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not introduce any new models.",
          "quote": "Given that our focus in this paper is on the dataset and task setup rather than system building, we do not focus on offering novel modeling types nor do we exhaustively evaluate all possible models; rather we view these results as estimating a starting point from some well-known modeling approaches and seeding contributions from the broader research community."
        },
        "is_executed": {
          "value": true,
          "justification": "PaLM is used for code-mixed data generation in the semantic parsing task.",
          "quote": "We use PaLM to convert the linearized query into a code-mixed query using few-shot prompting."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper mentions PaLM as the base model for Flan-PaLM.",
          "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "PaLM: Scaling language modeling with Pathways.",
          "justification": "The paper cites the relevant PaLM paper.",
          "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 35118
  }
}