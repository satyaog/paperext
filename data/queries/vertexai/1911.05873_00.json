{
  "paper": "1911.05873.txt",
  "words": 18973,
  "extractions": {
    "title": {
      "value": "A Reduction from Reinforcement Learning to No-Regret Online Learning",
      "justification": "The paper presents a reduction framework from reinforcement learning to no-regret online learning, suggesting a way to systematically design RL algorithms using tools from online learning.",
      "quote": "A Reduction from Reinforcement Learning to No-Regret Online Learning"
    },
    "description": "This paper presents a reduction from reinforcement learning to no-regret online learning to systematically design new RL algorithms. The authors propose a simple RL algorithm based on mirror descent and the generative-model oracle for tabular MDPs and its extension to linearly parameterized function approximators.",
    "type": {
      "value": "theoretical",
      "justification": "The paper provides a theoretical framework for designing RL algorithms based on a reduction to online learning. It also provides theoretical guarantees on the sample complexity of the proposed mirror descent algorithm.",
      "quote": "Therefore, the proposed reduction can be used as a tool to systematically design new RL algorithms. We demonstrate this idea by devising a simple RL algorithm based on mirror descent and the generative-model oracle. For any γ-discounted tabular RL problem, with probability at least 1 − δ, \u0010it learns an \u0011 ǫ|S||A| log( δ1 ) optimal policy using at most Õ (1−γ)4 ǫ2 samples."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper focuses on reducing reinforcement learning to no-regret online learning, which is a subfield of machine learning.",
        "quote": "We present a reduction from reinforcement learning (RL) to no-regret online learning based on the saddle-point formulation of RL, by which any online algorithm with sublinear regret can generate policies with provable performance guarantees."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper focuses on reducing reinforcement learning to no-regret online learning.",
          "quote": "We present a reduction from reinforcement learning (RL) to no-regret online learning based on the saddle-point formulation of RL, by which any online algorithm with sublinear regret can generate policies with provable performance guarantees."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Online Learning",
          "justification": "The paper focuses on reducing reinforcement learning to no-regret online learning.",
          "quote": "We present a reduction from reinforcement learning (RL) to no-regret online learning based on the saddle-point formulation of RL, by which any online algorithm with sublinear regret can generate policies with provable performance guarantees."
        },
        "aliases": []
      }
    ],
    "models": [],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 33077
  }
}