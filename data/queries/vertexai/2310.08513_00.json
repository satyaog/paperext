{
  "paper": "2310.08513.txt",
  "words": 14576,
  "extractions": {
    "title": {
      "value": "",
      "justification": "",
      "quote": ""
    },
    "description": "The paper explores the impact of initial weight structure, particularly its effective rank, on the richness or laziness of task learning in neural networks. The authors argue that high-rank initializations typically lead to lazier learning, characterized by smaller network changes during training. They support this claim with theoretical analysis and simulations of recurrent neural networks (RNNs) trained on neuroscience tasks. The study also identifies specific scenarios where low-rank initializations can result in lazier learning, particularly when the initialization aligns with task and data statistics.",
    "type": {
      "value": "empirical",
      "justification": "The study involves empirical simulations and experiments.",
      "quote": "Through both empirical and theoretical analyses, we discover that high-rank initializations typically yield smaller network changes indicative of lazier learning"
    },
    "primary_research_field": {
      "name": {
        "value": "Theoretical Neuroscience",
        "justification": "The paper primarily focuses on theoretical neuroscience, using deep learning tools to study neural circuit dynamics.",
        "quote": "In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics."
      },
      "aliases": []
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "Recurrent Neural Network",
          "justification": "The paper focuses on Recurrent Neural Networks, particularly their learning dynamics when initialized with different weight ranks.",
          "quote": "We examine recurrent neural networks (RNNs) because they are commonly adopted for modeling neural circuits (Barak, 2017; Song et al., 2016)."
        },
        "aliases": [
          "RNNs",
          "recurrent neural networks"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't introduce any new model.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The authors train RNNs on various neuroscience tasks.",
          "quote": "We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known neuroscience tasks (Figure 1) and demonstrate the applicability to different initial connectivity structures extracted from neuroscience data (Figure 2)."
        },
        "is_compared": {
          "value": true,
          "justification": "The authors compare different RNN initializations with varying ranks.",
          "quote": "This study examines how initial weight structure, particularly the effective rank, modulates the effective richness or laziness of task learning within the standard training regime."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Two-layer Linear Network",
          "justification": "The paper uses a two-layer linear network for its theoretical analysis.",
          "quote": "For a two-layer linear network with input data X ∈ Rd×m , W1 ∈ RN ×d and W2 ∈ R1×N as weights for layers 1 and 2, respectively, the NTK throughout training, K, is:"
        },
        "aliases": [
          "two-layer linear network",
          "linear teacher network"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't introduce any new model.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "The authors don't execute this model in experiments, but rather use it for theoretical analysis.",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "The paper uses a two-layer linear network to derive theoretical results.",
          "quote": "In this section, our theoretical results are framed in a simplified feedforward setting, as we use a two-layer network with linear activations."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Two-layer Feedforward Network",
          "justification": "The paper utilizes a two-layer feedforward network.",
          "quote": "For the MNIST task shown in Appendix Figure 4B, we used a two-layer feedforward network with a ReLU activation function."
        },
        "aliases": [
          "two-layer feedforward network"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't introduce any new model.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses this model in its experiments.",
          "quote": "For the MNIST task shown in Appendix Figure 4B, we used a two-layer feedforward network with a ReLU activation function."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper investigates a two-layer feedforward network with ReLU activation.",
          "quote": "We also demonstrated this trend for a non-idealized feedforward setting in Appendix Figure 4B"
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 26558
  }
}