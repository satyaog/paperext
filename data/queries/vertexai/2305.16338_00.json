{
  "paper": "2305.16338.txt",
  "words": 9242,
  "extractions": {
    "title": {
      "value": "Think Before You Act: Decision Transformers with Internal Working Memory",
      "justification": "Extracted from the paper's title.",
      "quote": "Think Before You Act: Decision Transformers with Internal Working Memory"
    },
    "description": "This research paper proposes a new deep learning model called Decision Transformers with Memory (DT-Mem) for reinforcement learning. The model is inspired by the concept of \"working memory\" in cognitive psychology and aims to improve the efficiency and generalization ability of existing large language model-based decision-making agents. DT-Mem consists of a Transformer module, a Memory module, and a Multi-layer perceptron (MLP) module. The authors evaluate DT-Mem on Atari games and Meta-World environments, demonstrating its superior performance over other state-of-the-art methods like Multi-game Decision Transformer (MDT), Prompt Decision Transformer (PDT), and Hyper-Decision Transformer (HDT). They also show that fine-tuning only the working memory module with a small amount of data can significantly improve the model\\'s adaptability to unseen tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a novel model and evaluates it empirically.",
      "quote": "Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper focuses on improving decision-making agents in reinforcement learning, a key aspect of Machine Learning.",
        "quote": "Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "LLM-based decision-making agents",
          "justification": "The paper focuses on enhancing these agents.",
          "quote": "Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. "
        },
        "aliases": [
          "LLM-based decision-making agents"
        ]
      },
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper explicitly frames the problem within the context of reinforcement learning.",
          "quote": "We formulate the RL problem as a Markov decision process (MDP) problem"
        },
        "aliases": [
          "Reinforcement Learning",
          "RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Decision Transformers with Memory",
          "justification": "The paper names the model as \\\"Decision Transformers with Memory (DT-Mem)\\\".",
          "quote": "Thus motivated, we propose Decision Transformers with Memory (DT-Mem)."
        },
        "aliases": [
          "DT-Mem"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper proposes DT-Mem as a novel contribution.",
          "quote": "We propose Decision Transformers with Memory (DT-Mem), a novel Transformer-based DT that improves model generalization, computational efficiency and model efficiency."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper presents experimental results for DT-Mem.",
          "quote": "Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only 10% of the model parameters compared to the state-of-the-art method."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper proposes and evaluates DT-Mem, comparing it to other models.",
          "quote": "Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only 10% of the model parameters compared to the state-of-the-art method."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No reference paper is mentioned for this model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Multi-game Decision Transformer",
          "justification": "The paper refers to the model as \\\"Multi-game Decision Transformer (MDT)\\\".",
          "quote": "Multi-game Decision Transformer [MDT, 22], which trains a large transformer-based model on multi-game domains."
        },
        "aliases": [
          "MDT",
          "Multi-game Decision Transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "MDT is not a contribution of this paper.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "The paper uses existing results for MDT.",
          "quote": "Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only 10% of the model parameters compared to the state-of-the-art method."
        },
        "is_compared": {
          "value": true,
          "justification": "DT-Mem is compared against MDT in the paper.",
          "quote": "Evaluation demonstrates that DT-Mem achieves better generalization on Atari games with only 10% of the model parameters compared to the state-of-the-art method."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper cites Lee et al. for MDT.",
          "quote": "Multi-game Decision Transformer [22]"
        }
      },
      {
        "name": {
          "value": "Hyper-Decision Transformer",
          "justification": "The paper refers to the model as \\\"Hyper-Decision Transformer (HDT)\\\".",
          "quote": "Hyper-Decision Transformer [HDT, 38], which utilizes a hyper-network module to help DT adapt rapidly to unseen tasks."
        },
        "aliases": [
          "HDT",
          "Hyper-Decision Transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "HDT is not a contribution of this paper.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "The paper uses existing results for HDT.",
          "quote": "Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38]."
        },
        "is_compared": {
          "value": true,
          "justification": "DT-Mem is compared against HDT.",
          "quote": "Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38]."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper cites Xu et al. for HDT.",
          "quote": "Hyper-Decision Transformer [38]"
        }
      },
      {
        "name": {
          "value": "Prompt Decision Transformer",
          "justification": "The paper refers to the model as \\\"Prompt Decision Transformer (PDT)\\\".",
          "quote": "The Prompt Decision Transformer [PDT, 37] generates actions by considering both recent context and pre-collected demonstrations from the target task."
        },
        "aliases": [
          "PDT",
          "Prompt Decision Transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "PDT is not a contribution of this paper.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "The paper uses existing results for PDT.",
          "quote": "Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38]."
        },
        "is_compared": {
          "value": true,
          "justification": "DT-Mem is compared against PDT.",
          "quote": "Furthermore, we demonstrate that fine-tuning DT-Memwith a small amount of data can produce state-of-the-art results on both Atari games and the Meta-World environment, when compared to MDT [22], PDT [37], and HDT [38]."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper cites Xu et al. for PDT.",
          "quote": "The Prompt Decision Transformer [37]"
        }
      },
      {
        "name": {
          "value": "Decision Transformer",
          "justification": "The paper mentions \\\"Decision Transformer (DT)\\\" in the context of pre-trained models.",
          "quote": "We fine-tune only the working memory in this work because we rely on the generalization capacity of a pre-trained Decision Transformer (DT)."
        },
        "aliases": [
          "DT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "DT is not a contribution of this paper.",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not execute DT, but leverages its pre-trained generalization capacity.",
          "quote": "We fine-tune only the working memory in this work because we rely on the generalization capacity of a pre-trained Decision Transformer (DT)."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper mentions DT in the context of memory module fine-tuning.",
          "quote": "We fine-tune only the working memory in this work because we rely on the generalization capacity of a pre-trained Decision Transformer (DT)."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No reference paper is explicitly mentioned for DT in this context.",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 17123
  }
}