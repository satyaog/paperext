{
  "paper": "2404.09939.txt",
  "words": 12371,
  "extractions": {
    "title": {
      "value": "A Survey on Deep Learning for Theorem Proving",
      "justification": "This is the title of the paper.",
      "quote": "A Survey on Deep Learning for Theorem Proving"
    },
    "description": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving by offering i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; ii) a meticulous summary of available datasets and strategies for data generation; iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art; and iv) a critical discussion on the persistent challenges and the promising avenues for future exploration.",
    "type": {
      "value": "theoretical",
      "justification": "The paper is a survey paper, therefore it is theoretical.",
      "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving by offering"
    },
    "primary_research_field": {
      "name": {
        "value": "Theorem Proving",
        "justification": "This paper is a survey about the application of deep learning to theorem proving.",
        "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving by offering"
      },
      "aliases": [
        "Theorem Proving"
      ]
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "PaLM",
          "justification": "The authors of this survey paper discuss PaLM in the context of autoformalization.",
          "quote": "Wu et al. (2022); Agrawal et al. (2022); Gadgil et al. (2022) study the prospects of autoformalization using PaLM (Chowdhery et al., 2023) and Codex (Chen et al., 2021) with few-shot prompting to translate mathematical problems into Isabelle and Lean."
        },
        "aliases": [
          "PaLM"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss PaLM in the context of autoformalization.",
          "quote": "Wu et al. (2022); Agrawal et al. (2022); Gadgil et al. (2022) study the prospects of autoformalization using PaLM (Chowdhery et al., 2023) and Codex (Chen et al., 2021) with few-shot prompting to translate mathematical problems into Isabelle and Lean."
        },
        "referenced_paper_title": {
          "value": "PaLM: Scaling language modeling with pathways",
          "justification": "The authors of this survey paper reference the paper \\\"PaLM: Scaling language modeling with pathways\\\" in the context of autoformalization.",
          "quote": "Wu et al. (2022); Agrawal et al. (2022); Gadgil et al. (2022) study the prospects of autoformalization using PaLM (Chowdhery et al., 2023) and Codex (Chen et al., 2021) with few-shot prompting to translate mathematical problems into Isabelle and Lean."
        }
      },
      {
        "name": {
          "value": "Codex",
          "justification": "The authors of this survey paper discuss Codex in the context of autoformalization.",
          "quote": "Wu et al. (2022); Agrawal et al. (2022); Gadgil et al. (2022) study the prospects of autoformalization using PaLM (Chowdhery et al., 2023) and Codex (Chen et al., 2021) with few-shot prompting to translate mathematical problems into Isabelle and Lean."
        },
        "aliases": [
          "Codex"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss Codex in the context of autoformalization.",
          "quote": "Wu et al. (2022); Agrawal et al. (2022); Gadgil et al. (2022) study the prospects of autoformalization using PaLM (Chowdhery et al., 2023) and Codex (Chen et al., 2021) with few-shot prompting to translate mathematical problems into Isabelle and Lean."
        },
        "referenced_paper_title": {
          "value": "\\\"Evaluating large language models trained on code\\\"",
          "justification": "The authors of this survey paper reference the paper '\\\"Evaluating large language models trained on code\\\" in the context of autoformalization.",
          "quote": "Wu et al. (2022); Agrawal et al. (2022); Gadgil et al. (2022) study the prospects of autoformalization using PaLM (Chowdhery et al., 2023) and Codex (Chen et al., 2021) with few-shot prompting to translate mathematical problems into Isabelle and Lean."
        }
      },
      {
        "name": {
          "value": "Minerva",
          "justification": "The authors of this survey paper discuss Minerva in the context of autoformalization.",
          "quote": "DSP (Jiang et al., 2023b) utilizes Minerva (Lewkowycz et al., 2022) to draft informal proofs and map them into formal sketches, with ATP systems employed to fill in the missing details in the proof sketch."
        },
        "aliases": [
          "Minerva"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss Minerva in the context of autoformalization.",
          "quote": "DSP (Jiang et al., 2023b) utilizes Minerva (Lewkowycz et al., 2022) to draft informal proofs and map them into formal sketches, with ATP systems employed to fill in the missing details in the proof sketch."
        },
        "referenced_paper_title": {
          "value": "Solving quantitative reasoning problems with language models",
          "justification": "The authors of this survey paper reference the paper \\\"Solving quantitative reasoning problems with language models\\\" in the context of autoformalization.",
          "quote": "DSP (Jiang et al., 2023b) utilizes Minerva (Lewkowycz et al., 2022) to draft informal proofs and map them into formal sketches, with ATP systems employed to fill in the missing details in the proof sketch."
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "The authors of this survey paper discuss GPT-4 in the context of autoformalization.",
          "quote": "Besides these efforts, Wu et al. (2022); Azerbayev et al. (2023); Jiang et al. (2023a) explore advanced LLMs like Codex and GPT-4 (Achiam et al., 2023) for informalization, i.e., the translation of formal statements to natural language."
        },
        "aliases": [
          "GPT-4"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss GPT-4 in the context of autoformalization.",
          "quote": "Besides these efforts, Wu et al. (2022); Azerbayev et al. (2023); Jiang et al. (2023a) explore advanced LLMs like Codex and GPT-4 (Achiam et al., 2023) for informalization, i.e., the translation of formal statements to natural language."
        },
        "referenced_paper_title": {
          "value": "GPT-4 technical report",
          "justification": "The authors of this survey paper reference the paper \\\"GPT-4 technical report\\\" in the context of autoformalization.",
          "quote": "Besides these efforts, Wu et al. (2022); Azerbayev et al. (2023); Jiang et al. (2023a) explore advanced LLMs like Codex and GPT-4 (Achiam et al., 2023) for informalization, i.e., the translation of formal statements to natural language."
        }
      },
      {
        "name": {
          "value": "BERT",
          "justification": "The authors of this survey paper discuss BERT in the context of premise selection.",
          "quote": "With the advancement of pre-trained language models, some efforts (Ferreira & Freitas, 2020a; Welleck et al., 2021) fine-tune BERT (Devlin et al., 2019)-like models to embed natural language statements into vectors and select premises using a linear classifier layer."
        },
        "aliases": [
          "BERT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss BERT in the context of premise selection.",
          "quote": "With the advancement of pre-trained language models, some efforts (Ferreira & Freitas, 2020a; Welleck et al., 2021) fine-tune BERT (Devlin et al., 2019)-like models to embed natural language statements into vectors and select premises using a linear classifier layer."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of deep bidirectional transformers for language understanding",
          "justification": "The authors of this survey paper reference the paper \\\"BERT: Pre-training of deep bidirectional transformers for language understanding\\\" in the context of premise selection.",
          "quote": "With the advancement of pre-trained language models, some efforts (Ferreira & Freitas, 2020a; Welleck et al., 2021) fine-tune BERT (Devlin et al., 2019)-like models to embed natural language statements into vectors and select premises using a linear classifier layer."
        }
      },
      {
        "name": {
          "value": "TreeLSTM",
          "justification": "The authors of this survey paper discuss TreeLSTM in the context of proofstep generation.",
          "quote": "For example, Gamepad (Huang et al., 2019) employs TreeLSTM (Tai et al., 2015) to encode the proof states and two distinct linear layers for tactic and argument prediction."
        },
        "aliases": [
          "TreeLSTM"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss TreeLSTM in the context of proofstep generation.",
          "quote": "For example, Gamepad (Huang et al., 2019) employs TreeLSTM (Tai et al., 2015) to encode the proof states and two distinct linear layers for tactic and argument prediction."
        },
        "referenced_paper_title": {
          "value": "Improved semantic representations from tree-structured long short-term memory networks",
          "justification": "The authors of this survey paper reference the paper \\\"Improved semantic representations from tree-structured long short-term memory networks\\\" in the context of proofstep generation.",
          "quote": "For example, Gamepad (Huang et al., 2019) employs TreeLSTM (Tai et al., 2015) to encode the proof states and two distinct linear layers for tactic and argument prediction."
        }
      },
      {
        "name": {
          "value": "GPT-f",
          "justification": "The authors of this survey paper discuss GPT-f in the context of proofstep generation.",
          "quote": "Specifically, GPT- f (Polu & Sutskever, 2020) first apply a conditional language modeling objective to train decoder-only Transformers to generate a proof step in the format of GOAL <GOAL> PROOFSTEP <PROOFSTEP><EOT>."
        },
        "aliases": [
          "GPT-f"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss GPT-f in the context of proofstep generation.",
          "quote": "Specifically, GPT- f (Polu & Sutskever, 2020) first apply a conditional language modeling objective to train decoder-only Transformers to generate a proof step in the format of GOAL <GOAL> PROOFSTEP <PROOFSTEP><EOT>."
        },
        "referenced_paper_title": {
          "value": "Generative language modeling for automated theorem proving",
          "justification": "The authors of this survey paper reference the paper \\\"Generative language modeling for automated theorem proving\\\" in the context of proofstep generation.",
          "quote": "Specifically, GPT- f (Polu & Sutskever, 2020) first apply a conditional language modeling objective to train decoder-only Transformers to generate a proof step in the format of GOAL <GOAL> PROOFSTEP <PROOFSTEP><EOT>."
        }
      },
      {
        "name": {
          "value": "GPT-3",
          "justification": "The authors of this survey paper discuss GPT-3 in the context of proofstep generation.",
          "quote": "NaturalProver (Welleck et al., 2022) trains GPT-3 (Brown et al., 2020) with constrained decoding to encourage using retrieved references in the proof steps."
        },
        "aliases": [
          "GPT-3"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss GPT-3 in the context of proofstep generation.",
          "quote": "NaturalProver (Welleck et al., 2022) trains GPT-3 (Brown et al., 2020) with constrained decoding to encourage using retrieved references in the proof steps."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The authors of this survey paper reference the paper \\\"Language models are few-shot learners\\\" in the context of proofstep generation.",
          "quote": "NaturalProver (Welleck et al., 2022) trains GPT-3 (Brown et al., 2020) with constrained decoding to encourage using retrieved references in the proof steps."
        }
      },
      {
        "name": {
          "value": "FMSCL",
          "justification": "The authors of this survey paper discuss FMSCL in the context of proof search.",
          "quote": "GPT- f (Polu & Sutskever, 2020) and FMSCL (Polu et al., 2023) train language models with outcome and proof size objectives as value functions to perform the best-first search."
        },
        "aliases": [
          "FMSCL"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This paper is a survey and does not contribute any new models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_executed": {
          "value": false,
          "justification": "This paper is a survey and does not execute any models.",
          "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving"
        },
        "is_compared": {
          "value": true,
          "justification": "The authors of this survey paper discuss FMSCL in the context of proof search.",
          "quote": "GPT- f (Polu & Sutskever, 2020) and FMSCL (Polu et al., 2023) train language models with outcome and proof size objectives as value functions to perform the best-first search."
        },
        "referenced_paper_title": {
          "value": "Formal mathematics statement curriculum learning",
          "justification": "The authors of this survey paper reference the paper \\\"Formal mathematics statement curriculum learning\\\" in the context of proof search.",
          "quote": "GPT- f (Polu & Sutskever, 2020) and FMSCL (Polu et al., 2023) train language models with outcome and proof size objectives as value functions to perform the best-first search."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 27071
  }
}