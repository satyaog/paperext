{
  "paper": "2309.03839.txt",
  "words": 10381,
  "extractions": {
    "title": {
      "value": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning",
      "justification": "The title is explicitly mentioned in the provided document.",
      "quote": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning"
    },
    "description": "This paper proposes the ORBIT (Offline Reinforcement Learning-Bootstrapped InTerface) method for training adaptive human-machine interfaces. It leverages a combination of offline reinforcement learning (RL) and online fine-tuning to map noisy, high-dimensional command signals from users to actions. The algorithm aims to optimize the interface for individual users using both offline pre-training and online human-in-the-loop RL. A user study showed that the ORBIT method improves user task performance over baseline directional interfaces for tasks like simulated navigation.",
    "type": {
      "value": "empirical",
      "justification": "The research includes user studies with participants performing tasks using the proposed ORBIT method and compares the performance with baseline methods.",
      "quote": "To evaluate ORBIT’s ability to learn an effective interface from real user data, we conduct a user study with 12 participants who perform a simulated navigation task by using their eye gaze to modulate a 128dimensional command signal from their webcam (illustrated in Fig. 3 and 5)."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The paper discusses the use of reinforcement learning algorithms and neural networks for improving human-machine interfaces, which are key areas in the field of deep learning.",
      "quote": "To our knowledge, ORBIT is the first human-in-the-loop RL algorithm that can learn an interface from both offline and online data."
    },
    "sub_research_fields": [
      {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on leveraging reinforcement learning techniques to optimize human-machine interfaces for better performance.",
        "quote": "We propose an offline RL algorithm for interface optimization that can learn from both an observational dataset of the user attempting to perform their desired tasks..."
      }
    ],
    "models": [
      {
        "name": {
          "value": "Policy πθ",
          "justification": "Mentioned as part of the proposed ORBIT method for training human-machine interfaces.",
          "quote": "In particular, we train eϕ end to end with a reward decoder dψ that predicts the reward rt′ given the pair (zt , st′ +1 ) for any t (including t ̸= t′ )."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning Model",
            "justification": "This policy is part of the reinforcement learning model designed to maximize the reward function.",
            "quote": "We then update the value functions Q and V by taking n gradient steps on the value losses from IQL (LQ and LV in [16]). Finally, we update the policy πθ and history encoder eη by taking m gradient steps on the loss in Eqn. 6."
          }
        ],
        "is_contributed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Expert User Data on Navigation Task",
          "justification": "Explicitly mentioned as the dataset collected from 12 participants for evaluating the ORBIT method.",
          "quote": "To evaluate ORBIT’s ability to learn an effective interface from real user data, we conduct a user study with 12 participants who perform a simulated navigation task by using their eye gaze to modulate a 128dimensional command signal from their webcam (illustrated in Fig. 3 and 5)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Simulated User Data on Navigation Task",
          "justification": "The paper mentions using simulated data to evaluate different components of the ORBIT method.",
          "quote": "We then update the value functions Q and V by taking n gradient steps on the value losses from IQL (LQ and LV in [16]). Finally, we update the policy πθ and history encoder eη by taking m gradient steps on the loss in Eqn. 6."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Lunar Lander Game Data",
          "justification": "Lunar Lander game with simulated user input is used to evaluate the ORBIT method.",
          "quote": "We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands, and find that our method improves over baseline interfaces in these domains as well."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper notes the usage of PyTorch, a common library for implementing deep learning models.",
          "quote": "To optimize all of our losses, we use the Adam optimizer [57] with an initial learning rate of 3 · 10−4. Following the IQL method, we use a constant learning rate for training the value functions, and cosine scheduling for training the encoders and policy."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Stable Baselines3",
          "justification": "The paper frequently mentions using the Stable Baselines3 library for implementing reinforcement learning algorithms like PPO.",
          "quote": "We train an agent to act as the simulated user using the proximal policy optimization algorithm (PPO) [58] and the default hyperparameter values from the Stable Baselines3 library [59]."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1022,
    "prompt_tokens": 18272,
    "total_tokens": 19294
  }
}