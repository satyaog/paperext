{
  "paper": "2308.12445.txt",
  "words": 10320,
  "extractions": {
    "title": {
      "value": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems",
      "justification": "Title is clearly mentioned at the top of the research paper.",
      "quote": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems"
    },
    "description": "This paper proposes Dr. DRL, a self-healing approach for deep reinforcement learning (DRL) systems that integrates intentional forgetting into continual learning (CL) to overcome issues like catastrophic forgetting, warm-starting failure, and slow convergence. The method prioritizes adapting key problem-solving skills while erasing minor behaviors, leading to reduced healing times and improved adaptability.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experiments and comparisons of the proposed Dr. DRL approach with vanilla CL on various drifted environments.",
      "quote": "To demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym environments with different drifting intensities."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The research focuses on improving self-healing mechanisms in deep reinforcement learning (DRL) systems, an area within deep learning.",
        "quote": "Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The paper specifically addresses issues related to deep reinforcement learning systems and proposes a novel solution to enhance their adaptability.",
          "quote": "Deep reinforcement learning (DRL), the blend of deep learning (DL) and reinforcement learning (RL), has shown promising achievements in recent years."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Deep Q-Learning",
          "justification": "The model is specifically mentioned as one of the well-established DRL algorithms used for evaluation.",
          "quote": "In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym environments... (ii) three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "aliases": [
          "DQN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Training"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Soft Actor-Critic",
          "justification": "The model is specifically mentioned as one of the well-established DRL algorithms used for evaluation.",
          "quote": "In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym environments... (ii) three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "aliases": [
          "SAC"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Training"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Proximal Policy Optimization",
          "justification": "The model is specifically mentioned as one of the well-established DRL algorithms used for evaluation.",
          "quote": "In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym environments... (ii) three well-established DRL algorithms, namely Deep Q-Learning (DQN), Soft Actor-Critic (SAC), and Proximal Policy Optimization (PPO)."
        },
        "aliases": [
          "PPO"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Training"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CartPole",
          "justification": "The dataset is specifically mentioned as one of the environments used for evaluating the proposed method.",
          "quote": "Dr. DRL performs an evaluation using purposefully drifted gym environments with varying levels of parameter shifts, including CartPole."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MountainCar",
          "justification": "The dataset is specifically mentioned as one of the environments used for evaluating the proposed method.",
          "quote": "Dr. DRL performs an evaluation using purposefully drifted gym environments with varying levels of parameter shifts, including MountainCar."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Acrobot",
          "justification": "The dataset is specifically mentioned as one of the environments used for evaluating the proposed method.",
          "quote": "Dr. DRL performs an evaluation using purposefully drifted gym environments with varying levels of parameter shifts, including Acrobot."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TensorFlow",
          "justification": "TensorFlow is mentioned as the deep learning framework used in the implementation of the proposed method, Dr. DRL.",
          "quote": "We implemented our approach as an open-source tool using Python 3.7 and it supports TensorFlow (version 2.4.4)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1243,
    "prompt_tokens": 16787,
    "total_tokens": 18030
  }
}