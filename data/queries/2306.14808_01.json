{
  "paper": "2306.14808.txt",
  "words": 13970,
  "extractions": {
    "title": {
      "value": "Maximum State Entropy Exploration using Predecessor and Successor Representations",
      "justification": "This is the exact title as presented in the document.",
      "quote": "Maximum State Entropy Exploration using Predecessor and Successor Representations"
    },
    "description": "The paper presents ηψ-Learning, an algorithm designed to improve exploration efficiency within reinforcement learning by combining predecessor and successor representations to maximize the state visitation distribution entropy of a single finite-length trajectory.",
    "type": {
      "value": "theoretical",
      "justification": "The paper focuses on the proposal and evaluation of a new algorithm (ηψ-Learning) for reinforcement learning, emphasizing theoretical contributions such as combining predecessor and successor representations.",
      "quote": "In this work, we present ηψ-Learning, an algorithm to learn an exploration policy that methodically searches through a task."
    },
    "primary_research_field": {
      "name": {
        "value": "deep learning",
        "justification": "The research is focused on advancing techniques within reinforcement learning, a subfield of deep learning, through the development of a novel exploration algorithm.",
        "quote": "This study of developing exploration behaviors in reinforcement learning is guided by a fundamental curiosity about the nature of autonomous learning."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "reinforcement learning",
          "justification": "The paper explicitly deals with solving problems in reinforcement learning by introducing ηψ-Learning.",
          "quote": "The domain of exploration in Reinforcement Learning (RL) focuses on discovering an agent’s environment via intrinsic motivation to accelerate learning optimal policies."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ηψ-Learning",
          "justification": "ηψ-Learning is the main model introduced and evaluated in this study to enhance exploration efficiency.",
          "quote": "In this work, we propose ηψ-Learning, an algorithm to compute an exploration strategy that methodically explores within a single finite-length trajectory"
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "RLHive",
          "justification": "RLHive was used to implement the proposed method ηψ-Learning and to conduct the experiments.",
          "quote": "The implementation of the proposed method was done using the RLHive (Patil et al., 2023) library."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 600,
    "prompt_tokens": 21997,
    "total_tokens": 22597
  }
}