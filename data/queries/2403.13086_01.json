{
  "paper": "2403.13086.txt",
  "words": 6513,
  "extractions": {
    "title": {
      "value": "Listenable Maps for Audio Classifiers",
      "justification": "The title is clearly mentioned at the top of the paper.",
      "quote": "Listenable Maps for Audio Classifiers"
    },
    "description": "This paper introduces a novel posthoc interpretation method called Listenable Maps for Audio Classifiers (L-MAC), designed to generate listenable interpretations for pretrained audio classifiers. L-MAC uses a decoder to produce binary masks, highlighting relevant portions of the input audio. These masks are then used to create listenable audio outputs through a special loss function that maximizes the classifier decision's confidence on the masked-in audio while minimizing it on the masked-out portion. Experimental evaluations show that L-MAC outperforms other gradient and masking-based interpretation methods both quantitatively and qualitatively, with users preferring its interpretations on average.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental evaluations and user studies to demonstrate the effectiveness of the proposed method, indicating that it is an empirical study.",
      "quote": "Quantitative evaluations on both in-domain and out-of-domain data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and masking-based methodologies. Furthermore, a user study confirms that, on average, users prefer the interpretations generated by the proposed technique."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The research involves the use of deep learning models for audio classification and the development of interpretation methods for these models.",
      "quote": "State-of-the-art models for speech and audio processing typically operate on less interpretable inputs, such as mel-spectrograms, as compared to standard images."
    },
    "sub_research_fields": [
      {
        "value": "Interpretability (Explainable AI)",
        "justification": "The primary focus of the paper is on generating interpretations for pre-trained machine learning models in the audio domain.",
        "quote": "Many existing posthoc interpretability methods are primarily designed for computer vision, where the task often involves classifying objects against a clean background."
      }
    ],
    "models": [
      {
        "name": {
          "value": "L-MAC",
          "justification": "L-MAC is the main model proposed and developed in this paper.",
          "quote": "This paper contributes to this emerging field by introducing a novel method called Listenable Maps for Audio Classifiers (L-MAC)."
        },
        "caracteristics": [
          {
            "value": "Interpretation Model",
            "justification": "L-MAC is designed to provide interpretations for audio classifiers by generating binary masks and listenable audio outputs.",
            "quote": "L-MAC outputs listenable explanations for pretrained audio classifiers that utilize mel-spectrograms or any other feature as input."
          }
        ],
        "is_contributed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CNN14",
          "justification": "CNN14 is one of the pretrained classifiers used throughout the experiments in the paper.",
          "quote": "In these experiments, we first train a CNN14 classifier (Kong et al., 2020) on the ESC-50 dataset (Piczak) augmented with WHAM! noise, to simulate real-world mixtures."
        },
        "caracteristics": [
          {
            "value": "Audio Classification Model",
            "justification": "CNN14 is used for classifying audio data in the experiments.",
            "quote": "In these experiments, we first train a CNN14 classifier (Kong et al., 2020) on the ESC-50 dataset (Piczak) augmented with WHAM! noise, to simulate real-world mixtures."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ESC-50",
          "justification": "ESC-50 is mentioned as the dataset used for training and evaluation throughout the paper.",
          "quote": "We utilized the ESC50 dataset (Piczak) which contains 50 environmental sound classes for both setups."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "VGG-Sound",
          "justification": "VGG-Sound is mentioned as the dataset used for pretraining the classifier CNN14.",
          "quote": "The CNN14 classifier we employed has 12 2D convolutional layers and is pre-trained on the VGG-sound dataset (Chen et al., 2020a) using SimCLR (Chen et al., 2020b)."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is a commonly used deep learning framework and is likely used here for implementing the models.",
          "quote": "We have used the Captum implementations (Kokhlikyan et al., 2020) for the gradient-based methods and SHAP and adapted the SpeechBrain (Ravanelli et al., 2021) implementation for L2I."
        },
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Captum",
          "justification": "Captum is mentioned for implementing the gradient-based interpretation methods used for comparison.",
          "quote": "We have used the Captum implementations (Kokhlikyan et al., 2020) for the gradient-based methods and SHAP and adapted the SpeechBrain (Ravanelli et al., 2021) implementation for L2I."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SHAP",
          "justification": "SHAP is mentioned as a gradient-based interpretation method used for comparison.",
          "quote": "We have used the Captum implementations (Kokhlikyan et al., 2020) for the gradient-based methods and SHAP and adapted the SpeechBrain (Ravanelli et al., 2021) implementation for L2I."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1071,
    "prompt_tokens": 12314,
    "total_tokens": 13385
  }
}