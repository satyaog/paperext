{
  "paper": "1910.02344.txt",
  "words": 9506,
  "extractions": {
    "description": "This paper introduces the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes from multiple sensory modalities. It proposes the Amortized Product-of-Experts (APoE) to improve computational efficiency and robustness to unseen modality combinations. The paper demonstrates the effectiveness of GMN in inferring robust 3D scene representations and performing accurate cross-modal generation using a newly developed environment, the Multisensory Embodied 3D-Scene Environment (MESE).",
    "title": {
      "value": "Neural Multisensory Scene Inference",
      "justification": "The title is clearly indicated at the top of the provided research paper.",
      "quote": "Neural Multisensory Scene Inference"
    },
    "type": {
      "value": "Empirical",
      "justification": "The paper focuses on introducing a new model (GMN), a novel method (APoE), and demonstrating their effectiveness through experimental results.",
      "quote": "Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations from arbitrary combinations of modalities and perform accurate cross-modal generation."
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The research involves developing a deep learning model to learn representations of 3D scenes from multisensory data.",
      "quote": "Motivated by the above desiderata, we propose the Generative Multisensory Network (GMN) for neural multisensory scene inference and rendering."
    },
    "sub_research_field": {
      "value": "Multimodal Learning",
      "justification": "The paper specifically focuses on learning representations from multiple sensory modalities.",
      "quote": "Motivated by human multisensory processing, we propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities."
    },
    "models": [
      {
        "name": {
          "value": "Generative Multisensory Network (GMN)",
          "justification": "The main model introduced and developed in the paper is the GMN.",
          "quote": "We propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities."
        },
        "role": "contributed",
        "type": {
          "value": "Generative Model",
          "justification": "The GMN is a type of generative model as it is designed to generate modality-invariant representations of 3D scenes.",
          "quote": "We introduce the Generative Multisensory Network (GMN) for neural multisensory scene inference and rendering."
        },
        "mode": "trained"
      },
      {
        "name": {
          "value": "Amortized Product-of-Experts (APoE)",
          "justification": "The APoE method is introduced as a novel component within the GMN to enhance its efficiency and robustness.",
          "quote": "We also introduce the Amortized Product-of-Experts network that allows for generalized cross-modal generation while resolving the problems in the Generative Query Network (GQN) and traditional Product-of-Experts."
        },
        "role": "contributed",
        "type": {
          "value": "Approximation Method",
          "justification": "The APoE is used to approximate the posterior distribution in the GMN model, enhancing computational efficiency.",
          "quote": "We introduce the Amortized Product-of-Experts (APoE) to improve the computational efficiency and robustness to unseen combinations of modalities at test time."
        },
        "mode": "trained"
      },
      {
        "name": {
          "value": "Consistent Generative Query Network (C-GQN)",
          "justification": "The paper references C-GQN as a base model which GMN builds upon.",
          "quote": "We adopt C-GQN network architecture from Kumar et al. (2018) for the proposed model, as well as the baseline."
        },
        "role": "referenced",
        "type": {
          "value": "Generative Model",
          "justification": "C-GQN is noted as a generative model specifically referenced for comparison and improvement.",
          "quote": "We use a C-GQN variant (Kumar et al., 2018). As a baseline model, we use a Generative Query Network (GQN)."
        },
        "mode": "referenced"
      },
      {
        "name": {
          "value": "Product-of-Experts (PoE)",
          "justification": "The PoE model is referenced in comparison to and as a precursor for the APoE introduced in the paper.",
          "quote": "As a result, the Amortized Product-of-Experts (APoE) allows the model to learn from a large number of modalities without tight coupling among the modalities."
        },
        "role": "referenced",
        "type": {
          "value": "Approximation Method",
          "justification": "PoE is noted as a technique for combining multiple experts to model complex distributions, influencing the development of APoE.",
          "quote": "We can model the conditional prior as a Product-of-Experts (PoE) network."
        },
        "mode": "referenced"
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MultiSensory Embodied 3D-Scene Environment (MESE)",
          "justification": "The MESE is the environment specifically developed for the experiments conducted in this paper.",
          "quote": "We also develop the Multisensory Embodied 3D-Scene Environment (MESE) used to develop and test the model."
        },
        "role": "contributed"
      },
      {
        "name": {
          "value": "Shepard-Metzler Dataset",
          "justification": "The Shepard-Metzler Dataset is used as a basis for generating the 3D scenes utilized in the experiments.",
          "quote": "The main task is similar to the Shepard-Metzler object experiments used in Eslami et al. (2018) but extends it with the MPL hand."
        },
        "role": "used"
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions using the PyTorch library for implementing the models.",
          "quote": "PyTorch (Paszke et al., 2017), CUDA-9.0 (Nickolls et al., 2008), and cuDNN7 (Chetlur et al., 2014) are used for the implementations."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "CUDA",
          "justification": "The CUDA library is mentioned as a part of the implementation framework.",
          "quote": "PyTorch (Paszke et al., 2017), CUDA-9.0 (Nickolls et al., 2008), and cuDNN7 (Chetlur et al., 2014) are used for the implementations."
        },
        "role": "used"
      },
      {
        "name": {
          "value": "cuDNN",
          "justification": "The cuDNN library is also mentioned as part of the implementation framework.",
          "quote": "PyTorch (Paszke et al., 2017), CUDA-9.0 (Nickolls et al., 2008), and cuDNN7 (Chetlur et al., 2014) are used for the implementations."
        },
        "role": "used"
      }
    ]
  },
  "usage": {
    "completion_tokens": 1657,
    "prompt_tokens": 15514,
    "total_tokens": 17171
  }
}