{
  "paper": "2312.14279.txt",
  "words": 16269,
  "extractions": {
    "title": {
      "value": "Characterizing and Classifying Developer Forum Posts with their Intentions",
      "justification": "This title accurately reflects the core subject and purpose of the paper, which is to classify developer forum posts according to their intentions.",
      "quote": "Characterizing and Classifying Developer Forum Posts with their Intentions"
    },
    "description": "This paper proposes an intention detection framework to classify developer forum posts by their intentions (such as asking for help, sharing information, etc.) using a transformer-based pre-trained model. The proposed model significantly outperforms existing state-of-the-art methods in classifying post intentions.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes an empirical study involving the collection and manual analysis of a dataset from developer forums, as well as the development and evaluation of a transformer-based model for intention classification.",
      "quote": "...we manually annotate the intentions of posts following a rigorous process according to the resulting taxonomy of technical forum post intentions. ... we propose an intention prediction framework for technical online posts."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on classifying posts from online forums using transformer-based language models, a prominent method in NLP.",
        "quote": "In the framework, we employ transformer-based pre-trained language models to generate embeddings for both title and description of posts."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text Classification",
          "justification": "The core task in the paper is to classify developer forum posts according to their intentions, which falls under the category of text classification.",
          "quote": "Our work is performed on a dataset of forum posts provided by our industrial partner that covers multiple developer communities (e.g., Stack Overflow, Discourse forums, etc.). Furthermore, we manually annotate the intentions of posts following a rigorous process according to the resulting taxonomy of technical forum post intentions. Based on the findings and insights from the qualitative study, we propose an intention prediction framework for technical online posts."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is one of the pre-trained language models used in the proposed framework.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs... We use the pooler output of the PTMs, which corresponds to the representation of the first token... As by fine-tuning this layer with our task, the quality of embedding may be improved for our downstream task."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa, another pre-trained language model, is evaluated in the proposed framework.",
          "quote": "RoBERTa (Liu et al., 2019) modified some hyper-parameters and training tasks while maintaining the original BERT architecture."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ALBERT",
          "justification": "ALBERT is another variant of BERT utilized in the study.",
          "quote": "ALBERT (Lan et al., 2019) further improve the original BERT by adopting parameter reduction techniques."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "DistilBERT is used as a more lightweight version of BERT in the framework.",
          "quote": "DistilBERT (Sanh et al., 2019) is a distilled version, which has 40% fewer parameters while maintaining over 95% of the BERT model."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BERTOverflow",
          "justification": "BERTOverflow is used for its domain-specific training on Stack Overflow data.",
          "quote": "BERTOverflow (Tabassum et al., 2020) is proposed with a named entity recognition technique. It is trained with sentences from Stack Overflow and can achieve better performance on domain-specific tasks."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CodeBERT",
          "justification": "CodeBERT is employed for its adaptation to both programming languages and natural languages.",
          "quote": "Pre-trained with both natural language corpus and programming language data, CodeBERT (Feng et al., 2020) is able to generate embeddings for both forms of input data."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Stack Overflow",
          "justification": "The dataset includes posts from Stack Overflow, a major source of technical discussions among developers.",
          "quote": "In our study, our primary goal is to narrow the gap by integrating industry insights into the construction of an intention detection approach for technical forum posts. ... Our work is performed on a dataset of forum posts provided by our industrial partner that covers multiple developer communities (e.g., Stack Overflow, Discourse forums, etc.)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Discourse forums",
          "justification": "The dataset also includes posts from Discourse forums, contributing to the diversity of the technical communities covered.",
          "quote": "Our work is performed on a dataset of forum posts provided by our industrial partner that covers multiple developer communities (e.g., Stack Overflow, Discourse forums, etc.). Furthermore, we manually annotate the intentions of posts following a rigorous process according to the resulting taxonomy of technical forum post intentions."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Lithium forums",
          "justification": "Lithium forums are another source of developer posts included in the dataset.",
          "quote": "The dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities), mainly from three different platforms: Stack Exchange3 , Lithium4 forums and Discourse5 forums."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face",
          "justification": "The Hugging Face library provided the transformer-based pre-trained models used in the study.",
          "quote": "We compare the performances of six variants of our framework with the PTMs mentioned above. We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1673,
    "prompt_tokens": 23862,
    "total_tokens": 25535
  }
}