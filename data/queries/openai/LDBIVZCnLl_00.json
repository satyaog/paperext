{
  "paper": "LDBIVZCnLl.txt",
  "words": 13876,
  "extractions": {
    "title": {
      "value": "Discrete Key-Value Bottleneck",
      "justification": "This is the title of the paper",
      "quote": "Discrete Key-Value Bottleneck"
    },
    "description": "The paper proposes a new model architecture built upon a discrete bottleneck that contains pairs of separate and learnable key-value codes. The model is designed to reduce catastrophic forgetting and improve generalization under input distribution changes, particularly in continual learning scenarios.",
    "type": {
      "value": "Theoretical Study",
      "justification": "The paper focuses on proposing a new model architecture and supporting its effectiveness through theoretical analysis and empirical validation.",
      "quote": "We theoretically investigate the ability of the discrete key-value bottleneck to minimize the effect of learning under distribution shifts and show that it reduces the complexity of the hypothesis class."
    },
    "primary_research_field": {
      "name": {
        "value": "Continual Learning",
        "justification": "The primary focus of the paper is on addressing challenges in learning from non-stationary data streams, which is a core issue in continual learning.",
        "quote": "Challenges emerge with non-stationary training data streams such as continual learning."
      },
      "aliases": [
        "CL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Catastrophic Forgetting",
          "justification": "The paper discusses mechanisms to mitigate catastrophic forgetting, which is crucial in the context of continual learning.",
          "quote": "In this way, we can integrate new data and thus gradually improve the models’ performance. Moreover, as we will show theoretically, the proposed architecture benefits from the fact that the decoder works with a discrete set of value codes, as opposed to directly predicting from the encoder representation."
        },
        "aliases": [
          "CF"
        ]
      },
      {
        "name": {
          "value": "Class-Incremental Learning",
          "justification": "The paper empirically evaluates the proposed model in class-incremental learning scenarios.",
          "quote": "We empirically verified the method under a challenging classincremental learning scenario on real-world data and show that the proposed model — without any task boundaries \n— reduces the common vulnerability to catastrophic forgetting."
        },
        "aliases": [
          "Class-IL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Discrete Key-Value Bottleneck",
          "justification": "This is the primary model introduced and discussed throughout the paper.",
          "quote": "In the present work, we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes."
        },
        "aliases": [
          "DKVB",
          "Key-Value Bottleneck"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The proposed model is a novel contribution and the focus of the research.",
          "quote": "we propose a model architecture to address this issue, building upon a discrete bottleneck containing pairs of separate and learnable key-value codes."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is empirically validated using various experiments.",
          "quote": "We empirically verify the proposed method under challenging class-incremental learning scenarios."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model is compared with other baseline models and methods in the experiments.",
          "quote": "we compare against the reported SDMLP performance on the ConvMixer backbone, as well as a 1-layer MLP with 128 hidden dimensions and a linear probe with and without bias term for all backbones."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "As the model is introduced in this paper, there is no need for a referenced paper title.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used as one of the main datasets for evaluating the proposed model.",
          "quote": "We present each set for 2000 epochs, which in an ordinary architecture would cause catastrophic forgetting of the previous sets. We perform five replications of each model with different random seeds, with the seed also re-sampling the class splits to avoid selecting any favourable class split. Importantly, we study one of the hardest yet most realistic settings in continual learning by not allowing any memory replay or provision of task boundaries which is required by the vast majority of existing continual learning methods. To the best of our knowledge, the only method that can deal with this learning scenario is (Bricken et al., 2023). We report experiments on five publicly available backbones including:."
        },
        "aliases": [
          "CIFAR10"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the foundational paper for the CIFAR-10 dataset.",
          "quote": "Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is used for initializing the keys in the experiments.",
          "quote": "We initialize keys on the unlabelled non-overlapping CIFAR100 dataset except for the ConvMixer where we used the embeddings from the downsampled Imagenet dataset for reasons of comparison."
        },
        "aliases": [
          "CIFAR100"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the foundational paper for the CIFAR-100 dataset.",
          "quote": "Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "The downsampled ImageNet dataset is used for experiments with the ConvMixer backbone.",
          "quote": "We initialize keys on the unlabelled non-overlapping CIFAR100 dataset except for the ConvMixer where we used the embeddings from the downsampled Imagenet dataset for reasons of comparison."
        },
        "aliases": [
          "ImageNet-1K",
          "ILSVRC"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet large scale visual recognition challenge",
          "justification": "This is the foundational paper for the ImageNet dataset.",
          "quote": "Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C. and Fei-Fei, L., 2015. ImageNet large scale visual recognition challenge. International journal of computer vision, 115(3), pp.211-252."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1562,
    "prompt_tokens": 23408,
    "total_tokens": 24970
  }
}