{
  "paper": "950758d68fd97078e161771685903c1b.txt",
  "words": 12759,
  "extractions": {
    "title": {
      "value": "Temporal encoding in deep reinforcement learning agents",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Temporal encoding in deep reinforcement learning agents"
    },
    "description": "This paper investigates the emergence and role of time cells and ramping cells in recurrent neural networks of deep reinforcement learning models performing interval timing and working memory tasks. Using in-silico models, it demonstrates that these types of cells naturally arise and contribute to the network's performance primarily through their effect on recurrent dynamics rather than direct temporal information encoding.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments using deep reinforcement learning models to understand the behavior of time and ramping cells, making it an empirical study.",
      "quote": "In the present study, we use in-silico models, namely deep reinforcement learning (DRL) agents, trained on simulated timing and working memory tasks to investigate the question of how time and ramping cells may contribute to behavior."
    },
    "primary_research_field": {
      "name": {
        "value": "Neural Networks",
        "justification": "The primary focus of the research is on the behavior of neural networks (specifically recurrent neural networks) within deep reinforcement learning agents.",
        "quote": "Here, we show that time cells and ramping cells naturally emerge in the recurrent neural networks of deep reinforcement learning models performing simulated interval timing and working memory tasks."
      },
      "aliases": [
        "Neural Nets"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The study emphasizes deep reinforcement learning as the framework within which the neural networks are trained.",
          "quote": "In the present study, we use in-silico models, namely deep reinforcement learning (DRL) agents, trained on simulated timing and working memory tasks to investigate the question of how time and ramping cells may contribute to behavior."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Actor-Critic",
          "justification": "The paper mentions the actor-critic architecture as part of the deep reinforcement learning agents used in the experiments.",
          "quote": "Here, we trained DRL agents that had an actor-critic architecture (Fig. 1b)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The actor-critic model is not a contribution of this paper; it is a standard architecture in reinforcement learning.",
          "quote": "The agents trained on the non-mnemonic DNMS task had the same architectures as the single pathway models trained on the normal DNMS task (Fig. 1b)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The actor-critic model was executed in the tasks described in the paper.",
          "quote": "Over episodes the agents’ performance improved, reaching almost 100% correct eventually (Fig. 1c, top)."
        },
        "is_compared": {
          "value": 0,
          "justification": "The focus of the paper is not on comparing the actor-critic model with other models but on the emergence of time and ramping cells.",
          "quote": "Leveraging the manipulability of artificial neural networks, we designed two types of virtual “knock-out \" experiments that took advantage of our ability to selectively manipulate activity in specific units and specific times of the RNNs."
        },
        "referenced_paper_title": {
          "value": "Asynchronous Methods for Deep Reinforcement Learning",
          "justification": "This is the reference paper for the actor-critic architecture used in this study.",
          "quote": "The models were trained with the Asynchronous Advantage Actor-Critic (A3C) algorithm."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "OpenAI Gym",
          "justification": "The paper mentions that the simulated environments for the tasks were designed to be compatible with the OpenAI Gym framework.",
          "quote": "The simulated environments for the DDC and DNMS tasks were designed to be compatible with the OpenAI gym framework."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "OpenAI Gym",
          "justification": "The library is referenced and used for building the task environments.",
          "quote": "The simulated environments for the DDC and DNMS tasks were designed to be compatible with the OpenAI gym framework."
        }
      },
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper states that all experiments were performed using PyTorch.",
          "quote": "All experiments were performed using PyTorch."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The library is referenced as it was used to implement the experiments.",
          "quote": "All experiments were performed using PyTorch."
        }
      },
      {
        "name": {
          "value": "scikit-learn",
          "justification": "The paper mentions that decoding analyses were performed using the scikit-learn packages.",
          "quote": "Decoding analyses were performed using scikit-learn packages."
        },
        "aliases": [
          "sklearn"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Scikit-learn: Machine Learning in Python",
          "justification": "The library is referenced for its use in decoding analyses.",
          "quote": "Decoding analyses were performed using scikit-learn packages."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1022,
    "prompt_tokens": 20562,
    "total_tokens": 21584
  }
}