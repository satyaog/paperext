{
  "paper": "2308.08736.txt",
  "words": 18892,
  "extractions": {
    "title": {
      "value": "On the Effectiveness of Log Representation for Log-based Anomaly Detection",
      "justification": "The title of the paper accurately reflects its focus and scope on evaluating log representations for anomaly detection.",
      "quote": "On the Effectiveness of Log Representation for Log-based Anomaly Detection"
    },
    "description": "This paper investigates and compares six commonly used log representation techniques in the context of log-based anomaly detection. Seven machine learning models and four public log datasets are used to evaluate these techniques. The study also examines the impact of log parsing and feature aggregation on the performance of the downstream models.",
    "type": {
      "value": "Empirical Study",
      "justification": "The study conducts experiments and evaluations to compare different log representation techniques empirically.",
      "quote": "Therefore, this work investigates and compares the commonly adopted log representation techniques from previous log analysis research."
    },
    "primary_research_field": {
      "name": {
        "value": "Log-based Anomaly Detection",
        "justification": "The primary focus of the research is on detecting anomalies in logs using different representation techniques.",
        "quote": "We select six log representation techniques and evaluate them with seven ML models and four public log datasets (i.e., HDFS, BGL, Spirit and Thunderbird) in the context of log-based anomaly detection."
      },
      "aliases": [
        "log anomaly detection"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "Several log representation techniques such as Word2Vec and BERT, originating from NLP, are evaluated in the study.",
          "quote": "For example, He et al. (2016b) match Message Count Vector representation with a logistic regression model to detect anomalies... Zhang et al. (2019) leverages pre-trained FastText model to generate log template embeddings to construct their anomaly detection workflow."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The study evaluates the performance of various machine learning models in the context of log anomaly detection.",
          "quote": "We select six log representation techniques and evaluate them with seven ML models..."
        },
        "aliases": [
          "ML"
        ]
      },
      {
        "name": {
          "value": "Deep Learning",
          "justification": "Deep learning-based techniques like BERT and FastText are evaluated for their effectiveness in log representation for anomaly detection.",
          "quote": "Instead of directly feeding the token-level representation to the follow-up models to fulfill downstream tasks...These sequential models leverage the contextual information presented by logs"
        },
        "aliases": [
          "DL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Logsy",
          "justification": "Logsy is a transformer-based model for log anomaly detection mentioned in the paper.",
          "quote": "Logsy (Nedelkoski et al., 2020) tokenizes the preprocessedlog messages and generates embeddings for tokens in log templates."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Logsy is cited from previous research and not contributed by this paper.",
          "quote": "Logsy (Nedelkoski et al., 2020)"
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that this model was executed as part of the experiments in the paper.",
          "quote": ""
        },
        "is_compared": {
          "value": 0,
          "justification": "There is no indication that this model was numerically compared to others in the paper.",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "Self-Attentive Classification-Based Anomaly Detection in Unstructured Logs",
          "justification": "This is the referenced paper for Logsy in the context of anomaly detection using transformer-based methods.",
          "quote": "Logsy (Nedelkoski et al., 2020) tokenizes the preprocessed log messages."
        }
      },
      {
        "name": {
          "value": "NeuralLog",
          "justification": "NeuralLog is a log-based anomaly detection model that uses BERT for generating representations, mentioned in the paper.",
          "quote": "Le and Zhang (2021) proposed NeuralLog, which does not rely on any log parsing."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "NeuralLog is cited from previous research and not contributed by this paper.",
          "quote": "NeuralLog (Le and Zhang, 2021)"
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that this model was executed as part of the experiments in the paper.",
          "quote": ""
        },
        "is_compared": {
          "value": 0,
          "justification": "There is no indication that this model was numerically compared to others in the paper.",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "Log-based Anomaly Detection without Log Parsing",
          "justification": "This is the referenced paper for NeuralLog in the context of anomaly detection using BERT embeddings.",
          "quote": "NeuralLog (Le and Zhang, 2021)"
        }
      },
      {
        "name": {
          "value": "DeepLog",
          "justification": "DeepLog is a sequential anomaly detection model that uses log keys to learn patterns, mentioned in the paper.",
          "quote": "DeepLog (Du et al., 2017) proposed the DeepLog anomaly detection framework...with log keys."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "DeepLog is cited from previous research: it is not contributed by this paper.",
          "quote": "DeepLog (Du et al., 2017)"
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that this model was executed as part of the experiments in the paper.",
          "quote": ""
        },
        "is_compared": {
          "value": 0,
          "justification": "There is no indication that this model was numerically compared to others in the paper.",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
          "justification": "This is the referenced paper for DeepLog in the context of detecting and diagnosing anomalies.",
          "quote": "DeepLog (Du et al., 2017) proposed the DeepLog anomaly detection framework"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "HDFS",
          "justification": "The HDFS dataset is used in the experiments to evaluate log representation techniques.",
          "quote": "The HDFS dataset (Xu et al., 2009) is collected from the Amazon EC2 platform."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Detecting Large-Scale System Problems by Mining Console Logs",
          "justification": "This is the referenced paper for the HDFS dataset, providing context for its origin and use.",
          "quote": "The HDFS dataset (Xu et al., 2009)"
        }
      },
      {
        "name": {
          "value": "BGL",
          "justification": "The BGL dataset is used in the experiments to evaluate log representation techniques.",
          "quote": "The BGL dataset (Oliner and Stearley, 2007) is recorded from the BlueGene/L (BG/L) supercomputer system."
        },
        "aliases": [
          "Blue Gene/L"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "What supercomputers say: A study of five system logs",
          "justification": "This is the referenced paper for the BGL dataset, providing context for its origin and use.",
          "quote": "The BGL dataset (Oliner and Stearley, 2007)"
        }
      },
      {
        "name": {
          "value": "Spirit",
          "justification": "The Spirit dataset is used in the experiments to evaluate log representation techniques.",
          "quote": "The Spirit dataset is also a well-used public log dataset (Oliner and Stearley, 2007)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "What supercomputers say: A study of five system logs",
          "justification": "This is the referenced paper for the Spirit dataset, providing context for its origin and use.",
          "quote": "The Spirit dataset is also a well-used public log dataset (Oliner and Stearley, 2007)"
        }
      },
      {
        "name": {
          "value": "Thunderbird",
          "justification": "The Thunderbird dataset is used in the experiments to evaluate log representation techniques.",
          "quote": "The Thunderbird dataset (Oliner and Stearley, 2007) is also a public log dataset from Sandia National Labs."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "What supercomputers say: A study of five system logs",
          "justification": "This is the referenced paper for the Thunderbird dataset, providing context for its origin and use.",
          "quote": "The Thunderbird dataset (Oliner and Stearley, 2007) is also a public log dataset from Sandia National Labs."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Word2Vec",
          "justification": "Word2Vec is used as a representation technique for log-based anomaly detection in the experiments.",
          "quote": "For Word2Vec, we use the word vectors generated by the model pre-trained with Google News dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning word vectors for 157 languages",
          "justification": "This is the referenced paper related to the implementation and usage of Word2Vec.",
          "quote": "For Word2Vec, we use the word vectors generated by the model pre-trained with Google News dataset."
        }
      },
      {
        "name": {
          "value": "FastText",
          "justification": "FastText is used as a representation technique for log-based anomaly detection in the experiments.",
          "quote": "For FastText, we leverage the off-the-shelf word vectors."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning word vectors for 157 languages",
          "justification": "This is the referenced paper related to the implementation and usage of FastText.",
          "quote": "For FastText, we leverage the off-the-shelf word vectors."
        }
      },
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is used as a representation technique for log-based anomaly detection in the experiments.",
          "quote": "For BERT (Devlin et al., 2018), we utilize the pre-trained base model."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "This is the referenced paper related to the implementation and usage of BERT.",
          "quote": "For BERT (Devlin et al., 2018), we utilize the pre-trained base model."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2038,
    "prompt_tokens": 30528,
    "total_tokens": 32566
  }
}