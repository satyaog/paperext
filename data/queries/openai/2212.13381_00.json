{
  "paper": "2212.13381.txt",
  "words": 9545,
  "extractions": {
    "title": {
      "value": "MixupE: Understanding and Improving Mixup from Directional Derivative Perspective",
      "justification": "This is the exact title of the paper provided in the information.",
      "quote": "MixupE: Understanding and Improving Mixup from Directional Derivative Perspective"
    },
    "description": "This paper introduces MixupE, a method that enhances the Mixup data augmentation technique by theoretically justifying the implicit regularization on directional derivatives. MixupE aims to improve generalization performance across various domains such as images, tabular data, speech, and graphs by incorporating a computationally efficient approximation of higher-order regularization terms.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper demonstrates the effectiveness of MixupE through various experiments across different datasets and architectures, which is characteristic of empirical research.",
      "quote": "To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs."
    },
    "primary_research_field": {
      "name": {
        "value": "Data Augmentation",
        "justification": "The paper focuses on improving the Mixup data augmentation technique, making Data Augmentation the primary research field.",
        "quote": "Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels."
      },
      "aliases": [
        "Data Augmentation Techniques"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generalization",
          "justification": "The paper aims to improve generalization performance using MixupE.",
          "quote": "Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup."
        },
        "aliases": [
          "Generalization in Deep Learning"
        ]
      },
      {
        "name": {
          "value": "Regularization Techniques",
          "justification": "The paper discusses the implicit and explicit regularization effects of Mixup and proposes MixupE for enhanced regularization.",
          "quote": "Our analysis provides a feasible insight to design the regularization in practice."
        },
        "aliases": [
          "Regularization Methods"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Mixup",
          "justification": "Mixup is a central model discussed and improved upon in the paper.",
          "quote": "Mixup is a popular data augmentation technique for training deep neural networks where additional samples are generated by linearly interpolating pairs of inputs and their labels."
        },
        "aliases": [
          "Mixup Data Augmentation"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Mixup was introduced in prior research and is not a contribution of this paper.",
          "quote": "despite its simplicity and minimal computation overhead, Mixup and its variants have been shown to achieve state-of-the-art in many tasks such as but not limited to, image classification."
        },
        "is_executed": {
          "value": 1,
          "justification": "The performance of Mixup is empirically tested in the paper.",
          "quote": "Despite its simplicity and minimal computation overhead, Mixup and its variants have been shown to achieve state-of-the-art in many tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "Mixup is compared to MixupE and other baseline models in the experiments.",
          "quote": "To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs."
        },
        "referenced_paper_title": {
          "value": "mixup: Beyond empirical risk minimization",
          "justification": "This is the reference paper where Mixup was first introduced.",
          "quote": "Mixup [Zhang et al., 2018] constructs a training sample as xi + (1 − λ)xj and ỹ = λyi + (1 − λ)yj with λ ∈ [0, 1]."
        }
      },
      {
        "name": {
          "value": "MixupE",
          "justification": "MixupE is the main contribution of the paper, proposed to improve the original Mixup technique.",
          "quote": "MixupE: Understanding and Improving Mixup from Directional Derivative Perspective"
        },
        "aliases": [
          "Mixup Enhanced"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "MixupE is introduced as an improved version of Mixup in this paper.",
          "quote": "Based on this new insight, we propose an improved version of Mixup, theoretically justified to deliver better generalization performance than the vanilla Mixup."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper provides empirical results for MixupE across various domains and architectures.",
          "quote": "To demonstrate the effectiveness of the proposed method, we conduct experiments across various domains such as images, tabular data, speech, and graphs."
        },
        "is_compared": {
          "value": 1,
          "justification": "MixupE is compared with the original Mixup and other baseline models in the experiments.",
          "quote": "This suggests that the proposed method MixupE is not overly sensitive to the hyperparameter η and works better than Mixup for a large range of η values."
        },
        "referenced_paper_title": {
          "value": "MixupE: Understanding and Improving Mixup from Directional Derivative Perspective",
          "justification": "MixupE is the main focus and contribution of this paper.",
          "quote": "We name this method as MixupE (Mixup Enhanced)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The paper uses the CIFAR-10 dataset for empirical evaluation of MixupE.",
          "quote": "Results: We show results for the CIFAR-10 (Table 1a), CIFAR-100 (Table 1b), SVHN (Table 2a), and Tiny-ImageNet (Table 2b) datasets."
        },
        "aliases": [
          "CIFAR10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-10 is a well-known dataset introduced by Alex Krizhevsky and Geoffrey Hinton.",
          "quote": "CIFAR-10 is a dataset commonly used for benchmarking image classification models."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "The paper uses the CIFAR-100 dataset for empirical evaluation of MixupE.",
          "quote": "Results: We show results for the CIFAR-10 (Table 1a), CIFAR-100 (Table 1b), SVHN (Table 2a), and Tiny-ImageNet (Table 2b) datasets."
        },
        "aliases": [
          "CIFAR100"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-100 is a well-known dataset introduced by Alex Krizhevsky and Geoffrey Hinton.",
          "quote": "CIFAR-100 is a dataset commonly used for benchmarking image classification models."
        }
      },
      {
        "name": {
          "value": "SVHN",
          "justification": "The paper uses the SVHN dataset for empirical evaluation of MixupE.",
          "quote": "Results: We show results for the CIFAR-10 (Table 1a), CIFAR-100 (Table 1b), SVHN (Table 2a), and Tiny-ImageNet (Table 2b) datasets."
        },
        "aliases": [
          "Street View House Numbers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Reading Digits in Natural Images with Unsupervised Feature Learning",
          "justification": "SVHN is a known dataset for digit classification tasks introduced by Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, Andrew Y. Ng.",
          "quote": "SVHN is commonly used for benchmarking digit classification models."
        }
      },
      {
        "name": {
          "value": "Tiny-ImageNet",
          "justification": "The paper uses the Tiny-ImageNet dataset for empirical evaluation of MixupE.",
          "quote": "Results: We show results for the CIFAR-10 (Table 1a), CIFAR-100 (Table 1b), SVHN (Table 2a), and Tiny-ImageNet (Table 2b) datasets."
        },
        "aliases": [
          "TinyImageNet"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Tiny ImageNet: A Student’s Guide",
          "justification": "Tiny-ImageNet is a subset of the ImageNet dataset often used for educational purposes and benchmarking image recognition models.",
          "quote": "Tiny-ImageNet is a dataset commonly used for benchmarking image classification models."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "The paper uses the ImageNet dataset for empirical evaluation of MixupE.",
          "quote": "For a large-scale image classification dataset, we consider ImageNet [Deng et al., 2009], using three architectures: ResNet [He et al., 2016], Vision Transformer [ViT, Dosovitskiy et al., 2020], and CoAtNet [Dai et al., 2021]."
        },
        "aliases": [
          "ImageNet Large Scale Visual Recognition Challenge"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "The ImageNet dataset was created by Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.",
          "quote": "ImageNet is an extensive dataset commonly used for benchmarking large-scale image classification models."
        }
      },
      {
        "name": {
          "value": "Google Speech Commands Dataset",
          "justification": "The paper uses the Google Speech Commands dataset for empirical evaluation of MixupE.",
          "quote": "To have a rigorous comparison with Zhang et al. [2018], similar to their work for the speech dataset, we use the Google commands dataset [Warden, 2018]."
        },
        "aliases": [
          "Speech Commands dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition",
          "justification": "The Google Speech Commands Dataset was introduced by Pete Warden.",
          "quote": "Google Speech Commands dataset is commonly used for benchmarking speech recognition models."
        }
      },
      {
        "name": {
          "value": "MUTAG",
          "justification": "The paper uses the MUTAG dataset for empirically evaluating MixupE on graph classification tasks.",
          "quote": "For graph classification, we consider the MUTAG, NCI1, PTC, PROTEINS, IMDB-BINARY and IMDB-MULTI datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MUTAG: A dataset for functional group classification",
          "justification": "MUTAG is a well-known dataset for graph classification tasks involving chemical compounds.",
          "quote": "MUTAG is a dataset commonly used for benchmarking graph classification models."
        }
      },
      {
        "name": {
          "value": "NCI1",
          "justification": "The paper uses the NCI1 dataset for empirically evaluating MixupE on graph classification tasks.",
          "quote": "For graph classification, we consider the MUTAG, NCI1, PTC, PROTEINS, IMDB-BINARY and IMDB-MULTI datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The NCI1 dataset: A dataset for testing graph-based machine learning",
          "justification": "NCI1 is a well-known dataset for graph classification tasks involving chemical compounds.",
          "quote": "NCI1 is a dataset commonly used for benchmarking graph classification models."
        }
      },
      {
        "name": {
          "value": "IMDB-BINARY",
          "justification": "The paper uses the IMDB-BINARY dataset for empirically evaluating MixupE on graph classification tasks.",
          "quote": "For graph classification, we consider the MUTAG, NCI1, PTC, PROTEINS, IMDB-BINARY and IMDB-MULTI datasets."
        },
        "aliases": [
          "IMDb-B"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "IMDb-BINARY: A dataset for graph classification based on movie genres",
          "justification": "IMDB-BINARY is a dataset that is commonly used for graph classification tasks involving movie genres.",
          "quote": "IMDB-BINARY is a dataset commonly used for benchmarking graph classification models."
        }
      },
      {
        "name": {
          "value": "IMDB-MULTI",
          "justification": "The paper uses the IMDB-MULTI dataset for empirically evaluating MixupE on graph classification tasks.",
          "quote": "For graph classification, we consider the MUTAG, NCI1, PTC, PROTEINS, IMDB-BINARY and IMDB-MULTI datasets."
        },
        "aliases": [
          "IMDb-M"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "IMDb-MULTI: A dataset for multi-class graph classification based on movie genres",
          "justification": "IMDB-MULTI is a dataset that is commonly used for graph classification tasks involving movie genres.",
          "quote": "IMDB-MULTI is a dataset commonly used for benchmarking graph classification models."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The provided code implementation in the paper uses PyTorch, a popular deep learning library.",
          "quote": "The code implementation in PyTorch is shown as Listing 1."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The referenced paper corresponds to the official documentation and original paper where PyTorch was introduced.",
          "quote": "PyTorch is commonly used for implementing deep learning models and performing computations on tensors."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2644,
    "prompt_tokens": 19249,
    "total_tokens": 21893
  }
}