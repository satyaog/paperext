{
  "paper": "2306.15794.txt",
  "words": 12538,
  "extractions": {
    "title": {
      "value": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
      "justification": "This is the title of the paper.",
      "quote": "HyenaDNA: Long-Range Genomic Sequence\nModeling at Single Nucleotide Resolution"
    },
    "description": "This paper presents HyenaDNA, a genomic foundation model pretrained on the human reference genome. It leverages Hyena's long-range capabilities, efficient training, and single nucleotide resolution to surpass state-of-the-art performance on various genomic benchmarks.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper involves empirical validation of the HyenaDNA model on numerous genomic benchmark datasets, comparing its performance with other models.",
      "quote": "We apply our pretrained HyenaDNA models to 29 diverse downstream\ngenomic tasks to showcase its long-range ability as well as fine-grain resolution. On fine-tuned benchmarks\nfrom the Nucleotide Transformer (Dalla-Torre et al., 2023), HyenaDNA achieves state-of-the-art (SotA) on 12\nof 18 datasets while using a model with orders of magnitude less parameters and pretraining data (see Tab.\n4.2)."
    },
    "primary_research_field": {
      "name": {
        "value": "Genomics",
        "justification": "The primary focus of the paper is on genomic sequence modeling using deep learning techniques.",
        "quote": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation, protein\nsynthesis, and numerous other cellular properties."
      },
      "aliases": [
        "Genomic Sequence Modeling"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper draws parallels between genomic sequence modeling and natural language processing.",
          "quote": "Similar to natural language models, researchers have\nproposed foundation models in genomics to learn generalizable features from unlabeled genome data\nthat can then be fine-tuned for downstream tasks such as identifying regulatory elements."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The paper extensively uses deep learning techniques to model genomic sequences.",
          "quote": "Understanding and learning from DNA sequences has long been a goal of biologists and deep learning researchers, as its “language” encodes instructions essential for all living things"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "HyenaDNA",
          "justification": "HyenaDNA is the primary deep learning model introduced and evaluated in this research paper.",
          "quote": "we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with\ncontext lengths of up to 1 million tokens at the single nucleotide-level – an up to 500x increase over previous dense attention-based models."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "HyenaDNA is the main contribution of this research paper.",
          "quote": "we present HyenaDNA, a genomic foundation model pretrained on the human reference genome"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed on GPUs during training and evaluation.",
          "quote": "Notably, with model parallelism, it becomes feasible to extend the context length by orders of magnitude beyond this current work, and leave that open to future research."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper includes comparisons of HyenaDNA against other models on several benchmarks.",
          "quote": "HyenaDNA achieves state-of-the-art (SotA) on 12 of 18 datasets while using a model with orders of magnitude less parameters and pretraining data"
        },
        "referenced_paper_title": {
          "value": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
          "justification": "The HyenaDNA model is based on the architecture proposed in the Hyena paper referenced in this work.",
          "quote": "A Hyena operator is composed of long convolutions and elementwise gate layers. The gates are fed projections of the input using dense layers and short convolutions. The\nlong convolutions are parameterized implicitly via an MLP that produces the convolutional filters."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Human reference genome",
          "justification": "The human reference genome is used extensively for pretraining the HyenaDNA model.",
          "quote": "HyenaDNA, a genomic foundation model pretrained on the human reference genome"
        },
        "aliases": [
          "Genome Reference Consortium human build 38 (GRCh38)"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Genome Reference Consortium, 2013",
          "justification": "The referenced paper provides the details about the human reference genome used in this research.",
          "quote": "Human reference genome (Genome Reference Consortium, 2013)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Pytorch",
          "justification": "Pytorch is used as a deep learning framework for training and evaluating the models in this research.",
          "quote": "Across all experiments, we use Pytorch and Pytorch Lightning."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "This is the main reference paper for PyTorch, which is used extensively in this research.",
          "quote": "Across all experiments, we use Pytorch and Pytorch Lightning."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1039,
    "prompt_tokens": 22768,
    "total_tokens": 23807
  }
}