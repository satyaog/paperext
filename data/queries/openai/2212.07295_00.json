{
  "paper": "2212.07295.txt",
  "words": 17194,
  "extractions": {
    "title": {
      "value": "Maximal Initial Learning Rates in Deep ReLU Networks",
      "justification": "The provided title is directly extracted from the document.",
      "quote": "Maximal Initial Learning Rates in Deep ReLU Networks"
    },
    "description": "This paper investigates the maximal initial learning rate for deep ReLU networks, defined as the largest learning rate at which a randomly initialized neural network can begin training and achieve a given threshold accuracy. The study explores the relationship between this learning rate, the network’s architecture, and the sharpness of the network at initialization. Empirical and theoretical analyses suggest a power-law relationship between the maximal initial learning rate and the product of the network’s depth and width.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper provides theoretical analyses and proofs regarding the maximal initial learning rate, supported by empirical results.",
      "quote": "We formally prove bounds for λ1 in terms of (depth × width) that align with our empirical results."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The research focuses on aspects fundamental to the training of deep neural networks, particularly the maximal initial learning rate.",
        "quote": "Training a neural network requires choosing a suitable learning rate, which involves a trade-off between speed and effectiveness of convergence."
      },
      "aliases": [
        "DL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper heavily focuses on the learning rate, which is a key hyperparameter optimization aspect in training neural networks.",
          "quote": "This ensures that networks trained at the maximal initial learning rate perform adequately while taking into account task difficulty."
        },
        "aliases": [
          "Hyperparameter Tuning"
        ]
      },
      {
        "name": {
          "value": "ReLU Networks",
          "justification": "The study specifically focuses on the behavior and training dynamics of deep ReLU networks.",
          "quote": "Using a simple approach to estimate η*, we observe that in constant-width fully-connected ReLU networks, η* behaves differently from the maximum learning rate later in training."
        },
        "aliases": [
          "Rectified Linear Units"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Fully-Connected ReLU Networks",
          "justification": "The study focuses primarily on fully-connected networks using ReLU activation functions for its analyses.",
          "quote": "Using a simple approach to estimate η*, we observe that in constant-width fully-connected ReLU networks, η* behaves differently from the maximum learning rate later in training."
        },
        "aliases": [
          "FC ReLU Networks"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper did not make a contribution of a new model but rather studied existing fully-connected ReLU networks.",
          "quote": "Using a simple approach to estimate η*, we observe that in constant-width fully-connected ReLU networks, η* behaves differently from the maximum learning rate later in training."
        },
        "is_executed": {
          "value": 1,
          "justification": "Empirical experiments were conducted on fully-connected ReLU networks as mentioned in the paper.",
          "quote": "We use depths ∈ {5, 7, 10, 12, 15, 18, 20, 23, 25, 27, 30}, for 25 initializations per architecture."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not mention numeric comparisons of fully-connected ReLU networks against other models.",
          "quote": "Using a simple approach to estimate η*, we observe that in constant-width fully-connected ReLU networks, η* behaves differently from the maximum learning rate later in training."
        },
        "referenced_paper_title": {
          "value": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
          "justification": "Referenced for understanding ReLU activations and their properties in the context of neural networks.",
          "quote": "He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification."
        }
      },
      {
        "name": {
          "value": "Residual Networks (ResNet-20)",
          "justification": "The paper includes experimental results involving ResNet-20 models to highlight the broader applicability of its findings.",
          "quote": "Performance of ResNet-20 (He et al., 2016) networks with different learning rate setups."
        },
        "aliases": [
          "ResNet-20"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper mentions using ResNet-20 but does not contribute it as a new model.",
          "quote": "Performance of ResNet-20 (He et al., 2016) networks with different learning rate setups."
        },
        "is_executed": {
          "value": 1,
          "justification": "The ResNet-20 models were executed to test the hypotheses regarding learning rates.",
          "quote": "Performance of ResNet-20 (He et al. 2016) networks with different learning rate setups. Each line in the figure is an average of 3 runs, along with error bars to indicate deviation in performance."
        },
        "is_compared": {
          "value": 0,
          "justification": "While empirical results were shown, there were no numeric comparisons made with other models.",
          "quote": "Performance of ResNet-20 (He et al. 2016) networks with different learning rate setups."
        },
        "referenced_paper_title": {
          "value": "Deep residual learning for image recognition",
          "justification": "Referenced for using ResNet-20 architecture in experiments.",
          "quote": "He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset was used for experiments to validate the theoretical findings.",
          "quote": "For MNIST and CIFAR-10 we use t = 0.925 and t = 0.34 respectively."
        },
        "aliases": [
          "Modified National Institute of Standards and Technology database"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition",
          "justification": "Standard reference paper for the MNIST dataset.",
          "quote": "LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The CIFAR-10 dataset was used for experiments to validate the theoretical findings.",
          "quote": "For MNIST and CIFAR-10 we use t = 0.925 and t = 0.34 respectively."
        },
        "aliases": [
          "Canadian Institute For Advanced Research 10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Krizhevsky, A. (2009). Learning multiple layers of features from tiny images",
          "justification": "Standard reference paper for the CIFAR-10 dataset.",
          "quote": "Krizhevsky, A. (2009). Learning multiple layers of features from tiny images."
        }
      },
      {
        "name": {
          "value": "Fashion-MNIST",
          "justification": "Fashion-MNIST was used to further experiment and validate empirical results.",
          "quote": "Experimental results for Fashion-MNIST. We obtain a threshold accuracy of 0.84 for Fashion-MNIST."
        },
        "aliases": [
          "FMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
          "justification": "Referenced as the source of the Fashion-MNIST dataset used in experiments.",
          "quote": "Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyHessian",
          "justification": "PyHessian was explicitly mentioned as being used for computing sharpness λ1.",
          "quote": "For computing the sharpness λ1, we use PyHessian (Yao et al., 2020)."
        },
        "aliases": [
          "PyHessian"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Yao, Z., Gholami, A., Keutzer, K., & Mahoney, M. W. (2020). PyHessian: Neural networks through the lens of the Hessian",
          "justification": "Standard reference paper for PyHessian.",
          "quote": "Yao, Z., Gholami, A., Keutzer, K., & Mahoney, M. W. (2020). PyHessian: Neural networks through the lens of the Hessian."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1725,
    "prompt_tokens": 36117,
    "total_tokens": 37842
  }
}