{
  "paper": "4GBGwVIEYJ.txt",
  "words": 12433,
  "extractions": {
    "title": {
      "value": "Sample-Efficient Reinforcement Learning by Breaking the Replay Ratio Barrier",
      "justification": "This is the title mentioned at the beginning of the paper.",
      "quote": "SAMPLE-EFFICIENT REINFORCEMENT LEARNING BY BREAKING THE REPLAY RATIO BARRIER"
    },
    "description": "This research investigates how scaling the replay ratio—the number of updates of an agent’s parameters per environment interaction—affects the sample efficiency of deep reinforcement learning algorithms. The study explores how resetting agent parameters can improve performance and applies this technique to SAC and SPR algorithms in both continuous and discrete control settings, breaking the established barriers and pushing the limits of sample efficiency.",
    "type": {
      "value": "theoretical",
      "justification": "The primary focus of the paper is on proposing and theoretically validating a new method for improving sample efficiency in deep reinforcement learning by scaling the replay ratio. Empirical evaluations support the theoretical claims.",
      "quote": "In this paper, we show that it is possible, with minimal but careful modifications to model-free algorithms mostly based on parameter resets (Ash & Adams, 2020; Nikishin et al., 2022), to reach new levels of replay ratio scaling and push the sample efficiency limits of deep RL."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on exploring and improving the sample efficiency of reinforcement learning algorithms by scaling the replay ratio.",
        "quote": "Increasing the replay ratio, the number of updates of an agent’s parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms."
      },
      "aliases": [
        "RL",
        "Deep RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The study specifically focuses on deep reinforcement learning algorithms and their sample efficiency.",
          "quote": "Increasing the replay ratio, the number of updates of an agent’s parameters per environment interaction, is an appealing strategy for improving the sample efficiency of deep reinforcement learning algorithms."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Continuous Control",
          "justification": "The paper evaluates the proposed method on continuous control tasks using SAC in the DeepMind Control Suite.",
          "quote": "Both in continuous control, with SAC in DeepMind Control Suite (Haarnoja et al., 2018; Tassa et al., 2018), and discrete control, with SPR in Atari 100k (Schwarzer et al., 2021a; Kaiser et al., 2020), we break the replay ratio barrier."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Discrete Control",
          "justification": "The paper evaluates the proposed method on discrete control tasks using SPR in the Atari 100k benchmark.",
          "quote": "Both in continuous control, with SAC in DeepMind Control Suite (Haarnoja et al., 2018; Tassa et al., 2018), and discrete control, with SPR in Atari 100k (Schwarzer et al., 2021a; Kaiser et al., 2020), we break the replay ratio barrier."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "SAC",
          "justification": "SAC's performance is compared to other baselines and modified versions of itself in the experimental section.",
          "quote": "Results In Figure 2, we compare a version of SR-SAC that uses a replay ratio of 128 to standard deep RL baselines."
        },
        "aliases": [
          "Soft Actor-Critic"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SAC is utilized as a baseline model and is not introduced as a new contribution in this paper.",
          "quote": "We consider Soft Actor-Critic (SAC) (Haarnoja et al., 2018), which optimizes an actor and a critic by maximizing policy entropy alongside the environment’s reward."
        },
        "is_executed": {
          "value": 1,
          "justification": "The SAC model is executed for comparison purposes in the experiments.",
          "quote": "Results In Figure 2, we compare a version of SR-SAC that uses a replay ratio of 128 to standard deep RL baselines."
        },
        "is_compared": {
          "value": 1,
          "justification": "SAC's performance is compared to other baselines and modified versions of itself.",
          "quote": "Results In Figure 2, we compare a version of SR-SAC that uses a replay ratio of 128 to standard deep RL baselines."
        },
        "referenced_paper_title": {
          "value": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
          "justification": "This is the referenced paper for the SAC model.",
          "quote": "We consider Soft Actor-Critic (SAC) (Haarnoja et al., 2018), which optimizes an actor and a critic by maximizing policy entropy alongside the environment’s reward."
        }
      },
      {
        "name": {
          "value": "SPR",
          "justification": "SPR's performance is compared to other baselines and modified versions of itself in the experimental section.",
          "quote": "We compare a version of SR-SPR that uses a replay ratio of 16 to standard baselines (DrQ, DER, Kostrikov et al., 2022; Van Hasselt et al., 2019) and recent work (IRIS, Micheli et al., 2022)."
        },
        "aliases": [
          "Self-Predictive Representations"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SPR is utilized as a baseline model and is not introduced as a new contribution in this paper.",
          "quote": "We consider SPR (Schwarzer et al., 2021a), a model-free DQN-based reinforcement learning algorithm that augments a sample-efficient variant of Rainbow with a model-based latent dynamics prediction objective."
        },
        "is_executed": {
          "value": 1,
          "justification": "The SPR model is executed for comparison purposes in the experiments.",
          "quote": "We compare a version of SR-SPR that uses a replay ratio of 16 to standard baselines (DrQ, DER, Kostrikov et al., 2022; Van Hasselt et al., 2019) and recent work (IRIS, Micheli et al., 2022)."
        },
        "is_compared": {
          "value": 1,
          "justification": "SPR's performance is compared to other baselines and modified versions of itself.",
          "quote": "We compare a version of SR-SPR that uses a replay ratio of 16 to standard baselines (DrQ, DER, Kostrikov et al., 2022; Van Hasselt et al., 2019) and recent work (IRIS, Micheli et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "Data-Efficient Reinforcement Learning with Self-Predictive Representations",
          "justification": "This is the referenced paper for the SPR model.",
          "quote": "We consider SPR (Schwarzer et al., 2021a), a model-free DQN-based reinforcement learning algorithm that augments a sample-efficient variant of Rainbow with a model-based latent dynamics prediction objective."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari 100k",
          "justification": "The Atari 100k benchmark is one of the benchmarks used for evaluating the modified reinforcement learning algorithms.",
          "quote": "train them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DeepMind Control Suite",
          "justification": "The DeepMind Control Suite is another benchmark used for evaluating the modified reinforcement learning algorithms.",
          "quote": "We push the limits of the sample efficiency of carefully-modified algorithms by training them using an order of magnitude more updates than usual, significantly improving their performance in the Atari 100k and DeepMind Control Suite benchmarks."
        },
        "aliases": [
          "DMC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3459,
    "prompt_tokens": 50845,
    "total_tokens": 54304
  }
}