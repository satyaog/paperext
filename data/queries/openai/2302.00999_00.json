{
  "paper": "2302.00999.txt",
  "words": 46506,
  "extractions": {
    "title": {
      "value": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance",
      "justification": "This is the title of the paper provided",
      "quote": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance"
    },
    "description": "The paper proposes several algorithms with high-probability convergence results under less restrictive assumptions for stochastic optimization and variational inequalities. The focus is on scenarios where the gradient/operator noise has bounded central α-th moment for α ∈ (1, 2], instead of the usual bounded variance assumption. The authors derive new high-probability convergence results in various optimization setups and establish the efficacy of their methods for problems that do not fit standard functional classes.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper introduces algorithms and provides empirical results demonstrating their high-probability convergence.",
      "quote": "In particular, we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central α-th moment for α ∈ (1, 2] in the following setups"
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization",
        "justification": "The primary research focus of this paper is on optimization techniques in the context of stochastic and variational inequalities.",
        "quote": "This paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions."
      },
      "aliases": [
        "Stochastic Optimization"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Stochastic Optimization",
          "justification": "The algorithms discussed in the paper are for stochastic optimization under specific noise assumptions.",
          "quote": "we derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central α-th moment for α ∈ (1, 2]"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Variational Inequalities",
          "justification": "The paper also addresses solutions for variational inequalities under the same noise assumptions.",
          "quote": "derive new high-probability convergence results under the assumption that the gradient/operator noise has bounded central α-th moment for α ∈ (1, 2] in the following setups...(ii) Lipschitz / star-cocoercive and monotone / quasi-strongly monotone variational inequalities."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Clipped-SGD",
          "justification": "The paper analyzes and experiments with Clipped Stochastic Gradient Descent under high-probability assumptions.",
          "quote": "In particular, we analyze Clipped Stochastic Gradient Descent (clipped-SGD)"
        },
        "aliases": [
          "Clipped Stochastic Gradient Descent"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Clipped-SGD is not introduced for the first time in this paper but is built upon previous works.",
          "quote": "Gorbunov et al. (2020; 2021) propose Clipped Stochastic Gradient Descent (clipped-SGD)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper runs experiments with Clipped-SGD.",
          "quote": "we analyze Clipped Stochastic Gradient Descent (clipped-SGD)"
        },
        "is_compared": {
          "value": 1,
          "justification": "Clipped-SGD is compared against other optimization methods in the experiments.",
          "quote": "In particular, we analyze Clipped Stochastic Gradient Descent (clipped-SGD) and compare its performance"
        },
        "referenced_paper_title": {
          "value": "Clipped Stochastic Methods for Smooth and Non-Smooth Convex Optimization Problems",
          "justification": "This paper is referenced for the introduction of Clipped-SGD.",
          "quote": "Gorbunov et al. (2020; 2021) propose Clipped Stochastic Gradient Descent (clipped-SGD)"
        }
      },
      {
        "name": {
          "value": "Clipped-SSTM",
          "justification": "The Clipped Stochastic Similar Triangles Method is introduced for accelerated optimization.",
          "quote": "Next, we focus on the accelerated version of clipped-SGD called Clipped Stochastic Similar Triangles Method (clipped-SSTM)"
        },
        "aliases": [
          "Clipped Stochastic Similar Triangles Method"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Clipped-SSTM builds on the existing Similar Triangles Method for optimization.",
          "quote": "Next, we focus on the accelerated version of clipped-SGD called Clipped Stochastic Similar Triangles Method (clipped-SSTM)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper runs experiments with Clipped-SSTM.",
          "quote": "Let Assumptions 1.1, 1.3, 1.6 with µ = 0 hold for Q = B3R (x∗ ), R ≥ kx0 − x∗ k2 and Clipped Stochastic Similar Triangles Method (clipped-SSTM)."
        },
        "is_compared": {
          "value": 1,
          "justification": "Clipped-SSTM is compared to Clipped-SGD in terms of performance.",
          "quote": "The obtained complexity bound matches the best-known ones for clipped-SGD."
        },
        "referenced_paper_title": {
          "value": "Fast gradient methods based on new search corrector (similar triangles/newton) for smooth stochastic objective functions",
          "justification": "The original Similar Triangles Method was introduced in this paper, upon which Clipped-SSTM is based.",
          "quote": "called Clipped Stochastic Similar Triangles Method (clipped-SSTM) (Gorbunov et al., 2020)."
        }
      },
      {
        "name": {
          "value": "Clipped-SEG",
          "justification": "Clipped Stochastic Extragradient Method is analyzed in this work for variational inequalities.",
          "quote": "Stochastic Extragradient method (clipped-SEG):"
        },
        "aliases": [
          "Clipped Stochastic Extragradient"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Clipped-SEG is not a new contribution in this paper but an existing method.",
          "quote": "consider Clipped Stochastic Extragradient method (clipped-SEG):"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper executes the Clipped-SEG method in its empirical evaluations.",
          "quote": "Let Assumptions 1.1, 1.7, 1.9, 1.10 hold for Q ="
        },
        "is_compared": {
          "value": 1,
          "justification": "Clipped-SEG is compared to other variational inequality solvers in the empirical results.",
          "quote": "In the star-cocoercive case, we focus on Clipped Stochastic Gradient Descent-Ascent (clipped-SGDA)."
        },
        "referenced_paper_title": {
          "value": "Clipped stochastic methods for variational inequalities with heavy-tailed noise.",
          "justification": "This method was previously analyzed in detail in the referenced paper.",
          "quote": "For (quasi strongly) monotone VIPs we consider Clipped Stochastic Extragradient method (clipped-SEG):"
        }
      },
      {
        "name": {
          "value": "Clipped-SGDA",
          "justification": "Using Clipped Stochastic Gradient Descent-Ascent algorithm for variational inequality.",
          "quote": "Stochastic Gradient Descent-Ascent (clipped-SGDA):"
        },
        "aliases": [
          "Clipped Stochastic Gradient Descent-Ascent"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper builds upon existing Clipped-SGDA.",
          "quote": "In the star-cocoercive case, we focus on Clipped Stochastic Gradient Descent-Ascent (clipped-SGDA)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper runs experiments with Clipped-SGDA.",
          "quote": "Algorithm 5 Clipped Stochastic Gradient Descent Ascent (clipped-SGDA)."
        },
        "is_compared": {
          "value": 1,
          "justification": "Clipped-SGDA is empirically compared with Clipped-SEG as well as other models.",
          "quote": "In the star-cocoercive case, we focus on Clipped Stochastic Gradient Descent-Ascent (clipped-SGDA)."
        },
        "referenced_paper_title": {
          "value": "Stochastic Gradient Descent-Ascent for Variational Inequalities",
          "justification": "The method is utilized here based on existing literature.",
          "quote": "In the star-cocoercive case, we focus on Clipped Stochastic Gradient Descent-Ascent (clipped-SGDA)."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3745,
    "prompt_tokens": 191231,
    "total_tokens": 194976
  }
}