{
  "paper": "2212.02614.txt",
  "words": 4732,
  "extractions": {
    "title": {
      "value": "Can Ensembling Pre-processing Algorithms Lead to Better Machine Learning Fairness?",
      "justification": "This is the title of the paper as mentioned at the beginning.",
      "quote": "Can Ensembling Pre-processing Algorithms Lead to Better Machine Learning Fairness?"
    },
    "description": "This paper evaluates the performance of three popular fairness pre-processing algorithms and investigates the potential for combining them into a more robust pre-processing ensemble.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments to evaluate the performance of several fairness pre-processing algorithms, making it an empirical study.",
      "quote": "Our study aims to evaluate the impact of fairness pre-processing algorithms and their ensemble on a model’s fairness and accuracy. In the following, we describe how we achieved our goal by selecting real-world datasets, widely-used ML models, the metrics that assess fairness and accuracy, and our experiment setup."
    },
    "primary_research_field": {
      "name": {
        "value": "Fairness in Machine Learning",
        "justification": "The primary focus of the paper is on evaluating and improving the fairness of machine learning models.",
        "quote": "As machine learning (ML) systems get adopted in more critical areas, it has become increasingly crucial to address the bias that could occur in these systems."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Ensemble Learning",
          "justification": "The paper investigates the potential for combining multiple fairness pre-processing algorithms into an ensemble.",
          "quote": "Specifically, we investigate whether ensembling multiple fairness pre-processing algorithms can provide further improvements to the model’s fairness and accuracy."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Fairness Algorithms",
          "justification": "The paper discusses and evaluates several fairness pre-processing algorithms.",
          "quote": "First, we evaluate the performance of several fairness pre-processing algorithms to highlight their strengths and weaknesses in fairness and accuracy."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Algorithm Performance Evaluation",
          "justification": "The paper involves evaluating the performance of different algorithms and comparing them.",
          "quote": "To address this challenge, we set out on two endeavors in this study. First, we evaluate the performance of several fairness pre-processing algorithms to highlight their strengths and weaknesses in fairness and accuracy."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Logistic Regression",
          "justification": "The paper mentions using Logistic Regression as one of the models for evaluation.",
          "quote": "We select Logistic Regression and Random Forest for our evaluation."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Logistic Regression is not a contribution of this paper; it is used as a baseline model.",
          "quote": "We select Logistic Regression and Random Forest for our evaluation."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed during the experiments for evaluation purposes.",
          "quote": "We select Logistic Regression and Random Forest for our evaluation."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Logistic Regression is compared with Random Forest in terms of fairness and accuracy.",
          "quote": "We select Logistic Regression and Random Forest for our evaluation. Both models have been subject to previous studies on fairness algorithms [7]."
        },
        "referenced_paper_title": {
          "value": "An empirical comparison of supervised learning algorithms",
          "justification": "The referenced paper about Logistic Regression is mentioned in the context of comparing different supervised learning algorithms.",
          "quote": "R. Caruana and A. Niculescu-Mizil, “An empirical comparison of supervised learning algorithms,” in Proceedings of the 23rd International Conference on Machine Learning, ser. ICML ’06. New York, NY, USA: Association for Computing Machinery, 2006, p. 161–168."
        }
      },
      {
        "name": {
          "value": "Random Forest",
          "justification": "The paper mentions using Random Forest as one of the models for evaluation.",
          "quote": "We select Logistic Regression and Random Forest for our evaluation."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Random Forest is not a contribution of this paper; it is used as a baseline model.",
          "quote": "We select Logistic Regression and Random Forest for our evaluation."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed during the experiments for evaluation purposes.",
          "quote": "We select Logistic Regression and Random Forest for our evaluation."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Random Forest is compared with Logistic Regression in terms of fairness and accuracy.",
          "quote": "We select Logistic Regression and Random Forest for our evaluation. Both models have been subject to previous studies on fairness algorithms [7]."
        },
        "referenced_paper_title": {
          "value": "Random forest versus logistic regression: a large-scale benchmark experiment",
          "justification": "The referenced paper about Random Forest is mentioned in the context of comparing it with Logistic Regression.",
          "quote": "R. Couronné, P. Probst, and A.-L. Boulesteix, “Random forest versus logistic regression: a large-scale benchmark experiment,” BMC bioinformatics, vol. 19, no. 1, pp. 1–14, 2018."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "German Credit Dataset",
          "justification": "The German Credit Dataset is one of the datasets used for evaluating fairness pre-processing algorithms.",
          "quote": "More specifically, we use (1) the German credit dataset [11] which includes data on 1,000 bank account holders and their credit evaluation (e.g., good or bad)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "UCI machine learning repository",
          "justification": "The dataset is retrieved from the UCI machine learning repository.",
          "quote": "D. Dua and C. Graff, “UCI machine learning repository,” 2017. [Online]. Available: http://archive.ics.uci.edu/ml"
        }
      },
      {
        "name": {
          "value": "COMPAS Dataset",
          "justification": "The COMPAS Dataset is one of the datasets used for evaluating fairness pre-processing algorithms.",
          "quote": "(2) The COMPAS dataset [12] has information on 10,000 criminal defendants and was used by courts to help judges and parole officers make better decisions."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Machine bias: There’s software used across the country to predict future criminals. and its biased against blacks",
          "justification": "The related paper discusses the bias inherent in the COMPAS dataset.",
          "quote": "J. Angwin, J. Larson, S. Mattu, and L. Kirchner, “Machine bias: There's software used across the country to predict future criminals. and its biased against blacks.” ProPublica, May 2016."
        }
      },
      {
        "name": {
          "value": "Adult Census Income Dataset",
          "justification": "The Adult Census Income Dataset is one of the datasets used for evaluating fairness pre-processing algorithms.",
          "quote": "(3) The Adult Census Income dataset [13] is used to determine whether a person earns more than $50,000 USD and includes 48,842 instances."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid",
          "justification": "The referenced paper provides context for using the Adult Census Income dataset.",
          "quote": "R. Kohavi et al., “Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid.” in Kdd, vol. 96, 1996, pp. 202–207."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "AIF360",
          "justification": "The AIF360 library is used for implementing the fairness pre-processing algorithms.",
          "quote": "Notably, we leverage the implementation of these algorithms provided from AIF3602."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias",
          "justification": "The referenced paper describes the AIF360 toolkit, which is used in this study.",
          "quote": "R. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovic, S. Nagar, K. N. Ramamurthy, J. Richards, D. Saha, P. Sattigeri, M. Singh, K. R. Varshney, and Y. Zhang, “AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias,” arXiv:1810.01943 [cs], Oct. 2018, arXiv: 1810.01943."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1756,
    "prompt_tokens": 9094,
    "total_tokens": 10850
  }
}