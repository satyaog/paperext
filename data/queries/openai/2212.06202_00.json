{
  "paper": "2212.06202.txt",
  "words": 8917,
  "extractions": {
    "title": {
      "value": "Doubly Right Object Recognition: A Why Prompt for Visual Rationales",
      "justification": "It is the title of the paper",
      "quote": "Doubly Right Object Recognition: A Why Prompt for Visual Rationales"
    },
    "description": "This paper investigates whether computer vision models can provide correct rationales for their predictions, introducing a benchmark called doubly right object recognition. The paper evaluates if state-of-the-art visual models can produce both correct categorical labels and accurate rationales. By transferring rationales from language models to visual representations through a tailored dataset, the study shows significant improvements in performance.",
    "type": {
      "value": "Empirical",
      "justification": "The paper conducts empirical experiments and evaluations to show that transferring rationales from language models to visual representations can improve the performance of visual models in producing correct rationales.",
      "quote": "Visualizations and empirical experiments show that our prompts significantly improve performance on doubly right object recognition, in addition to zero-shot transfer to unseen tasks and datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper primarily deals with visual recognition models and their ability to provide correct rationales for their predictions, which falls under the domain of Computer Vision.",
        "quote": "In this paper, we investigate whether computer vision models can also provide correct rationales for their predictions."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Visual Reasoning",
          "justification": "The paper addresses visual reasoning by proposing a benchmark to evaluate if visual models can justify their predictions with correct rationales.",
          "quote": "Visual reasoning for doubly right object recognition task. Motivated by prompting in NLP, we learn a why prompt from multimodal data, which allows us to instruct visual models to predict both the right category and the correct rationales that justify the prediction."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Explainability",
          "justification": "The paper aims to make visual recognition models more interpretable by ensuring they provide correct rationales for their predictions.",
          "quote": "Learning models that can explain their own decision is important for building trustworthy systems, especially in applications that require human-machine interactions. Rationales that justify the prediction can largely improve user trust."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP",
          "justification": "The paper mentions that state-of-the-art visual models such as CLIP often provide incorrect rationales for their categorical predictions and evaluates this model.",
          "quote": "We find that state-of-the-art visual models, such as CLIP, often provide incorrect rationales for their categorical predictions."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CLIP is used as a baseline model to evaluate the proposed why prompt method but is not a contribution of this paper.",
          "quote": "We find that state-of-the-art visual models, such as CLIP, often provide incorrect rationales for their categorical predictions."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper evaluates the CLIP model on the proposed benchmark for doubly right object recognition.",
          "quote": "We find that state-of-the-art visual models, such as CLIP, often provide incorrect rationales for their categorical predictions."
        },
        "is_compared": {
          "value": 1,
          "justification": "CLIP is compared against other models to show its performance on the doubly right object recognition task.",
          "quote": "Benchmarking six large-scale image-language pretrained models on doubly right recognition over six datasets. The accuracy for doubly right is gray-scaled. We bold the best accuracy."
        },
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "The reference paper provides the details about the CLIP model which is evaluated in the current study.",
          "quote": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision."
        }
      },
      {
        "name": {
          "value": "FLAVA",
          "justification": "FLAVA is mentioned in the paper as one of the models evaluated for doubly right object recognition.",
          "quote": "We study FLAVA [49] and five variants of CLIP [41], where evaluation results are in Table 2."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "FLAVA is used as a baseline model to evaluate the proposed why prompt method but is not a contribution of this paper.",
          "quote": "We study FLAVA [49] and five variants of CLIP [41], where evaluation results are in Table 2."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper evaluates the FLAVA model on the proposed benchmark for doubly right object recognition.",
          "quote": "We study FLAVA [49] and five variants of CLIP [41], where evaluation results are in Table 2."
        },
        "is_compared": {
          "value": 1,
          "justification": "FLAVA is compared against other models to show its performance on the doubly right object recognition task.",
          "quote": "Benchmarking six large-scale image-language pretrained models on doubly right recognition over six datasets. The accuracy for doubly right is gray-scaled. We bold the best accuracy."
        },
        "referenced_paper_title": {
          "value": "FLAVA: A foundational language and vision alignment model",
          "justification": "The reference paper provides the details about the FLAVA model which is evaluated in the current study.",
          "quote": "Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and\nDouwe Kiela. Flava: A foundational language and vision\nalignment model."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10+",
          "justification": "The paper mentions CIFAR-10+ as part of its experimental datasets for evaluating doubly right object recognition.",
          "quote": "Ours CIFAR-10+\n10\n63\n2,201\nYes"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The CIFAR-10 dataset is used as a reference to explain the CIFAR-10+ dataset.",
          "quote": "Alex Krizhevsky, Learning multiple layers of features from tiny images."
        }
      },
      {
        "name": {
          "value": "CIFAR-100+",
          "justification": "The paper mentions CIFAR-100+ as part of its experimental datasets for evaluating doubly right object recognition.",
          "quote": "Ours CIFAR-100+\n100\n540\n18,318\nYes"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The CIFAR-100 dataset is used as a reference to explain the CIFAR-100+ dataset.",
          "quote": "Alex Krizhevsky, Learning multiple layers of features from tiny images."
        }
      },
      {
        "name": {
          "value": "Food101+",
          "justification": "The paper mentions Food101+ as part of its experimental datasets for evaluating doubly right object recognition.",
          "quote": "Ours Food101+\n101\n435\n15,212\nYes"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Food-101 – Mining Discriminative Components with Random Forests",
          "justification": "The Food101 dataset is used as a reference to explain the Food101+ dataset.",
          "quote": "Bossard, L.; Guillaumin, M.; Van Gool, L. (2014). Food-101 – Mining Discriminative Components with Random Forests."
        }
      },
      {
        "name": {
          "value": "Caltech101+",
          "justification": "The paper mentions Caltech101+ as part of its experimental datasets for evaluating doubly right object recognition.",
          "quote": "Ours Caltech101+\n101\n516\n16,849\nYes"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Caltech 101 Object Categories",
          "justification": "The Caltech101 dataset is used as a reference to explain the Caltech101+ dataset.",
          "quote": "L. Fei-Fei, R. Fergus and P. Perona. (2007). Caltech 101 Object Categories."
        }
      },
      {
        "name": {
          "value": "SUN+",
          "justification": "The paper mentions SUN+ as part of its experimental datasets for evaluating doubly right object recognition.",
          "quote": "Ours SUN+\n397\n2,170\n75,381\nYes"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "SUN Attribute Database: Discovering, Annotating, and Recognizing Scene Attributes",
          "justification": "The SUN dataset is used as a reference to explain the SUN+ dataset.",
          "quote": "G. Patterson and J. Hays. (2012). SUN Attribute Database: Discovering, Annotating, and Recognizing Scene Attributes."
        }
      },
      {
        "name": {
          "value": "ImageNet+",
          "justification": "The paper mentions ImageNet+ as part of its experimental datasets for evaluating doubly right object recognition.",
          "quote": "Ours ImageNet+\n1000\n5,810\n271,016\nYes"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "The ImageNet dataset is used as a reference to explain the ImageNet+ dataset.",
          "quote": "Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton, ImageNet Large Scale Visual Recognition Challenge."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "GPT-3",
          "justification": "GPT-3 is used to generate rationales for the doubly right object recognition task.",
          "quote": "Recent advances in large-scale language models, such as the GPT3, demonstrate the ability to provide various commonsense knowledge in language."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The reference paper provides details about GPT-3, which is used in the current study to generate rationales.",
          "quote": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei. Language Models are Few-Shot Learners."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2192,
    "prompt_tokens": 16746,
    "total_tokens": 18938
  }
}