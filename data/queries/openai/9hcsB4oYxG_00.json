{
  "paper": "9hcsB4oYxG.txt",
  "words": 4397,
  "extractions": {
    "title": {
      "value": "Adaptive Resolution Residual Networks",
      "justification": "Title of the paper is clearly stated at the beginning.",
      "quote": "Adaptive Resolution Residual Networks"
    },
    "description": "The paper introduces Adaptive Resolution Residual Networks (ARRNs), which are neural operators that can adapt to varying signal resolutions. The networks are built with Laplacian residuals and Laplacian dropout to enhance computational efficiency and robustness to low-bandwidth signals.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes theoretical formulation, but it also demonstrates experimental results on datasets like CIFAR10 and CIFAR100, showing the practical advantages of ARRNs.",
      "quote": "We provide theoretical analysis for the advantageous properties of ARRNs in section 2, along with a set of experiments that demonstrate these properties in practice in section 3, where we train ARRN and competing methods at a single resolution, then evaluate them at various resolutions."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The research focuses on image classification tasks using datasets like CIFAR10 and CIFAR100.",
        "quote": "We construct a pair of experimental setups each evaluated on the CIFAR10 and CIFAR100 (Krizhevsky et al., 2009) image classification datasets."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Architecture",
          "justification": "The paper introduces a new neural network architecture called Adaptive Resolution Residual Networks (ARRNs).",
          "quote": "We introduce Adaptive Resolution Residual Networks (ARRNs), which are neural operators that can be rediscretized easily and robustly thanks to two components: Laplacian residuals, which define the structure of ARRNs and allow rediscretization, and Laplacian dropout, which improves the robustness of rediscretized ARRNs through a training augmentation."
        },
        "aliases": [
          "NAS"
        ]
      },
      {
        "name": {
          "value": "Image Classification",
          "justification": "The experiments focus on image classification tasks on the CIFAR10 and CIFAR100 datasets.",
          "quote": "We construct a pair of experimental setups each evaluated on the CIFAR10 and CIFAR100 (Krizhevsky et al., 2009) image classification datasets."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Adaptive Resolution Residual Networks",
          "justification": "The paper introduces and focuses on Adaptive Resolution Residual Networks (ARRNs).",
          "quote": "We introduce Adaptive Resolution Residual Networks (ARRNs), which are neural operators that can be rediscretized easily and robustly thanks to two components: Laplacian residuals, which define the structure of ARRNs and allow rediscretization, and Laplacian dropout, which improves the robustness of rediscretized ARRNs through a training augmentation."
        },
        "aliases": [
          "ARRNs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The primary focus of the paper is to introduce ARRNs as a novel model.",
          "quote": "We introduce Adaptive Resolution Residual Networks (ARRNs), which are neural operators that can be rediscretized easily and robustly thanks to two components: Laplacian residuals, which define the structure of ARRNs and allow rediscretization, and Laplacian dropout, which improves the robustness of rediscretized ARRNs through a training augmentation."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed in experimental setups to demonstrate its properties and benefits.",
          "quote": "We provide theoretical analysis for the advantageous properties of ARRNs in section 2, along with a set of experiments that demonstrate these properties in practice in section 3, where we train ARRN and competing methods at a single resolution, then evaluate them at various resolutions."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model was compared with other models such as MobileNetV3, WideResNet, ResNet, ConvNeXt, and EfficientNetV2.",
          "quote": "We compare our ARRN against a wide range of convolutional network families that are well-suited for the classification task we test: MobileNetV3 (1.52M-4.21M) Howard et al. (2019), WideResNetV2 (66.8M-124M) (Zagoruyko and Komodakis, 2016), ResNet (11.1M-42.5M) He et al. (2016a), ConvNeXt (27.8M-196M) Liu et al. (2022), and EfficientNetV2 (20.2M-117.2M)."
        },
        "referenced_paper_title": {
          "value": "Deep residual learning for image recognition",
          "justification": "The ARRNs build on the concept of residual learning introduced in this paper.",
          "quote": "We formulate Laplacian residuals by combining the properties of standard residuals (He et al., 2016a,b) and Laplacian pyramids (Burt and Adelson, 1987)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR10",
          "justification": "The CIFAR10 dataset was used in the experimental evaluation of the models.",
          "quote": "Our experiments demonstrate that rediscretized ARRNs have identical performance to nonrediscretized ARRNs; that rediscretized ARRNs have vastly lower inference time than nonrediscretized ARRNs ; and that ARRNs are robust to low bandwidth signals. We construct a pair of experimental setups each evaluated on the CIFAR10 and CIFAR100 (Krizhevsky et al., 2009) image classification datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is the foundational paper for the CIFAR10 dataset.",
          "quote": "CIFAR10 and CIFAR100 (Krizhevsky et al., 2009)"
        }
      },
      {
        "name": {
          "value": "CIFAR100",
          "justification": "The CIFAR100 dataset was also used in the experimental evaluation of the models.",
          "quote": "Our experiments demonstrate that rediscretized ARRNs have identical performance to nonrediscretized ARRNs; that rediscretized ARRNs have vastly lower inference time than nonrediscretized ARRNs ; and that ARRNs are robust to low bandwidth signals. We construct a pair of experimental setups each evaluated on the CIFAR10 and CIFAR100 (Krizhevsky et al., 2009) image classification datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is the foundational paper for the CIFAR100 dataset.",
          "quote": "CIFAR10 and CIFAR100 (Krizhevsky et al., 2009)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1359,
    "prompt_tokens": 8787,
    "total_tokens": 10146
  }
}