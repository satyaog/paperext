{
  "paper": "2308.16557.txt",
  "words": 13837,
  "extractions": {
    "title": {
      "value": "Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing",
      "justification": "The title of the paper is explicitly mentioned at the top.",
      "quote": "Effective Test Generation Using Pre-trained Large Language Models and Mutation Testing"
    },
    "description": "This paper introduces MuTAP, a technique that employs pre-trained Large Language Models (LLMs) to improve the effectiveness of test cases generated for software testing. By augmenting prompts with surviving mutants from mutation testing, MuTAP enhances the bug-revealing capabilities of test cases. The approach was shown to outperform state-of-the-art automated test generation tools in multiple benchmarks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results showing the effectiveness of MuTAP compared to other methods.",
      "quote": "Our results show that our proposed method is able to detect up to 28% more faulty human-written code snippets."
    },
    "primary_research_field": {
      "name": {
        "value": "Software Testing",
        "justification": "The main focus of the paper is on improving test case generation and effectiveness in software testing.",
        "quote": "The goal of automated test generation tools is to ease the development of tests by suggesting efficient bug-revealing tests."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Mutation Testing",
          "justification": "The paper leverages mutation testing techniques to improve test generation.",
          "quote": "To improve over this limitation, in this paper, we introduce MuTAP (Mutation Test case generation using Augmented Prompt) for improving the effectiveness of test cases generated by LLMs in terms of revealing bugs by leveraging mutation testing."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Machine Learning Code Generation",
          "justification": "The paper employs pre-trained Large Language Models for generating code, including test cases.",
          "quote": "Recently, researchers have leveraged Large Language Models (LLMs) of code to generate unit tests."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Codex",
          "justification": "Codex is one of the LLMs employed in MuTAP for code-related tasks.",
          "quote": "We employ two types of LLMs as the LLMC of MuTAP: Codex, which is designed for code-related tasks, and llama-2-chat, which is optimized for dialog use cases and versatile enough to accommodate a range of tasks, including programming."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Codex is used but not contributed in this paper.",
          "quote": "We employ two types of LLMs as the LLMC of MuTAP: Codex, which is designed for code-related tasks, and llama-2-chat, which is optimized for dialog use cases and versatile enough to accommodate a range of tasks, including programming."
        },
        "is_executed": {
          "value": 1,
          "justification": "Codex is executed to generate test cases as part of the study.",
          "quote": "The initial prompt generated by zero-shot technique contains three units, following the approach in [29]. The component indicated by 1 in Figure 1 shows an example of such a prompt. The first unit in this component is an instruction in a natural language named ùêºùëÅùëÜ 1 and it clarifies the task by asking: ‚ÄúGenerate test case for the following code‚Äù."
        },
        "is_compared": {
          "value": 1,
          "justification": "Codex's performance is compared with that of another LLM, llama-2-chat, and also with conventional tools.",
          "quote": "MuTAP, using both llama-2-chat and Codex, demonstrates better performance compared to Pynguin in terms of killing mutants and detecting buggy code."
        },
        "referenced_paper_title": {
          "value": "Evaluating Large Language Models Trained on Code",
          "justification": "The reference paper for Codex is mentioned in the reference section.",
          "quote": "Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al., 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374 ."
        }
      },
      {
        "name": {
          "value": "llama-2-chat",
          "justification": "llama-2-chat is the other LLM employed in MuTAP for versatile tasks including programming.",
          "quote": "We employ two types of LLMs as the LLMC of MuTAP: Codex, which is designed for code-related tasks, and llama-2-chat, which is optimized for dialog use cases and versatile enough to accommodate a range of tasks, including programming."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "llama-2-chat is used but not contributed in this paper.",
          "quote": "We employ two types of LLMs as the LLMC of MuTAP: Codex, which is designed for code-related tasks, and llama-2-chat, which is optimized for dialog use cases and versatile enough to accommodate a range of tasks, including programming."
        },
        "is_executed": {
          "value": 1,
          "justification": "llama-2-chat is executed to generate test cases as part of the study.",
          "quote": "The initial prompt generated by zero-shot technique contains three units, following the approach in [29]. The component indicated by 1 in Figure 1 shows an example of such a prompt. The first unit in this component is an instruction in a natural language named ùêºùëÅùëÜ 1 and it clarifies the task by asking: ‚ÄúGenerate test case for the following code‚Äù."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of llama-2-chat is compared with Codex and conventional tools.",
          "quote": "MuTAP, using both llama-2-chat and Codex, demonstrates better performance compared to Pynguin in terms of killing mutants and detecting buggy code."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open Foundation and Fine-tuned Chat Models",
          "justification": "The reference paper for llama-2-chat is mentioned in the reference section.",
          "quote": "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al., 2023. Llama 2: Open Foundation and Fine-tuned Chat Models. arXiv preprint arXiv:2307.09288 ."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "HumanEval",
          "justification": "HumanEval is one of the benchmarks used for evaluating MuTAP.",
          "quote": "We evaluate MuTAP on both synthetic bugs of 164 PUTs [12] and 1710 buggy programs collected from a Python bug repairing benchmark [23]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Evaluating Large Language Models Trained on Code",
          "justification": "The reference paper for HumanEval is mentioned in the reference section.",
          "quote": "Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al., 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374 ."
        }
      },
      {
        "name": {
          "value": "Refactory",
          "justification": "Refactory is the other benchmark used for evaluating MuTAP.",
          "quote": "We evaluate MuTAP on both synthetic bugs of 164 PUTs [12] and 1710 buggy programs collected from a Python bug repairing benchmark [23]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Re-factoring based program repair applied to programming assignments",
          "justification": "The reference paper for Refactory is mentioned in the reference section.",
          "quote": "Hu, Y., Ahmed, U.Z., Mechtaev, S., Leong, B., Roychoudhury, A., 2019. Re-factoring based program repair applied to programming assignments, in: 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), IEEE. pp. 388‚Äì398."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "MutPy",
          "justification": "MutPy is used for mutation testing in the study.",
          "quote": "For this purpose, we use MutPy version 2.0 [21]. MutPy is a MT tool for code in Python 3.3+. It benefits from different mutation operators to generate the mutants."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MutPy: A Mutation Testing Tool for Python 3.x Source Code",
          "justification": "The reference paper for MutPy is mentioned in the reference section.",
          "quote": "Ha≈Ças, K., 2019. Mutpy: a mutation testing tool for Python 3.x source code."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1838,
    "prompt_tokens": 23993,
    "total_tokens": 25831
  }
}