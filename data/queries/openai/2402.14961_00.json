{
  "paper": "2402.14961.txt",
  "words": 7505,
  "extractions": {
    "title": {
      "value": "Reinforcement Learning with Elastic Time Steps",
      "justification": "It's the title provided in the prompt.",
      "quote": "Reinforcement Learning with Elastic Time Steps"
    },
    "description": "The paper presents a novel off-policy actor-critic algorithm named Soft Elastic Actor-Critic (SEAC) that aims to improve Reinforcement Learning (RL) by introducing elastic time steps with a variable duration. This allows the agent to adapt its control frequency based on the situation, thereby optimizing computational resources and data usage. SEAC is evaluated in both a Newtonian kinematics maze navigation task and a 3D racing video game called Trackmania, where it outperforms the baseline Soft Actor-Critic (SAC) in energy efficiency and training stability.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves empirical evaluations of the proposed SEAC algorithm in simulation environments including a maze navigation task and a racing video game, where performance metrics are compared with SAC and CTCO models.",
      "quote": "We evaluate SEAC’s capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania. SEAC outperforms the SAC baseline in terms of energy efficiency and overall time management, and most importantly without the need to identify a control frequency for the learned controller."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper proposes and evaluates a new algorithm within the domain of Reinforcement Learning, focusing on the use of elastic time steps.",
        "quote": "We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Robotics",
          "justification": "The paper discusses the application of RL in robotics, specifically in controlling agents through dynamic control frequencies.",
          "quote": "Traditional Reinforcement Learning (RL) algorithms are usually applied in robotics to learn controllers that act with a fixed control rate."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Simulation",
          "justification": "Evaluations are performed in simulation environments such as the Newtonian kinematics maze and the 3D racing game Trackmania.",
          "quote": "We evaluate SEAC’s capabilities in simulation in a Newtonian kinematics maze navigation task and on a 3D racing video game, Trackmania."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Soft Elastic Actor-Critic (SEAC)",
          "justification": "SEAC is the primary model introduced and evaluated in the paper, emphasizing elastic time steps for RL.",
          "quote": "We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue."
        },
        "aliases": [
          "SEAC"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The SEAC model is the primary contribution of this research paper.",
          "quote": "We propose Soft Elastic Actor-Critic (SEAC), a novel off-policy actor-critic algorithm to address this issue."
        },
        "is_executed": {
          "value": 1,
          "justification": "The SEAC model was executed on a computer with NVIDIA RTX 4070 GPU for the evaluations.",
          "quote": "We conducted experiments with SEAC on Trackmania for over 742.3 hours. These experiments were conducted on a I5-13600K computer with an NVIDIA RTX 4070 GPU."
        },
        "is_compared": {
          "value": 1,
          "justification": "SEAC is compared with the SAC baseline and the CTCO model in terms of performance metrics such as energy efficiency and training speed.",
          "quote": "We also compared SEAC with a similar approach, the Continuous-Time Continuous-Options (CTCO) model, and SEAC resulted in better task performance."
        },
        "referenced_paper_title": {
          "value": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
          "justification": "SEAC is compared with the SAC algorithm, which is referenced in the paper.",
          "quote": "We also compared SEAC with a similar approach, the Continuous-Time Continuous-Options (CTCO) model, and SEAC resulted in better task performance."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Newtonian Kinematics Maze",
          "justification": "The Newtonian Kinematics Maze is used to evaluate the SEAC algorithm by testing its capabilities in a simulated maze environment.",
          "quote": "We validate SEAC on two maze environments based on Newtonian kinematics as well as a racing game (TrackMania 2023), comparing it with SAC [10] at different fixed frequencies."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "An Optimal Algorithm for Finding Segments Intersections",
          "justification": "The Newtonian Maze environment is based on Newtonian kinematics principles, and intersection judgment methods referred to in the research.",
          "quote": "We employ the line segment intersection judgment method to ascertain whether the agent traverses through walls, as outlined by Balaban [26]."
        }
      },
      {
        "name": {
          "value": "TrackMania",
          "justification": "The TrackMania 2023 3D racing game is used to validate and test the SEAC algorithm's performance in a dynamic and real-time environment.",
          "quote": "We validate SEAC on two maze environments based on Newtonian kinematics as well as a racing game (TrackMania 2023), comparing it with SAC [10] at different fixed frequencies."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Trackmania main page",
          "justification": "The reference for the TrackMania racing game is given for the evaluation of the SEAC model.",
          "quote": "Trackmania: Trackmania is a real-time racing game. This dynamic environment is ideal for testing SEAC’s strategic planning, adaptability, and real-time decision-making skills."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Gymnasium",
          "justification": "The Gymnasium library, also known as OpenAI Gym, is used for implementing and testing the maze environment in the SEAC evaluation.",
          "quote": "Maze Environment: we use a maze based on Newtonian Kinematics with two maps, implemented in gymnasium."
        },
        "aliases": [
          "OpenAI Gym",
          "Gym"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Gymnasium main page",
          "justification": "The paper references the Gymnasium library for the maze environment.",
          "quote": "Maze Environment: we use a maze based on Newtonian Kinematics with two maps, implemented in gymnasium [25]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1293,
    "prompt_tokens": 12922,
    "total_tokens": 14215
  }
}