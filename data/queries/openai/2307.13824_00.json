{
  "paper": "2307.13824.txt",
  "words": 8595,
  "extractions": {
    "title": {
      "value": "Offline Reinforcement Learning with On-Policy Q-Function Regularization",
      "justification": "This is the main title presented on the first page of the paper.",
      "quote": "Offline Reinforcement Learning with On-Policy Q-Function Regularization"
    },
    "description": "The paper addresses the core challenge of offline reinforcement learning (RL) which is handling the extrapolation error caused by the distribution shift between the history dataset and the desired policy. It proposes regularizing towards the Q-function of the behavior policy instead of the behavior policy itself, and introduces two algorithms leveraging this approach. The proposed methods show strong performance on the D4RL benchmarks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments to demonstrate the performance of the proposed algorithms on the D4RL benchmarks.",
      "quote": "We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper discusses challenges and proposes solutions within the context of reinforcement learning, specifically offline reinforcement learning.",
        "quote": "The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy."
      },
      "aliases": [
        "RL",
        "Offline Reinforcement Learning",
        "Batch Reinforcement Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Policy Regularization",
          "justification": "The paper focuses on regularizing the learning policy towards the Q-function of the behavior policy to address the challenges in offline reinforcement learning.",
          "quote": "In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Q-Function Estimation",
          "justification": "The methodology involves estimating the Q-function of the behavior policy which is key to the proposed solutions.",
          "quote": "Estimating Qπb is easier than πb . Qπb can be estimated by Qsarsa via minimizing a SARSA-style objective"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "TD3",
          "justification": "TD3 is one of the baseline models mentioned in the paper for comparison.",
          "quote": "vanilla TD3 method from online RL (Fujimoto et al., 2018)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "TD3 is used as a baseline for comparison and is referenced from prior work, not contributed by this paper.",
          "quote": "vanilla TD3 method from online RL (Fujimoto et al., 2018)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes results from running the TD3 model as a baseline for comparison.",
          "quote": "a comparison between our Qsarsa-regularized TD3 method (TD3 with Qsarsa), the vanilla TD3 method"
        },
        "is_compared": {
          "value": 1,
          "justification": "TD3 is used as a baseline for numerical comparison with the proposed model.",
          "quote": "a comparison between our Qsarsa-regularized TD3 method (TD3 with Qsarsa), the vanilla TD3 method"
        },
        "referenced_paper_title": {
          "value": "Addressing function approximation error in actor-critic methods",
          "justification": "This is the referenced paper for the TD3 model, as cited in the paper.",
          "quote": "vanilla TD3 method from online RL (Fujimoto et al., 2018)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "D4RL",
          "justification": "D4RL is the benchmark dataset used in the paper to demonstrate the performance of the proposed methods.",
          "quote": "demonstrate they exhibit strong performance on the D4RL benchmarks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "D4rl: Datasets for deep data-driven reinforcement learning",
          "justification": "The cited reference for D4RL mentioned in the paper.",
          "quote": "D4RL benchmark (Fu et al., 2020)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Acme",
          "justification": "The Acme framework is mentioned as the source for implementations of the baselines used in the experiments.",
          "quote": "We re-run the implementations in the Acme framework (Hoffman et al., 2020) to keep an identical evaluation process for all methods."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Acme: A research framework for distributed reinforcement learning",
          "justification": "This is the referenced paper for the Acme framework as cited in the paper.",
          "quote": "Acme framework (Hoffman et al., 2020)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 948,
    "prompt_tokens": 17776,
    "total_tokens": 18724
  }
}