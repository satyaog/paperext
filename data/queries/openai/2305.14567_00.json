{
  "paper": "2305.14567.txt",
  "words": 12860,
  "extractions": {
    "title": {
      "value": "Memory Efficient Neural Processes via Constant Memory Attention Block",
      "justification": "This is the title of the paper.",
      "quote": "Memory Efficient Neural Processes via Constant Memory Attention Block"
    },
    "description": "The paper proposes Constant Memory Attentive Neural Processes (CMANPs), an efficient variant of Neural Processes for memory-constrained environments. It introduces a novel attention block, Constant Memory Attention Block (CMAB), that performs computations in constant memory and updates efficiently. The paper demonstrates that CMANPs achieve state-of-the-art performance on several benchmarks while being significantly more memory efficient than existing methods.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experiments comparing the proposed model CMANPs against various baselines on multiple benchmarks, demonstrating its performance and efficiency.",
      "quote": "Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Efficient Deep Learning",
        "justification": "The primary research focus is on developing memory-efficient variants of Neural Processes for use in low-resource settings.",
        "quote": "In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention."
      },
      "aliases": [
        "Efficient Algorithms",
        "Memory Efficient Models"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Meta-learning",
          "justification": "The paper focuses on Neural Processes, which fall under the category of meta-learning methods.",
          "quote": "Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty."
        },
        "aliases": [
          "Meta-Learning"
        ]
      },
      {
        "name": {
          "value": "Attention Mechanism",
          "justification": "The paper introduces a novel attention block to improve memory efficiency.",
          "quote": "Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates."
        },
        "aliases": [
          "Attention",
          "Attention Blocks"
        ]
      },
      {
        "name": {
          "value": "Low-Resource Machine Learning",
          "justification": "The paper aims to develop models particularly useful in low-resource settings such as mobile phones and IoT devices.",
          "quote": "Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings."
        },
        "aliases": [
          "Low-Resource Settings",
          "Mobile AI"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "CMANP",
          "justification": "CMANP (Constant Memory Attentive Neural Processes) is the primary model introduced in the paper.",
          "quote": "Tackling this issue, we propose Constant Memory Attentive Neural Processes (CMANPs), a novel attention-based NP that is competitive with prior state-of-the-art while only requiring a constant amount of memory."
        },
        "aliases": [
          "Constant Memory Attentive Neural Processes"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "CMANP is a new model proposed and evaluated in this paper.",
          "quote": "In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was empirically evaluated on multiple benchmarks.",
          "quote": "Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares CMANP to other state-of-the-art models on several benchmarks.",
          "quote": "We compare CMANPs against a large variety of members of the Neural Process family on standard NP benchmarks."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "CMANP is introduced in this paper, so there is no referenced paper for this model.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "CMAB",
          "justification": "CMAB (Constant Memory Attention Block) is the novel attention mechanism introduced in the paper to improve memory efficiency.",
          "quote": "Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates."
        },
        "aliases": [
          "Constant Memory Attention Block"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "CMAB is a key contribution of this paper.",
          "quote": "Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results for the CMAB as part of the CMANP model.",
          "quote": "In this section, we introduce Constant Memory Attentive Neural Processes (CMANPs), a memory efficient variant of Neural Processes (Figure 2) that leverages the stacked CMAB blocks for efficiency."
        },
        "is_compared": {
          "value": 0,
          "justification": "CMAB itself is not compared against other models, but it is a component of the CMANP model, which is compared.",
          "quote": "We compare CMANPs against a large variety of members of the Neural Process family on standard NP benchmarks."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "CMAB is a novel attention block proposed in this paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CelebA",
          "justification": "CelebA is one of the datasets used for evaluating the proposed model.",
          "quote": "The results show that CMANP-AND achieves clear state-of-the-art results on CelebA (32x32), CelebA (64x64), and CelebA (128x128) while being scalable to more data points than prior Not-Diagonal variants."
        },
        "aliases": [
          "CelebA Dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep learning face attributes in the wild",
          "justification": "The CelebA dataset is a well-known dataset referenced in the work.",
          "quote": "CelebA (Liu et al., 2015) comprises coloured images of celebrity faces. Methods are evaluated on various resolutions to show the scalability of the methods."
        }
      },
      {
        "name": {
          "value": "EMNIST",
          "justification": "EMNIST is another dataset used for evaluating the proposed model.",
          "quote": "The results show that CMANP-AND achieves clear state-of-the-art results on CelebA (32x32), CelebA (64x64), and CelebA (128x128) while being scalable to more data points than prior Not-Diagonal variants. Furthermore, CMANP-AND achieves results competitive with state-of-the-art EMNIST."
        },
        "aliases": [
          "EMNIST Dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "EMNIST: Extending MNIST to handwritten letters",
          "justification": "The paper references the original EMNIST dataset paper.",
          "quote": "EMNIST (Cohen et al., 2017) comprises black and white images of handwritten letters of 32 Ã— 32 resolution."
        }
      },
      {
        "name": {
          "value": "Mooc",
          "justification": "Mooc is used in the Temporal Point Processes experiments section for evaluating the extension of CMAB beyond Neural Processes.",
          "quote": "Mooc Dataset (Kumar et al., 2019) comprises of 7, 047 sequences. Each sequence contains the action times of an individual user of an online Mooc course with 98 categories for the marks."
        },
        "aliases": [
          "Mooc Dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks",
          "justification": "Mooc is a dataset referenced in the work and used for experimentation.",
          "quote": "Mooc Dataset (Kumar et al., 2019) comprises of 7, 047 sequences. Each sequence contains the action times of an individual user of an online Mooc course with 98 categories for the marks."
        }
      },
      {
        "name": {
          "value": "Reddit",
          "justification": "Reddit is used in the Temporal Point Processes experiments section for evaluating the extension of CMAB beyond Neural Processes.",
          "quote": "Reddit Dataset (Kumar et al., 2019) comprises of 10, 000 sequences. Each sequence contains the action times from the most active users with marks being one of the 984 the subreddit categories of each sequence."
        },
        "aliases": [
          "Reddit Dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks",
          "justification": "Reddit is a dataset referenced in the work and used for experimentation.",
          "quote": "Reddit Dataset (Kumar et al., 2019) comprises of 10, 000 sequences. Each sequence contains the action times from the most active users with marks being one of the 984 the subreddit categories of each sequence."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments in the paper are implemented using PyTorch.",
          "quote": "The models were implemented using the PyTorch library."
        },
        "aliases": [
          "torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "The paper references the original PyTorch library paper.",
          "quote": "PyTorch: An imperative style, high-performance deep learning library."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1887,
    "prompt_tokens": 23243,
    "total_tokens": 25130
  }
}