{
  "paper": "2302.06540.txt",
  "words": 7564,
  "extractions": {
    "title": {
      "value": "Imitation from Observation With Bootstrapped Contrastive Learning",
      "justification": "It is the exact title of the paper.",
      "quote": "Imitation from Observation With Bootstrapped Contrastive Learning"
    },
    "description": "This paper presents BootIfOL, a novel algorithm within the Imitation from Observation (IfO) paradigm. It focuses on training autonomous agents in a Markov Decision Process (MDP) using only visual demonstrations of expert trajectories. This approach leverages contrastive learning to derive a reward function that measures the similarity between agent and expert behaviors. The primary goal is to improve the agent's learning efficiency and effectiveness in various control tasks using a limited number of expert demonstrations.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experimental evaluations on various control tasks to demonstrate the effectiveness of the proposed BootIfOL algorithm.",
      "quote": "We evaluate our approach on a variety of control tasks showing that we can train effective policies using a limited number of demonstrative trajectories, greatly improving on prior approaches that consider raw observations."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary research focus is on training autonomous agents using reinforcement learning paradigms, specifically within the Imitation from Observation (IfO) framework.",
        "quote": "...we train the reward function progressively with new visual trajectories generated by the agent. In contrast to [1], our method trains the trajectory encoding function on a set of failure trajectories before the agent training starts."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper involves using visual demonstrations and pixel-based learning to train agents, which is central to the field of Computer Vision.",
          "quote": "In this paper, we adopt a challenging, but more realistic problem formulation, learning control policies that operate on a learned latent space with access only to visual demonstrations of an expert completing a task."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Imitation Learning",
          "justification": "The paper's core focus is on Imitation from Observation (IfO), which is a subset of Imitation Learning that involves learning from visual demonstrations without access to action data.",
          "quote": "Imitation from observation (IfO) is a learning paradigm that consists of training autonomous agents in a Markov Decision Process (MDP) by observing expert demonstrations without access to its actions."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BootIfOL",
          "justification": "BootIfOL is the main algorithm proposed in the paper for Imitation from Observation using bootstrapped contrastive learning.",
          "quote": "We present BootIfOL, an IfO algorithm that aims to learn a reward function that takes an agent trajectory and compares it to an expert, providing rewards based on similarity to agent behavior and implicit goal."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "BootIfOL is the novel algorithm introduced and evaluated in the research paper.",
          "quote": "In this paper, we present BootIfOL, a novel reward learning IfO algorithm..."
        },
        "is_executed": {
          "value": true,
          "justification": "The BootIfOL algorithm is executed as part of the experimental evaluations.",
          "quote": "We evaluate our approach on a variety of control tasks showing that we can train effective policies using a limited number of demonstrative trajectories, greatly improving on prior approaches that consider raw observations."
        },
        "is_compared": {
          "value": true,
          "justification": "BootIfOL is compared to several other methods such as Context Translation (CT), ViRL and GAIfO in the experimental results.",
          "quote": "We compare our algorithm to the Context Translation (CT) [17] method for IfO by varying the number n of expert trajectory samples used at each training episode to predict the CT agent trajectory. We also baseline against ViRL [1] and GAIfO [26]."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "This is a novel model introduced in this paper.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Deepmind Control Suite",
          "justification": "This dataset is used in the experimental evaluation part of the BootIfOL algorithm.",
          "quote": "Our method successfully solves a range of tasks in the Deepmind Control Suite [23] and the Meta-world environment [31], approaching episodic rewards of task experts."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deepmind control suite",
          "justification": "This is the referenced paper for the Deepmind Control Suite.",
          "quote": "Deepmind control suite"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "DrQ-v2",
          "justification": "DrQ-v2 is explicitly mentioned as the Reinforcement Learning algorithm used to train the agent policy in the paper.",
          "quote": "In this work, we use DrQ-v2 [30] RL algorithm to train the agent policy, π, and its associated q-value function, Q, though our method is agnostic to the choice of the RL algorithm."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Mastering visual continuous control: Improved data-augmented reinforcement learning",
          "justification": "This is the referenced paper for the DrQ-v2 algorithm.",
          "quote": "In this work, we use DrQ-v2 [30] RL algorithm to train the agent policy, π, and its associated q-value function, Q, though our method is agnostic to the choice of the RL algorithm."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1096,
    "prompt_tokens": 13038,
    "total_tokens": 14134
  }
}