{
  "paper": "2310.02567.txt",
  "words": 9458,
  "extractions": {
    "title": {
      "value": "Improving Automatic VQA Evaluation Using Large Language Models",
      "justification": "The title directly reflects the main focus and contribution of the paper.",
      "quote": "Improving Automatic VQA Evaluation Using Large Language Models"
    },
    "description": "This paper proposes a new metric for automatic Visual Question Answering (VQA) evaluation named LAVE (LLM-Assisted VQA Evaluation), which leverages instruction-tuned large language models (LLMs) to better correlate with human judgment compared to existing metrics.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper conducts experiments analyzing the effectiveness of the proposed LAVE metric by collecting human judgments and comparing them to existing metrics.",
      "quote": "We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper deals with Visual Question Answering (VQA), a key benchmark in the field of Computer Vision.",
        "quote": "Visual question answering (VQA) (Antol et al. 2015) has become an essential benchmark for assessing the progress of multimodal vision-language systems."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Visual Question Answering",
          "justification": "The study focuses specifically on improving the evaluation of Visual Question Answering systems.",
          "quote": "We propose a novel automatic VQA evaluation metric, LAVE (LLM-Assisted VQA Evaluation), which leverages the in-context learning capabilities of instruction-tuned LLMs."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "LAVE",
          "justification": "The metric introduced by the paper is called LAVE (LLM-Assisted VQA Evaluation).",
          "quote": "We propose a novel automatic VQA evaluation metric, LAVE (LLM-Assisted VQA Evaluation), which leverages the in-context learning capabilities of instruction-tuned LLMs."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Evaluation"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "VQAv2",
          "justification": "The VQAv2 dataset is one of the datasets on which the proposed metric, LAVE, is evaluated.",
          "quote": "We use these VQA models to generate answers for three VQA datasets: VQAv2 (Goyal et al. 2017), VG-QA (Krishna et al. 2017) and OK-VQA (Marino et al. 2019)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "VG-QA",
          "justification": "The VG-QA dataset is used for evaluating the performance of LAVE.",
          "quote": "We use these VQA models to generate answers for three VQA datasets: VQAv2 (Goyal et al. 2017), VG-QA (Krishna et al. 2017) and OK-VQA (Marino et al. 2019)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "OK-VQA",
          "justification": "The OK-VQA dataset is mentioned as one of the benchmarks for evaluating the proposed metric, LAVE.",
          "quote": "We use these VQA models to generate answers for three VQA datasets: VQAv2 (Goyal et al. 2017), VG-QA (Krishna et al. 2017) and OK-VQA (Marino et al. 2019)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "HuggingFace Transformers",
          "justification": "The paper mentions using the HuggingFace Transformers library for implementation purposes.",
          "quote": "We leverage the HuggingFace Transformers’ (Wolf et al. 2020) implementation of Flan-T5 and LLaMA (for Vicuna), and use GPT-3.5-Turbo through OpenAI’s API1."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 822,
    "prompt_tokens": 16129,
    "total_tokens": 16951
  }
}