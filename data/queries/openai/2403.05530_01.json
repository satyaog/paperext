{
  "paper": "2403.05530.txt",
  "words": 30556,
  "extractions": {
    "title": {
      "value": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "justification": "The title is clearly stated on the first page of the paper.",
      "quote": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
    },
    "description": "This paper presents Gemini 1.5 Pro, a multimodal mixture-of-experts model capable of recalling and reasoning over information from millions of tokens of context, achieving high performance in long-context tasks across text, video, and audio.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper provides empirical results on the performance of Gemini 1.5 Pro and compares it with other models.",
      "quote": "In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, ... achieving high performance in long-context tasks across text, video, and audio."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on developing and evaluating a deep learning model, specifically a multimodal mixture-of-experts model.",
        "quote": "We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro. This is our first release from Gemini 1.5, a new family of highly-capable multimodal models which incorporates a novel mixture-of-experts architecture..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The research involves a model handling multiple data modalities including text, video, and audio.",
          "quote": "Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs), and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours of video, and almost five days long of audio."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Gemini 1.5 Pro",
          "justification": "This is the main model introduced and evaluated in the paper.",
          "quote": "We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Gemini 1.0 Ultra",
          "justification": "This model is used for comparison with the proposed model, Gemini 1.5 Pro.",
          "quote": "Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Used"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Claude 2.1",
          "justification": "This model is used for comparison in the experiments evaluating long-context capabilities.",
          "quote": "Gemini 1.5 Pro achieves a 100% recall at 200k tokens, surpassing Claude 2.1’s 98%. This 100% recall is maintained up to 530k tokens, and recall is 99.7% at 1M tokens."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Used"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-4 Turbo",
          "justification": "This model is used for comparison in the experiments evaluating long-context capabilities.",
          "quote": "Gemini 1.5 Pro sets a new state-of-the-art of 64.5% accuracy on EgoSchema using only 16 frames (vs 55.6% for GPT-4V (Balažević et al., 2024))."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Used"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-4V",
          "justification": "This model is used for comparison in the video needle-in-a-haystack task.",
          "quote": "Gemini 1.5 Pro sets a new state-of-the-art of 64.5% accuracy on EgoSchema using only 16 frames (vs 55.6% for GPT-4V (Balažević et al., 2024))."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Used"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "USM",
          "justification": "This model is used for comparison in automatic speech recognition tasks.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023)... achieving a WER of 8.8%."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Used"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Whisper",
          "justification": "This model is used for comparison in automatic speech recognition tasks.",
          "quote": "Note that ASR tasks report a word error rate (WER) metric, where a lower number is better. The Table 5 below shows that... Whisper is not robust to long segments and hence requires audio to be segmented every 30 seconds to achieve a WER of 7.3%."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Used"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "LLaMA",
          "justification": "This model is used for comparison in terms of training data memorization.",
          "quote": "We found that LLaMA and Mistral emit training data at a rate around 0.1% with this attack."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Used"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MATH",
          "justification": "This dataset is used to evaluate mathematical problem-solving capabilities.",
          "quote": "We find that 1.5 Pro consistently outperforms both 1.0 Ultra and 1.0 Pro on grade-school math (i.e., GSM8K) and even shows material improvement over the more demanding benchmarks where there is more headroom for improvement, i.e., +3.5% over 1.0 Ultra for middle- and high-school math problems (i.e., Hendrycks MATH)..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Les Misérables",
          "justification": "This dataset is used for long-document QA tasks in the paper.",
          "quote": "We test the model's ability to answer them correctly when the entire 1,462 page book (i.e., 710K tokens) is provided as input."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "FLEURS",
          "justification": "This dataset is used to evaluate automatic speech recognition in multiple languages.",
          "quote": "On FLEURS we evaluate a subset of 55 languages for which we have coverage our training data... On AST we report BLEU scores."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "WMT23",
          "justification": "This dataset is used for evaluating multilingual translation capabilities.",
          "quote": "For our multilingual evaluations we use a multilingual math reasoning (MGSM; Shi et al., 2023a) benchmark and a machine translation benchmark (WMT23; Kocmi et al., 2023) which was constructed after the model’s training data cut-off hence minimizing test set leakage risks."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "EgoSchema",
          "justification": "This dataset is used for evaluating video understanding capabilities.",
          "quote": "The publicly available question answering benchmark with the longest videos is EgoSchema (Mangalam et al., 2023)... we introduce a new benchmark, 1H-VideoQA, composed of 125 five-way multiple-choice questions over public videos 40-105 minutes long."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "COVOST-2",
          "justification": "This dataset is used for evaluating speech-to-text translation capabilities.",
          "quote": "On CoVoST-2 we evaluate on translating speech in 20 languages into English, reporting on the subset of languages that were seen by the model during pre-training."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BigBench",
          "justification": "This dataset is used to evaluate reasoning capabilities.",
          "quote": "On reasoning tasks, 1.5 Pro outperforms 1.0 Pro by a large margin and shows a comparable performance to 1.0 Ultra, slightly underperforming on DROP and slightly outperforming on BBH."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "The training of Gemini 1.5 Pro was done using JAX.",
          "quote": "Training was done using JAX (Bradbury et al., 2018) and ML Pathways (Dean, 2021)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SymPy",
          "justification": "The SymPy library is used for solving mathematical problems and integrating into the evaluation of the model.",
          "quote": "We present a simple example of MATH Intermediate Algebra problem where the solution involved SymPy. Here is the Python code to solve the problem: ... import sympy as sp"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2902,
    "prompt_tokens": 52052,
    "total_tokens": 54954
  }
}