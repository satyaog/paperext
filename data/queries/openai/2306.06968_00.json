{
  "paper": "2306.06968.txt",
  "words": 10670,
  "extractions": {
    "title": {
      "value": "Can Forward Gradient Match Backpropagation?",
      "justification": "Title is explicitly provided in the research paper.",
      "quote": "Can Forward Gradient Match Backpropagation?"
    },
    "description": "The paper explores the potential of using forward gradients, which involve directional derivatives in forward differentiation mode, as an alternative to the traditional backpropagation algorithm for training neural networks. The study conducts a rigorous examination of various combinations of gradient targets and guesses for a standard computer vision neural network to understand if this method can offer competitive accuracy while avoiding the challenges associated with backpropagation.",
    "type": {
      "value": "Empirical",
      "justification": "The paper presents experimental results and data that evaluate the performance of different methods rather than purely theoretical or mathematical analysis.",
      "quote": "For a standard computer vision neural network, we conduct a rigorous study systematically covering a variety of combinations of gradient targets and gradient guesses, including those previously presented in the literature."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The research focuses on exploring forward gradients as an alternative method for training neural networks, which falls under the subfield of Machine Learning.",
        "quote": "Stochastic Gradient Descent (SGD) (Amari, 1967; Bottou, 2012) using end-to-end backpropagation for gradient computation is the ubiquitous training method of state-of-the-art Deep Neural Networks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The models and experiments in the paper are applied to standard image classification tasks, commonly tackled in the Computer Vision field.",
          "quote": "For a standard computer vision neural network, we conduct a rigorous study systematically covering a variety of combinations of gradient targets and gradient guesses."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet-18",
          "justification": "The paper utilizes the ResNet-18 architecture in its experiments to test the different gradient computation methods.",
          "quote": "Our work attempts to study and understand the limits of Forward Gradient descent on a well-established architecture (Resnet-18) and a difficult computer-vision task (ImageNet32)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet-18 is a well-known pre-existing architecture and is not introduced for the first time in this paper.",
          "quote": "Our work attempts to study and understand the limits of Forward Gradient descent on a well-established architecture (Resnet-18)..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The ResNet-18 model was executed to conduct the experiments and evaluate the Forward Gradient methods.",
          "quote": "The experiments were conducted using standard image classification tasks on well-established architectures like ResNet-18."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of ResNet-18 with Forward Gradient was compared with traditional backpropagation methods.",
          "quote": "We show that Forward Gradient can lead to competitive accuracy with end-to-end training."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "The original reference for ResNet-18 is the paper by He et al., 2016.",
          "quote": "Our work attempts to study and understand the limits of Forward Gradient descent on a well-established architecture (Resnet-18)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used as one of the datasets for training and testing the models in the experiments.",
          "quote": "We considered the CIFAR-10 and ImageNet32 datasets, used with standard data augmentation."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The referenced paper by Krizhevsky, 2009, is the original reference for the CIFAR-10 dataset.",
          "quote": "We considered the CIFAR-10 and ImageNet32 datasets, used with standard data augmentation."
        }
      },
      {
        "name": {
          "value": "ImageNet32",
          "justification": "ImageNet32 is used as another dataset for training and testing the models in the experiments.",
          "quote": "We considered the CIFAR-10 and ImageNet32 datasets, used with standard data augmentation."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "A downsampled variant of ImageNet as an alternative to CIFAR datasets",
          "justification": "The referenced paper by Chrabaszcz et al., 2017, is the original reference for the ImageNet32 dataset.",
          "quote": "We considered the CIFAR-10 and ImageNet32 datasets, used with standard data augmentation."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1149,
    "prompt_tokens": 20182,
    "total_tokens": 21331
  }
}