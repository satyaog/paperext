{
  "paper": "2110.05740.txt",
  "words": 27557,
  "extractions": {
    "title": {
      "value": "Temporal Abstraction in Reinforcement Learning with the Successor Representation",
      "justification": "This is the formal title of the research paper as given.",
      "quote": "Temporal Abstraction in Reinforcement Learning with the Successor Representation"
    },
    "description": "This paper investigates how the successor representation (SR) can serve as a foundation for temporal abstractions in reinforcement learning (RL), particularly focusing on option discovery for temporally-extended exploration. The authors propose the Representation-driven Option Discovery (ROD) cycle, demonstrating its effectiveness through methods like eigenoptions and covering options.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper presents various empirical evaluations of proposed methods and algorithms such as eigenoptions and covering options, explicitly showing their impact on tasks like diffusion time and reward maximization.",
      "quote": "Our empirical evaluation focuses on options discovered for temporally-extended exploration and on the use of the successor representation to combine them."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper is primarily focused on advancements in reinforcement learning, particularly dealing with temporal abstractions and the successor representation.",
        "quote": "In the reinforcement learning problem, an agent interacts with its environment such that the agent receives an observation from the environment and takes an action based on the received observations."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Option Discovery",
          "justification": "The paper contributes significantly to the field of option discovery by presenting and evaluating methods like eigenoptions and covering options.",
          "quote": "In this paper, we argue that the successor representation, which encodes states based on the pattern of state visitation that follows them, can be seen as a natural substrate for the discovery and use of temporal abstractions."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Temporal Abstraction",
          "justification": "The paper deals extensively with temporal abstraction concepts within RL, proposing methods to discover and utilize temporal abstractions.",
          "quote": "Reasoning at multiple levels of temporal abstraction is one of the key attributes of intelligence."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Eigenoptions",
          "justification": "Eigenoptions are central to the proposed methods for option discovery in this paper, utilizing the eigenvectors of the successor representation.",
          "quote": "Eigenoptions are options defined by the eigenvectors of the SR."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "Eigenoptions are one of the new methods proposed by the authors in this paper.",
          "quote": "In this section, we discuss eigenoptions (Machado et al., 2017, 2018)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The method is empirically evaluated in the paper through various experiments.",
          "quote": "Empirically evaluating them and shedding light on decisions every option discovery method should make."
        },
        "is_compared": {
          "value": 1,
          "justification": "Eigenoptions are compared to other methods like covering options across multiple metrics such as diffusion time and reward maximization.",
          "quote": "We present evidence on the potential of instantiating a cycle in which both the representation and the options are constantly refined based on each other (Section 7), and on the synergy of different approaches based on the SR (Section 9)."
        },
        "referenced_paper_title": {
          "value": "A Laplacian Framework for Option Discovery in Reinforcement Learning",
          "justification": "This paper is the primary reference for eigenoptions, as indicated by multiple citations in the current paper.",
          "quote": "In this section, we discuss eigenoptions (Machado et al., 2017, 2018)."
        }
      },
      {
        "name": {
          "value": "Covering Options",
          "justification": "Covering options are another core method introduced in the paper for discovering useful options in RL.",
          "quote": "Covering options are options defined by the bottom eigenvector of the graph Laplacian (i.e., the first PVF)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "Covering options are introduced by the authors in this work as a novel method for option discovery.",
          "quote": "Covering options (Jinnai et al., 2019b)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper empirically evaluates covering options through various experiments.",
          "quote": "Covering options only do so when discovered in closed-form, the setting they were originally introduced."
        },
        "is_compared": {
          "value": 1,
          "justification": "Covering options are compared to other methods like eigenoptions in the paper.",
          "quote": "Eigenoptions tend to be more effective than covering options."
        },
        "referenced_paper_title": {
          "value": "Discovering Options for Exploration by Minimizing Cover Time.",
          "justification": "This paper is the primary reference for covering options, as indicated by multiple citations in the current paper.",
          "quote": "Covering options (Jinnai et al., 2019b)."
        }
      },
      {
        "name": {
          "value": "Option Keyboard",
          "justification": "The paper discusses how the option keyboard can be used to combine multiple eigenoptions.",
          "quote": "The option keyboard, Barreto et al., 2019"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The option keyboard was proposed in a previous work and is built upon in this paper.",
          "quote": "The option keyboard, Barreto et al., 2019"
        },
        "is_executed": {
          "value": 1,
          "justification": "The method is empirically evaluated in the paper for combining eigenoptions.",
          "quote": "The combination of eigenoptions and the option keyboard allow these approaches to heavily benefit from each other."
        },
        "is_compared": {
          "value": 0,
          "justification": "The option keyboard is not compared in the same way as eigenoptions and covering options, but rather used as a tool to combine them.",
          "quote": "We can think of the process above as implementing a mapping from w ∈ Rd to an approximation of the corresponding option π c+"
        },
        "referenced_paper_title": {
          "value": "The Option Keyboard: Combining Skills in Reinforcement Learning",
          "justification": "This is the reference paper for the option keyboard.",
          "quote": "Option Keyboard (Barreto et al., 2019)"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1542,
    "prompt_tokens": 41684,
    "total_tokens": 43226
  }
}