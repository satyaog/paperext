{
  "paper": "2308.08708.txt",
  "words": 41462,
  "extractions": {
    "title": {
      "value": "Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
      "justification": "The title mentioned in the user's prompt and throughout the content",
      "quote": "Consciousness in Artificial Intelligence: Insights from the Science of Consciousness"
    },
    "description": "This report argues for a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness.",
    "type": {
      "value": "empirical study",
      "justification": "The paper systematically evaluates current AI systems against empirical neuroscientific theories of consciousness.",
      "quote": "This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper discusses the implementation of neuroscientific theories of consciousness within deep learning frameworks and evaluates current AI models.",
        "quote": "We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higherorder theories, predictive processing, and attention schema theory."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "AI Consciousness",
          "justification": "The primary focus of the paper is on understanding and assessing AI systems for consciousness based on deep learning and neuroscience.",
          "quote": "Our method for studying consciousness in AI has three main tenets."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Perceiver",
          "justification": "The Perceiver is evaluated concerning its implementation of self-attention and its applicability to global workspace theory.",
          "quote": "The Perceiver architecture allows for sequences of inputs to be processed diachronically, with the latent space state updating with each new input, but also influenced by its previous state."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Transformer",
          "justification": "Transformers are discussed for their architecture featuring self-attention and potential similarities to global workspace theory.",
          "quote": "In a Transformer, an operation called 'self-attention' is used to integrate information from different parts of an input, which are often positions in a sequence."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PaLM-E",
          "justification": "The PaLM-E multilodal model is discussed for its integration of visual and textual inputs to generate outputs, potentially illustrating features of agency and embodiment.",
          "quote": "PaLM-E generates high-level plans while the policy unit provides low-level vision-guided motor control."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DeepMind Adaptive Agent",
          "justification": "AdA uses reinforcement learning for complex task execution in a simulated environment, illustrating elements of agency and adaptive behavior.",
          "quote": "AdA is a large Transformer-based, RL-trained 'adaptive agent'."
        },
        "aliases": [
          "AdA"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "training"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Virtual Rodent",
          "justification": "The Virtual Rodent is examined for learning and embodying physical interactions within a simulated environment.",
          "quote": "The 'virtual rodent' of Merel et al. (2019) is a promising candidate for these attributes."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "training"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is commonly used in training and implementing various deep learning models discussed in the paper.",
          "quote": ""
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "TensorFlow",
          "justification": "TensorFlow is another prominent deep learning library used for implementing models discussed.",
          "quote": ""
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1266,
    "prompt_tokens": 58277,
    "total_tokens": 59543
  }
}