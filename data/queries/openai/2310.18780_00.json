{
  "paper": "2310.18780.txt",
  "words": 24551,
  "extractions": {
    "title": {
      "value": "Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions",
      "justification": "This is the exact title of the paper as given by the user.",
      "quote": "Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions"
    },
    "description": "The paper focuses on improving the efficiency of long convolution sequence models (LCSMs) by distilling them into low-dimensional linear state-space models (SSMs), aiming to achieve O(1) compute and memory cost per token. New methods including rational interpolation and model-order reduction techniques are introduced, along with architectural improvements to the convolution-based Hyena model.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper includes various experiments, model training, and benchmarking, indicating that it is empirical in nature.",
      "quote": "We perform a series of extensive experiments on all variants of LCSM, including pre-trained H3 models of sizes 125 million, 355 million, 1.3 billion and 2.7 billion parameters; Hyena of size 153 million parameters, and MultiHyena of size 153 million parameters."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper specifically focuses on methods and improvements in long convolution sequence models and state-space models, which are core topics within the field of Deep Learning.",
        "quote": "Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers."
      },
      "aliases": [
        "DL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Sequence Modeling",
          "justification": "The paper focuses on long convolution sequence models (LCSMs) and their distillation into state-space models (SSMs) for efficient sequence generation.",
          "quote": "Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers."
        },
        "aliases": [
          "Seq2Seq"
        ]
      },
      {
        "name": {
          "value": "Model Compression",
          "justification": "A significant portion of the paper is dedicated to model distillation, rational interpolation, and model-order reduction techniques to compress long convolution sequence models into state-space models.",
          "quote": "Distilling fast recurrences We introduce LaughingHyena, the first distillation approach for LCSMs that enables recurrent inference without impacting downstream quality."
        },
        "aliases": [
          "Model Distillation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "LaughingHyena",
          "justification": "This model is explicitly introduced and explained as the main contribution of the paper for distilling long convolution sequence models.",
          "quote": "Distilling fast recurrences We introduce LaughingHyena, the first distillation approach for LCSMs that enables recurrent inference without impacting downstream quality."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "LaughingHyena is presented as a novel contribution in the paper.",
          "quote": "We introduce LaughingHyena, the first distillation approach for LCSMs that enables recurrent inference without impacting downstream quality."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes extensive benchmarking and performance metrics, indicating that the model was executed.",
          "quote": "Distilling fast recurrences We introduce LaughingHyena, the first distillation approach for LCSMs that enables recurrent inference without impacting downstream quality."
        },
        "is_compared": {
          "value": 1,
          "justification": "LaughingHyena is compared against other models such as Transformers, Hybrid H3, and Hyena in terms of efficiency and performance.",
          "quote": "At model sizes above one billion parameters, LaughingHyena achieves 10× higher peak throughput over comparable Transformers, and can process larger batch sizes."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "LaughingHyena is a contribution of the current paper.",
          "quote": "Distilling fast recurrences We introduce LaughingHyena, the first distillation approach for LCSMs that enables recurrent inference without impacting downstream quality."
        }
      },
      {
        "name": {
          "value": "Hyena",
          "justification": "The Hyena model is one of the baseline models compared and improved upon in the paper.",
          "quote": "We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The Hyena model is not presented as a novel contribution in this paper.",
          "quote": "We further introduce architectural improvements to convolution-based layers such as Hyena."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Hyena model was trained and its performance was benchmarked against other models.",
          "quote": "To validate the multi-head formulation, we train 150 and 350 million parameter MultiHyena models on The Pile using 8 heads and otherwise the same architecture as equivalent Hyena models."
        },
        "is_compared": {
          "value": 1,
          "justification": "Hyena is frequently used as a baseline for performance comparisons.",
          "quote": "We further introduce architectural improvements to convolution-based layers such as Hyena. The resulting model achieves 10× higher throughput than Transformers and 1.5× higher than Hyena at 1.3B parameters."
        },
        "referenced_paper_title": {
          "value": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
          "justification": "The Hyena model is introduced and elaborated in this referenced paper.",
          "quote": "Michael Poli et al. “Hyena Hierarchy: Towards Larger Convolutional Language Models”. In: (2023). arXiv: 2302.10866"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "The Pile",
          "justification": "The dataset is explicitly mentioned as used for training and benchmarking in the paper.",
          "quote": "We pretrain a suite of MultiHyena language models on The Pile, investigating scaling of perplexity with different amounts of total tokens (5, 10, 15 billion), as well as larger training runs for 300 billion tokens."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
          "justification": "The Pile is introduced and elaborated in this referenced paper.",
          "quote": "Leo Gao et al. “The pile: An 800gb dataset of diverse text for language modeling”. In: (2020). arXiv: 2101.00027"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper involves extensive model training and benchmarking, which are commonly implemented using PyTorch.",
          "quote": "Experimentally, we observe that standard coefficient normalization techniques overly restrict the parameters space and lead to poor distillation performances at reasonable order."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "This reference is used to support the utilization of the PyTorch library.",
          "quote": "Adam Paszke et al. “Automatic differentiation in PyTorch”. In: (2017)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1412,
    "prompt_tokens": 42379,
    "total_tokens": 43791
  }
}