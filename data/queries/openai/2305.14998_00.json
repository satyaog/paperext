{
  "paper": "2305.14998.txt",
  "words": 7814,
  "extractions": {
    "title": {
      "value": "An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics",
      "justification": "This title represents the focus of the research on evaluating the robustness of metrics for image captioning without references.",
      "quote": "Title: An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics"
    },
    "description": "The paper evaluates the robustness of reference-free metrics for image captioning, including CLIPScore, UMIC, and PAC-S, under various conditions to identify their strengths and limitations.",
    "type": {
      "value": "empirical",
      "justification": "The study conducts several controlled experiments to examine the robustness of reference-free metrics in various scenarios, involving data collection, analysis, and evaluation.",
      "quote": "In this work, we evaluate the robustness of these reference-free metrics in scenarios where the correct and incorrect captions have high lexical overlap... several controlled experiments, varying one aspect at a time."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The research is centered around evaluating image captioning metrics, which falls under the domain of Computer Vision.",
        "quote": "In this work, we evaluate the robustness of these reference-free metrics in scenarios where the correct and incorrect captions have high lexical overlap..."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Captioning",
          "justification": "The specific focus of the paper is on evaluating metrics for image captioning.",
          "quote": "Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al.,2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions."
        },
        "aliases": [
          "img-cap"
        ]
      },
      {
        "name": {
          "value": "Evaluation Metrics",
          "justification": "The paper explores the effectiveness of various evaluation metrics in image captioning.",
          "quote": "Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings."
        },
        "aliases": [
          "eval-metrics"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIPScore",
          "justification": "CLIPScore is one of the primary metrics discussed and evaluated in the paper.",
          "quote": "Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper discusses and evaluates CLIPScore but does not claim to have developed it.",
          "quote": "Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021)"
        },
        "is_executed": {
          "value": true,
          "justification": "CLIPScore is evaluated using empirical experiments within the paper.",
          "quote": "We evaluate the robustness of CLIPScore in scenarios..."
        },
        "is_compared": {
          "value": true,
          "justification": "CLIPScore's performance is compared to other metrics like UMIC and PAC-S in the study.",
          "quote": "Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors."
        },
        "referenced_paper_title": {
          "value": "CLIPScore: A reference-free evaluation metric for image captioning",
          "justification": "The paper cites CLIPScore as a reference-free evaluation metric in the context of image captioning.",
          "quote": "CLIPScore (Hessel et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "VQAv2",
          "justification": "The paper mentions using questions from the VQAv2 dataset for generating caption-like sentences.",
          "quote": "We use the questions from the popular VQAv2 (Goyal et al., 2016) dataset"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
          "justification": "The paper references the VQAv2 dataset in the context of visual question answering and generating captions for evaluation.",
          "quote": "We use the questions from the popular VQAv2 (Goyal et al., 2016) dataset"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "GPT-J",
          "justification": "The paper uses GPT-J for generating caption-like sentences by transforming visual question-answer pairs.",
          "quote": "We employ GPT-J prompting to transform visual question-answer pairs into caption-like sentences."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
          "justification": "The paper references GPT-J in the context of generating transformed sentences for evaluation.",
          "quote": "We employ GPT-J prompting to transform visual question-answer pairs into caption-like sentences."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 946,
    "prompt_tokens": 13673,
    "total_tokens": 14619
  }
}