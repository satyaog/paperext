{
  "paper": "E0LWTN1xPX.txt",
  "words": 3998,
  "extractions": {
    "title": {
      "value": "Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning",
      "justification": "This is the exact title mentioned in the provided paper.",
      "quote": "Surprise-Adaptive Intrinsic Motivation for Unsupervised Reinforcement Learning"
    },
    "description": "The paper introduces a novel approach to unsupervised reinforcement learning by proposing an agent that adapts its objective between surprise-minimization and surprise-maximization depending on the entropy conditions of the environment. The agent uses a multi-armed bandit problem to select its objective and demonstrates emergent behaviors in both high- and low-entropy environments.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experiments and empirical validations of the proposed agent in various environments, as shown in the sections detailing experiments and performance evaluations.",
      "quote": "Experiments are conducted to validate the surprise-adaptive agent in both low- and high-entropy environments."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on developing an adaptive reinforcement learning agent that can operate in both high and low-entropy environments.",
        "quote": "However, thus far, no single intrinsic motivation function has succeeded in capturing the complexity of motivation that gives rise to intelligent systems."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Unsupervised Learning",
          "justification": "The research centers around unsupervised reinforcement learning, where the agent learns without access to extrinsic rewards.",
          "quote": "Unsupervised reinforcement learning (URL), or learning without access to an extrinsic reward function, has recently gained significant attention, often as a pretraining method."
        },
        "aliases": [
          "URL"
        ]
      },
      {
        "name": {
          "value": "Intrinsic Motivation",
          "justification": "The paper investigates intrinsic motivation as a method for the agent to develop emergent behaviors.",
          "quote": "A recent focus has been on developing objectives where the agent has no access to extrinsic rewards and instead develops emergent behaviors from an intrinsic motivation alone."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Surprise-Adaptive Agent",
          "justification": "This is the main model proposed and analyzed in the paper, designed to adapt its objective between surprise-minimization and surprise-maximization.",
          "quote": "In this work, we propose an adaptive mechanism to select between maximizing and minimizing surprise in a given environment, based on the agentâ€™s ability to exert control over its entropy conditions, which we frame as a multi-armed bandit problem."
        },
        "aliases": [
          "S-Adapt"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper introduces and validates this new model.",
          "quote": "In this work, we propose an adaptive mechanism to select between maximizing and minimizing surprise in a given environment."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed in various environments to validate its performance.",
          "quote": "Experiments are conducted to validate the surprise-adaptive agent in both low- and high-entropy environments."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the Surprise-Adaptive Agent is compared to other agents such as S-Min, S-Max, and RND.",
          "quote": "Our method (S-Adapt) is compared against exclusively surprise-minimizing (S-Min) and exclusively surprise-maximizing (S-Max) agents."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This is the primary model being introduced in the paper.",
          "quote": "In this work, we propose an adaptive mechanism to select between maximizing and minimizing surprise."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Tetris",
          "justification": "The Tetris environment is used as a high-entropy environment for validating the performance of the proposed agent.",
          "quote": "For the high-entropy environment, we select the Tetris environment used in Berseth et al. [3] while for the low-entropy environment, we construct a maze environment (Maze), in which the agent navigates to a goal."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SMiRL: Surprise Minimizing Reinforcement Learning in Unstable Environments",
          "justification": "The Tetris environment was used in a prior work referenced as Berseth et. al [3].",
          "quote": "For the high-entropy environment, we select the Tetris environment used in Berseth et al. [3]."
        }
      },
      {
        "name": {
          "value": "Maze",
          "justification": "The Maze environment is constructed as a low-entropy environment for validating the performance of the proposed agent.",
          "quote": "For the low-entropy environment, we construct a maze environment (Maze), in which the agent navigates to a goal."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The Maze environment is newly constructed for this paper's experiments.",
          "quote": "For the low-entropy environment, we construct a maze environment (Maze), in which the agent navigates to a goal."
        }
      },
      {
        "name": {
          "value": "MinAtar",
          "justification": "The MinAtar suite is used for more comprehensive evaluations of the proposed agent's ability to adapt to different environments.",
          "quote": "Next, we apply our agent to the MinAtar suite of tasks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MinAtar: An Atari-Inspired Testbed for Thorough and Reproducible Reinforcement Learning Experiments",
          "justification": "The MinAtar suite is used for evaluations and is referenced in the paper as Young and Tian [20].",
          "quote": "Next, we apply our agent to the MinAtar [20] suite of tasks."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "DQN",
          "justification": "The DQN algorithm is the reinforcement learning method used to train the agents in the experiments.",
          "quote": "All agents were trained using DQN [12]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Human-level control through deep reinforcement learning",
          "justification": "The paper references Mnih et al. [12] for the DQN algorithm used in the experiments.",
          "quote": "In our experiments, we use the value-based method DQN [12] to solve Equation 1."
        }
      },
      {
        "name": {
          "value": "Griddly",
          "justification": "The Griddly platform is used for constructing the custom Maze environment.",
          "quote": "We constructed a custom maze environment using the Griddly platform [2]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Griddly: A platform for AI research in games",
          "justification": "The paper references Bamford [2] for the Griddly platform used in the experiments.",
          "quote": "We constructed a custom maze environment using the Griddly platform [2]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1363,
    "prompt_tokens": 7835,
    "total_tokens": 9198
  }
}