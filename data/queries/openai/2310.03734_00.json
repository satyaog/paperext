{
  "paper": "2310.03734.txt",
  "words": 9034,
  "extractions": {
    "title": {
      "value": "LEVERAGING UNPAIRED DATA FOR VISION-LANGUAGE GENERATIVE MODELS VIA CYCLE CONSISTENCY",
      "justification": "The value is directly taken from the title of the research paper.",
      "quote": "LEVERAGING UNPAIRED DATA FOR VISION-LANGUAGE GENERATIVE MODELS VIA CYCLE CONSISTENCY"
    },
    "description": "This paper introduces ITIT (Integrating Image Text), a training paradigm grounded in cycle consistency for vision-language generative models that allows training with unpaired image and text data. ITIT utilizes a joint image-text encoder with distinct image and text decoders enabling bidirectional image-to-text and text-to-image generation. The model leverages a small set of paired image-text data to generate pseudo paired data from unpaired images or texts and imposes cycle consistency between the original and the generated outputs. The paper demonstrates that ITIT achieves performance on par with state-of-the-art models using significantly fewer paired data.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results that demonstrate the performance of the ITIT model on various benchmarks, illustrating its empirical nature.",
      "quote": "Our experiments show that ITIT with unpaired datasets exhibits similar scaling behavior as using high-quality paired data. We demonstrate image generation and captioning performance on par with state-of-the-art text-to-image and image-to-text models with orders of magnitude fewer (only 3M) paired image-text data."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary focus of the paper is on vision-language generative models, which falls under the domain of Computer Vision.",
        "quote": "Current vision-language generative models rely on expansive corpora of paired image-text data to attain optimal performance and generalization capabilities."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generative Models",
          "justification": "The paper focuses on generative models for vision and language using both paired and unpaired data.",
          "quote": "We introduce ITIT (Integrating Image Text): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The paper addresses multimodal learning by integrating both image and text modalities in a single framework.",
          "quote": "ITIT is comprised of a joint image-text encoder with disjoint image and text decoders that enable bidirectional image-to-text and text-to-image generation in a single framework."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "As the paper deals with text generation tasks from images (image-to-text) and text-to-image models, it also falls under the domain of Natural Language Processing.",
          "quote": "The major problem with using unpaired data during vision-language training is the lack of supervision. To overcome this problem, we introduce ITIT, a novel training paradigm that uses cycle consistency losses between cycle-generated images/texts and their corresponding original inputs to provide supervision for image-only and text-only data (Figure 1)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ITIT",
          "justification": "ITIT (Integrating Image Text) is the model introduced in the paper for training vision-language generative models using unpaired data.",
          "quote": "We introduce ITIT (Integrating Image Text): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The ITIT model is the main contribution of the paper, presenting a new approach for using unpaired data in vision-language tasks.",
          "quote": "We introduce ITIT (Integrating Image Text): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments involving ITIT were conducted and results were reported in the paper, indicating that the model was executed.",
          "quote": "The major problem with using unpaired data during vision-language training is the lack of supervision. To overcome this problem, we introduce ITIT, a novel training paradigm that uses cycle consistency losses between cycle-generated images/texts and their corresponding original inputs to provide supervision for image-only and text-only data (Figure 1)."
        },
        "is_compared": {
          "value": 1,
          "justification": "ITIT was compared to state-of-the-art text-to-image and image-to-text models in the paper.",
          "quote": "Our experiments show that ITIT with unpaired datasets exhibits similar scaling behavior as using high-quality paired data. We demonstrate image generation and captioning performance on par with state-of-the-art text-to-image and image-to-text models with orders of magnitude fewer (only 3M) paired image-text data."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "ITIT is a novel model introduced in this paper itself, and therefore does not have a referenced paper.",
          "quote": "We introduce ITIT (Integrating Image Text): an innovative training paradigm grounded in the concept of cycle consistency which allows vision-language training on unpaired image and text data."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CC3M",
          "justification": "CC3M (Conceptual Captions) is one of the datasets mentioned in the paper used for experiments.",
          "quote": "We use three datasets in our experiments: CC3M (Sharma et al., 2018), WebLI (Chen et al., 2023), and Shutterstock (Shutterstock, 2023)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset For Automatic Image Captioning",
          "justification": "The conceptual captions (CC3M) dataset is well-known and referenced paper is provided in the citations.",
          "quote": "We use three datasets in our experiments: CC3M (Sharma et al., 2018), WebLI (Chen et al., 2023), and Shutterstock (Shutterstock, 2023)."
        }
      },
      {
        "name": {
          "value": "WebLI",
          "justification": "WebLI (Web Language Image) is another dataset mentioned in the paper used for experiments.",
          "quote": "We use three datasets in our experiments: CC3M (Sharma et al., 2018), WebLI (Chen et al., 2023), and Shutterstock (Shutterstock, 2023)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pali: A Jointly-Scaled Multilingual Language-Image Model",
          "justification": "The WebLI dataset is referred to in the paper and the referenced paper is provided in the citations.",
          "quote": "We use three datasets in our experiments: CC3M (Sharma et al., 2018), WebLI (Chen et al., 2023), and Shutterstock (Shutterstock, 2023)."
        }
      },
      {
        "name": {
          "value": "Shutterstock",
          "justification": "Shutterstock dataset is another one used in the experiments as mentioned in the paper.",
          "quote": "We use three datasets in our experiments: CC3M (Sharma et al., 2018), WebLI (Chen et al., 2023), and Shutterstock (Shutterstock, 2023)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The Shutterstock dataset is a well-known dataset of images, and there's no specific reference paper for it.",
          "quote": "We use three datasets in our experiments: CC3M (Sharma et al., 2018), WebLI (Chen et al., 2023), and Shutterstock (Shutterstock, 2023)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "T5",
          "justification": "The T5 model is leveraged for text embeddings in the ITIT model.",
          "quote": "We tokenize images into discrete visual tokens (Van Den Oord et al., 2017) and combine them with text embeddings from a pre-trained T5 model (Raffel et al., 2020) as input to the joint image-text encoder."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "justification": "The referenced paper for T5 is provided in the citations.",
          "quote": "We tokenize images into discrete visual tokens (Van Den Oord et al., 2017) and combine them with text embeddings from a pre-trained T5 model (Raffel et al., 2020) as input to the joint image-text encoder."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1753,
    "prompt_tokens": 16799,
    "total_tokens": 18552
  }
}