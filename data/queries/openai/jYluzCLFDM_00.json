{
  "paper": "jYluzCLFDM.txt",
  "words": 6524,
  "extractions": {
    "title": {
      "value": "Lag-Llama: Towards Foundation Models for Time Series Forecasting",
      "justification": "Title is clearly given at the start of the paper",
      "quote": "Lag-Llama: Towards Foundation Models for Time Series Forecasting"
    },
    "description": "This paper presents Lag-Llama, a univariate probabilistic time-series forecasting model trained on a large collection of time-series data. The paper studies the model's zero-shot prediction capabilities, performance on out-of-distribution datasets, and its scaling behavior. The model shows good zero-shot prediction capabilities on unseen datasets, outperforming several supervised baselines. Additionally, the study includes fitting neural scaling laws to predict the model's performance based on model size.",
    "type": {
      "value": "empirical study",
      "justification": "The paper involves training and testing a new model, Lag-Llama, and it provides empirical results demonstrating its performance and scalability.",
      "quote": "In this paper, we present preliminary results of our ongoing work along those lines. We train a transformer model on a large collection of time-series datasets and evaluate its performance on an unseen “out-of-distribution” dataset."
    },
    "primary_research_field": {
      "name": {
        "value": "Time Series Forecasting",
        "justification": "The paper introduces Lag-Llama, a model specifically for univariate probabilistic time-series forecasting.",
        "quote": "Our strategy is to train a single model over a large corpus of time series, the details of which are in App. Table 4."
      },
      "aliases": [
        "Time-Series Forecasting"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Scaling",
          "justification": "The paper fits neural scaling laws to predict the performance of the model.",
          "quote": "We fit empirical scaling laws of the zero-shot test performance of the model as a function of the model size, allowing us to potentially extrapolate and predict generalization beyond the models used in this paper."
        },
        "aliases": [
          "Scaling Laws"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Lag-Llama",
          "justification": "The paper introduces and extensively discusses the Lag-Llama model.",
          "quote": "Lag-Llama’s zero-shot performance beats or compares favorably to supervised baselines, and as the model size increases, its performance improves and stabilizes across hyperparameter specifications."
        },
        "aliases": [
          "Lag-Llama"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is newly introduced and studied in this paper.",
          "quote": "we aim to develop foundation models for time-series, investigate their behavior at scale, and push the limits of transfer achievable across diverse time-series domains. In this paper, we present preliminary results of our ongoing work along those lines. We train a transformer model on a large collection of time-series datasets and evaluate its performance on an unseen “out-of-distribution” dataset. Specifically, we investigate the use of pre-trained time series models for the univariate probabilistic time series forecasting use case and introduce the Lag-Llama model"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper provides empirical evaluations of the Lag-Llama model.",
          "quote": "We train a transformer model on a large collection of time-series datasets and evaluate its performance on an unseen “out-of-distribution” dataset."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of Lag-Llama with several supervised baselines.",
          "quote": "We then use the Continuous Ranked Probability Score (CRPS) metric [15, 28] score as our evaluation metric. CRPS is a proper scoring rule that measures the compatibility of a predicted cumulative distribution function (CDF) F with the ground-truth sample x as"
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The Lag-Llama model is newly introduced in this paper, and hence there is no referenced paper title.",
          "quote": "Specifically, we investigate the use of pre-trained time series models for the univariate probabilistic time series forecasting use case and introduce the Lag-Llama model"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Monash Time Series Repository",
          "justification": "The paper states that Lag-Llama is trained on time series datasets from the Monash Time Series Repository.",
          "quote": "introduce the Lag-Llama model1 trained on a large collection of time series from the Monash Time Series Repository [16]."
        },
        "aliases": [
          "Monash TSR"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Monash Time Series Forecasting Archive",
          "justification": "This is the reference that provides the datasets from the Monash Time Series Repository.",
          "quote": "These datasets have diverse frequencies and come from different application domains; the properties of these datasets are outlined in App. Table 4. In total, our training set comprises a total of 305,443 individual time series."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions the use of PyTorch for implementing the model.",
          "quote": "The open-source code is made available at https://github.com/kashif/pytorch-transformer-ts."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the reference paper for PyTorch mentioned in the paper.",
          "quote": "We acknowledge and thank the authors and contributors of all the open-source libraries that were used in this work, especially: GluonTS [1], NumPy [17], Pandas [42], Matplotlib [18] and PyTorch [31]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1099,
    "prompt_tokens": 13125,
    "total_tokens": 14224
  }
}