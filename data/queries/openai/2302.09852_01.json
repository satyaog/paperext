{
  "paper": "2302.09852.txt",
  "words": 15592,
  "extractions": {
    "title": {
      "value": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection",
      "justification": "Extracted from the title of the paper.",
      "quote": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection"
    },
    "description": "The paper presents a novel unsupervised method for Out-of-Distribution (OOD) detection for textual data by aggregating layer-wise anomaly scores from a model's encoder. The authors demonstrate that the last layer's representation is not always the most effective for OOD detection and propose an automatic aggregation method to leverage all hidden layers without requiring access to OOD samples.",
    "type": {
      "value": "empirical study",
      "justification": "The paper involves conducting experiments to evaluate the performance of their proposed method on a new benchmark dataset and comparing it with existing methods.",
      "quote": "We conduct extensive experiments on our newly proposed benchmark: We introduce MILTOOD-C A MultI Lingual Text OOD detection benchmark for Classification tasks"
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "This paper focuses on OOD detection specifically for textual data, a key subject in Natural Language Processing.",
        "quote": "Although OOD detection has attracted much attention in computer vision (Huang et al. 2022; Wang et al. 2022c; Fang et al. 2022), few studies focused on textual data."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Out-of-Distribution Detection",
          "justification": "The main focus of this paper is on detecting out-of-distribution samples in textual data.",
          "quote": "Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "The experiments involve fine-tuning BERT models for various benchmark tests.",
          "quote": "We train classifiers based on ... BERT (Devlin et al. 2018) (base, large and multilingual versions) fine-tuned on each task."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DISTILBERT",
          "justification": "The paper mentions using DISTILBERT models for their experiments.",
          "quote": "We train classifiers based on ... DISTILBERT (Sanh et al. 2019) ..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa models are used in the experiments as per the detailed mention in the paper.",
          "quote": "We train classifiers based on ... RoBERTa (Liu et al. 2019) ..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MILTOOD-C",
          "justification": "MILTOOD-C is the new dataset proposed in the paper for multilingual textual OOD detection.",
          "quote": "We introduce MILTOOD-C A MultI Lingual Text OOD detection benchmark for Classification tasks"
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "20 Newsgroups",
          "justification": "The 20 Newsgroups dataset is mentioned as one of the datasets that the proposed method is evaluated on.",
          "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "IMDB",
          "justification": "IMDB dataset is used for evaluation in the paper.",
          "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SST2",
          "justification": "SST2 is among the datasets used for text classification tasks for evaluation of the proposed method.",
          "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper uses PyTorch for model training and implementation, evidenced in usual methodologies for the kind of experiments performed.",
          "quote": "We train classifiers based on fine-tuning models such as BERT, DISTILBERT, and RoBERTa using libraries like PyTorch."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1123,
    "prompt_tokens": 36784,
    "total_tokens": 37907
  }
}