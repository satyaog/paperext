{
  "paper": "1910.02344.txt",
  "words": 9506,
  "extractions": {
    "title": {
      "value": "Neural Multisensory Scene Inference",
      "justification": "The title is clearly indicated at the top of the provided research paper.",
      "quote": "Neural Multisensory Scene Inference"
    },
    "description": "This paper introduces the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes from multiple sensory modalities. It proposes the Amortized Product-of-Experts (APoE) to improve computational efficiency and robustness to unseen modality combinations. The paper demonstrates the effectiveness of GMN in inferring robust 3D scene representations and performing accurate cross-modal generation using a newly developed environment, the Multisensory Embodied 3D-Scene Environment (MESE).",
    "type": {
      "value": "Empirical",
      "justification": "The paper focuses on introducing a new model (GMN), a novel method (APoE), and demonstrating their effectiveness through experimental results.",
      "quote": "Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations from arbitrary combinations of modalities and perform accurate cross-modal generation."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The research involves developing a deep learning model to learn representations of 3D scenes from multisensory data.",
        "quote": "Motivated by the above desiderata, we propose the Generative Multisensory Network (GMN) for neural multisensory scene inference and rendering."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The paper specifically focuses on learning representations from multiple sensory modalities.",
          "quote": "Motivated by human multisensory processing, we propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Generative Multisensory Network",
          "justification": "The main model introduced and developed in the paper is the GMN.",
          "quote": "We propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities."
        },
        "aliases": [
          "GMN"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Amortized Product-of-Experts",
          "justification": "The APoE method is introduced as a novel component within the GMN to enhance its efficiency and robustness.",
          "quote": "We also introduce the Amortized Product-of-Experts network that allows for generalized cross-modal generation while resolving the problems in the Generative Query Network (GQN) and traditional Product-of-Experts."
        },
        "aliases": [
          "APoE"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Consistent Generative Query Network",
          "justification": "The paper references C-GQN as a base model which GMN builds upon.",
          "quote": "We adopt C-GQN network architecture from Kumar et al. (2018) for the proposed model, as well as the baseline."
        },
        "aliases": [
          "C-GQN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "referenced"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Product-of-Experts",
          "justification": "The PoE model is referenced in comparison to and as a precursor for the APoE introduced in the paper.",
          "quote": "As a result, the Amortized Product-of-Experts (APoE) allows the model to learn from a large number of modalities without tight coupling among the modalities."
        },
        "aliases": [
          "PoE"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "referenced"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MultiSensory Embodied 3D-Scene Environment",
          "justification": "The MESE is the environment specifically developed for the experiments conducted in this paper.",
          "quote": "We also develop the Multisensory Embodied 3D-Scene Environment (MESE) used to develop and test the model."
        },
        "aliases": [
          "MESE"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Shepard-Metzler Dataset",
          "justification": "The Shepard-Metzler Dataset is used as a basis for generating the 3D scenes utilized in the experiments.",
          "quote": "The main task is similar to the Shepard-Metzler object experiments used in Eslami et al. (2018) but extends it with the MPL hand."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions using the PyTorch library for implementing the models.",
          "quote": "PyTorch (Paszke et al., 2017), CUDA-9.0 (Nickolls et al., 2008), and cuDNN7 (Chetlur et al., 2014) are used for the implementations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CUDA",
          "justification": "The CUDA library is mentioned as a part of the implementation framework.",
          "quote": "PyTorch (Paszke et al., 2017), CUDA-9.0 (Nickolls et al., 2008), and cuDNN7 (Chetlur et al., 2014) are used for the implementations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "cuDNN",
          "justification": "The cuDNN library is also mentioned as part of the implementation framework.",
          "quote": "PyTorch (Paszke et al., 2017), CUDA-9.0 (Nickolls et al., 2008), and cuDNN7 (Chetlur et al., 2014) are used for the implementations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1657,
    "prompt_tokens": 15514,
    "total_tokens": 17171
  }
}