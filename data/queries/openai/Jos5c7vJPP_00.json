{
  "paper": "Jos5c7vJPP.txt",
  "words": 20358,
  "extractions": {
    "title": {
      "value": "E XCHANGEABLE DATASET A MORTIZATION FOR BAYESIAN P OSTERIOR I NFERENCE",
      "justification": "The title is clearly stated at the beginning of the paper, 'E XCHANGEABLE DATASET A MORTIZATION FOR BAYESIAN P OSTERIOR I NFERENCE.'",
      "quote": "E XCHANGEABLE DATASET A MORTIZATION FOR BAYESIAN P OSTERIOR I NFERENCE"
    },
    "description": "The paper examines how to efficiently perform Bayesian posterior inference using amortized methods that leverage permutation invariant, set-based neural network architectures. It aims to generalize posterior predictions to datasets of varying cardinalities and orderings, comparing its methods to Maximum Likelihood and MCMC approaches.",
    "type": {
      "value": "Empirical Study",
      "justification": "The study involves empirical evaluations of various neural network architectures and training objectives to perform amortized Bayesian posterior inference.",
      "quote": "Our experiments explore the effectiveness of this approach for both posterior estimation directly as well as model predictive performance."
    },
    "primary_research_field": {
      "name": {
        "value": "Bayesian Inference",
        "justification": "The paper focuses on improving Bayesian posterior inference using neural network-based amortization techniques.",
        "quote": "Bayesian inference is a natural approach to reasoning about uncertainty. Unfortunately, in practice it generally requires expensive iterative methods like MCMC to approximate posterior distributions."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The methods discussed involve neural network architectures and training objectives common in machine learning.",
          "quote": "In this work, we amortize the posterior parameter inference for probabilistic models by leveraging permutation invariant, set-based network architectures which respect the inherent exchangeability of independent observations of a dataset."
        },
        "aliases": [
          "ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DeepSets",
          "justification": "DeepSets is explicitly mentioned as a set-based network architecture used for their amortized inference.",
          "quote": "Motivated by work in set-based neural network architectures like Transformers and DeepSets (Zaheer et al., 2017; Vaswani et al., 2017; Lee et al., 2019), we design a function that can take an arbitrary set as input as opposed to an ordered list of fixed length and explore a number of different choices which respect permutation invariance"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "DeepSets is a referenced model, not a contribution of this paper.",
          "quote": "Motivated by work in set-based neural network architectures like Transformers and DeepSets (Zaheer et al., 2017; Vaswani et al., 2017; Lee et al., 2019)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments to compare its effectiveness.",
          "quote": "DeepSets (Zaheer et al., 2017; Vaswani et al., 2017; Lee et al., 2019)"
        },
        "is_compared": {
          "value": 1,
          "justification": "DeepSets was compared to other architectures like Transformers.",
          "quote": "highlighting the superior performance of our proposed approach when compared to existing baselines, especially in the presence of model misspecification and real-world data."
        },
        "referenced_paper_title": {
          "value": "Deep sets",
          "justification": "The reference to DeepSets is backed by its citation.",
          "quote": "DeepSets (Zaheer et al., 2017)"
        }
      },
      {
        "name": {
          "value": "Transformers",
          "justification": "Transformers are explicitly mentioned as a set-based network architecture used for their amortized inference.",
          "quote": "Motivated by work in set-based neural network architectures like Transformers and DeepSets (Zaheer et al., 2017; Vaswani et al., 2017; Lee et al., 2019), we design a function that can take an arbitrary set as input as opposed to an ordered list of fixed length and explore a number of different choices which respect permutation invariance"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Transformers is a referenced model, not a contribution of this paper.",
          "quote": "Motivated by work in set-based neural network architectures like Transformers and DeepSets (Zaheer et al., 2017; Vaswani et al., 2017; Lee et al., 2019)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments to compare its effectiveness.",
          "quote": "Transformers (Zaheer et al., 2017; Vaswani et al., 2017; Lee et al., 2019)"
        },
        "is_compared": {
          "value": 1,
          "justification": "Transformers was compared to other architectures like DeepSets.",
          "quote": "highlighting the superior performance of our proposed approach when compared to existing baselines, especially in the presence of model misspecification and real-world data."
        },
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "The reference to Transformers is backed by its citation.",
          "quote": "Transformers (Zaheer et al., 2017; Vaswani et al., 2017; Lee et al., 2019)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "OpenML-CTR23",
          "justification": "OpenML-CTR23 is specifically mentioned as a source of tasks for the regression problems in the experiments.",
          "quote": "For the tabular regression problems, we consider the suite of tasks outlined in OpenML-CTR23 - A curated tabular regression benchmarking suite (Fischer et al., 2023)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "OpenML-CTR23 â€“ a curated tabular regression benchmarking suite",
          "justification": "The dataset is used for experimental analysis.",
          "quote": "For the tabular regression problems, we consider the suite of tasks outlined in OpenML-CTR23 - A curated tabular regression benchmarking suite (Fischer et al., 2023)"
        }
      },
      {
        "name": {
          "value": "OpenML-CC18",
          "justification": "OpenML-CC18 is specifically mentioned as a source of tasks for the classification problems in the experiments.",
          "quote": "Similarly, we consider the OpenML-CC18 Curated Classification benchmark (Bischl et al., 2019) suite of tasks for classification and perform a similar filtering algorithm."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Openml benchmarking suites",
          "justification": "The dataset is used for experimental analysis.",
          "quote": "Similarly, we consider the OpenML-CC18 Curated Classification benchmark (Bischl et al., 2019) suite of tasks for classification and perform a similar filtering algorithm."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3014,
    "prompt_tokens": 78290,
    "total_tokens": 81304
  }
}