{
  "paper": "2211.14666.txt",
  "words": 18476,
  "extractions": {
    "title": {
      "value": "Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning",
      "justification": "The name of the paper as stated in the provided text",
      "quote": "Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning"
    },
    "description": "This paper explores the connections between disentangled representations and sparse task-specific predictors in improving generalization and identifiability in multi-task learning. The authors provide theoretical insights and propose a bi-level optimization framework to learn disentangled representations. They validate their theoretical findings with empirical results on few-shot learning benchmarks.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes both theoretical insights and empirical validation through experiments.",
      "quote": "Finally, we draw a connection between this bi-level optimization problem and formulations from the meta-learning literature. Inspired by our identifiability result, we enhance an existing method (Lee et al., 2019), where the task-specific predictors are now group-sparse SVMs. We show that this new meta-learning algorithm achieves competitive performance on the miniImageNet benchmark (Vinyals et al., 2016), while only using a fraction of the representation."
    },
    "primary_research_field": {
      "name": {
        "value": "Multi-Task Learning",
        "justification": "The main focus of the paper is on improving multi-task learning through disentanglement and sparsity.",
        "quote": "In this work, we explore synergies between disentanglement and sparse task-specific predictors in the context of multi-task learning."
      },
      "aliases": [
        "MTL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The work discusses learning disentangled representations, which is a core area within representation learning.",
          "quote": "In this section, we first provide a new identification result (Theorem 3.1, Section 3.2), which states that in the multi-task learning setting, regularizing the task-specific predictors to be sparse can yield disentangled representations."
        },
        "aliases": [
          "RepL"
        ]
      },
      {
        "name": {
          "value": "Meta-Learning",
          "justification": "The experimental validations include applications in meta-learning scenarios.",
          "quote": "We explore a meta-learning version of this algorithm based on group Lasso multiclass SVM predictors, for which we derive a tractable dual formulation."
        },
        "aliases": [
          "MetaL"
        ]
      },
      {
        "name": {
          "value": "Few-Shot Learning",
          "justification": "The experimental part includes benchmarks standard in few-shot learning tasks.",
          "quote": "It obtains competitive results on standard few-shot classification benchmarks, while each task is using only a fraction of the learned representations."
        },
        "aliases": [
          "FSL"
        ]
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "miniImageNet",
          "justification": "The few-shot learning experiments were conducted on the miniImageNet dataset.",
          "quote": "We show that this new meta-learning algorithm achieves competitive performance on the miniImageNet benchmark (Vinyals et al., 2016), while only using a fraction of the representation."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Matching Networks for One Shot Learning",
          "justification": "This paper is where the miniImageNet dataset was introduced.",
          "quote": "Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al. Matching networks for one shot learning. Advances in Neural Information Processing Systems, 29, 2016."
        }
      },
      {
        "name": {
          "value": "3D Shapes",
          "justification": "The 3D Shapes dataset was used for validating the theoretical results.",
          "quote": "We validate our theory by showing our approach can indeed disentangle latent factors on tasks constructed from the 3D Shapes dataset (Burgess & Kim, 2018)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "3D Shapes dataset",
          "justification": "The dataset is referenced directly in the context of the experiments.",
          "quote": "Burgess, C. and Kim, H. Scott. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/, 2018."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "JAX was used for implementation purposes.",
          "quote": "Our implementation relies on jax and jaxopt (Bradbury et al., 2018; Blondel et al., 2022) and is available here: https://github.com/tristandeleu/synergies-disentanglement-sparsity."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "JAX: composable transformations of Python+NumPy programs",
          "justification": "This is the primary reference for JAX.",
          "quote": "Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. JAX: composable transformations of Python+NumPy programs, 2018."
        }
      },
      {
        "name": {
          "value": "JAXOPT",
          "justification": "JAXOPT was used for implementation purposes.",
          "quote": "Our implementation relies on jax and jaxopt (Bradbury et al., 2018; Blondel et al., 2022) and is available here: https://github.com/tristandeleu/synergies-disentanglement-sparsity."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Efficient and modular implicit differentiation",
          "justification": "This is the primary reference for JAXOPT.",
          "quote": "Blondel, M., Berthet, Q., Cuturi, M., Frostig, R., Hoyer, S., Llinares-LÃ³pez, F., Pedregosa, F., and Vert, J.-P. Efficient and modular implicit differentiation. NeurIPS, 2022."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1466,
    "prompt_tokens": 35187,
    "total_tokens": 36653
  }
}