{
  "paper": "2312.15536.txt",
  "words": 21418,
  "extractions": {
    "title": {
      "value": "Harnessing Pre-trained Generalist Agents for Software Engineering Tasks",
      "justification": "Title of the paper",
      "quote": "Harnessing Pre-trained Generalist Agents for Software Engineering Tasks"
    },
    "description": "This paper investigates the applicability of pre-trained generalist agents for solving two important software engineering (SE) tasks: the detection of bugs in games and the minimization of makespan in a scheduling task.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts an empirical study to assess the performance of generalist agents.",
      "quote": "This paper investigates the potential of generalist agents for solving SE tasks. Specifically, we conduct an empirical study aimed at assessing the performance of two generalist agents on two important SE tasks"
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on using reinforcement learning algorithms for software engineering tasks.",
        "quote": "Recently, DRL has been increasingly leveraged in Software Engineering (SE) tasks, thanks to the availability of DRL algorithms such as Proximal Policy Optimization (PPO), Advantage Actor Critic (A2C), Deep Q-Networks (DQN) that can train agents to accomplish the task at hand."
      },
      "aliases": [
        "DRL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Automated Game Testing",
          "justification": "The paper applies reinforcement learning for the detection of bugs in games.",
          "quote": "...assessing the performance of two generalist agents on two important SE tasks: the detection of bugs in games..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Job-Shop Scheduling",
          "justification": "The paper uses reinforcement learning to minimize makespan in scheduling tasks.",
          "quote": "...the minimization of makespan in a scheduling task, to solve the job-shop scheduling problem..."
        },
        "aliases": [
          "JSSP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Multi-Game Decision Transformer (MGDT)",
          "justification": "One of the generalist agents evaluated in the study is the Multi-Game Decision Transformer.",
          "quote": "In this paper, we leverage generalist agents, namely a Multi-Game Decision Transformer (MGDT) and a Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (IMPALA)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "While the paper evaluated MGDT, it did not claim to have developed it.",
          "quote": "We did not pre-train the MGDT agent as the checkpoint of the pre-training done by Lee et al. [31] is available online."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper fine-tunes and evaluates pre-trained MGDT on multiple tasks, indicating it was executed.",
          "quote": "We employ the pre-trained models of these generalist agents (MGDT and IMPALA) and empirically compare their performance against the performance of specialist agents."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares MGDT performance against other models.",
          "quote": "We employ the pre-trained models of these generalist agents and empirically compare their performance against the performance of specialist agents."
        },
        "referenced_paper_title": {
          "value": "Multi-game decision transformers",
          "justification": "This is the title of the reference paper where MGDT is introduced.",
          "quote": "A Decision Transformer (DT) is an architecture that tackles DRL as a sequential modeling problem [16]. It leverages transformer [50] architecture to predict future actions. Specifically, a trajectory is represented as a sequence of states, actions and return-to-go...Table 1, 2 summarizes the differences between a DRL-based agent, IMPALA [18] and Multi-game Decision Transformer(MGDT) [31] with Gym benchmark...."
        }
      },
      {
        "name": {
          "value": "Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (IMPALA)",
          "justification": "Another generalist agent evaluated in the study is IMPALA.",
          "quote": "In this paper, we leverage generalist agents, namely a Multi-Game Decision Transformer (MGDT) and a Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures (IMPALA)."
        },
        "aliases": [
          "IMPALA"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "While the paper evaluated IMPALA, it did not claim to have developed it.",
          "quote": "Given that pre-trained version of IMPALA are not available, we pre-trained it on 57 games of the Atari learning environment [11] using V-trace [18], an off-policy actor-critic algorithm."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper fine-tunes and evaluates pre-trained IMPALA on multiple tasks, indicating it was executed.",
          "quote": "We employ the pre-trained models of these generalist agents and empirically compare their performance against the performance of specialist agents."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares IMPALA performance against other models.",
          "quote": "We employ the pre-trained models of these generalist agents and empirically compare their performance against the performance of specialist agents."
        },
        "referenced_paper_title": {
          "value": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
          "justification": "This is the title of the reference paper where IMPALA is introduced.",
          "quote": "Table 1, 2 summarizes the differences between a DRL-based agent, IMPALA [18] and Multi-game Decision Transformer (MGDT) [31]...."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Blockmaze",
          "justification": "The dataset is selected for evaluating generalist agents in the study.",
          "quote": "A Blockmaze game, Figure 5, from Zheng et al. [58], is selected for the evaluation of the generalist agents."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Wuji: Automatic online combat game testing using evolutionary deep reinforcement learning",
          "justification": "This is the title of the reference paper from which the dataset is selected.",
          "quote": "A Blockmaze game, Figure 5, from Zheng et al. [58], is selected for the evaluation of the generalist agents."
        }
      },
      {
        "name": {
          "value": "MsPacman",
          "justification": "The dataset is selected for evaluating generalist agents in the study.",
          "quote": "The objective of the MsPacman game is to eat all the dots without touching the ghosts."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Gym: A toolkit for developing and comparing reinforcement learning algorithms",
          "justification": "This is the title of the reference paper where the environment is included in the gym toolkit.",
          "quote": "The environment of the MsPacman game is integrated into the gym Python toolkit."
        }
      },
      {
        "name": {
          "value": "Job-Shop Scheduling Problem Dataset (Taillard's method)",
          "justification": "The dataset is selected for evaluating generalist agents in the study.",
          "quote": "The instances we considered ((6 × 6), (30 × 20)) are generated following the Taillard’s method...."
        },
        "aliases": [
          "JSSP Dataset"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Benchmarks for basic scheduling problems",
          "justification": "This is the title of the reference paper where the dataset is introduced.",
          "quote": "The instances we considered ((6 × 6), (30 × 20)) are generated following the Taillard’s method."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Gym Python toolkit",
          "justification": "The library is used for the MsPacman game environment.",
          "quote": "The environment of the MsPacman game is integrated into the gym Python toolkit."
        },
        "aliases": [
          "gym"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Gym: A toolkit for developing and comparing reinforcement learning algorithms",
          "justification": "This is the title of the reference paper where the gym toolkit is introduced.",
          "quote": "The environment of the MsPacman game is integrated into the gym Python toolkit."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1601,
    "prompt_tokens": 54819,
    "total_tokens": 56420
  }
}