{
  "paper": "2308.14456.txt",
  "words": 11008,
  "extractions": {
    "title": {
      "value": "Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads",
      "justification": "Confirmed from the provided research paper text.",
      "quote": "Speech Self-Supervised Representations Benchmarking: a Case for Larger Probing Heads"
    },
    "description": "This paper examines the impact of varying probing head architectures on the performance benchmarking of self-supervised learning (SSL) models for speech tasks. It investigates how changes in the probing head structure influence the performance ranking of evaluated SSL models across various speech processing tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The research involves empirical evaluations and comparisons of different SSL models and probing head architectures through experiments and performance benchmarking.",
      "quote": "This study examines how benchmarking results are affected by changes in the probing head architecture."
    },
    "primary_research_field": {
      "name": {
        "value": "Speech Processing",
        "justification": "The paper primarily focuses on benchmarking SSL models for various speech processing tasks.",
        "quote": "Self-supervised learning (SSL) leverages large datasets of unlabeled speech to reach impressive performance with reduced amounts of annotated data."
      },
      "aliases": [
        "Speech Processing"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-supervised Learning",
          "justification": "The paper investigates self-supervised learning models for speech tasks.",
          "quote": "Self-supervised learning (SSL) offers a compelling solution for benefiting from abundant unlabeled data to achieve notable performance improvements in various downstream tasks such as speech or speaker recognition."
        },
        "aliases": [
          "SSL",
          "Self-supervised Learning"
        ]
      },
      {
        "name": {
          "value": "Speech Recognition",
          "justification": "The downstream tasks investigated in the paper include automatic speech recognition (ASR).",
          "quote": "The SUPERB benchmark has chosen a probing family FT (i.e. a downstream architecture with its hyperparameters, such as an MLP with given number of layers and hidden sizes) for every considered downstream task T and, for every considered SSL encoder ϕ, it shows a task error rate equal to: min Et ( f ◦ ϕ);"
        },
        "aliases": [
          "ASR",
          "Speech Recognition"
        ]
      },
      {
        "name": {
          "value": "Speaker Verification",
          "justification": "The downstream tasks investigated in the paper include speaker verification (SV).",
          "quote": "Automatic Speaker Verification (ASV). The ASV task consists of a binary classification procedure aimed at determining whether speakers in a pair of utterances are the same."
        },
        "aliases": [
          "ASV",
          "Speaker Verification"
        ]
      },
      {
        "name": {
          "value": "Emotion Recognition",
          "justification": "The downstream tasks investigated in the paper include emotion recognition (ER).",
          "quote": "For ER, we utilize the IEMOCAP dataset, which comprises 10, 039 utterances from 10 distinct speakers."
        },
        "aliases": [
          "ER",
          "Emotion Recognition"
        ]
      },
      {
        "name": {
          "value": "Intent Classification",
          "justification": "The downstream tasks investigated in the paper include intent classification (IC).",
          "quote": "The SLURP collection consists of approximately 72, 000 audio recordings that capture user interactions with a home assistant in single-turn scenarios."
        },
        "aliases": [
          "IC",
          "Intent Classification"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Wav2vec 2.0",
          "justification": "Wav2vec 2.0 is one of the SSL models evaluated in the paper, both in its Base and Large versions.",
          "quote": "For our study, we focused on a subset of state-of-the-art models from the SUPERB benchmark due to their wide adoption within the community. We selected nine SSL models that extract representations directly from the waveform: Wav2vec 2.0, HuBERT, WavLM, and Data2Vec."
        },
        "aliases": [
          "Wav2vec 2.0"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Wav2vec 2.0 is a pre-existing model and is not introduced as a new contribution in this paper.",
          "quote": "For our study, we focused on a subset of state-of-the-art models from the SUPERB benchmark due to their wide adoption within the community."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Wav2vec 2.0 model was executed as part of the benchmarking experiments.",
          "quote": "We obtained all the pre-trained checkpoints from their respective HuggingFace (HF) official cards, except for Wav2vec2.0 Large, for which we used the Fairseq checkpoint since the HF version underperformed compared to the results reported in SUPERB."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Wav2vec 2.0 model's performance is compared to other SSL models in the benchmark experiments.",
          "quote": "The proliferation of approaches for speech SSL has, therefore, fomented the need for “universal” benchmarks evaluating their performance across multiple downstream tasks."
        },
        "referenced_paper_title": {
          "value": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
          "justification": "The referenced paper for Wav2vec 2.0 is cited in the research paper under evaluation.",
          "quote": "A. Baevski, H. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” Neurips, 2020."
        }
      },
      {
        "name": {
          "value": "HuBERT",
          "justification": "HuBERT is one of the SSL models evaluated in the paper, both in its Base and Large versions.",
          "quote": "For our study, we focused on a subset of state-of-the-art models from the SUPERB benchmark due to their wide adoption within the community. We selected nine SSL models that extract representations directly from the waveform: Wav2vec 2.0, HuBERT, WavLM, and Data2Vec."
        },
        "aliases": [
          "HuBERT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "HuBERT is a pre-existing model and is not introduced as a new contribution in this paper.",
          "quote": "For our study, we focused on a subset of state-of-the-art models from the SUPERB benchmark due to their wide adoption within the community."
        },
        "is_executed": {
          "value": 1,
          "justification": "The HuBERT model was executed as part of the benchmarking experiments.",
          "quote": "We obtained all the pre-trained checkpoints from their respective HuggingFace (HF) official cards, except for Wav2vec2.0 Large, for which we used the Fairseq checkpoint since the HF version underperformed compared to the results reported in SUPERB."
        },
        "is_compared": {
          "value": 1,
          "justification": "The HuBERT model's performance is compared to other SSL models in the benchmark experiments.",
          "quote": "The proliferation of approaches for speech SSL has, therefore, fomented the need for “universal” benchmarks evaluating their performance across multiple downstream tasks."
        },
        "referenced_paper_title": {
          "value": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
          "justification": "The referenced paper for HuBERT is cited in the research paper under evaluation.",
          "quote": "W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Trans. Audio, Speech and Lang. Proc., vol. 29, oct 2021."
        }
      },
      {
        "name": {
          "value": "WavLM",
          "justification": "WavLM is one of the SSL models evaluated in the paper, both in its Base+ and Large versions.",
          "quote": "For our study, we focused on a subset of state-of-the-art models from the SUPERB benchmark due to their wide adoption within the community. We selected nine SSL models that extract representations directly from the waveform: Wav2vec 2.0, HuBERT, WavLM, and Data2Vec."
        },
        "aliases": [
          "WavLM"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "WavLM is a pre-existing model and is not introduced as a new contribution in this paper.",
          "quote": "For our study, we focused on a subset of state-of-the-art models from the SUPERB benchmark due to their wide adoption within the community."
        },
        "is_executed": {
          "value": 1,
          "justification": "The WavLM model was executed as part of the benchmarking experiments.",
          "quote": "We obtained all the pre-trained checkpoints from their respective HuggingFace (HF) official cards, except for Wav2vec2.0 Large, for which we used the Fairseq checkpoint since the HF version underperformed compared to the results reported in SUPERB."
        },
        "is_compared": {
          "value": 1,
          "justification": "The WavLM model's performance is compared to other SSL models in the benchmark experiments.",
          "quote": "The proliferation of approaches for speech SSL has, therefore, fomented the need for “universal” benchmarks evaluating their performance across multiple downstream tasks."
        },
        "referenced_paper_title": {
          "value": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
          "justification": "The referenced paper for WavLM is cited in the research paper under evaluation.",
          "quote": "S. Chen, C. Wang, Z. Chen et al., “WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing,” IEEE JSTSP, vol. 16, no. 6, pp. 1505–1518, oct 2021."
        }
      },
      {
        "name": {
          "value": "Data2Vec",
          "justification": "Data2Vec is one of the SSL models evaluated in the paper, both in its Base and Large versions.",
          "quote": "For our study, we focused on a subset of state-of-the-art models from the SUPERB benchmark due to their wide adoption within the community. We selected nine SSL models that extract representations directly from the waveform: Wav2vec 2.0, HuBERT, WavLM, and Data2Vec."
        },
        "aliases": [
          "Data2Vec"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Data2Vec is a pre-existing model and is not introduced as a new contribution in this paper.",
          "quote": "For our study, we focused on a subset of state-of-the-art models from the SUPERB benchmark due to their wide adoption within the community."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Data2Vec model was executed as part of the benchmarking experiments.",
          "quote": "We obtained all the pre-trained checkpoints from their respective HuggingFace (HF) official cards, except for Wav2vec2.0 Large, for which we used the Fairseq checkpoint since the HF version underperformed compared to the results reported in SUPERB."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Data2Vec model's performance is compared to other SSL models in the benchmark experiments.",
          "quote": "The proliferation of approaches for speech SSL has, therefore, fomented the need for “universal” benchmarks evaluating their performance across multiple downstream tasks."
        },
        "referenced_paper_title": {
          "value": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
          "justification": "The referenced paper for Data2Vec is cited in the research paper under evaluation.",
          "quote": "A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, “Data2vec: A general framework for self-supervised learning in speech, vision and language,” in International Conference on Machine Learning. PMLR, 2022."
        }
      },
      {
        "name": {
          "value": "DistilHuBERT",
          "justification": "DistilHuBERT is one of the SSL models evaluated in the paper.",
          "quote": "We selected nine SSL models that extract representations directly from the waveform: Wav2vec 2.0, HuBERT, WavLM, and Data2Vec in both their Base and Large versions. We also included DistilHuBERT, which is a distilled version of Hubert Base with four times fewer transformer layers."
        },
        "aliases": [
          "DistilHuBERT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "DistilHuBERT is a pre-existing model and is not introduced as a new contribution in this paper.",
          "quote": "We also included DistilHuBERT, which is a distilled version of Hubert Base with four times fewer transformer layers."
        },
        "is_executed": {
          "value": 1,
          "justification": "The DistilHuBERT model was executed as part of the benchmarking experiments.",
          "quote": "We obtained all the pre-trained checkpoints from their respective HuggingFace (HF) official cards, except for Wav2vec2.0 Large, for which we used the Fairseq checkpoint since the HF version underperformed compared to the results reported in SUPERB."
        },
        "is_compared": {
          "value": 1,
          "justification": "The DistilHuBERT model's performance is compared to other SSL models in the benchmark experiments.",
          "quote": "The proliferation of approaches for speech SSL has, therefore, fomented the need for “universal” benchmarks evaluating their performance across multiple downstream tasks."
        },
        "referenced_paper_title": {
          "value": "DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT",
          "justification": "The referenced paper for DistilHuBERT is cited in the research paper under evaluation.",
          "quote": "H.-J. Chang, S.-w. Yang, and H.-y. Lee, “DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT,” ICASSP, 2021."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "LibriSpeech",
          "justification": "LibriSpeech is one of the downstream datasets used for evaluating the SSL models.",
          "quote": "For the first one, LibriSpeech train-clean-100/dev-clean subsets are used for training and validation while test-clean and test-other are kept for testing."
        },
        "aliases": [
          "LibriSpeech"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "LibriSpeech: an ASR corpus based on public domain audio books",
          "justification": "The referenced paper for LibriSpeech is cited in the research paper under evaluation.",
          "quote": "V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An asr corpus based on public domain audio books,” in 2015 (ICASSP), 2015, pp. 5206–5210."
        }
      },
      {
        "name": {
          "value": "Buckeye",
          "justification": "Buckeye is one of the downstream datasets used for evaluating the SSL models.",
          "quote": "The Buckeye dataset is considered as a second ASR task, allowing for testing the ability of the models with fewer labeled data and in a more complex spontaneous setting of English speech."
        },
        "aliases": [
          "Buckeye"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Buckeye Corpus of Conversational Speech: Labeling Conventions and a Test of Transcriber Reliability",
          "justification": "The referenced paper for Buckeye is cited in the research paper under evaluation.",
          "quote": "M. Pitt, K. Johnson, E. Hume, S. Kiesling, and W. Raymond, “The buckeye corpus of conversational speech: Labeling conventions and a test of transcriber reliability,” Speech Communication, vol. 45, pp. 89–95, 01 2005."
        }
      },
      {
        "name": {
          "value": "CommonVoice",
          "justification": "CommonVoice is one of the downstream datasets used for evaluating the SSL models.",
          "quote": "Since low-resource languages are one of the main applications of SSL methods, two low-resource language tasks, extracted from the CommonVoice 11.0 release, are considered: Welsh (Cymraeg) and Basque (Euskera)."
        },
        "aliases": [
          "CommonVoice"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Common voice: A massively-multilingual speech corpus",
          "justification": "The referenced paper for CommonVoice is cited in the research paper under evaluation.",
          "quote": "R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, “Common voice: A massively-multilingual speech corpus,” 2020."
        }
      },
      {
        "name": {
          "value": "VoxCeleb1",
          "justification": "VoxCeleb1 is one of the downstream datasets used for evaluating the SSL models.",
          "quote": "The ASV task consists of a binary classification procedure aimed at determining whether speakers in a pair of utterances are the same. Similar to the SUPERB benchmark, we utilize the VoxCeleb1 train and test splits for this task."
        },
        "aliases": [
          "VoxCeleb1"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Voxceleb: A large-scale speaker identification dataset",
          "justification": "The referenced paper for VoxCeleb1 is cited in the research paper under evaluation.",
          "quote": "A. Nagrani, J. S. Chung, and A. Zisserman, “Voxceleb: A large-scale speaker identification dataset,” Interspeech 2017, Aug 2017."
        }
      },
      {
        "name": {
          "value": "IEMOCAP",
          "justification": "IEMOCAP is one of the downstream datasets used for evaluating the SSL models.",
          "quote": "For ER, we utilize the IEMOCAP dataset, which comprises 10, 039 utterances from 10 distinct speakers."
        },
        "aliases": [
          "IEMOCAP"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "IEMOCAP: Interactive emotional dyadic motion capture database",
          "justification": "The referenced paper for IEMOCAP is cited in the research paper under evaluation.",
          "quote": "C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive emotional dyadic motion capture database,” Language resources and evaluation, vol. 42, pp. 335–359, 2008."
        }
      },
      {
        "name": {
          "value": "SLURP",
          "justification": "SLURP is one of the downstream datasets used for evaluating the SSL models.",
          "quote": "While the SUPERB benchmark evaluates the semantic content of SSL representations using the Speech Commands (SC), we employ the more challenging SLURP dataset for Intent Classification, as error rates with SC are extremely low."
        },
        "aliases": [
          "SLURP"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "SLURP: A spoken language understanding resource package",
          "justification": "The referenced paper for SLURP is cited in the research paper under evaluation.",
          "quote": "E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser, “Slurp: A spoken language understanding resource package,” EMNLP 2020."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "The paper mentions the use of the SpeechBrain library to build the benchmarking code.",
          "quote": "We release the code base developed within the SpeechBrain library for replication and to encourage further investigations and comparisons between models."
        },
        "aliases": [
          "SpeechBrain"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A general-purpose speech toolkit",
          "justification": "The referenced paper for SpeechBrain is cited in the research paper under evaluation.",
          "quote": "M. Ravanelli et al., “Speechbrain: A general-purpose speech toolkit,” 2021."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 4006,
    "prompt_tokens": 19885,
    "total_tokens": 23891
  }
}