{
  "paper": "2209.10015.txt",
  "words": 8562,
  "extractions": {
    "title": {
      "value": "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics",
      "justification": "Title extracted directly from the paper",
      "quote": "Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics"
    },
    "description": "This paper presents a unified framework called Metadata Archaeology via Probe Dynamics (MAP-D) for uncovering and inferring metadata of data examples in a dataset. The approach involves curating different subsets of data based on training dynamics, such as mislabeled, atypical, or out-of-distribution examples, and then leveraging differences in learning dynamics to infer metadata. The method is shown to be competitive with more complex solutions in a range of tasks, including identifying and correcting mislabeled examples, classifying minority-group samples, and prioritizing points relevant for training.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical results from experiments across multiple datasets and tasks to validate the proposed MAP-D approach.",
      "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper's primary research field focuses on image classification datasets and related tasks.",
        "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Data Quality",
          "justification": "The paper deals with data quality issues such as label noise, out-of-distribution samples, and mislabeled data.",
          "quote": "Existing methods for dealing with these challenges tend to make strong assumptions about the particular issues at play, and often require a priori knowledge or metadata such as domain labels."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Data Auditing",
          "justification": "One of the primary applications of the proposed MAP-D method is scalable human auditing of datasets to surface problematic examples.",
          "quote": "We show how MAP-D could be leveraged to audit large-scale datasets or debug model training, with negligible added cost."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Fairness",
          "justification": "The paper also deals with the problem of identifying minority group samples to ensure model fairness.",
          "quote": "Finally, we show how to use MAP-D to identify minority group samples, or surface examples for data-efficient prioritized training."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Metadata Archaeology via Probe Dynamics (MAP-D)",
          "justification": "The paper introduces this new model as a contribution.",
          "quote": "Our approach, which we term Metadata Archaeology via Probe Dynamics (MAP-D), leverages distinct differences in training dynamics for different curated subsets to enable specialized treatment and effective labelling of different metadata categories."
        },
        "aliases": [
          "MAP-D"
        ],
        "is_contributed": {
          "value": true,
          "justification": "MAP-D is introduced as the main contribution of the paper.",
          "quote": "We propose Metadata Archeology, a unifying and general framework for uncovering latent metadata categories."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments utilize GPUs for training deep learning models such as ResNet-50.",
          "quote": "In Figure 3, we present the training dynamics on the probe suites given a ResNet-50 model on ImageNet."
        },
        "is_compared": {
          "value": true,
          "justification": "MAP-D is compared with other methods in tasks such as label noise correction, prioritized training, and minority group detection.",
          "quote": "We benchmark against a series of different baselines (Arazo et al. [5], Zhang et al. [73], Patrini et al. [48], Reed et al. [51]), some of which are specifically developed to deal with label noise."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The model is newly introduced in this paper, and thus there is no reference paper.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "One of the six datasets used for evaluating the proposed MAP-D approach.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The CIFAR-10 dataset was originally introduced in this paper.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "One of the six datasets used for evaluating the proposed MAP-D approach.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The CIFAR-100 dataset was originally introduced in this paper.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "One of the six datasets used for evaluating the proposed MAP-D approach.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "The ImageNet dataset was originally introduced in this paper.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        }
      },
      {
        "name": {
          "value": "Waterbirds",
          "justification": "One of the six datasets used for evaluating the proposed MAP-D approach.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation",
          "justification": "The Waterbirds dataset was originally introduced in this paper.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        }
      },
      {
        "name": {
          "value": "CelebA",
          "justification": "One of the six datasets used for evaluating the proposed MAP-D approach.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep Learning Face Attributes in the Wild",
          "justification": "The CelebA dataset was originally introduced in this paper.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        }
      },
      {
        "name": {
          "value": "Clothing1M",
          "justification": "One of the six datasets used for evaluating the proposed MAP-D approach.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning from massive noisy labeled data for image classification",
          "justification": "The Clothing1M dataset was originally introduced in this paper.",
          "quote": "We present consistent results across six image classification datasets, CIFAR-10/CIFAR-100 [35], ImageNet [14], Waterbirds [54], CelebA [40], Clothing1M [69] and two models from the ResNet family [22]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments use PyTorch as the underlying deep learning framework.",
          "quote": "We use AdamW optimizer with default hyperparameters as in PyTorch [47] and ImageNet pretrained ResNet-50."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The reference paper introduces PyTorch as a deep learning library.",
          "quote": "We use AdamW optimizer with default hyperparameters as in PyTorch [47] and ImageNet pretrained ResNet-50."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2077,
    "prompt_tokens": 15410,
    "total_tokens": 17487
  }
}