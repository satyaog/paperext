{
  "paper": "2210.12236.txt",
  "words": 9749,
  "extractions": {
    "title": {
      "value": "Uncertain Evidence in Probabilistic Models and Stochastic Simulators",
      "justification": "The title is extracted directly from the research paper.",
      "quote": "Uncertain Evidence in Probabilistic Models and Stochastic Simulators"
    },
    "description": "This paper explores the problem of performing Bayesian inference in probabilistic models where observations come with uncertainty, termed as 'uncertain evidence.' It examines different methods for interpreting such evidence and their implications for inference about latent variables. The study revisits older methods like Jeffrey's rule and virtual evidence, and evaluates a newer method called distributional evidence. Through experiments, the paper illustrates how different interpretations of uncertain evidence can lead to significantly different inference results.",
    "type": {
      "value": "empirical",
      "justification": "The paper carries out experiments to showcase the impact of uncertain evidence interpretations on inference results.",
      "quote": "To showcase the impact of different interpretations of the same uncertain evidence, we carry out experiments in which one interpretation is defined as 'correct.'"
    },
    "primary_research_field": {
      "name": {
        "value": "Probabilistic Models",
        "justification": "The paper deals with making Bayesian inference in probabilistic models where observations come with uncertainty.",
        "quote": "This paper deals with the case where y is not observed exactly; rather it is associated with uncertainty which we refer to as 'uncertain evidence.'"
      },
      "aliases": []
    },
    "sub_research_fields": [],
    "models": [],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "PYPROB",
          "justification": "The paper mentions using the PYPROB library for probabilistic programming in the experiments.",
          "quote": "For distributional evidence we notice the form of q(t ∼ Dq |g) is the same as in Section 4.1 which allows for an analytical likelihood. In the case of Jeffrey’s rule we similarly trivially satisfy Theorem 3.3 (II), as g, t, and t̂ are one-dimensional, as well as Theorem 3.3 (III) ... but infer those using approximate Bayesian inference via the probabilistic programming language PYPROB"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "pyprob",
          "justification": "The PYPROB library was used for the experiments mentioned in the study.",
          "quote": "Armed with the formula x = gt2 /2 our student can convert measurements of t into estimates of g if x is known... but infer those using approximate Bayesian inference via the probabilistic programming language PYPROB (Baydin & Le, 2018)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 599,
    "prompt_tokens": 18529,
    "total_tokens": 19128
  }
}