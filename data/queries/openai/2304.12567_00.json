{
  "paper": "2304.12567.txt",
  "words": 12160,
  "extractions": {
    "title": {
      "value": "PROTO-VALUE NETWORKS: SCALING REPRESENTATION LEARNING WITH AUXILIARY TASKS",
      "justification": "The title of the paper is found at the top of the document.",
      "quote": "PROTO-VALUE NETWORKS : SCALING REPRESENTATION LEARNING WITH AUXILIARY TASKS"
    },
    "description": "The paper focuses on enhancing representation learning in deep reinforcement learning using a new family of auxiliary tasks based on the successor measure, termed Proto-Value Networks (PVNs). The experimental results on the Arcade Learning Environment highlight PVNs' ability to produce rich features with fewer interactions compared to established algorithms.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts a series of experiments on the Arcade Learning Environment to demonstrate the effectiveness of the proposed Proto-Value Networks, indicating that it is an empirical study.",
      "quote": "Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper primarily focuses on improving representation learning within the scope of deep reinforcement learning.",
        "quote": "In deep reinforcement learning (RL), an agent maps observations to a policy or return prediction by means of a neural network."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper emphasizes the importance of representation learning and introduces Proto-Value Networks to enhance it.",
          "quote": "An effective method for learning state representations is to have the network predict a collection of auxiliary tasks associated with each state."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Auxiliary Tasks",
          "justification": "The core contribution of the paper revolves around designing and scaling auxiliary tasks to improve state representation learning.",
          "quote": "Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Proto-Value Networks (PVN)",
          "justification": "The paper introduces and focuses on a new model called Proto-Value Networks (PVN).",
          "quote": "we call the resulting object proto-value networks."
        },
        "aliases": [
          "PVN"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "Proto-Value Networks is introduced by the authors in this paper.",
          "quote": "Accordingly, we call the resulting object proto-value networks."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is implemented and executed as part of the experiments in the paper.",
          "quote": "Specifically, we implement the successor measure (Blier et al., 2021; Touati & Ollivier, 2021), which extends the successor representation (Dayan, 1993) by replacing state-equality with set-inclusion."
        },
        "is_compared": {
          "value": 1,
          "justification": "Proto-Value Networks are compared with other representation learning methods in the study.",
          "quote": "we find that PVN produces state features that are rich enough to support linear value approximations that are comparable to those of DQN (Mnih et al., 2015) on a number of games."
        },
        "referenced_paper_title": {
          "value": "Proto-value functions: A laplacian framework for learning representation and control in markov decision processes",
          "justification": "The referenced paper for the proto-value functions is mentioned as the theoretical foundation for the model.",
          "quote": "extending Mahadevan & Maggioni (2007)â€™s proto-value functions to deep reinforcement learning"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Arcade Learning Environment (ALE)",
          "justification": "The experiments conducted in the paper utilize the Arcade Learning Environment datasets.",
          "quote": "We study the effectiveness of this method on the Arcade Learning Environment (ALE) (Bellemare et al., 2013)."
        },
        "aliases": [
          "ALE"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Arcade Learning Environment: An evaluation platform for general agents",
          "justification": "The referenced paper provides the background and details of the Arcade Learning Environment used in the experiments.",
          "quote": "(Bellemare et al., 2013)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Acme",
          "justification": "The Acme library is utilized for implementing the RL agents used in the experiments.",
          "quote": "Our agents are implemented using the Acme library (Hoffman et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Acme: A research framework for distributed reinforcement learning",
          "justification": "The referenced paper provides details of the Acme framework which is used for implementing the agents.",
          "quote": "(Hoffman et al., 2020)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 943,
    "prompt_tokens": 22618,
    "total_tokens": 23561
  }
}