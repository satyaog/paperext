{
  "paper": "a473845f2009dca1e2660b56d0aaaf05.txt",
  "words": 12922,
  "extractions": {
    "title": {
      "value": "Resolution Enhancement with a Task-Assisted GAN to Guide Optical Nanoscopy Image Analysis and Acquisition",
      "justification": "This is the title of the paper",
      "quote": "Resolution Enhancement with a\nTask-Assisted GAN to Guide Optical\nNanoscopy Image Analysis and Acquisition"
    },
    "description": "This paper introduces the Task-Assisted Generative Adversarial Network (TA-GAN) for enhancing the resolution of diffraction-limited microscopy images, especially in the context of biological nanostructures. The work evaluates the generative accuracy of TA-GAN using confocal and STED microscopy images and explores its utility in online live-cell imaging assistance to mitigate light exposure and optimize acquisition sequences.",
    "type": {
      "value": "Empirical",
      "justification": "The paper involves experimental evaluation and comparisons using datasets and developed models, indicating it's an empirical study.",
      "quote": "We evaluate how TA-GAN improves generative accuracy over unassisted methods\nusing images acquired with two modalities: confocal (diffraction-limited)\nand STimulated Emission Depletion (STED, super-resolved) microscopy."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The study focuses on enhancing resolution in microscopy images, a common application in the field of computer vision.",
        "quote": "We propose to guide the image generation process using a complementary\ntask that is closely related to the biological question at hand. This approach\nimproves the applicability of synthetic data generation using deep learning in\nmicroscopy and ensures that the generated features in synthetic images are\nconsistent with the observed biological structures in real images."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Microscopy Image Analysis",
          "justification": "The paper specifically addresses challenges and solutions related to microscopy images.",
          "quote": "Deep learning-based super-resolution approaches have been gaining similar interest for\nmicroscopy images, as they achieve high performance in reducing blurring artifacts and noise [13, 14]."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Generative Adversarial Networks (GANs)",
          "justification": "The main contributions of the paper involve advancements in GANs for image resolution enhancement.",
          "quote": "We designed a Task-Assisted GAN (TA-GAN)\nfor confocal-to-STED super-resolution image generation, which is optimized\nto perform well over a complementary task associated with the nanostructures\nof interest that are unresolved in confocal images."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Task-Assisted Generative Adversarial Network (TA-GAN)",
          "justification": "The primary model proposed and evaluated in this study is the Task-Assisted Generative Adversarial Network (TA-GAN).",
          "quote": "Our model, named Task-Assisted Generative Adversarial Network (TA-GAN), incorporates an auxiliary task (e.g. segmentation, localization) that is closely related to the characterization of the observed biological nanostructures."
        },
        "aliases": [
          "TA-GAN"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The TA-GAN is explicitly developed and evaluated in the scope of this paper.",
          "quote": "We designed a Task-Assisted GAN (TA-GAN) for confocal-to-STED super-resolution image generation..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments and evaluations involving TA-GAN were executed on computational resources, likely involving GPUs given the context of deep learning models.",
          "quote": "Steps 2, 3 and 4 needed to be computed with a graphical processing unit (GPU) to avoid computation induced delays."
        },
        "is_compared": {
          "value": 1,
          "justification": "TA-GAN is compared against other baseline models in this study to evaluate its performance",
          "quote": "Our results demonstrate that the TA-GAN and TA-CycleGAN models improve the synthetic representation of biological nanostructures in comparison to other deep learning-based super-resolution approaches."
        },
        "referenced_paper_title": {
          "value": "Image-to-Image Translation with Conditional Adversarial Networks",
          "justification": "The TA-GAN model is based on the conditional GAN architecture, which was introduced in this paper.",
          "quote": "The TA-GAN was developed from the conditional GAN model for\nimage-to-image translation [2] available at https://github.com/junyanz/\npytorch-CycleGAN-and-pix2pix."
        }
      },
      {
        "name": {
          "value": "TA-CycleGAN",
          "justification": "Another model introduced in the paper is the TA-CycleGAN, which adapts the CycleGAN model for unpaired datasets.",
          "quote": "We expand the applicability\nof the method with a variation called TA-CycleGAN, based on the CycleGAN\nmodel "
        },
        "aliases": [
          "TA-CycleGAN"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "TA-CycleGAN is introduced and developed in the paper as a variant of TA-GAN for unpaired datasets.",
          "quote": "We expand the applicability\nof the method with a variation called TA-CycleGAN, based on the CycleGAN\nmodel"
        },
        "is_executed": {
          "value": 1,
          "justification": "TA-CycleGAN, like TA-GAN, is executed using computational resources, probably involving GPUs.",
          "quote": "Using our task-assisted strategy, we trained a CycleGAN [2] model, as it\nwas precisely developed for image domain translation for unpaired datasets."
        },
        "is_compared": {
          "value": 1,
          "justification": "TA-CycleGAN's performance is evaluated and compared against other approaches for synthetic image generation and domain adaptation.",
          "quote": "Our results demonstrate that the TA-GAN and TA-CycleGAN models improve the synthetic representation of biological nanostructures in comparison to other deep learning-based super-resolution approaches."
        },
        "referenced_paper_title": {
          "value": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",
          "justification": "TA-CycleGAN is based on the CycleGAN architecture introduced in this referenced paper.",
          "quote": "We expand the applicability of the method with a variation called TA-CycleGAN, based on the CycleGAN model [37]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Axonal F-actin dataset",
          "justification": "This dataset is used to train and evaluate the TA-GAN model for generating super-resolved images of axonal F-actin.",
          "quote": "The first TA-GAN model is trained to generate STED images of the axonal\nF-actin lattice from confocal images (Figure 1b)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Neuronal activity remodels the F-actin based submembrane lattice in dendrites but not axons of hippocampal neurons",
          "justification": "The dataset originates from the study by Lavoie-Cardinal et al. referenced in this paper.",
          "quote": "The Axonal F-actin dataset was used to train the TA-GAN for confocal-to-STED super-resolution of axonal F-actin rings using a binary segmentation\ntask. The original dataset consisted of 516 paired confocal and STED images\n(224 x 224 pixels) of axonal F-actin from Lavoie-Cardinal et al. [3]."
        }
      },
      {
        "name": {
          "value": "Dendritic F-actin dataset",
          "justification": "This dataset is essential for evaluating the TA-GAN and TA-CycleGAN models, focusing on dendritic F-actin structures.",
          "quote": "The performance of the TA-GAN is next evaluated on a more complex task,\nwhich is the semantic segmentation of two nanostructures that can be differentiated only with super-resolution optical microscopy: dendritic F-actin rings\nand fibers [3, 44]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Neuronal activity remodels the F-actin based submembrane lattice in dendrites but not axons of hippocampal neurons",
          "justification": "The dataset comes from Lavoie-Cardinal et al.'s study, which is referenced in the paper.",
          "quote": "The Dendritic F-actin dataset was used to train the TA-GAN for confocal-to-\nSTED super-resolution of dendritic F-actin nanostructures using a semantic\nsegmentation task as well as to train the TA-CycleGAN for live and fixed\ndomain adaptation. The original dataset from Lavoie-Cardinal et al. [3] was\nsplit into a training set (304 images), a validation set (54 images), and a\ntesting set (26 images, 12 for low activity and 14 for high activity)."
        }
      },
      {
        "name": {
          "value": "Synaptic protein dataset",
          "justification": "This dataset is utilized to train the TA-GAN model for super-resolution tasks involving synaptic proteins.",
          "quote": "We use automatically generated localization annotations to train the\nTA-GAN for confocal-to-STED super-resolution of the Synaptic protein dataset\nconsisting of two-channels images of synaptic proteins clusters [4]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Activity-dependent remodeling of synaptic protein organization revealed by high throughput analysis of STED nanoscopy images",
          "justification": "The dataset comes from the study by Wiesner et al., as referenced in the paper.",
          "quote": "Images from the original dataset from Wiesner et al. [4] were split into a training set (81 images), a validation set (22 images) and a testing set (12\nPSD95-Bassoon and 7 PSD95-Homer1c images)."
        }
      },
      {
        "name": {
          "value": "Live F-actin dataset",
          "justification": "This dataset is crucial for the evaluation of the TA-CycleGAN model, enabling adaptation to live-cell conditions.",
          "quote": "A large FOV was first imaged at low-resolution from which regions were manually\nselected by an expert. For each region selected, a confocal and a STED image\nwere acquired sequentially. The Live F-actin dataset consists in 904 paired\nSTED and confocal images of F-actin stained with the fluorogenic dye SiR-\nActin (Spirochrome, US) in living hippocampal cultured neurons. The dataset\nwas split into a training set (833 images) and a validation set (71 images)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Fluorogenic probes for live-cell imaging of the cytoskeleton",
          "justification": "The dataset is based on methodologies involving SiR-Actin, which was introduced in the referenced paper by Lukinavičius et al.",
          "quote": "The Live F-actin dataset was used to train the TA-GAN for resolution enhancement of live-cell images. The original dataset is from Lavoie-Cardinal et al.'s work that was cited in this paper."
        }
      },
      {
        "name": {
          "value": "Dendritic F-actin dataset adapted to the live-cell STED imaging domain",
          "justification": "This adapted dataset is generated using the TA-CycleGAN to train a network for live-cell imaging.",
          "quote": "This dataset corresponds to the Dendritic F-actin dataset adapted to the live-cell STED imaging domain using the TA-CycleGAN for fixed-to-live domain adaptation"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Neuronal activity remodels the F-actin based submembrane lattice in dendrites but not axons of hippocampal neurons",
          "justification": "The adapted dataset is derived from the Dendritic F-actin dataset introduced in Lavoie-Cardinal et al.'s work.",
          "quote": "This dataset corresponds to the Dendritic F-actin dataset adapted to the live-cell STED imaging domain using the TA-CycleGAN for fixed-to-live domain adaptation (TA-CycleGAN for domain adaptation section)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The models in the paper, such as TA-GAN and TA-CycleGAN, are implemented using PyTorch.",
          "quote": "The TA-GAN was developed from the conditional GAN model for image-to-image translation [2], available at https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "PyTorch is a core library used for developing the GAN models in this paper.",
          "quote": "The TA-GAN was developed from the conditional GAN model for image-to-image translation [2], available at https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2552,
    "prompt_tokens": 22172,
    "total_tokens": 24724
  }
}