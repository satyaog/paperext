{
  "paper": "UaCrxyUeyE.txt",
  "words": 5222,
  "extractions": {
    "title": {
      "value": "On the Varied Faces of Overparameterization in Supervised and Self-Supervised Learning",
      "justification": "The paper title is clearly stated at the beginning of the provided excerpt.",
      "quote": "On the Varied Faces of Overparameterization in Supervised and Self-Supervised Learning"
    },
    "description": "This paper explores how overparameterization affects neural networks' representation quality and generalization in both supervised and self-supervised learning settings. The authors use information geometric measures like the spectral norm of the feature Jacobian and the coefficient of spectral decay to assess representation quality.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental analysis on different datasets and models to study the impact of overparameterization.",
      "quote": "In this work, we experimentally connect α to input sensitivity of the features fθ , by studying the expected spectral norm of the input Jacobian J."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on representation learning, a core area within Deep Learning.",
        "quote": "The quality of the representations learned by neural networks depends on several factors."
      },
      "aliases": [
        "Representation Learning",
        "Neural Networks"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper evaluates the representation quality in self-supervised learning contexts.",
          "quote": "Self-supervised learning (SSL) models learn representations from large unlabeled datasets."
        },
        "aliases": [
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Supervised Learning",
          "justification": "The paper also evaluates the impact of supervised learning on representation quality under overparameterization.",
          "quote": "In particular, we show that in self-supervised learning, the worst-case sensitivity of the model to inputs monotonically increases with overparameterization in conjunction with the emergence of heavier tails in the feature eigenspectrum."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet18",
          "justification": "The model name is explicitly stated in the context of various experiments using it.",
          "quote": "Comparisons are made with both Barlow Twins and BYOL models using a ResNet18 encoder."
        },
        "aliases": [
          "ResNet-18"
        ],
        "is_contributed": {
          "value": false,
          "justification": "ResNet18 is not introduced in this paper but is used for experimentation.",
          "quote": "As our backbones, we use convolutional networks with residual connections (ResNet18)."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was used for experimentation as mentioned multiple times in the context of tests conducted.",
          "quote": "In particular, increasing model width monotonically improves both train and test loss."
        },
        "is_compared": {
          "value": true,
          "justification": "ResNet18 is compared to other models in terms of performance metrics.",
          "quote": "For our SSL experiments, we use a standard ResNet18 backbone with batch normalization layers as the feature encoder."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "This is the reference paper for ResNet18.",
          "quote": "He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778)."
        }
      },
      {
        "name": {
          "value": "Barlow Twins",
          "justification": "The model name is explicitly mentioned in the context of self-supervised learning experiments.",
          "quote": "For learning objectives, we consider supervised learning as the baseline, and compare learning dynamics to Barlow Twins."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Barlow Twins is used as an existing self-supervised learning model.",
          "quote": "For learning objectives, we consider supervised learning as the baseline, and compare learning dynamics to Barlow Twins."
        },
        "is_executed": {
          "value": true,
          "justification": "Barlow Twins is actually used in the experiments.",
          "quote": "We train Resnet-18 backbones with varying base widths using the Barlow-Twins learning objective."
        },
        "is_compared": {
          "value": true,
          "justification": "Barlow Twins is compared to other SSL models in the experiments.",
          "quote": "In particular, increasing model width monotonically improves both train and test loss. We report similar observations on STL10, and with BYOL as the pretraining objective in the Appendix C."
        },
        "referenced_paper_title": {
          "value": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "justification": "This is the reference paper for Barlow Twins.",
          "quote": "Zbontar, J., Jing, L., Misra, I., LeCun, Y., & Deny, S. (2021). Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning (pp. 12310-12320). PMLR."
        }
      },
      {
        "name": {
          "value": "BYOL",
          "justification": "The model name is explicitly mentioned in the context of self-supervised learning experiments.",
          "quote": "We report similar observations on STL10, and with BYOL as the pretraining objective in the Appendix C."
        },
        "aliases": [
          "Bootstrap Your Own Latent"
        ],
        "is_contributed": {
          "value": false,
          "justification": "BYOL is used as an existing self-supervised learning model.",
          "quote": "Similarly to the supervised experiments, we control model size by varying the base width ω = 1, . . . , 64. Accordingly, the embedding dimensionality varies from dimension d = 32 to d = 2048. To ensure good performance, we use a non-linear projector head consisting of one hidden MLP layer with batch normalization and ReLU activations, with width matching the embedding dimensionality of the encoder. SSL features are learned on CIFAR-10 by using Barlow Twins (Zbontar et al., 2021) and BYOL (Grill et al., 2020). At the end of SSL training, the projection layer is discarded, and the ResNet18 encoder is used to generate features fθ (x)."
        },
        "is_executed": {
          "value": true,
          "justification": "BYOL is actually used in the experiments.",
          "quote": "In our experiments, following Nakkiran et al. (2019), we vary the base width in the range ω = 1, . . . , 64. By controlling the network size through the width, we produce a range of models presenting model-wise double descent in the test error, which captures the essence of the benign overfitting phenomenon (Bartlett et al., 2020) observed for large interpolating networks, while also presenting malign overfitting for models near the interpolation threshold."
        },
        "is_compared": {
          "value": true,
          "justification": "BYOL is compared to other SSL models in the experiments.",
          "quote": "For learning objectives, we consider supervised learning as the baseline, and compare learning dynamics to Barlow Twins (Zbontar et al., 2021), BYOL (Grill et al., 2020) with 2 augmentations."
        },
        "referenced_paper_title": {
          "value": "Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning",
          "justification": "This is the reference paper for BYOL.",
          "quote": "Grill, J., Strub, F., Altché, F., Tallec, C., Richemond, P. H., Buchatskaya, E., ... & others. (2020). Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33, 21271-21284."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used extensively in the experiments mentioned in the paper.",
          "quote": "For learning objectives, we consider supervised learning as the baseline, and compare learning dynamics to Barlow Twins (Zbontar et al., 2021), BYOL (Grill et al., 2020) with 2 augmentations. Experimental details are provided in appendix A. In line with prior work, we consider the noisy-label regime to test the sensitivity and quality of features (Neyshabur et al., 2017). To measure information geometry of the learned mappings, we approximate α and the Jacobian spectral norm, per protocols elucidated in Appendix B. We also present additional results on purely convolutional models in the Appendix C."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the reference paper for the CIFAR-10 dataset.",
          "quote": "Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical report, University of Toronto."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is also extensively used in the experiments mentioned in the paper.",
          "quote": "We train all networks for 1000 epochs. To stabilize prolonged training, we use learning rate warmup over the first 5 epochs of training, starting from a learning rate η0 = 10−1 × η."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the reference paper for the CIFAR-100 dataset.",
          "quote": "Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images. Technical report, University of Toronto."
        }
      },
      {
        "name": {
          "value": "STL-10",
          "justification": "STL-10 is used in the self-supervised learning experiments mentioned in the paper.",
          "quote": "We report similar observations on STL10, and with BYOL as the pretraining objective in the Appendix C."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "An Analysis of Single-Layer Networks in Unsupervised Feature Learning",
          "justification": "This is the reference paper for the STL-10 dataset.",
          "quote": "Coates, A., Ng, A., & Lee, H. (2011). An analysis of single-layer networks in unsupervised feature learning. In Proceeding of the fourteenth international conference on artificial intelligence and statistics (pp. 215-223)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The library is used for implementing the experimental codebase.",
          "quote": "Our codebase is implemented in Pytorch version 1.11."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "This is the reference paper for PyTorch.",
          "quote": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S. (2019). Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32."
        }
      },
      {
        "name": {
          "value": "Adam",
          "justification": "The Adam optimizer is used for SSL training.",
          "quote": "SSL learning is carried out using the Adam optimizer (Kingma & Ba, 2014), with starting learning rate 0.001."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A Method for Stochastic Optimization",
          "justification": "This is the reference paper for the Adam optimizer.",
          "quote": "Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2844,
    "prompt_tokens": 10368,
    "total_tokens": 13212
  }
}