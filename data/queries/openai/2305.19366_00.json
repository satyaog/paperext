{
  "paper": "2305.19366.txt",
  "words": 17586,
  "extractions": {
    "title": {
      "value": "Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network",
      "justification": "This is the title of the paper.",
      "quote": "Joint Bayesian Inference of Graphical Structure and Parameters with a Single Generative Flow Network"
    },
    "description": "This paper introduces a new method called JSP-GFN for approximating the joint posterior distribution over the structure and parameters of a Bayesian Network using a single Generative Flow Network. The approach involves a two-phase sampling process: generating the DAG sequentially one edge at a time, followed by sampling the parameters. The method is validated empirically against existing methods using simulated and real datasets, and demonstrates favorable results.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents a new method and evaluates its performance using both simulated and real data.",
      "quote": "We validate empirically that JSP-GFN provides an accurate approximation of the posterior when those conditions are approximately satisfied by a learned sampling model, and compares favorably against existing methods on simulated and real data."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper focuses on methods for Bayesian inference and structure learning, which are subfields of Machine Learning.",
        "quote": "Since they provide a framework for generative modeling of discrete and composite objects, Generative Flow Networks (GFlowNets; Bengio et al., 2021, 2023) proved to be an effective method for Bayesian structure learning."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Bayesian Inference",
          "justification": "The paper aims to improve upon methods for Bayesian inference, specifically for learning the joint posterior distribution of a Bayesian Network's structure and parameters.",
          "quote": "In this paper, we propose to infer the joint posterior over graphical structures G and parameters of the conditional probability distributions θ of a Bayesian Network using a single GFlowNet called JSP-GFN (for Joint Structure and Parameters GFlowNet), leveraging recent advances extending GFlowNets to continuous sample spaces."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Structure Learning",
          "justification": "The proposed JSP-GFN method is used to approximate the joint posterior over the structure (graphical model) and parameters of Bayesian Networks, which is a key problem in structure learning.",
          "quote": "We use a single GFlowNet whose sampling policy follows a two-phase process: the DAG is first generated sequentially one edge at a time, and then the corresponding parameters are picked once the full structure is known."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "JSP-GFN (Joint Structure and Parameters GFlowNet)",
          "justification": "The paper introduces the JSP-GFN model for Bayesian inference of graphical structures and parameters.",
          "quote": "We propose to infer the joint posterior over graphical structures G and parameters of the conditional probability distributions θ of a Bayesian Network using a single GFlowNet called JSP-GFN."
        },
        "aliases": [
          "JSP-GFN"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The JSP-GFN model is introduced as the primary contribution of the paper.",
          "quote": "We propose to infer the joint posterior over graphical structures G and parameters of the conditional probability distributions θ of a Bayesian Network using a single GFlowNet called JSP-GFN."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of empirical validation against existing methods.",
          "quote": "We validate empirically that JSP-GFN provides an accurate approximation of the posterior when those conditions are approximately satisfied by a learned sampling model, and compares favorably against existing methods on simulated and real data."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the JSP-GFN model is compared numerically to other Bayesian inference methods on both simulated and real data.",
          "quote": "We validate empirically that JSP-GFN provides an accurate approximation of the posterior when those conditions are approximately satisfied by a learned sampling model, and compares favorably against existing methods on simulated and real data."
        },
        "referenced_paper_title": {
          "value": "GFlowNet Foundations",
          "justification": "The GFlowNet model and its extensions are foundational to the JSP-GFN method.",
          "quote": "In Deleu et al. (2022), the problem of generating a sample DAG from the marginal posterior P (G | D) was treated as a sequential decision process, where edges are added one at a time, starting from the empty graph over d variables, following a learned transition probability."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Flow Cytometry Data",
          "justification": "The paper evaluates the JSP-GFN approach using flow cytometry data for discovering protein signaling networks.",
          "quote": "The flow cytometry dataset consists of N = 4, 200 measurements of d = 11 phosphoproteins from 7 different experiments, meaning that this dataset contains a mixture of both observational and interventional data."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Causal protein-signaling networks derived from multiparameter single-cell data.",
          "justification": "The referenced paper is a key source for the flow cytometry dataset used in the evaluation.",
          "quote": "Sachs, K., Perez, O., Pe'er, D., Lauffenburger, D. A., & Nolan, G. P. (2005). Causal protein-signaling networks derived from multiparameter single-cell data. Science, 308(5721), 523-529."
        }
      },
      {
        "name": {
          "value": "Gene Expression Data",
          "justification": "The paper evaluates the JSP-GFN approach using gene expression data for discovering a small gene regulatory network.",
          "quote": "For the gene expression dataset, we used a subset of N = 2, 628 observations of d = 61 genes from (Sethuraman et al., 2023)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "NODAGS-Flow: Nonlinear Cyclic Causal Structure Learning.",
          "justification": "The referenced paper is a key source for the gene expression dataset used in the evaluation.",
          "quote": "Sethuraman, M. G., Lopez, R., Mohan, R., Fekri, F., Biancalani, T., & Huetter, J. C. (2023). NODAGS-Flow: Nonlinear Cyclic Causal Structure Learning. AISTATS."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions using neural network libraries for training models, and PyTorch is a common choice for such tasks.",
          "quote": "Note that Pϕ (θi | G, stop) effectively approximates the posterior distribution P (θi | G, D) once fully trained. Moreover, in addition to being an approximation of the joint posterior P (G, θ | D), the GFlowNet also provides an approximation of the marginal posterior P (G | D), by only following the first phase of the generation process (to generate G) until the “stop” action is selected, and not continuing into the generation of θ."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The referenced paper details the PyTorch library, which is commonly used in deep learning research for training neural networks.",
          "quote": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G.,... & Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1550,
    "prompt_tokens": 30244,
    "total_tokens": 31794
  }
}