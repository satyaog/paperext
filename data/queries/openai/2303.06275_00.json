{
  "paper": "2303.06275.txt",
  "words": 7399,
  "extractions": {
    "title": {
      "value": "A Systematic Study of Joint Representation Learning on Protein Sequences and Structures",
      "justification": "This is the title of the paper as given in the provided text.",
      "quote": "A Systematic Study of Joint Representation Learning on Protein Sequences and Structures"
    },
    "description": "This paper explores joint protein representation learning by integrating a state-of-the-art Protein Language Model (ESM-2) with various structure encoders (GVP, GearNet, CDConv). It introduces fusion strategies and different pre-training techniques to improve protein function prediction tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments to evaluate the integration of PLMs and structure encoders, introduces new methods, and reports results based on empirical data.",
      "quote": "Our method achieves significant improvements over existing sequence- and structure-based methods, setting new state-of-the-art for function annotation."
    },
    "primary_research_field": {
      "name": {
        "value": "Computational Biology",
        "justification": "The paper focuses on protein representation learning, which is a core topic within computational biology.",
        "quote": "Learning effective protein representations is critical in a variety of tasks in biology such as predicting protein functions."
      },
      "aliases": [
        "Computational Biology"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Protein Function Prediction",
          "justification": "The goal of the paper is to enhance protein function prediction using joint representation learning.",
          "quote": "This study underscores several important design choices for fusing protein sequence and structure information. Our method achieves significant improvements over existing sequence- and structure-based methods, setting new state-of-the-art for function annotation."
        },
        "aliases": [
          "Function Annotation",
          "Function Prediction"
        ]
      },
      {
        "name": {
          "value": "Structural Biology",
          "justification": "The paper involves learning from protein structures and integrating structure-based encoders.",
          "quote": "In contrast, structure-based methods leverage 3D structural information with graph neural networks and geometric pre-training methods show potential in function prediction tasks."
        },
        "aliases": [
          "Protein Structure Analysis"
        ]
      },
      {
        "name": {
          "value": "Graph Neural Networks",
          "justification": "The paper utilizes graph neural networks for structure-based protein representation learning.",
          "quote": "In contrast, structure-based methods leverage 3D structural information with graph neural networks and geometric pre-training methods show potential in function prediction tasks."
        },
        "aliases": [
          "GNNs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ESM-2",
          "justification": "ESM-2 is frequently mentioned in the paper as a state-of-the-art protein language model used in the study.",
          "quote": "Our study undertakes a comprehensive exploration of joint protein representation learning by integrating a state-of-the-art PLM (ESM-2) with distinct structure encoders (GVP, GearNet, CDConv)."
        },
        "aliases": [
          "ESM"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ESM-2 is a pre-existing model referenced in the study.",
          "quote": "These protein language models (PLMs)... notable instances include... ESM (Rives et al. 2021; Lin et al. 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and tested in the context of this study.",
          "quote": "We employ the fusion of three structure encoders with ESM-2-650M using three distinct fusion strategies."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of ESM-2 is compared with other models.",
          "quote": "we observe that while the raw performance of vanilla GearNet lags behind other encoders like CDConv, its integration with PLMs yields better outcomes"
        },
        "referenced_paper_title": {
          "value": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
          "justification": "The title of the referenced paper from which ESM-2 is derived.",
          "quote": "These protein language models (PLMs)... notable instances include... ESM (Rives et al. 2021; Lin et al. 2023)."
        }
      },
      {
        "name": {
          "value": "GearNet",
          "justification": "GearNet is one of the structure encoders used in the study.",
          "quote": "our study undertakes a comprehensive exploration of joint protein representation learning by integrating a state-of-the-art PLM (ESM-2) with distinct structure encoders (GVP, GearNet, CDConv)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GearNet is a pre-existing model referenced in the study.",
          "quote": "Typical examples include GearNet (Zhang et al. 2022b), GVP (Jing et al. 2021), CDConv (Fan et al. 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and tested in the context of this study.",
          "quote": "We employ the fusion of three structure encoders with ESM-2-650M using three distinct fusion strategies."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of GearNet is compared with other models.",
          "quote": "we observe that while the raw performance of vanilla GearNet lags behind other encoders like CDConv, its integration with PLMs yields better outcomes"
        },
        "referenced_paper_title": {
          "value": "Protein Representation Learning by Geometric Structure Pretraining",
          "justification": "The title of the referenced paper from which GearNet is derived.",
          "quote": "GearNet (Zhang et al. 2022b)"
        }
      },
      {
        "name": {
          "value": "GVP",
          "justification": "GVP is one of the structure encoders used in the study.",
          "quote": "our study undertakes a comprehensive exploration of joint protein representation learning by integrating a state-of-the-art PLM (ESM-2) with distinct structure encoders (GVP, GearNet, CDConv)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GVP is a pre-existing model referenced in the study.",
          "quote": "Typical examples include GearNet (Zhang et al. 2022b), GVP (Jing et al. 2021), CDConv (Fan et al. 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and tested in the context of this study.",
          "quote": "We employ the fusion of three structure encoders with ESM-2-650M using three distinct fusion strategies."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of GVP is compared with other models.",
          "quote": "we observe that while the raw performance of vanilla GearNet lags behind other encoders like CDConv, its integration with PLMs yields better outcomes"
        },
        "referenced_paper_title": {
          "value": "Learning from Protein Structure with Geometric Vector Perceptrons",
          "justification": "The title of the referenced paper from which GVP is derived.",
          "quote": "GVP (Jing et al. 2021)"
        }
      },
      {
        "name": {
          "value": "CDConv",
          "justification": "CDConv is one of the structure encoders used in the study.",
          "quote": "our study undertakes a comprehensive exploration of joint protein representation learning by integrating a state-of-the-art PLM (ESM-2) with distinct structure encoders (GVP, GearNet, CDConv)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CDConv is a pre-existing model referenced in the study.",
          "quote": "Typical examples include GearNet (Zhang et al. 2022b), GVP (Jing et al. 2021), CDConv (Fan et al. 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and tested in the context of this study.",
          "quote": "We employ the fusion of three structure encoders with ESM-2-650M using three distinct fusion strategies."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of CDConv is compared with other models.",
          "quote": "we observe that while the raw performance of vanilla GearNet lags behind other encoders like CDConv, its integration with PLMs yields better outcomes"
        },
        "referenced_paper_title": {
          "value": "Continuous-Discrete Convolution for Geometry-Sequence Modeling in Proteins",
          "justification": "The title of the referenced paper from which CDConv is derived.",
          "quote": "CDConv (Fan et al. 2023)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AlphaFold Database",
          "justification": "The AlphaFold Database is used for pre-training in the study.",
          "quote": "We further explore six diverse pre-training techniques, employing the optimal model from the aforementioned choices and leveraging pre-training on the AlphaFold Database."
        },
        "aliases": [
          "AlphaFold"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "AlphaFold Protein Structure Database: massively expanding the structural coverage of protein-sequence space with high-accuracy models",
          "justification": "The AlphaFold Database is referenced for pre-training in this study.",
          "quote": "We perform 50 epochs of pre-training on the AlphaFold Database, following the hyperparameters in (Zhang et al. 2022b)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TorchDrug",
          "justification": "The paper mentions that the models are implemented using TorchDrug.",
          "quote": "These models are implemented using the TorchDrug library (Zhu et al. 2022) and trained across 4 A100 GPUs."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "TorchDrug: A Powerful and Flexible Machine Learning Platform for Drug Discovery",
          "justification": "TorchDrug is referenced as the implementing library in the study.",
          "quote": "These models are implemented using the TorchDrug library (Zhu et al. 2022) and trained across 4 A100 GPUs."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2434,
    "prompt_tokens": 15044,
    "total_tokens": 17478
  }
}