{
  "paper": "2211.15533.txt",
  "words": 10706,
  "extractions": {
    "title": {
      "value": "The Stack: 3 TB of permissively licensed source code",
      "justification": "Title can be found at the beginning of the paper.",
      "quote": "The Stack: 3 TB of permissively licensed source code\nDenis Kocetkov∗\nServiceNow Research\n\nRaymond Li∗"
    },
    "description": "The paper introduces The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages. It details the collection process, the construction of subsets, data governance plans, limitations, and results from training models on Python subsets.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents data collection processes, experimental results on different datasets, and performance metrics, which are key components of empirical research.",
      "quote": "We describe how we collect the full dataset, construct a permissively licensed subset, present a data governance plan, discuss limitations, and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus is on leveraging large language models for code understanding and generation, a subfield of NLP.",
        "quote": "Large Language Models (LLMs) play an ever-increasing role in the field of Artificial Intelligence (AI)–not only for natural language processing but also for code understanding and generation."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Code Understanding and Generation",
          "justification": "A significant portion of the study is dedicated to exploring how LLMs can be used for code-related tasks.",
          "quote": "To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages."
        },
        "aliases": [
          "Code Understanding",
          "Code Generation",
          "Code LLMs"
        ]
      },
      {
        "name": {
          "value": "Dataset Collection and Preprocessing",
          "justification": "The paper details the data collection, preprocessing, and dataset construction processes extensively.",
          "quote": "We present The Stack, a large dataset with 3.1 TB of permissively licensed source code in 30 programming languages. We release this dataset along with a near-deduplicated version at https://hf.co/BigCode."
        },
        "aliases": [
          "Data Collection",
          "Data Preprocessing"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Codex",
          "justification": "Codex is mentioned as a model used for comparison in text2code performance benchmarks.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Codex was not introduced in this paper but used for comparison purposes.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not report executing Codex, but only comparing against it.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "is_compared": {
          "value": 1,
          "justification": "Codex was used for numerical comparison.",
          "quote": "We outperform these models by a large margin if we train on the all-license version of the dataset."
        },
        "referenced_paper_title": {
          "value": "Evaluating large language models trained on code",
          "justification": "This is the referenced paper for Codex as noted in the text.",
          "quote": "Codex (Chen et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "The Stack",
          "justification": "The primary dataset discussed and introduced in the paper.",
          "quote": "To stimulate open and responsible research on LLMs for code, we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages."
        },
        "aliases": [
          "The Stack v1.1",
          "BigCode Dataset"
        ],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The Stack is introduced for the first time in this paper.",
          "quote": "We introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Megatron-LM",
          "justification": "Megatron-LM is explicitly mentioned as the library used for training the models.",
          "quote": "We use a fork of Megatron-LM (Shoeybi et al., 2019) for training."
        },
        "aliases": [
          "Megatron"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
          "justification": "This is the referenced paper for Megatron-LM as noted in the text.",
          "quote": "We use a fork of Megatron-LM (Shoeybi et al., 2019) for training."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1073,
    "prompt_tokens": 22407,
    "total_tokens": 23480
  }
}