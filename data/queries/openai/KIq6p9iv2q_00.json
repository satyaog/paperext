{
  "paper": "KIq6p9iv2q.txt",
  "words": 18661,
  "extractions": {
    "title": {
      "value": "Towards Perpetually Trainable Neural Networks",
      "justification": "The given title of the research paper.",
      "quote": "Towards Perpetually Trainable Neural Networks"
    },
    "description": "This paper explores the loss of plasticity in neural networks trained on nonstationary data distributions, identifies the independent mechanisms causing this loss, and proposes a unified training protocol to maintain plasticity. It validates its approach in synthetic nonstationary tasks, reinforcement learning, and natural distribution shifts.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experiments and empirical analyses in various domains like synthetic nonstationary tasks, reinforcement learning, and distribution shifts.",
      "quote": "We validate this approach in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on two naturally arising nonstationarities: reinforcement learning in the arcade learning environment, and by an adaptation of the WiLDS distribution shift benchmark."
    },
    "primary_research_field": {
      "name": {
        "value": "Continual Learning",
        "justification": "The paper focuses on maintaining plasticity in neural networks over continuous and nonstationary data distributions.",
        "quote": "Our analysis of these mechanisms sheds new light onto the well-known phenomenon of dormant neurons, revealing that saturated ReLU units are a symptom of a deeper underlying pathology in the network’s optimization dynamics characterized by shifts in the distribution of preactivations, which can cause plasticity loss even in non-saturating nonlinearities."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper analyzes the loss of plasticity in deep reinforcement learning settings and validates its mitigation strategies in the arcade learning environment.",
          "quote": "Critically, we show that mitigation strategies to these mechanisms can be combined to produce learners which are significantly more robust to nonstationarity than would be obtained by any single strategy in isolation."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Neural Network Optimization",
          "justification": "The paper delves into mechanisms like unit saturation, preactivation distribution shift, and parameter growth, which are directly related to neural network optimization.",
          "quote": "One factor driving this instability is the loss of plasticity, meaning that updating the network’s predictions in response to new information becomes more difficult as training progresses."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Multilayer Perceptron (MLP)",
          "justification": "The paper refers to using MLPs for training on nonstationary image classification tasks.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "aliases": [
          "MLP"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The MLP is not an original contribution of this paper.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "is_executed": {
          "value": true,
          "justification": "The MLP was executed in several experiments involving synthetic tasks and reinforcement learning.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "is_compared": {
          "value": true,
          "justification": "The MLP was compared against other architectures like CNN and ResNet-18 to evaluate performance.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The MLP is a standard model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Convolutional Neural Network (CNN)",
          "justification": "CNNs are referenced as part of the empirical evaluation for various tasks.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "aliases": [
          "CNN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The CNN is not an original contribution of this paper.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "is_executed": {
          "value": true,
          "justification": "The CNN was executed in several experiments involving synthetic tasks and reinforcement learning.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "is_compared": {
          "value": true,
          "justification": "The CNN was compared against other architectures like MLP and ResNet-18 to evaluate performance.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The CNN is a standard model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Residual Network (ResNet-18)",
          "justification": "ResNet-18 is explicitly mentioned and evaluated as part of the study.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "aliases": [
          "ResNet-18"
        ],
        "is_contributed": {
          "value": false,
          "justification": "ResNet-18 is not an original contribution of this paper.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "is_executed": {
          "value": true,
          "justification": "ResNet-18 was executed in several experiments involving synthetic tasks and reinforcement learning.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "is_compared": {
          "value": true,
          "justification": "ResNet-18 was compared against other architectures like MLP and CNN to evaluate performance.",
          "quote": "We evaluate three architectures: a MLP, a CNN, and a ResNet-18."
        },
        "referenced_paper_title": {
          "value": "\"Deep Residual Learning for Image Recognition\" by He et al.",
          "justification": "The ResNet architecture is attributed to the widely recognized paper by He et al.",
          "quote": "He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep Residual Learning for Image Recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used as a standard dataset for evaluating various classification tasks.",
          "quote": "we have considered CIFAR-100, CIFAR-10, and MNIST and observe consistent results across all base datasets. We primarily use CIFAR-10 in our evaluations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "\"Learning Multiple Layers of Features from Tiny Images\" by Alex Krizhevsky",
          "justification": "CIFAR-10 is a well-established dataset introduced by Alex Krizhevsky.",
          "quote": "Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images. Master’s thesis, University of Toronto, 2009."
        }
      },
      {
        "name": {
          "value": "iWildCam",
          "justification": "iWildCam is used to validate the approach on natural distribution shifts.",
          "quote": "The dataset used is the iwildcam dataset, which consists of photos of animals taken in a variety of locations and times of day."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "\"Wilds: A benchmark of in-the-wild distribution shifts\" by Koh et al.",
          "justification": "iWildCam is part of the WiLDS benchmark introduced by Koh et al.",
          "quote": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning, 2021."
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is mentioned as an alternative dataset for evaluating classification tasks.",
          "quote": "we have considered CIFAR-100, CIFAR-10, and MNIST and observe consistent results across all base datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "\"Gradient-Based Learning Applied to Document Recognition\" by Yann LeCun et al.",
          "justification": "MNIST is a well-established dataset introduced by Yann LeCun et al.",
          "quote": "LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-Based Learning Applied to Document Recognition. Proceedings of the IEEE, 86(11), 2278-2324."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2132,
    "prompt_tokens": 31413,
    "total_tokens": 33545
  }
}