{
  "paper": "2210.14709.txt",
  "words": 7624,
  "extractions": {
    "title": {
      "value": "LEARNING ON LARGE-SCALE TEXT-ATTRIBUTED GRAPHS VIA VARIATIONAL INFERENCE",
      "justification": "The title is found on the first page of the paper",
      "quote": "LEARNING ON LARGE-SCALE TEXT-ATTRIBUTED GRAPHS VIA VARIATIONAL INFERENCE"
    },
    "description": "This paper presents GLEM, an approach designed for learning on large text-attributed graphs by integrating graph structure and text using a variational Expectation-Maximization framework. The method aims to address scalability issues of combining large language models and graph neural networks by training these models separately and iteratively in an E-step and M-step.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes extensive experiments on multiple datasets to demonstrate the efficiency and effectiveness of the proposed approach.",
      "quote": "Extensive experiments on multiple datasets demonstrate the efficiency and effectiveness of the proposed approach."
    },
    "primary_research_field": {
      "name": {
        "value": "Graph Machine Learning",
        "justification": "The main focus of the paper is on learning representations for text-attributed graphs, which is a subfield of graph machine learning.",
        "quote": "Learning on TAG has become an important research topic in multiple areas including graph learning, information retrieval, and natural language processing."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The integration of large language models into the learning process indicates the involvement of NLP techniques.",
          "quote": "An ideal solution for such a problem would be integrating both the text and graph structure information with large language models and graph neural networks (GNNs)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GLEM",
          "justification": "The proposed approach, named Graph and Language Learning by Expectation Maximization (GLEM), is the main contribution of the paper.",
          "quote": "we propose such an approach, named Graph and Language Learning by Expectation Maximization GLEM."
        },
        "aliases": [
          "GLEM"
        ],
        "is_contributed": {
          "value": true,
          "justification": "GLEM is proposed in this paper as the main model for learning on large text-attributed graphs.",
          "quote": "we propose such an approach, named Graph and Language Learning by Expectation Maximization GLEM."
        },
        "is_executed": {
          "value": true,
          "justification": "GLEM is executed and evaluated in the experiments section of the paper.",
          "quote": "Extensive experiments on multiple datasets demonstrate the efficiency and effectiveness of the proposed approach"
        },
        "is_compared": {
          "value": true,
          "justification": "GLEM's performance is compared with several other models in the experimental results section.",
          "quote": "we compare GLEM-LM and GLEM-GNN against LMs, GNNs, and methods combining both of worlds."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The model GLEM is newly introduced in this paper and is not based on a prior reference.",
          "quote": "we propose such an approach, named Graph and Language Learning by Expectation Maximization GLEM."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ogbn-arxiv",
          "justification": "ogbn-arxiv is listed as one of the datasets used for evaluating GLEM’s performance.",
          "quote": "Notably, GLEM achieves new state-of-the-art results on ogbn-arxiv, ogbn-product, and ogbn-papers100M."
        },
        "aliases": [
          "ogbn-arxiv"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Open graph benchmark: Datasets for machine learning on graphs",
          "justification": "The ogbn-arxiv dataset is part of the Open Graph Benchmark.",
          "quote": "OGB datasets (Hu et al., 2020)"
        }
      },
      {
        "name": {
          "value": "ogbn-products",
          "justification": "ogbn-products is listed as one of the datasets used for evaluating GLEM’s performance.",
          "quote": "Notably, GLEM achieves new state-of-the-art results on ogbn-arxiv, ogbn-product, and ogbn-papers100M."
        },
        "aliases": [
          "ogbn-products"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Open graph benchmark: Datasets for machine learning on graphs",
          "justification": "The ogbn-products dataset is part of the Open Graph Benchmark.",
          "quote": "OGB datasets (Hu et al., 2020)"
        }
      },
      {
        "name": {
          "value": "ogbn-papers100M",
          "justification": "ogbn-papers100M is listed as one of the datasets used for evaluating GLEM’s performance.",
          "quote": "Notably, GLEM achieves new state-of-the-art results on ogbn-arxiv, ogbn-product, and ogbn-papers100M."
        },
        "aliases": [
          "ogbn-papers100M"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Open graph benchmark: Datasets for machine learning on graphs",
          "justification": "The ogbn-papers100M dataset is part of the Open Graph Benchmark.",
          "quote": "OGB datasets (Hu et al., 2020)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "DeBERTa",
          "justification": "The DeBERTa library is explicitly used in the implementation of the language model (LM) for text encoding.",
          "quote": "We adopt the DeBERTa (He et al., 2021) as the LM model and fine-tune it for node classification."
        },
        "aliases": [
          "DeBERTa"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deberta: Decoding-enhanced bert with disentangled attention",
          "justification": "The DeBERTa model mentioned is referenced to its original paper.",
          "quote": "We adopt the DeBERTa (He et al., 2021) as the LM model and fine-tune it for node classification."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2604,
    "prompt_tokens": 31170,
    "total_tokens": 33774
  }
}