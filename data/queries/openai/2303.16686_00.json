{
  "paper": "2303.16686.txt",
  "words": 6039,
  "extractions": {
    "title": {
      "value": "Communication Load Balancing via Efficient Inverse Reinforcement Learning",
      "justification": "The title of the paper is extracted from the beginning header.",
      "quote": "Communication Load Balancing via Efficient Inverse Reinforcement Learning"
    },
    "description": "This paper tackles the problem of communication load balancing using an Inverse Reinforcement Learning (IRL) approach. It formulates load balancing as a Markov decision process and proposes a novel IRL framework to infer reward functions from demonstrations, which are then used to train reinforcement learning policies. The paper claims notable improvements in efficiency and load balancing performance across various simulated traffic scenarios.",
    "type": {
      "value": "empirical study",
      "justification": "The paper presents experimental results and evaluations, implementing IRL on different simulated traffic scenarios and comparing its effectiveness to other methods.",
      "quote": "Experimental evaluations implemented on different simulated traffic scenarios have shown our method to be effective and better than other baselines by a considerable margin."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper primarily revolves around using Inverse Reinforcement Learning for communication load balancing.",
        "quote": "In this work, we tackle the communication load balancing problem from an inverse reinforcement learning (IRL) approach."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Inverse Reinforcement Learning",
          "justification": "The paper specifically employs Inverse Reinforcement Learning as its core approach.",
          "quote": "In this work, we tackle the communication load balancing problem from an inverse reinforcement learning (IRL) approach."
        },
        "aliases": [
          "IRL"
        ]
      },
      {
        "name": {
          "value": "Communication Networks",
          "justification": "The application domain of the paper is communication load balancing within communication networks.",
          "quote": "Communication load balancing aims to balance the load between different available resources, and thus improve the quality of service for network systems."
        },
        "aliases": [
          "Networking"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Inverse Reinforcement Learning (IRL)",
          "justification": "The paper focuses on applying the Inverse Reinforcement Learning model to the problem of communication load balancing.",
          "quote": "In this work, we tackle the communication load balancing problem from an inverse reinforcement learning (IRL) approach."
        },
        "aliases": [
          "IRL"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper contributes by applying IRL to communication load balancing for the first time and proposes an IRL-based framework specifically tailored for this application.",
          "quote": "Our proposed method, TRajectory EXtrapolation(T-REX) using Temporally Consistent Sampling (TCS) follows a similar training pipeline of T-REX along with the inclusion of a data-augmentation module."
        },
        "is_executed": {
          "value": 1,
          "justification": "The IRL model was executed as part of the experimental evaluations on simulated traffic scenarios.",
          "quote": "Experimental evaluations implemented on different simulated traffic scenarios have shown our method to be effective and better than other baselines by a considerable margin."
        },
        "is_compared": {
          "value": 1,
          "justification": "The IRL model was compared numerically to other baselines and demonstrated better performance.",
          "quote": "Experimental evaluations implemented on different simulated traffic scenarios have shown our method to be effective and better than other baselines by a considerable margin."
        },
        "referenced_paper_title": {
          "value": "Apprenticeship learning via inverse reinforcement learning",
          "justification": "The model references prior work on Inverse Reinforcement Learning by Abbeel and Ng.",
          "quote": "Inverse reinforcement learning (IRL) [15] can act as a potential solution by circumventing the need for tedious reward designing instead inferring it from expert data."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Simulation Datasets of RAN",
          "justification": "The datasets used are simulations of Radio Access Network (RAN) environments to test the proposed models.",
          "quote": "The experiments are conducted on a system-level RAN simulator[36, 37] consisting of seven eNBs."
        },
        "aliases": [
          "RAN Simulations"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "not cited",
          "justification": "The paper does not provide a specific title for the reference dataset paper.",
          "quote": "The experiments are conducted on a system-level RAN simulator[36, 37] consisting of seven eNBs."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Proximal Policy Optimization (PPO)",
          "justification": "PPO is mentioned as the optimization algorithm used for policy training in the proposed IRL framework.",
          "quote": "Once a reward network rˆ is trained, it is used to train a policy using Proximal Policy Optimization (PPO) [35]."
        },
        "aliases": [
          "PPO"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "The referenced paper for PPO is titled 'Proximal Policy Optimization Algorithms' by Schulman et al.",
          "quote": "Once a reward network rˆ is trained, it is used to train a policy using Proximal Policy Optimization (PPO) [35]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1005,
    "prompt_tokens": 11296,
    "total_tokens": 12301
  }
}