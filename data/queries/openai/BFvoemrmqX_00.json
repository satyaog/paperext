{
  "paper": "BFvoemrmqX.txt",
  "words": 13065,
  "extractions": {
    "title": {
      "value": "Bridging the Gap Between Target Networks and Functional Regularization",
      "justification": "The title of the paper as it appears at the beginning of the document.",
      "quote": "Bridging the Gap Between Target Networks and Functional Regularization"
    },
    "description": "This research paper explores the role of Target Networks in deep Reinforcement Learning (RL), emphasizing their regularization effect and associated instabilities. The authors propose a novel method called Functional Regularization (FR) as an alternative to Target Networks to enhance the stability and performance of value function learning. The paper provides theoretical analyses and empirical experiments to compare the effectiveness of FR against Target Networks across various environments.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves conducting experiments across a range of environments with empirical results to investigate the effectiveness of the proposed Functional Regularization.",
      "quote": "We conduct an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness of the regularization induced by Target Networks and Functional Regularization in terms of performance, accuracy, and stability."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper primarily discusses concepts and methods related to deep Reinforcement Learning, such as Target Networks and value function learning.",
        "quote": "Bootstrapping is behind much of the successes of deep Reinforcement Learning. However, learning the value function via bootstrapping often leads to unstable training due to fast-changing target values."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "Temporal Difference (TD)",
          "justification": "The paper discusses Temporal Difference (TD) learning extensively as a fundamental algorithm in Reinforcement Learning.",
          "quote": "Bootstrapping can unfortunately be unstable as the target value is estimated with constantly updated parameters."
        },
        "aliases": [
          "TD"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Temporal Difference is an established algorithm and not a new contribution of this paper.",
          "quote": "Learning the value function via bootstrapping often leads to unstable training due to fast-changing target values. Temporal Difference (TD) algorithm addresses these issues by using bootstrapping."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical studies where TD is executed.",
          "quote": "Temporal Difference (TD) algorithm addresses these issues by using bootstrapping."
        },
        "is_compared": {
          "value": 1,
          "justification": "TD is compared with other methods such as Target Networks and the proposed Functional Regularization.",
          "quote": "we propose a simple Functional Regularization (FR) alternative that has the advantages of provably ensuring TD remains stable while being more flexible than TN."
        },
        "referenced_paper_title": {
          "value": "Learning to predict by the methods of temporal differences",
          "justification": "The referenced paper is likely the seminal work by Richard S. Sutton on Temporal Difference learning.",
          "quote": "Temporal Difference (TD) (Sutton, 1988) algorithm addresses these issues by using bootstrapping."
        }
      },
      {
        "name": {
          "value": "Functional Regularization (FR)",
          "justification": "Functional Regularization (FR) is the primary contribution of the paper proposed as an alternative to Target Networks.",
          "quote": "To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence."
        },
        "aliases": [
          "FR"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "Functional Regularization is a new method introduced in this paper.",
          "quote": "To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence."
        },
        "is_executed": {
          "value": 1,
          "justification": "FR is empirically tested in the paper across various environments.",
          "quote": "We conduct an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness of the regularization induced by Target Networks and Functional Regularization in terms of performance, accuracy, and stability."
        },
        "is_compared": {
          "value": 1,
          "justification": "FR is compared with existing methods such as TD and Target Networks to demonstrate its effectiveness.",
          "quote": "We conduct an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness of the regularization induced by Target Networks and Functional Regularization in terms of performance, accuracy, and stability."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Functional Regularization is introduced in this paper and does not have a previous reference.",
          "quote": "To overcome these issues, we propose an explicit Functional Regularization alternative that is flexible and a convex regularizer in function space and we theoretically study its convergence."
        }
      },
      {
        "name": {
          "value": "Target Networks (TN)",
          "justification": "The paper extensively discusses the role of Target Networks in stabilizing Reinforcement Learning and compares it with Functional Regularization.",
          "quote": "Target Networks are employed to stabilize training by using an additional set of lagging parameters to estimate the target values."
        },
        "aliases": [
          "TN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Target Networks are a well-established method in the field of Reinforcement Learning.",
          "quote": "Today TNs are central to most modern deep RL algorithms (Mnih et al., 2013; Lillicrap et al., 2015; Abdolmaleki et al., 2018b; Haarnoja et al., 2018; Fujimoto et al., 2018; Hausknecht & Stone, 2015; Van Hasselt et al., 2016; Hessel et al., 2018)."
        },
        "is_executed": {
          "value": 1,
          "justification": "Target Networks are executed in the scope of the empirical experiments conducted in this paper.",
          "quote": "Empirical evaluation shows that Functional Regularization without regularization weight tuning can be used as a drop-in replacement for Target Networks."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance and stability of Target Networks with Functional Regularization.",
          "quote": "We conduct an experimental study across a range of environments, discount factors, and off-policiness data collections to investigate the effectiveness of the regularization induced by Target Networks and Functional Regularization in terms of performance, accuracy, and stability."
        },
        "referenced_paper_title": {
          "value": "Playing atari with deep reinforcement learning",
          "justification": "The paper references the original work by Mnih et al., 2013, which introduced Target Networks.",
          "quote": "Target Networks (TN) (Mnih et al., 2013) a popular technique in deep RL which uses an additional set of lagging parameters to estimate the target value."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari suite",
          "justification": "The Atari suite is used in the experimental setup to evaluate the performance of the proposed Functional Regularization.",
          "quote": "In our experimental study, we explored a variety of environments, including the two-state MDP (Tsitsiklis & Van Roy, 1996), the Four Rooms environment (Sutton et al., 1999), and the Atari suite (Bellemare et al., 2013)."
        },
        "aliases": [
          "ALE"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The arcade learning environment: An evaluation platform for general agents",
          "justification": "The paper references the original work introducing the Atari suite as an evaluation platform.",
          "quote": "the Atari suite (Bellemare et al., 2013)"
        }
      },
      {
        "name": {
          "value": "Four Rooms environment",
          "justification": "The Four Rooms environment is one of the datasets used for empirical evaluation in the paper.",
          "quote": "In our experimental study, we explored a variety of environments, including the two-state MDP (Tsitsiklis & Van Roy, 1996), the Four Rooms environment (Sutton et al., 1999), and the Atari suite (Bellemare et al., 2013)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
          "justification": "The referenced paper is the one by Sutton et al., 1999 introducing the Four Rooms environment.",
          "quote": "the Four Rooms environment (Sutton et al., 1999)"
        }
      },
      {
        "name": {
          "value": "two-state MDP",
          "justification": "The two-state MDP is used as a dataset in theoretical and empirical experiments to illustrate convergence properties.",
          "quote": "In our experimental study, we explored a variety of environments, including the two-state MDP (Tsitsiklis & Van Roy, 1996), the Four Rooms environment (Sutton et al., 1999), and the Atari suite (Bellemare et al., 2013)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Feature-based methods for large scale dynamic programming",
          "justification": "The referenced paper is likely the one by Tsitsiklis & Van Roy, 1996 which introduces simple MDPs used for TD learning studies.",
          "quote": "the two-state MDP (Tsitsiklis & Van Roy, 1996)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2220,
    "prompt_tokens": 24146,
    "total_tokens": 26366
  }
}