{
  "paper": "1911.05873.txt",
  "words": 18973,
  "extractions": {
    "title": {
      "value": "A Reduction from Reinforcement Learning to No-Regret Online Learning",
      "justification": "The title is explicitly mentioned at the beginning of the paper.",
      "quote": "A Reduction from Reinforcement Learning to No-Regret Online Learning"
    },
    "description": "The paper presents a novel reduction from Reinforcement Learning (RL) to no-regret Online Learning (OL) by leveraging the saddle-point formulation of RL. This approach enables any no-regret online learning algorithm to design RL policies with provable performance guarantees. The reduction decouples RL into regret minimization and function approximation, facilitating more systematic RL algorithm design. An RL algorithm based on this reduction, using the mirror descent algorithm and a generative-model oracle, is proposed and shown to achieve improved sample complexity.",
    "type": {
      "value": "empirical study",
      "justification": "The paper includes the design and analysis of a new RL algorithm, backed by theoretical guarantees and empirical performance evaluations, suggesting it is an empirical study.",
      "quote": "We demonstrate this idea by devising a simple RL algorithm based on mirror descent and the generative-model oracle."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The focus on developing strategies to optimize RL algorithms places this research within the broader domain of Deep Learning, which encompasses various machine learning methodologies including RL.",
        "quote": "This new perspective decouples the RL problem into two parts: regret minimization and function approximation."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper explicitly deals with advancements and methodologies in Reinforcement Learning (RL), aiming to improve RL algorithms' efficiency and performance.",
          "quote": "We present a reduction from reinforcement learning (RL) to no-regret online learning based on the saddle-point formulation of RL."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "mirror descent algorithm",
          "justification": "The model is central to the proposed reduction framework and algorithm design for RL within the paper.",
          "quote": "We demonstrate this idea by devising a simple RL algorithm based on mirror descent and the generative-model oracle."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "stochastic mirror descent",
          "justification": "The paper discusses utilizing stochastic mirror descent, particularly in the context of RL problem formulation and optimization.",
          "quote": "While it is possible to devise a minor modification of the previous stochastic mirror descent algorithms, e.g. [5], achieving the same rate with our new analysis"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "mirror-prox",
          "justification": "Mirror-prox is mentioned as a potential optimization algorithm that can be leveraged within the framework.",
          "quote": "one can further foresee that algorithms leveraging the continuity in COL—e.g. mirror-prox [25] or PicCoLO [18]—and variance reduction can lead to more sample efficient RL algorithms."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "tabular Markov decision process",
          "justification": "The dataset used to test the proposed RL algorithm is a tabular representation of MDP, which is central to the empirical validation.",
          "quote": "For any γ-discounted tabular RL problem, with probability at least 1 − δ,  it learns an  ǫ|S||A| log( 1/δ ) optimal policy using at most Õ (1−γ)^4 ǫ^2 samples."
        },
        "aliases": [
          "MDP"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1010,
    "prompt_tokens": 32391,
    "total_tokens": 33401
  }
}