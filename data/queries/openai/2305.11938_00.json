{
  "paper": "2305.11938.txt",
  "words": 17798,
  "extractions": {
    "title": {
      "value": "X TREME -U P: A User-Centric Scarce-Data Benchmark for Under-Represented Languages",
      "justification": "The title captures the essence of the benchmark's focus on under-represented languages and user-centric tasks.",
      "quote": "X TREME -U P: A User-Centric Scarce-Data Benchmark for Under-Represented Languages"
    },
    "description": "The paper presents X TREME -U P, a benchmark designed to evaluate the capabilities of language models on under-represented languages (ULs) using scarce data. It focuses on user-centric tasks prevalent among high-resource languages. The benchmark comprises 88 languages and nine key technologies, including OCR, ASR, MT, autocomplete, semantic parsing, and transliteration. The paper provides baseline results and recommends the development of more inclusive multilingual NLP technologies.",
    "type": {
      "value": "empirical study",
      "justification": "The study provides experimental results using the X TREME -U P benchmark to evaluate different language models.",
      "quote": "We evaluate commonly used models on the benchmark."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The study is centered on developing and evaluating NLP models for multilingual tasks.",
        "quote": "We propose X TREME -U P, a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multilingual NLP",
          "justification": "The paper specifically targets the evaluation of multilingual models across various language tasks in under-represented languages.",
          "quote": "X TREME -U P evaluates the capabilities of language models across 88 under-represented languages."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "mT5",
          "justification": "mT5 is one of the commonly used models evaluated on the X TREME -U P benchmark.",
          "quote": "We provide baseline results on a handful of baseline systems... mT5-base."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ByT5",
          "justification": "ByT5 is another model evaluated on the X TREME -U P benchmark and outperformed mT5 in various tasks.",
          "quote": "The byte-based ByT5 outperforms the subword-based mT5 across most tasks."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Flan-PaLM",
          "justification": "Flan-PaLM was used for in-context learning evaluation in the study.",
          "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "FLEURS",
          "justification": "FLEURS is used for evaluating ASR tasks in the benchmark.",
          "quote": "We employ the FLEURS dataset (Conneau et al., 2023) consisting of recordings in 102 languages for sentences from FLORES-101 (Goyal et al., 2022), which were translated from English Wikipedia to 101 languages."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Universal Dependencies",
          "justification": "This dataset was used to test models' predictive capabilities rather than their memorization capabilities.",
          "quote": "We process high-quality natural language data from Universal Dependencies (de Marneffe et al., 2021), which we deduplicate against mC4 (Xue et al., 2021), the most common multilingual pre-training corpus in order to test models predictive rather than memorization capabilities."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Dakshina",
          "justification": "Used for the transliteration task in the benchmark.",
          "quote": "Most of the data for the task comes from the romanized full-string subset of the Dakshina dataset (Roark et al., 2020), in which 10,000 Wikipedia sentences written in the native scripts of the 12 languages were human-romanized by native speakers, resulting in parallel sentences in the native and Latin scripts."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "FLORES-101",
          "justification": "Adapted for machine translation tasks in the benchmark.",
          "quote": "The dataset is adapted from FLORES-101 (Goyal et al., 2022), repurposing half of the datasetâ€™s original development set as a training set."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "TyDi QA",
          "justification": "Used for evaluating question answering tasks in the X TREME -U P benchmark.",
          "quote": "In the in-language QA task, both the question and passage are in the same language. In this task, original questions and passages are from the TyDi QA dataset (Clark et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MasakhaNER",
          "justification": "The dataset provides data for the named entity recognition task in the benchmark.",
          "quote": "The dataset contains processed data from MasakhaNER (Adelani et al., 2021) and MasakhaNER 2.0 (Adelani et al., 2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "T5X",
          "justification": "T5X was used to train models on the XTREME-UP benchmark.",
          "quote": "Models were trained using seqio and T5X (Roberts et al., 2022) on TPUs (Kumar et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "seqio",
          "justification": "seqio was used for training models on the benchmark.",
          "quote": "Models were trained using seqio and T5X (Roberts et al., 2022) on TPUs (Kumar et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1337,
    "prompt_tokens": 31495,
    "total_tokens": 32832
  }
}