{
  "paper": "2404.11568.txt",
  "words": 18913,
  "extractions": {
    "title": {
      "value": "On the Scalability of GNNs for Molecular Graphs",
      "justification": "The title is explicitly stated at the beginning of the document.",
      "quote": "On the Scalability of GNNs for Molecular Graphs"
    },
    "description": "The paper explores the scalability of Graph Neural Networks (GNNs) for molecular graph reasoning. It compares message-passing networks, graph Transformers, and hybrid models on large-scale molecular datasets to analyze the impact of scale on performance. The study finds that increasing model size, dataset size, and diversity result in significant performance gains, paving the way for foundational GNNs in pharmaceutical drug discovery.",
    "type": {
      "value": "empirical",
      "justification": "The paper is an empirical study as it involves conducting experiments and analyzing the scalability of different GNN architectures on various molecular graph datasets.",
      "quote": "We analyze the scaling behavior of message-passing networks, graph Transformers, and hybrid architectures with respect to the increasing scale of width, depth, number of molecules, number of labels, and diversity of datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on the scalability and performance of Graph Neural Networks (GNNs), which are a subset of deep learning architectures.",
        "quote": "We analyze the scaling behavior of message-passing networks, graph Transformers, and hybrid architectures with respect to the increasing scale of width, depth, number of molecules, number of labels, and diversity of datasets."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Graph Neural Networks",
          "justification": "The paper exclusively focuses on the scalability and performance of different Graph Neural Network architectures for molecular graph reasoning.",
          "quote": "Specifically, we analyze message-passing networks, graph Transformers, and hybrid architectures on the largest public collection of 2D molecular graphs."
        },
        "aliases": [
          "GNNs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "MPNN++",
          "justification": "MPNN++ is one of the models studied in the paper for scaling behavior in molecular GNNs.",
          "quote": "We select MPNN++ (Masters et al., 2022) which improves quantum prediction over the MPNN (Gilmer et al., 2017)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Training"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Graph Transformer",
          "justification": "The paper examines the scaling behavior of Graph Transformers as one of the studied models.",
          "quote": "We analyze the scaling behavior of message-passing networks, graph Transformers, and hybrid architectures with respect to the increasing scale of width, depth, number of molecules, number of labels, and diversity of datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Training"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Hybrid GPS++",
          "justification": "The Hybrid GPS++ model is investigated for its scalability and performance in the paper.",
          "quote": "Our graph Transformer and hybrid models make use of GPS++ model, which is known for its scalable nature on quantum property predictions."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Training"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Chemprop-RDKit",
          "justification": "Chemprop-RDKit is mentioned as one of the baseline models in the TDC leaderboards.",
          "quote": "SOTA on TDC is established by a group of 8 different models, namely Chemprop-RDKit (Yang et al., 2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "LargeMix",
          "justification": "The LargeMix dataset is explicitly mentioned as the primary dataset mixture used for studying scaling behavior of GNNs.",
          "quote": "We study the scaling behavior of GNNs on the LargeMix dataset mixture."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "L1000",
          "justification": "L1000_VCAP and L1000_MCF7 are subsets of the LargeMix dataset used for studying the scaling behavior.",
          "quote": "We study the scaling behavior of GNNs on the LargeMix dataset mixture... L1000_VCAP and L1000_MCF7 are two datasets of 16k and 20k molecules, respectively, with 998 graph-level classification labels."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PCBA_1328",
          "justification": "PCBA_1328 is a part of the LargeMix dataset used for the study.",
          "quote": "We study the scaling behavior of GNNs on the LargeMix dataset mixture... PCBA_1328 is a dataset of 1.6M molecules with 1,328 binary classification labels."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PCQM4M",
          "justification": "PCQM4M is included in the LargeMix dataset mixture used for scalability analysis.",
          "quote": "We study the scaling behavior of GNNs on the LargeMix dataset mixture... PCQM4M_G25 and PCQM4M_N4 are two datasets of 3.8M molecules with 25 graph-level labels and 4 node-level labels."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Therapeutics Data Commons",
          "justification": "Therapeutics Data Commons (TDC) is used as the benchmark for evaluating the performance of finetuned models.",
          "quote": "Finally, we evaluate our models on a range of public benchmarks with 38 datasets from TDC (Huang et al., 2021)."
        },
        "aliases": [
          "TDC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Polaris",
          "justification": "Polaris is one of the benchmark datasets used for evaluating model performance.",
          "quote": "Polaris. This is a recent collection of benchmarks addressing concerns over previous datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MoleculeNet",
          "justification": "MoleculeNet is used for evaluating the finetuned models and comparing their state-of-the-art performance.",
          "quote": "Finally, we evaluate our models on a range of public benchmarks with 38 datasets from TDC (Huang et al., 2021), Polaris, and MoleculeNet (Wu et al., 2018)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PCQM4M_G25",
          "justification": "PCQM4M_G25 is included in the LargeMix dataset mixture used for scalability analysis.",
          "quote": "PCQM4M_G25 and PCQM4M_N4 are two datasets of 3.8M molecules with 25 graph-level labels and 4 node-level labels."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PCQM4M_N4",
          "justification": "PCQM4M_N4 is included in the LargeMix dataset mixture used for scalability analysis.",
          "quote": "PCQM4M_G25 and PCQM4M_N4 are two datasets of 3.8M molecules with 25 graph-level labels and 4 node-level labels."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Graphium",
          "justification": "The Graphium library is used for the large-scale multi-task pretraining of GNNs in the study.",
          "quote": "Since our analysis consists of multiple tasks on a large scale, we utilize the Graphium library (Beaini et al., 2024)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1679,
    "prompt_tokens": 42488,
    "total_tokens": 44167
  }
}