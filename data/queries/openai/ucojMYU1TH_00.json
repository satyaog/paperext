{
  "paper": "ucojMYU1TH.txt",
  "words": 14522,
  "extractions": {
    "title": {
      "value": "On learning history-based policies for controlling Markov decision processes",
      "justification": "This is the title of the paper as it appears at the beginning.",
      "quote": "On learning history-based policies for controlling Markov decision processes"
    },
    "description": "This paper introduces a theoretical framework for studying Reinforcement Learning (RL) algorithms that use history-based feature abstraction mappings for controlling Markov Decision Processes (MDPs). The paper also designs a corresponding RL algorithm and evaluates its effectiveness on continuous control tasks.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper focuses on the theoretical analysis and development of history-based RL methods and their formal properties.",
      "quote": "In this paper, we introduce a theoretical framework for studying the behaviour of RL algorithms that learn to control an MDP using history-based feature abstraction mappings."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary topic revolves around developing and analyzing Reinforcement Learning algorithms.",
        "quote": "Reinforcement learning (RL) folklore suggests"
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "History-based Feature Abstraction",
          "justification": "This refers to the specific methods studied in the paper for addressing the challenges in RL with MDPs.",
          "quote": "history-based feature abstraction"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Continuous Control Learning",
          "justification": "The paper evaluates the proposed RL algorithm on continuous control tasks.",
          "quote": "we numerically evaluate its effectiveness on a set of continuous control tasks."
        },
        "aliases": []
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "MuJoCo",
          "justification": "The paper uses MuJoCo environments for empirical evaluations.",
          "quote": "We answer question (1) by comparing our approach with the proximal policy gradient (PPO) (Schulman et al., 2017) and the policy-gradient version of DeepMDP framework (Gelada et al., 2019). For question (2) we compare our approach with modified versions of PlaNet (Hafner et al., 2019), Dreamer (Hafner et al., 2020), and VariBAD (Zintgraf et al., 2020). For question (3) we compare the performance of our method using different MMD kernels and KL-divergence based approximation of Wasserstein distance. All the approaches are evaluated on six continuous control tasks from the MuJoCo (Todorov et al., 2012) OpenAI-Gym suite."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MuJoCo: A physics engine for model-based control",
          "justification": "This is the reference title as per the citation provided in the empirical evaluation section.",
          "quote": "(Todorov et al., 2012)"
        }
      },
      {
        "name": {
          "value": "OpenAI Gym",
          "justification": "The paper uses the OpenAI Gym environment suite for the empirical evaluations.",
          "quote": "For question (3) we compare the performance of our method using different MMD kernels and KL-divergence based approximation of Wasserstein distance. All the approaches are evaluated on six continuous control tasks from the MuJoCo (Todorov et al., 2012) OpenAI-Gym suite."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Openai gym",
          "justification": "This is the reference title as per the citation provided in the empirical evaluation section.",
          "quote": "(Brockman et al., 2016)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PPO",
          "justification": "Proximal Policy Optimization (PPO) algorithm is used as a baseline for comparisons.",
          "quote": "We answer question (1) by comparing our approach with the proximal policy gradient (PPO) (Schulman et al., 2017)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Proximal policy optimization algorithms",
          "justification": "This is the reference title as per the citation provided in the empirical evaluation section.",
          "quote": "(Schulman et al., 2017)"
        }
      },
      {
        "name": {
          "value": "DeepMDP",
          "justification": "DeepMDP framework is used as a baseline for comparison in the empirical evaluation.",
          "quote": "We answer question (1) by comparing our approach [...] the policy-gradient version of DeepMDP framework (Gelada et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepMDP: Learning continuous latent space models for representation learning",
          "justification": "This is the reference title as per the citation provided in the empirical evaluation section.",
          "quote": "(Gelada et al., 2019)"
        }
      },
      {
        "name": {
          "value": "PlaNet",
          "justification": "The PlaNet framework is modified for comparison in the empirical evaluation.",
          "quote": "For question (2) we compare our approach with modified versions of PlaNet (Hafner et al., 2019), Dreamer (Hafner et al., 2020), and VariBAD (Zintgraf et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning latent dynamics for planning from pixels",
          "justification": "This is the reference title as per the citation provided in the empirical evaluation section.",
          "quote": "(Hafner et al., 2019)"
        }
      },
      {
        "name": {
          "value": "Dreamer",
          "justification": "The Dreamer framework is modified for comparison in the empirical evaluation.",
          "quote": "For question (2) we compare our approach with modified versions of PlaNet (Hafner et al., 2019), Dreamer (Hafner et al., 2020), and VariBAD (Zintgraf et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Dream to control: Learning behaviors by latent imagination",
          "justification": "This is the reference title as per the citation provided in the empirical evaluation section.",
          "quote": "(Hafner et al., 2020)"
        }
      },
      {
        "name": {
          "value": "VariBAD",
          "justification": "The VariBAD framework is used for comparison in the empirical evaluation.",
          "quote": "For question (2) we compare our approach with modified versions of PlaNet (Hafner et al., 2019), Dreamer (Hafner et al., 2020), and VariBAD (Zintgraf et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "VariBAD: A very good method for bayes-adaptive deep RL via meta-learning",
          "justification": "This is the reference title as per the citation provided in the empirical evaluation section.",
          "quote": "(Zintgraf et al., 2020)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1701,
    "prompt_tokens": 28121,
    "total_tokens": 29822
  }
}