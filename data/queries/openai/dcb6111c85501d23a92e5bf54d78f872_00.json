{
  "paper": "dcb6111c85501d23a92e5bf54d78f872.txt",
  "words": 23307,
  "extractions": {
    "title": {
      "value": "Recall, Robustness, and Lexicographic Evaluation",
      "justification": "This is the title of the paper provided in the prompt.",
      "quote": "Recall, Robustness, and Lexicographic Evaluation"
    },
    "description": "This paper addresses the use and evaluation of recall metrics in machine learning systems, particularly for ranking tasks. It introduces a new metric, lexirecall, which aims to improve the sensitivity, stability, and fairness of recall-oriented evaluation by focusing on worst-case performance. The paper provides extensive empirical analysis to validate lexirecall's effectiveness.",
    "type": {
      "value": "empirical study",
      "justification": "The paper is empirical as it provides extensive empirical analysis and validations of the newly proposed lexirecall metric.",
      "quote": "Through extensive empirical analysis across three recommendation tasks and 17 information retrieval tasks, we establish that our new evaluation method, lexirecall, has convergent validity."
    },
    "primary_research_field": {
      "name": {
        "value": "Information Retrieval",
        "justification": "The paper primarily focuses on the evaluation metrics for retrieval systems, which is a core topic in the field of Information Retrieval.",
        "quote": "Recall is often used to evaluate rankings of items, including those produced by recommender, retrieval, and other machine learning systems."
      },
      "aliases": [
        "IR"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Metric Evaluation",
          "justification": "The paper introduces a new evaluation metric, lexirecall, and validates its effectiveness through empirical analysis.",
          "quote": "Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison."
        },
        "aliases": [
          "Evaluation Metrics"
        ]
      },
      {
        "name": {
          "value": "Recommender Systems",
          "justification": "The paper mentions the application of recall metrics in recommender systems and includes empirical analysis across multiple recommendation tasks.",
          "quote": "Throughout this article, when we assess or compare evaluation methods, we will focus on these properties."
        },
        "aliases": [
          "Recommendation Systems"
        ]
      },
      {
        "name": {
          "value": "Fairness in Machine Learning",
          "justification": "The paper connects recall to concepts of robustness and fairness, aiming to improve fair evaluation methods.",
          "quote": "Moreover, by developing TSE as the counterpart to RR, we directly connected recall-orientation to robustness and Rawlsian notions of fairness, providing a normative argument for improving techniques for gathering complete relevance judgments."
        },
        "aliases": [
          "Fair ML"
        ]
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "TREC 2019 Fair Ranking Track",
          "justification": "The paper uses data from the TREC 2019 Fair Ranking Track for empirical analysis.",
          "quote": "In this light, our approach is similar to the TREC 2019 Fair Ranking Track, which considered fairness and robustness."
        },
        "aliases": [
          "TREC Fair Ranking",
          "TREC2019-FR"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Overview of the TREC 2019 Fair Ranking Track",
          "justification": "This is the title of the referenced paper in the empirical analysis section about the used dataset.",
          "quote": "Overview of the TREC 2019 Fair Ranking Track. In The Twenty-Eighth Text REtrieval Conference (TREC 2019) Proceedings."
        }
      },
      {
        "name": {
          "value": "MovieLens",
          "justification": "The paper uses data from the MovieLens dataset for empirical analysis in recommendation tasks.",
          "quote": "The recommender systems datasets, consistent with [97], we converted graded judgments to binary labels by considering any rating below 4 as nonrelevant and otherwise relevant for these datasets. This includes the MovieLens dataset."
        },
        "aliases": [
          "MovieLens Dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "MovieLens Datasets: History and Context",
          "justification": "This is the title of the referenced paper about the MovieLens dataset used in the empirical analysis.",
          "quote": "For MovieLens dataset, we referred to MovieLens Datasets: History and Context."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1851,
    "prompt_tokens": 103798,
    "total_tokens": 105649
  }
}