{
  "paper": "2309.07445.txt",
  "words": 13372,
  "extractions": {
    "title": {
      "value": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
      "justification": "This is the title of the paper.",
      "quote": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects"
    },
    "description": "This paper introduces SIB-200, a large-scale open-source benchmark dataset for topic classification in 205 languages and dialects. The study explores the performance gap between high-resource and low-resource languages using various models and evaluation settings.",
    "type": {
      "value": "empirical study",
      "justification": "The paper provides empirical results and evaluations using different language models and datasets.",
      "quote": "Our evaluation shows that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus of the paper is on Natural Language Processing, particularly on topic classification across multiple languages.",
        "quote": "Despite the progress in building multilingual language models, evaluation is often limited to a few languages with available datasets which excludes a large number of low-resource languages."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multilingual Learning",
          "justification": "The paper deals with multilingual evaluation and learning, addressing the gap between high-resource and low-resource languages.",
          "quote": "Our evaluation shows that there is still a large gap between the performance of high-resource and low-resource languages when multilingual evaluation is scaled to numerous world languages."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Topic Classification",
          "justification": "The primary task evaluated in the paper is topic classification across 205 languages and dialects.",
          "quote": "In this paper, we create SIB-200—a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "XLM-R",
          "justification": "XLM-R is mentioned multiple times as a key model used for evaluation in the paper.",
          "quote": "We also fine-tune region-specific PLM trained on multiple country-level or continent-level languages: AfriBERTa (126M), Serengeti (278M), AfroXLMR (550M), MuRIL (236M) and IndicBERTv2 (278M)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "XLM-R is used but not contributed by this work.",
          "quote": "We also fine-tune region-specific PLM trained on multiple country-level or continent-level languages: AfriBERTa (126M), Serengeti (278M), AfroXLMR (550M), MuRIL (236M) and IndicBERTv2 (278M)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the evaluation within the paper.",
          "quote": "We fine-tune XLM-R-base (270M parameters), XLM-R (550M), Glot-500 (395M), which are trained on several languages: XLM-R and Glot-500 were trained on 100 and 500 languages respectively."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model is numerically compared to other models such as Glot-500 and MLP.",
          "quote": "Comparing English versus other languages, fine-tuning XLM-R on English achieved an accuracy of 92.1%, indicating that the task itself is not difficult if given a properly pre-trained MLM and ∼ 700 training samples. However, when fine-tuning the same model in other languages, the performance drops vastly to an average accuracy of 75.9%."
        },
        "referenced_paper_title": {
          "value": "Unsupervised cross-lingual representation learning at scale",
          "justification": "This is the title of the original XLM-R paper.",
          "quote": "Conneau et al., 2020"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SIB-200",
          "justification": "SIB-200 is the main dataset introduced and utilized in the paper for topic classification in 205 languages and dialects.",
          "quote": "In this paper, we create SIB-200—a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU)."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects",
          "justification": "This is the current paper which introduces the SIB-200 dataset.",
          "quote": "In this paper, we create SIB-200—a large-scale open-sourced benchmark dataset for topic classification in 205 languages and dialects to address the lack of evaluation dataset for Natural Language Understanding (NLU)."
        }
      },
      {
        "name": {
          "value": "Flores-200",
          "justification": "Flores-200 is used as a base dataset for creating SIB-200.",
          "quote": "The dataset is based on Flores-200 machine translation corpus."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Flores-101 evaluation benchmark for low-resource and multilingual machine translation",
          "justification": "This is the reference paper for the Flores-200 dataset.",
          "quote": "The dataset is based on Flores-200 machine translation corpus."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "scikit-learn",
          "justification": "The paper mentions the use of scikit-learn for training MLP models.",
          "quote": "The MLP models were trained for 300 iterations, and we used either word n-gram tokens or XLM-R tokens. For the multilingual PLM, we fine-tune each language training data for 20 epochs, with a maximum sequence length of 164, batch size of 16, and learning rate of 1e−5 on a single Nvidia A10 GPU. Here, we assume access to labelled data in the target language."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Scikit-learn: Machine Learning in Python",
          "justification": "This is the reference paper for the scikit-learn library.",
          "quote": "Pedregosa et al., 2011"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1302,
    "prompt_tokens": 32528,
    "total_tokens": 33830
  }
}