{
  "paper": "2307.16071.txt",
  "words": 4763,
  "extractions": {
    "title": {
      "value": "ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus",
      "justification": "This is the title of the research paper as mentioned at the beginning of the document.",
      "quote": "ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus"
    },
    "description": "This paper introduces ÌròyìnSpeech, a new Yorùbá speech corpus aimed at increasing the amount of high-quality, contemporary Yorùbá speech data for Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) tasks. The corpus comprises about 42 hours of speech data recorded by 80 volunteers and includes contributions to the Mozilla Common Voice platform. The paper also presents evaluation results for TTS and ASR models, providing baselines for future research.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical data collected from various sources, describes the methodology used for data collection and preprocessing, and reports the results of baseline experiments for TTS and ASR.",
      "quote": "We provide extensive baseline experiments using state-of-the-art approaches for TTS and ASR."
    },
    "primary_research_field": {
      "name": {
        "value": "Speech Processing",
        "justification": "The paper primarily deals with tasks related to speech, including Text-to-Speech (TTS) and Automatic Speech Recognition (ASR).",
        "quote": "We introduce ÌròyìnSpeech, a new corpus influenced by the desire to increase the amount of high quality, contemporary Yorùbá speech data, which can be used for both Text-to-Speech (TTS) and Automatic Speech Recognition (ASR) tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text-to-Speech (TTS)",
          "justification": "The paper discusses the creation and evaluation of Text-to-Speech models using the Yorùbá speech data.",
          "quote": "Our TTS evaluation suggests that a high-fidelity, general domain, single-speaker Yorùbá voice is possible with as little as 5 hours of speech."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Automatic Speech Recognition (ASR)",
          "justification": "The paper also focuses on creating and evaluating Automatic Speech Recognition models.",
          "quote": "Similarly, for ASR we obtained a baseline word error rate (WER) of 23.8."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech)",
          "justification": "The paper trains and evaluates several variants of the VITS model for Yorùbá speech synthesis.",
          "quote": "We train and evaluate three variants of the VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) model."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The VITS model is used in the paper, but it is not introduced or contributed by this paper.",
          "quote": "We train and evaluate three variants of the VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) model."
        },
        "is_executed": {
          "value": true,
          "justification": "The VITS models were trained and evaluated as part of the research.",
          "quote": "The models were trained with a batch size of 16 using an NVIDIA A10 GPU with 24GB of GPU memory."
        },
        "is_compared": {
          "value": true,
          "justification": "The results of the VITS model are compared to other models and variations in the study.",
          "quote": "The results are mixed. Although the MOS score is higher for the continued pre-training voice, the MUSHRA and MCD scores are higher for the model where we train from scratch."
        },
        "referenced_paper_title": {
          "value": "Variational Inference with adversarial learning for end-to-end Text-to-Speech",
          "justification": "The VITS model is a known model in the field and is referenced accordingly.",
          "quote": "Train and evaluate three variants of the VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) model."
        }
      },
      {
        "name": {
          "value": "Conformer",
          "justification": "The Conformer model is used for Automatic Speech Recognition (ASR) tasks in the study.",
          "quote": "We use ESPNet to train a 12-layer Conformer model end-to-end with an RNN language model (LM) for decoding."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The Conformer model is used in the paper, but it is not introduced or contributed by this paper.",
          "quote": "We use ESPNet to train a 12-layer Conformer model end-to-end with an RNN language model (LM) for decoding."
        },
        "is_executed": {
          "value": true,
          "justification": "The Conformer model was trained and evaluated as part of the research.",
          "quote": "We use ESPNet to train a 12-layer Conformer model end-to-end with an RNN language model (LM) for decoding."
        },
        "is_compared": {
          "value": true,
          "justification": "The Conformer model's performance is compared to other models in the study.",
          "quote": "We observe that finetuning wav2vec 2.0 leads to significantly better performance versus training the Conformer model end-to-end."
        },
        "referenced_paper_title": {
          "value": "Conformer: Convolution-augmented Transformer for Speech Recognition",
          "justification": "The Conformer model is a known model in the field and is referenced accordingly.",
          "quote": "We use ESPNet to train a 12-layer Conformer model end-to-end with an RNN language model (LM) for decoding."
        }
      },
      {
        "name": {
          "value": "wav2vec 2.0",
          "justification": "The paper uses the wav2vec 2.0 model for Automatic Speech Recognition (ASR) tasks after finetuning it.",
          "quote": "We finetune wav2vec 2.0 XLSR-300m (Babu et al., 2022) and train an end-to-end Conformer model (Gulati et al., 2020)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The wav2vec 2.0 model is used in the paper, but it is not introduced or contributed by this paper.",
          "quote": "We finetune wav2vec 2.0 XLSR-300m (Babu et al., 2022) and train an end-to-end Conformer model (Gulati et al., 2020)."
        },
        "is_executed": {
          "value": true,
          "justification": "The wav2vec 2.0 model was finetuned and evaluated as part of the research.",
          "quote": "We finetune wav2vec 2.0 XLSR-300m (Babu et al., 2022) and train an end-to-end Conformer model (Gulati et al., 2020)."
        },
        "is_compared": {
          "value": true,
          "justification": "The finetuned wav2vec 2.0 model's performance is compared to other models in the study.",
          "quote": "We observe that finetuning wav2vec 2.0 leads to significantly better performance versus training the Conformer model end-to-end."
        },
        "referenced_paper_title": {
          "value": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
          "justification": "The wav2vec 2.0 model is a known model in the field and is referenced accordingly.",
          "quote": "We finetune wav2vec 2.0 XLSR-300m (Babu et al., 2022) and train an end-to-end Conformer model (Gulati et al., 2020)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ÌròyìnSpeech",
          "justification": "The primary dataset contributed by the paper is the ÌròyìnSpeech corpus, designed to increase the amount of high-quality Yorùbá speech data.",
          "quote": "We introduce the ÌròyìnSpeech— a new dataset created to increase the amount of high quality, contemporary Yorùbá speech. The dataset has a total of 42 hours of audio, recorded by 80 volunteers."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "ÌròyìnSpeech: A multi-purpose Yorùbá Speech Corpus",
          "justification": "The dataset is the primary subject of the paper and is contributed by the authors.",
          "quote": "We introduce the ÌròyìnSpeech— a new dataset created to increase the amount of high quality, contemporary Yorùbá speech."
        }
      },
      {
        "name": {
          "value": "MENYO-20k dataset",
          "justification": "The MENYO-20k dataset is used as a source for the corpus text in the ÌròyìnSpeech dataset.",
          "quote": "The corpus text was obtained from two sources, firstly the MENYO-20k dataset (Adelani et al., 2021), an open-source, multi-domain English-Yorùbá machine translation corpus."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The effect of domain and diacritics in Yoruba–English neural machine translation",
          "justification": "The MENYO-20k dataset is acknowledged and cited in the references section.",
          "quote": "The corpus text was obtained from two sources, firstly the MENYO-20k dataset (Adelani et al., 2021), an open-source, multi-domain English-Yorùbá machine translation corpus."
        }
      },
      {
        "name": {
          "value": "MasakhaNER 2.0",
          "justification": "The MasakhaNER 2.0 dataset is used as a source for the corpus text in the ÌròyìnSpeech dataset.",
          "quote": "The corpus text was obtained from two sources... and secondly, the Yorùbá portion of the MasakhaNER 2.0 dataset (Adelani et al., 2022) (i.e MasakhaNER-YOR) based on the Asejere newspaper."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MasakhaNER 2.0: Africa-centric transfer learning for named entity recognition",
          "justification": "The MasakhaNER 2.0 dataset is acknowledged and cited in the references section.",
          "quote": "The corpus text was obtained from two sources... and secondly, the Yorùbá portion of the MasakhaNER 2.0 dataset (Adelani et al., 2022) (i.e MasakhaNER-YOR) based on the Asejere newspaper."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "ESPNet",
          "justification": "The ESPNet toolkit is used for training the Conformer model for ASR tasks.",
          "quote": "We use ESPNet to train a 12-layer Conformer model end-to-end with an RNN language model (LM) for decoding."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ESPNet: End-to-End Speech Processing Toolkit",
          "justification": "ESPNet is a known toolkit in the field and is referenced accordingly.",
          "quote": "We use ESPNet to train a 12-layer Conformer model end-to-end with an RNN language model (LM) for decoding."
        }
      },
      {
        "name": {
          "value": "Coqui TTS",
          "justification": "The Coqui TTS toolkit is used for training the VITS models for TTS tasks.",
          "quote": "We trained the VITS models using the Coqui TTS toolkit (Meyer et al., 2022b)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Coqui TTS: A Toolbox for Text-to-Speech Research",
          "justification": "Coqui TTS is a known toolkit in the field and is referenced accordingly.",
          "quote": "We trained the VITS models using the Coqui TTS toolkit (Meyer et al., 2022b)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2443,
    "prompt_tokens": 10020,
    "total_tokens": 12463
  }
}