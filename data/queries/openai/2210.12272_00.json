{
  "paper": "2210.12272.txt",
  "words": 6315,
  "extractions": {
    "title": {
      "value": "Implicit Offline Reinforcement Learning with Supervised Learning",
      "justification": "The title comes directly from the header of the paper, where all authors and affiliations are listed.",
      "quote": "Implicit Offline Reinforcement Learning with Supervised Learning"
    },
    "description": "This paper proposes a novel algorithm for offline reinforcement learning (RL) named Implicit Reinforcement Learning via Supervised Learning (IRvS). It explores the use of implicit models, specifically Energy-Based Models (EBMs), to better handle multi-modal and discontinuous reward distributions in offline RL. The approach is demonstrated to be advantageous on high-dimensional manipulation and locomotion tasks compared to explicit models.",
    "type": {
      "value": "Empirical",
      "justification": "The paper presents experimental results to validate the effectiveness of the proposed IRvS algorithm on various benchmarks, including robot manipulation and locomotion tasks.",
      "quote": "Finally, we demonstrate the effectiveness of our method on high-dimension manipulation and locomotion tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The core contribution of the paper is in the field of Reinforcement Learning, specifically offline reinforcement learning using supervised learning techniques.",
        "quote": "Offline Reinforcement Learning (RL) via Supervised Learning is a simple and effective way to learn robotic skills from a dataset collected by policies of different expertise levels."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Robotic Manipulation",
          "justification": "The paper demonstrates the effectiveness of IRvS on high-dimensional manipulation tasks, implying a substantial focus on robotic manipulation.",
          "quote": "Finally, we show that our method achieves state-of-the-art on the difficult ADROIT and FrankaKitchen manipulation tasks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Locomotion",
          "justification": "Several experiment benchmarks focus on locomotion tasks, highlighting this as a substantial area of application.",
          "quote": "Finally, we show that our method achieves state-of-the-art on the difficult ADROIT and FrankaKitchen manipulation tasks and is competitive with Offline RL and RvS algorithms on a suite of robotic locomotion environments."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Implicit Reinforcement Learning via Supervised Learning (IRvS)",
          "justification": "The paper proposes the IRvS model as its primary contribution.",
          "quote": "In this work, we bridge the gap between implicit models and RvS, and propose the first implicit RvS (IRvS) algorithm."
        },
        "aliases": [
          "IRvS"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "IRvS is introduced as the novel contribution of this paper.",
          "quote": "we bridge the gap between implicit models and RvS, and propose the first implicit RvS (IRvS) algorithm"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experimental results, which suggests that the model has been implemented and executed.",
          "quote": "Finally, we demonstrate the effectiveness of our method on high-dimension manipulation and locomotion tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "IRvS is compared numerically with other models in various experiments.",
          "quote": "Finally, we show that our method achieves state-of-the-art on the difficult ADROIT and FrankaKitchen manipulation tasks and is competitive with Offline RL and RvS algorithms on a suite of robotic locomotion environments."
        },
        "referenced_paper_title": {
          "value": "Implicit behavioral cloning",
          "justification": "The concept of Implicit Models, specifically Energy-Based Models (EBMs), builds on prior work in implicit behavioral cloning.",
          "quote": "Florence et al. (2021) introduced Implicit Behavior Cloning where they leverage an EBM to learn a policy from offline data."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ADROIT",
          "justification": "The ADROIT benchmark is mentioned explicitly as a dataset used for evaluating the proposed model.",
          "quote": "Finally, we show that our method achieves state-of-the-art on the difficult ADROIT and FrankaKitchen manipulation tasks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations",
          "justification": "The ADROIT dataset is taken from this referenced paper.",
          "quote": "The ADROIT benchmark (Kumar et al., 2013; Rajeswaran et al., 2017) consists of 4 environments (relocate, pen, door, hammer) and 3 datasets (human, cloned, expert) for a total of 12 tasks."
        }
      },
      {
        "name": {
          "value": "FrankaKitchen",
          "justification": "The FrankaKitchen dataset is mentioned explicitly as a dataset used for evaluating the proposed model.",
          "quote": "Finally, we show that our method achieves state-of-the-art on the difficult ADROIT and FrankaKitchen manipulation tasks."
        },
        "aliases": [
          "Kitchen"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning",
          "justification": "The FrankaKitchen dataset is taken from this referenced paper.",
          "quote": "The FrankaKitchen (Gupta et al., 2019) (kitchen) environment places a 9-DoF robot in a realistic kitchen environment where it can interact freely with different objects."
        }
      },
      {
        "name": {
          "value": "D4RL",
          "justification": "The D4RL benchmark suite is mentioned explicitly as a dataset used for evaluating the proposed model.",
          "quote": "We compare both IRvS and our version of RvS (as defined by Equation (9) and Equation (10)) to implicit and explicit state-of-the-art algorithms on a suite of offline RL tasks provided by d4rl (Fu et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "D4rl: Datasets for deep data-driven reinforcement learning",
          "justification": "The D4RL dataset is taken from this referenced paper.",
          "quote": "The offline mujoco benchmark (Todorov et al., 2012) consists of 3 locomotion environments (halfcheetah, walker2d, and hopper) and 3 datasets (medium, medium-replay, medium-expert) for a total of 9 tasks."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "MuJoCo",
          "justification": "The MuJoCo physics engine is used for benchmarking locomotion tasks.",
          "quote": "The offline mujoco benchmark (Todorov et al., 2012) consists of 3 locomotion environments (halfcheetah, walker2d, and hopper) and 3 datasets (medium, medium-replay, medium-expert) for a total of 9 tasks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Mujoco: A physics engine for model-based control",
          "justification": "The MuJoCo library is taken from this referenced paper.",
          "quote": "The offline mujoco benchmark (Todorov et al., 2012) consists of 3 locomotion environments (halfcheetah, walker2d, and hopper) and 3 datasets (medium, medium-replay, medium-expert) for a total of 9 tasks."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1458,
    "prompt_tokens": 12014,
    "total_tokens": 13472
  }
}