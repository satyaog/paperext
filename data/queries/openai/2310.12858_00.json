{
  "paper": "2310.12858.txt",
  "words": 4524,
  "extractions": {
    "title": {
      "value": "Audio Editing with Non-Rigid Text Prompts",
      "justification": "This is the title of the paper as mentioned at the beginning of the document.",
      "quote": "Audio Editing with Non-Rigid Text Prompts Francesco Paissan1,2 , Luca Della Libera3,2 , Zhepei Wang4 , Paris Smaragdis4 , Mirco Ravanelli3,2 , Cem Subakan5,3,2"
    },
    "description": "In this paper, the authors explore audio editing with non-rigid text prompts via Latent Diffusion Models. They carry out a fine-tuning step on the latent diffusion model to increase the faithfulness of the generated edits to the input audio. The methodology outperforms state-of-the-art neural audio editing pipelines for tasks like addition, style transfer, and inpainting.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experimental setups, benchmarking, and a user study to validate the proposed methodology, indicating an empirical approach.",
      "quote": "We quantitatively and qualitatively show that our pipeline obtains results which outperform current state-of-the-art neural audio editing pipelines for addition, style transfer, and inpainting. Through a user study, we show that our method results in higher user preference compared to several baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Generative Models for Audio",
        "justification": "The paper focuses on generative models applied to audio editing using text prompts.",
        "quote": "Index Terms: Latent diffusion, audio editing, generative models for audio."
      },
      "aliases": [
        "Audio Generative Models",
        "Audio Generation"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Latent Diffusion Models",
          "justification": "The paper uses Latent Diffusion Models for the audio editing tasks.",
          "quote": "The text-prompted audio generation pipeline we use in this paper is a latent diffusion model (LDM) [6]."
        },
        "aliases": [
          "LDMs"
        ]
      },
      {
        "name": {
          "value": "Neural Audio Editing",
          "justification": "The paper deals with neural approaches for editing audio using non-rigid text prompts.",
          "quote": "We quantitatively and qualitatively show that our pipeline obtains results which outperform current state-of-the-art neural audio editing pipelines for addition, style transfer, and inpainting."
        },
        "aliases": [
          "Audio Editing with Neural Networks"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Latent Diffusion Model",
          "justification": "The primary model used in the paper for text-prompted audio generation.",
          "quote": "The text-prompted audio generation pipeline we use in this paper is a latent diffusion model (LDM) [6]."
        },
        "aliases": [
          "LDM"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model itself is not a contribution of this paper but utilized in the study.",
          "quote": "The text-prompted audio generation pipeline we use in this paper is a latent diffusion model (LDM) [6]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experimental setup to validate its performance on audio editing tasks.",
          "quote": "The text-prompted audio generation pipeline we use in this paper is a latent diffusion model (LDM) [6]."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance is compared numerically to other baselines in the study.",
          "quote": "We benchmarked it against AudioLDM [11], AUDIT [16], and SDEdit [17] both quantitatively and qualitatively."
        },
        "referenced_paper_title": {
          "value": "High-resolution image synthesis with latent diffusion models",
          "justification": "This is the primary reference paper for the Latent Diffusion Model used in the study.",
          "quote": "The text-prompted audio generation pipeline we use in this paper is a latent diffusion model (LDM) [6]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Unspecified Acoustic Scenes",
          "justification": "The paper mentions using 10-second long audio samples of acoustic scenes for benchmarking the proposed methodology.",
          "quote": "Reported results are obtained on twenty-seven 10 s long audio samples of acoustic scenes, each with an associated edit prompt."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Not specified",
          "justification": "The specific referenced paper for this dataset is not provided in the document.",
          "quote": "Reported results are obtained on twenty-seven 10 s long audio samples of acoustic scenes, each with an associated edit prompt."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "HiFi-GAN",
          "justification": "The HiFi-GAN library is used as a vocoder in the pipeline described in the paper.",
          "quote": "Afterwards, this representation is fed to the VAE decoder to create a mel-spectrogram and passed through a pretrained HiFi-GAN [21] that works as a vocoder."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "HiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis",
          "justification": "This is the primary reference paper for HiFi-GAN used in the study.",
          "quote": "Afterwards, this representation is fed to the VAE decoder to create a mel-spectrogram and passed through a pretrained HiFi-GAN [21] that works as a vocoder."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1068,
    "prompt_tokens": 8498,
    "total_tokens": 9566
  }
}