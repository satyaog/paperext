{
  "paper": "2406.01361.txt",
  "words": 9079,
  "extractions": {
    "title": {
      "value": "Learning to Play Atari in a World of Tokens",
      "justification": "This is the exact title mentioned in the provided paper.",
      "quote": "Learning to Play Atari in a World of Tokens"
    },
    "description": "The paper introduces DART, a model-based reinforcement learning algorithm that uses discrete representations for both world and policy modeling. It leverages transformers for auto-regressive world modeling and behavior learning. DART outperforms previous state-of-the-art methods on the Atari 100k sample efficiency benchmark.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper's primary focus is on introducing and empirically evaluating the performance of the DART algorithm, suggesting it focuses on experiments and results rather than theoretical exploration.",
      "quote": "In this work, we introduce discrete abstract representation for transformer-based learning (DART), a novel approach that leverages transformers for learning both the world model and policy... DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The work focuses on model-based reinforcement learning, particularly in leveraging discrete representations and transformers for improved sample efficiency.",
        "quote": "Model-based reinforcement learning agents utilizing transformers have shown improved sample efficiency due to their ability to model extended context... In this work, we introduce discrete abstract representations for transformer-based learning (DART)"
      },
      "aliases": [
        "RL",
        "MBRL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper discusses the use of visual data from Atari games and how transformers can handle long-range dependencies typical in visual processing tasks.",
          "quote": "This motivates the need to use transformers (Vaswani et al., 2017; Lin et al., 2022), which have proven highly effective in capturing long-range dependencies in various natural language processing (NLP) tasks and addressing complex visual reasoning challenges in computer vision (CV) tasks."
        },
        "aliases": [
          "CV"
        ]
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper draws parallels between the use of transformers in NLP tasks and their application in reinforcement learning for modeling long-range dependencies.",
          "quote": "This motivates the need to use transformers (Vaswani et al., 2017; Lin et al., 2022), which have proven highly effective in capturing long-range dependencies in various natural language processing (NLP) tasks and addressing complex visual reasoning challenges in computer vision (CV) tasks."
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DART",
          "justification": "DART is the primary model introduced and evaluated in the paper, designed to handle discrete representations in reinforcement learning tasks using transformers.",
          "quote": "In this work, we introduce discrete abstract representations for transformer-based learning (DART), a sample-efficient method utilizing discrete representations for modeling both the world and learning behavior."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "DART is the main novel contribution of the paper.",
          "quote": "In this work, we introduce discrete abstract representations for transformer-based learning (DART), a novel approach that leverages transformers for learning both the world model and policy."
        },
        "is_executed": {
          "value": 1,
          "justification": "DART was empirically tested and its performance was measured on the Atari 100k benchmark.",
          "quote": "DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games."
        },
        "is_compared": {
          "value": 1,
          "justification": "DART's performance was compared to other state-of-the-art methods like DreamerV3, TWM, and IRIS.",
          "quote": "DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "DART is the central model of this paper and was not taken from an external reference.",
          "quote": "In this work, we introduce discrete abstract representations for transformer-based learning (DART), a novel approach that leverages transformers for learning both the world model and policy."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari 100k benchmark",
          "justification": "This dataset is used to evaluate the performance of DART and other compared methods.",
          "quote": "DART outperforms previous state-of-the-art methods that do not use look-ahead search on the Atari 100k sample efficiency benchmark with a median human-normalized score of 0.790 and beats humans in 9 out of 26 games."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The Atari 100k benchmark is a well-known dataset and was not contributed by the authors of this paper.",
          "quote": "We evaluated our model’s performance based on several metrics, including the mean and median of the human-normalized score, which measures how well the agent performs compared to human and random players given as scoreagent −scorerandom scorehuman −scorerandom ."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The authors mentioned using PyTorch for implementing their model.",
          "quote": "Our implementation utilized the PyTorch framework for all neural network components."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "PyTorch is a well-known library used in this paper and not the focus of another reference.",
          "quote": "Our implementation utilized the PyTorch framework for all neural network components. "
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1201,
    "prompt_tokens": 17518,
    "total_tokens": 18719
  }
}