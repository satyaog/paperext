{
  "paper": "2211.16031.txt",
  "words": 10135,
  "extractions": {
    "title": {
      "value": "Syntactic Substitutability as Unsupervised Dependency Syntax",
      "justification": "The title is taken directly from the research paper.",
      "quote": "Syntactic Substitutability as Unsupervised Dependency Syntax"
    },
    "description": "This research paper explores the hypothesis that syntactic dependencies can be represented in language model attention distributions and proposes a theory-agnostic method to induce these structures. The method, named Syntactic Substitutability as Unsupervised Dependency Syntax (SSUD), leverages the property of syntactic substitutability, where words in dependencies can be substituted with words from the same category. The effectiveness of this method is tested through parsing tasks, achieving significant improvements and demonstrating its generalizability.",
    "type": {
      "value": "Empirical Study",
      "justification": "The research involves empirical testing of a new method for inducing syntactic structures from language models and provides quantitative results from various experiments.",
      "quote": "We demonstrate that our method, Syntactic Substitutability as Unsupervised Dependency Syntax (SSUD) leads to improvements in dependency parsing accuracy."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on dependency parsing and syntactic structure induction from language models, which are primary topics within Natural Language Processing (NLP).",
        "quote": "In recent years, large pretrained language models (LLMs), like BERT, have led to impressive performance gains across many natural language processing tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Dependency Parsing",
          "justification": "The paper's core contribution is a method for inducing syntactic dependencies from language model attention distributions.",
          "quote": "We demonstrate that our method, Syntactic Substitutability as Unsupervised Dependency Syntax (SSUD) leads to improvements in dependency parsing accuracy."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Language Model Interpretability",
          "justification": "The paper includes analysis and methods to understand what syntactic structures are captured by the attention mechanisms of language models.",
          "quote": "Papadimitriou et al. (2022) show that BERT systematically learns to use word-order information to syntactically distinguish subjects and objects even when the respective nouns are swapped."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is extensively used in the paper as the primary language model for generating attention distributions and substitution predictions.",
          "quote": "In recent years, large pretrained language models (LLMs), like BERT (Devlin et al., 2019), have led to impressive performance gains across many natural language processing tasks."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "BERT was not introduced in this paper; it was used to perform experiments.",
          "quote": "In recent years, large pretrained language models (LLMs), like BERT (Devlin et al., 2019), have led to impressive performance gains across many natural language processing tasks."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments conducted in the paper used the BERT model to generate attention distributions for syntactic dependency induction.",
          "quote": "The language model investigated here is BERT."
        },
        "is_compared": {
          "value": 1,
          "justification": "Results from using BERT are compared with results from other methods to demonstrate the improvements achieved by SSUD.",
          "quote": "We also quantitatively show that the induced parses align more with an annotation schema where function words are treated as heads ... On long-distance subject-verb agreement constructions, SSUD achieves an increase in recall of >70% compared to a previous unsupervised parsing method."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "This is the original paper that introduced BERT, which is used prominently in this research.",
          "quote": "In recent years, large pretrained language models (LLMs), like BERT (Devlin et al., 2019), have led to impressive performance gains across many natural language processing tasks."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "WSJ10",
          "justification": "The WSJ10 subset of the Penn Treebank is used for evaluating the parsing methods proposed in the paper.",
          "quote": "We assess our method using two gold-standard English dependency parsing datasets: (1) the sentence length ≤ 10 test split (section 23) of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) annotated with Stanford Dependencies (de Marneffe et al., 2006) (WSJ10; 389 sentences)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Building a large annotated corpus of English: The Penn Treebank",
          "justification": "This is the original paper describing the Penn Treebank, from which the WSJ10 dataset is derived.",
          "quote": "We assess our method using two gold-standard English dependency parsing datasets: (1) the sentence length ≤ 10 test split (section 23) of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) annotated with Stanford Dependencies (de Marneffe et al., 2006) (WSJ10; 389 sentences)."
        }
      },
      {
        "name": {
          "value": "EN-PUD",
          "justification": "The EN-PUD dataset is used for evaluating the parsing methods proposed in the paper.",
          "quote": "We assess our method using two gold-standard English dependency parsing datasets: ... (2) the English section of the Parallel Universal Dependencies dataset annotated with Universal Dependencies (Nivre et al., 2020) (EN-PUD; 1000 sentences)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Universal Dependencies v2: An evergrowing multilingual treebank collection",
          "justification": "This is the reference paper for the Universal Dependencies treebank, from which the EN-PUD dataset is part.",
          "quote": "We assess our method using two gold-standard English dependency parsing datasets: ... (2) the English section of the Parallel Universal Dependencies dataset annotated with Universal Dependencies (Nivre et al., 2020) (EN-PUD; 1000 sentences)."
        }
      },
      {
        "name": {
          "value": "Marvin and Linzen dataset",
          "justification": "The Marvin and Linzen dataset is used for controlled experiments on syntactic constructions.",
          "quote": "We also test our method on a more difficult, long-distance subject-verb agreement dataset from Marvin and Linzen (2018) (see Experiment 2)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Targeted syntactic evaluation of language models",
          "justification": "This is the reference paper for the dataset used to evaluate long-distance subject-verb agreement constructions in language models.",
          "quote": "We also test our method on a more difficult, long-distance subject-verb agreement dataset from Marvin and Linzen (2018) (see Experiment 2)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2774,
    "prompt_tokens": 41257,
    "total_tokens": 44031
  }
}