{
  "paper": "2206.15387.txt",
  "words": 8330,
  "extractions": {
    "title": {
      "value": "W HERE TO B EGIN ? O N THE I MPACT OF P RE T RAINING AND I NITIALIZATION IN F EDERATED L EARNING",
      "justification": "This is the title of the paper.",
      "quote": "W HERE TO B EGIN ? O N THE I MPACT OF P RE T RAINING AND I NITIALIZATION IN F EDERATED L EARNING"
    },
    "description": "The paper investigates the impact of starting federated learning from a pre-trained model versus a randomly initialized model. The authors conduct an extensive empirical study using four standard federated learning benchmark datasets, comparing the performance of various federated optimization methods under both initialization strategies. The study reveals that pre-training not only reduces the time to reach a target accuracy but also mitigates the effects of data and system heterogeneity.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves extensive empirical evaluations comparing different initialization strategies and their impact on federated learning.",
      "quote": "We perform an extensive empirical study, comparing 15 variations of federated optimization methods on four commonly-used FL benchmark datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The core topic of the paper is on federated learning and the role of pre-training and initialization in this domain.",
        "quote": "Federated learning (FL) has emerged as a popular distributed machine learning paradigm."
      },
      "aliases": [
        "FL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The study focuses on the impact of initialization on various federated optimization methods.",
          "quote": "How does model initialization (random or pre-trained) impact the behavior of federated optimization methods?"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Data Heterogeneity",
          "justification": "The study addresses the challenge of data heterogeneity in federated learning and examines if pre-training affects it.",
          "quote": "Surprisingly, we also find that starting federated learning from a pre-trained initialization reduces the effect of both data and system heterogeneity."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "System Heterogeneity",
          "justification": "The study also explores how system heterogeneity is impacted by starting from a pre-trained model.",
          "quote": "Surprisingly, we also find that starting federated learning from a pre-trained initialization reduces the effect of both data and system heterogeneity."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Squeezenet",
          "justification": "Squeezenet is one of the model architectures used in the experiments.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Squeezenet is not a novel model introduced by the paper; it is used as a baseline architecture.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Squeezenet model is used and trained as part of the experiments.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Squeezenet is compared under different initialization strategies.",
          "quote": "Our study reveals three key findings:... enabling the training of more accurate models (up to 40%) than is possible when starting from random initialization."
        },
        "referenced_paper_title": {
          "value": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size",
          "justification": "This is the original paper where the Squeezenet model was introduced.",
          "quote": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size."
        }
      },
      {
        "name": {
          "value": "ResNet18",
          "justification": "ResNet18 is another model architecture used in the experiments.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet18 is not a novel model introduced by the paper; it is used as a baseline architecture.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "is_executed": {
          "value": 1,
          "justification": "The ResNet18 model is used and trained as part of the experiments.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of ResNet18 is compared under different initialization strategies.",
          "quote": "Our study reveals three key findings:... enabling the training of more accurate models (up to 40%) than is possible when starting from random initialization."
        },
        "referenced_paper_title": {
          "value": "Deep residual learning for image recognition",
          "justification": "This is the original paper where the ResNet model was introduced.",
          "quote": "Deep residual learning for image recognition."
        }
      },
      {
        "name": {
          "value": "DistilGPT2",
          "justification": "DistilGPT2 is used as one of the model architectures in the empirical study.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "DistilGPT2 is not a novel model introduced by the paper; it is used as a baseline architecture.",
          "quote": "We use the model weights provided in the HuggingFace library that has been distilled from a pre-trained GPT2."
        },
        "is_executed": {
          "value": 1,
          "justification": "The DistilGPT2 model is used and trained as part of the experiments.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of DistilGPT2 is compared under different initialization strategies.",
          "quote": "Our study reveals three key findings:... enabling the training of more accurate models (up to 40%) than is possible when starting from random initialization."
        },
        "referenced_paper_title": {
          "value": "DistilGPT2",
          "justification": "This is a reference to the model provided by HuggingFace.",
          "quote": "DistilGPT2"
        }
      },
      {
        "name": {
          "value": "CharLM",
          "justification": "CharLM is another model architecture evaluated in the study.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CharLM is not a novel model introduced by the paper; it is used as a baseline architecture.",
          "quote": "Character-aware neural language models"
        },
        "is_executed": {
          "value": 1,
          "justification": "The CharLM model is used and trained as part of the experiments.",
          "quote": "We train Squeezenet and ResNet18 models for CIFAR-10 and FEMNIST, and DistilGPT2 and CharLM models for Stackoverflow and Reddit pushift.io."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of CharLM is compared under different initialization strategies.",
          "quote": "Pre-trained models should be the first step for any practical deployment to save on communication bandwidth and achieve high model accuracy."
        },
        "referenced_paper_title": {
          "value": "Character-aware neural language models",
          "justification": "This is the original paper where CharLM was introduced.",
          "quote": "Character-aware neural language models."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used as one of the benchmark datasets in the empirical study.",
          "quote": "We experiment on four FL benchmark datasets: CIFAR-10, FEMNIST, Stack Overflow, and Reddit."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is the original paper where CIFAR-10 was introduced.",
          "quote": "Learning multiple layers of features from tiny images."
        }
      },
      {
        "name": {
          "value": "FEMNIST",
          "justification": "FEMNIST is used as one of the benchmark datasets in the empirical study.",
          "quote": "We experiment on four FL benchmark datasets: CIFAR-10, FEMNIST, Stack Overflow, and Reddit."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Federated EMNIST-62",
          "justification": "This is the adapted federated version of the original EMNIST dataset.",
          "quote": "Federated EMNIST-62 (FEMNIST) consists of digits and English characters, totaling 62 classes."
        }
      },
      {
        "name": {
          "value": "Stack Overflow",
          "justification": "Stack Overflow dataset is used as one of the benchmark datasets in the empirical study.",
          "quote": "We experiment on four FL benchmark datasets: CIFAR-10, FEMNIST, Stack Overflow, and Reddit."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Federated visual classification with real-world data distribution",
          "justification": "This paper discusses the use of Stack Overflow dataset in federated learning applications.",
          "quote": "Federated visual classification with real-world data distribution."
        }
      },
      {
        "name": {
          "value": "Reddit pushift.io",
          "justification": "Reddit pushift.io dataset is used as one of the benchmark datasets in the empirical study.",
          "quote": "We experiment on four FL benchmark datasets: CIFAR-10, FEMNIST, Stack Overflow, and Reddit."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pretrained models for multilingual federated learning",
          "justification": "This paper discusses the use of Reddit dataset in federated learning applications.",
          "quote": "Pretrained models for multilingual federated learning."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The open-source federated learning framework FLSim used in the paper is built on PyTorch.",
          "quote": "Our experiments are implemented using the open-source federated learning simulation framework FLSim FLSim Authors (2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in pytorch",
          "justification": "This is the original paper where PyTorch was introduced and discussed.",
          "quote": "Automatic differentiation in pytorch."
        }
      },
      {
        "name": {
          "value": "FLSim",
          "justification": "The federated learning simulation framework used for the experiments.",
          "quote": "Our experiments are implemented using the open-source federated learning simulation framework FLSim FLSim Authors (2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Federated learning simulator (flsim)",
          "justification": "This is the reference to the FLSim framework's source and documentation.",
          "quote": "Federated learning simulator (flsim)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2984,
    "prompt_tokens": 16450,
    "total_tokens": 19434
  }
}