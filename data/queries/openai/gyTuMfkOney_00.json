{
  "paper": "gyTuMfkOney.txt",
  "words": 9902,
  "extractions": {
    "title": {
      "value": "A Functional Perspective on Multi-Layer Out-of-Distribution Detection",
      "justification": "The title encapsulates the core focus of the paper, which is on viewing multi-layer Out-of-Distribution detection from a functional perspective.",
      "quote": "A F UNCTIONAL P ERSPECTIVE ON M ULTI -L AYER O UT- OF -D ISTRIBUTION D ETECTION"
    },
    "description": "This paper introduces a novel approach to Out-of-Distribution (OOD) detection by leveraging the functional trajectories of samples through the layers of a pre-trained multi-layer neural network. The method is designed to identify OOD samples whose trajectories differ statistically from the reference trajectories characterized by the training set. It does not require additional OOD data and demonstrates competitive performance on ImageNet-1k and other OOD datasets.",
    "type": {
      "value": "Empirical",
      "justification": "The paper reports on experiments conducted on various datasets, including ImageNet-1k, and discusses the empirical performance of the proposed OOD detection method.",
      "quote": "We validate the value of the proposed method using a mid-size OOD detection benchmark on ImageNet-1k."
    },
    "primary_research_field": {
      "name": {
        "value": "Out-of-Distribution Detection",
        "justification": "The paper primarily deals with detecting Out-of-Distribution (OOD) samples using neural networks.",
        "quote": "To address safety issues arising from the presence of OOD samples, a successful line of work aims at augmenting ML models with an OOD binary detector to distinguish between abnormal and in-distribution examples."
      },
      "aliases": [
        "OOD Detection"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The empirical validation is performed on the ImageNet-1k dataset, which is a well-known benchmark in the field of Computer Vision.",
          "quote": "We validate the value of the proposed method using a mid-size OOD detection benchmark on ImageNet-1k."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "DenseNet-121",
          "justification": "The model is explicitly mentioned as one of the pre-trained neural networks used for evaluating the proposed OOD detection method.",
          "quote": "A DenseNet-121 pre-trained on ILSVRC-2012 with 8M parameters and test set top-1 accuracy of 74.43%."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used for evaluation purposes and is not a novel contribution of this work.",
          "quote": "The ability of a Deep Neural Network (DNN) to generalize to new data is mainly restricted to priorly known concepts in the training dataset."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is used in experiments conducted for the paper.",
          "quote": "We ran experiments with three models of different architectures. A DenseNet-121 pre-trained on ILSVRC-2012."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of DenseNet-121 is compared with other models and across different OOD detection methods.",
          "quote": "We report our main results in Table 1, which includes the performance for the three model architectures, four OOD datasets, and seven detection methods."
        },
        "referenced_paper_title": {
          "value": "Densely Connected Convolutional Networks",
          "justification": "The referenced paper title is provided in the context of describing the DenseNet-121 model.",
          "quote": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely connected convolutional networks. In CVPR."
        }
      },
      {
        "name": {
          "value": "BiT-S-101",
          "justification": "The model is explicitly mentioned as one of the pre-trained neural networks used for evaluating the proposed OOD detection method.",
          "quote": "A BiT-S-101 model based on a ResNetv2-101 architecture with top-1 test set accuracy of 77.41% and 44.5M parameters."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used for evaluation purposes and is not a novel contribution of this work.",
          "quote": "The ability of a Deep Neural Network (DNN) to generalize to new data is mainly restricted to priorly known concepts in the training dataset."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is used in experiments conducted for the paper.",
          "quote": "We ran experiments with three models of different architectures...A BiT-S-101 model based on a ResNetv2-101 architecture."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of BiT-S-101 is compared with other models and across different OOD detection methods.",
          "quote": "We report our main results in Table 1, which includes the performance for the three model architectures, four OOD datasets, and seven detection methods."
        },
        "referenced_paper_title": {
          "value": "Big Transfer (BiT): General Visual Representation Learning",
          "justification": "The referenced paper title is provided in the context of describing the BiT-S-101 model.",
          "quote": "Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In ECCV, 2020."
        }
      },
      {
        "name": {
          "value": "ViT-B-16",
          "justification": "The model is explicitly mentioned as one of the pre-trained neural networks used for evaluating the proposed OOD detection method.",
          "quote": "We also ran experiments with a Vision Transformer (ViT-B-16), which is trained on the ILSVRC2012 dataset with 82.64% top-1 test accuracy and 70M parameters."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used for evaluation purposes and is not a novel contribution of this work.",
          "quote": "The ability of a Deep Neural Network (DNN) to generalize to new data is mainly restricted to priorly known concepts in the training dataset."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is used in experiments conducted for the paper.",
          "quote": "We ran experiments with three models of different architectures...a Vision Transformer (ViT-B-16)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of ViT-B-16 is compared with other models and across different OOD detection methods.",
          "quote": "We report our main results in Table 1, which includes the performance for the three model architectures, four OOD datasets, and seven detection methods."
        },
        "referenced_paper_title": {
          "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "justification": "The referenced paper title is provided in the context of describing the ViT-B-16 model.",
          "quote": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet-1k",
          "justification": "The dataset is prominently mentioned as the primary dataset used for training and evaluating the models in the context of OOD detection.",
          "quote": "We validate the value of the proposed method using a mid-size OOD detection benchmark on ImageNet-1k."
        },
        "aliases": [
          "ILSVRC2012"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "The referenced paper title is provided in the context of describing the ImageNet-1k dataset.",
          "quote": "J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09, 2009."
        }
      },
      {
        "name": {
          "value": "iNaturalist",
          "justification": "The dataset is used as an out-of-distribution dataset to evaluate the performance of the proposed method.",
          "quote": "The iNaturalist dataset contains over 5,000 species of plant and animals. We consider a split with 10,000 test samples with concepts from 110 classes different from the in-distribution ones."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The iNaturalist Challenge 2017 Dataset",
          "justification": "The referenced paper title is provided in the context of describing the iNaturalist dataset.",
          "quote": "Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig Adam, Pietro Perona, and Serge J. Belongie. The iNaturalist challenge 2017 dataset. ArXiv, abs/1707.06642, 2017."
        }
      },
      {
        "name": {
          "value": "Sun",
          "justification": "The dataset is used as an out-of-distribution dataset to evaluate the performance of the proposed method.",
          "quote": "The Sun dataset is a scene dataset of 397 categories. We considered a split with 10,000 randomly sampled test examples belonging to 50 categories."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "SUN Database: Large-Scale Scene Recognition from Abbey to Zoo",
          "justification": "The referenced paper title is provided in the context of describing the Sun dataset.",
          "quote": "J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. SUN Database: Large-Scale Scene Recognition from Abbey to Zoo. In CVPR, 2010."
        }
      },
      {
        "name": {
          "value": "Places365",
          "justification": "The dataset is used as an out-of-distribution dataset to evaluate the performance of the proposed method.",
          "quote": "The Places365 is also a scenes dataset with 365 different concepts. We also considered a random split with 10,000 samples from 50 disjoint categories."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Places: A 10 million image database for scene recognition",
          "justification": "The referenced paper title is provided in the context of describing the Places365 dataset.",
          "quote": "Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017."
        }
      },
      {
        "name": {
          "value": "Textures",
          "justification": "The dataset is used as an out-of-distribution dataset to evaluate the performance of the proposed method.",
          "quote": "For the Textures dataset is composed of textural patterns. We considered all of the 5,640 available test samples."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Describing Textures in the Wild",
          "justification": "The referenced paper title is provided in the context of describing the Textures dataset.",
          "quote": "M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, and A. Vedaldi. Describing textures in the wild. In Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition, 2014."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2325,
    "prompt_tokens": 20126,
    "total_tokens": 22451
  }
}