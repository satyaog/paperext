{
  "paper": "2302.00871.txt",
  "words": 14073,
  "extractions": {
    "title": {
      "value": "Using In-Context Learning to Improve Dialogue Safety",
      "justification": "This is the title of the paper as it appears at the beginning of the document.",
      "quote": "Using In-Context Learning to Improve Dialogue Safety"
    },
    "description": "This paper investigates a retrieval-based approach for reducing bias and toxicity in responses from chatbots. It utilizes in-context learning to steer a model toward generating safer responses by retrieving demonstrations of safe responses to similar dialogue contexts. The study demonstrates the method's effectiveness through both automatic and human evaluations.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experiments and evaluations to measure the effectiveness of the proposed retrieval-based approach using dialogue models and various datasets.",
      "quote": "We find our method performs competitively with existing approaches to dialogue safety without requiring training. We also show, using automatic and human evaluation, that reductions in toxicity obtained using our approach are not at the cost engagingness or coherency."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper investigates methods for improving dialogue safety in conversational AI, a key sub-field within Natural Language Processing.",
        "quote": "While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Dialogue Systems",
          "justification": "The primary focus of the paper is improving the safety of responses generated by dialogue systems.",
          "quote": "We investigate a retrieval-based approach for reducing bias and toxicity in responses from chatbots."
        },
        "aliases": [
          "Conversational Agents",
          "Chatbots"
        ]
      },
      {
        "name": {
          "value": "Bias and Fairness",
          "justification": "One of the main concerns addressed by the paper is reducing social biases and promoting fairness in dialogue responses.",
          "quote": "We investigate a retrieval-based approach for reducing bias and toxicity in responses from chatbots."
        },
        "aliases": [
          "Bias Mitigation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "OPT",
          "justification": "OPT models are used in the experiments to evaluate the proposed method.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "aliases": [
          "Open Pre-trained Transformer"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses OPT models for evaluation but they are not novel contributions of this paper.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experiments and results based on executing the OPT models.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the OPT models is compared with other models and methods in the paper.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "referenced_paper_title": {
          "value": "OPT: Open Pre-trained Transformer Language Models",
          "justification": "The title of the referenced paper describing the OPT models.",
          "quote": "OPT (Zhang et al., 2022)"
        }
      },
      {
        "name": {
          "value": "LLaMA",
          "justification": "LLaMA models are used in the experiments to evaluate the proposed method.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "aliases": [
          "Large Language Model Meta AI"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses LLaMA models for evaluation but they are not novel contributions of this paper.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experiments and results based on executing the LLaMA models.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the LLaMA models is compared with other models and methods in the paper.",
          "quote": "We observe similar trends in our LLaMA and Vicuna results to OPT."
        },
        "referenced_paper_title": {
          "value": "LLaMA: Open and Efficient Foundation Language Models",
          "justification": "The title of the referenced paper describing the LLaMA models.",
          "quote": "LLaMA (Touvron et al., 2023)"
        }
      },
      {
        "name": {
          "value": "Vicuna",
          "justification": "Vicuna models are used in the experiments to evaluate the proposed method.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses Vicuna models for evaluation but they are not novel contributions of this paper.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experiments and results based on executing the Vicuna models.",
          "quote": "To answer Q1 (§5), we evaluate our approach in three families of models: OPT (Zhang et al., 2022), LLaMA (Touvron et al., 2023), and Vicuna (Chiang et al., 2023)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the Vicuna models is compared with other models and methods in the paper.",
          "quote": "We observe similar trends in our LLaMA and Vicuna results to OPT."
        },
        "referenced_paper_title": {
          "value": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
          "justification": "The title of the referenced paper describing the Vicuna models.",
          "quote": "Vicuna (Chiang et al., 2023)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ProsocialDialog",
          "justification": "The ProsocialDialog dataset is used both as a source of safety demonstrations and for evaluating the models.",
          "quote": "ProsocialDialog (Kim et al., 2022). ProsocialDialog contains unsafe utterances with prosocial responses. We use the 42K conversations from the training split of ProsocialDialog as our source of safety demonstrations for all our experiments."
        },
        "aliases": [
          "Prosocial Dialogues"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ProsocialDialog: A Prosocial Backbone for Conversational Agents",
          "justification": "The title of the referenced paper describing the ProsocialDialog dataset.",
          "quote": "ProsocialDialog (Kim et al., 2022)."
        }
      },
      {
        "name": {
          "value": "DiaSafety",
          "justification": "The DiaSafety dataset is used for evaluating model responses to unsafe inputs.",
          "quote": "DiaSafety (Sun et al., 2022). DiaSafety is a collection of adversarial utterances which can illicit unsafe responses from conversational models."
        },
        "aliases": [
          "Dialogue Safety"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "On the Safety of Conversational Models: Taxonomy, Dataset, and Benchmark",
          "justification": "The title of the referenced paper describing the DiaSafety dataset.",
          "quote": "DiaSafety (Sun et al., 2022)."
        }
      },
      {
        "name": {
          "value": "Commonsense-Dialogues",
          "justification": "The Commonsense-Dialogues dataset is used to evaluate model responses to safe inputs.",
          "quote": "Commonsense-Dialogues is a collection of conversations grounded in social contexts. We experiment with generating responses to the 1K conversations from the validation set of Commonsense-Dialogues."
        },
        "aliases": [
          "Common-Sense Dialogs"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Commonsense-Focused Dialogues for Response Generation: An Empirical Study",
          "justification": "The title of the referenced paper describing the Commonsense-Dialogues dataset.",
          "quote": "Commonsense-Dialogues is a collection of conversations grounded in social contexts."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PerspectiveAPI",
          "justification": "PerspectiveAPI is used as one of the methods for automatically evaluating response safeness.",
          "quote": "PERSPECTIVE. We use PerspectiveAPI to quantify response toxicity."
        },
        "aliases": [
          "Perspective"
        ],
        "role": "used/referenced",
        "referenced_paper_title": {
          "value": "PerspectiveAPI",
          "justification": "The referenced title from the text which explains the library usage.",
          "quote": "For more information on PerspectiveAPI, see: https://perspectiveapi.com"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2035,
    "prompt_tokens": 26701,
    "total_tokens": 28736
  }
}