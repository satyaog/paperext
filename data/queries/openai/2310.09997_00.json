{
  "paper": "2310.09997.txt",
  "words": 2866,
  "extractions": {
    "title": {
      "value": "Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels",
      "justification": "The title 'Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels' is clearly stated at the beginning of the manuscript and aligns with the content discussed.",
      "quote": "Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels"
    },
    "description": "This paper introduces 'Forecaster,' a deep hierarchical reinforcement learning approach designed to plan over high-level goals by leveraging a temporally abstract world model. The model aids in envisioning long-term outcomes by using a tree-search planning procedure. The approach is validated in the AntMaze domain for both single-task learning and generalization to new tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experiments and evaluations on the AntMaze domain, showcasing its empirical nature.",
      "quote": "We empirically demonstrate Forecaster’s potential in both single-task learning and generalization to new tasks in the AntMaze domain."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper is centered on hierarchical reinforcement learning and planning in high-dimensional state spaces.",
        "quote": "We introduce Forecaster, a deep hierarchical reinforcement learning approach which plans over high-level goals leveraging a temporally abstract world model."
      },
      "aliases": [
        "Reinforcement Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Hierarchical Reinforcement Learning (HRL)",
          "justification": "The paper relates to hierarchical RL by decomposing decision-making into high-level and low-level actions.",
          "quote": "Hierarchical Reinforcement Learning (HRL) offers a paradigm for autonomously decompose a sequential decision making problem into a hierarchy of subtasks."
        },
        "aliases": [
          "HRL"
        ]
      },
      {
        "name": {
          "value": "Model-Based Reinforcement Learning",
          "justification": "The approach involves learning and using a world model to predict future states and rewards, characteristic of model-based RL.",
          "quote": "Forecaster is a deep model-based hierarchical reinforcement learning algorithm that learns to plan ahead and maximize long-term success in complex pixel environments."
        },
        "aliases": [
          "Model-Based RL"
        ]
      },
      {
        "name": {
          "value": "Deep Learning for Control",
          "justification": "The paper discusses using deep learning models to manage control tasks through hierarchical planning and abstract modeling.",
          "quote": "Our work proposes Forecaster, a deep model-based reinforcement learning approach which optimizes long-term performance in complex visual environments."
        },
        "aliases": [
          "DL for Control"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Forecaster",
          "justification": "Forecaster is the primary model introduced and evaluated in the paper.",
          "quote": "We introduce Forecaster, a deep hierarchical reinforcement learning approach which plans over high-level goals leveraging a temporally abstract world model."
        },
        "aliases": [
          "Forecaster"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Forecaster is the novel contribution of this work.",
          "quote": "We introduce Forecaster, a deep hierarchical reinforcement learning approach..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes empirical evaluations demonstrating Forecaster in action.",
          "quote": "We empirically demonstrate Forecaster’s potential in both single-task learning and generalization to new tasks in the AntMaze domain."
        },
        "is_compared": {
          "value": true,
          "justification": "Forecaster is compared with baselines like Director in the experimental section.",
          "quote": "We implemented Forecaster on top of Director [4], reusing its default hyperparameters. Director uses the same manager-worker approach but is limited as it does not involve the notion of options, of the abstract world model, of the tree-search planning or the goal-conditioning."
        },
        "referenced_paper_title": {
          "value": "Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels",
          "justification": "The model is introduced and explained within this paper.",
          "quote": "Forecaster is a deep model-based hierarchical reinforcement learning algorithm that learns to plan ahead and maximize long-term success in complex pixel environments."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AntMaze",
          "justification": "The AntMaze domain is used for evaluating the proposed Forecaster approach.",
          "quote": "We empirically demonstrate Forecaster’s potential in both single-task learning and generalization to new tasks in the AntMaze domain."
        },
        "aliases": [
          "AntMaze"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep hierarchical planning from pixels",
          "justification": "The AntMaze environment is commonly referenced in hierarchical reinforcement learning literature, including 'Deep hierarchical planning from pixels.'",
          "quote": "Deep hierarchical planning from pixels, 2022."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PlaNet",
          "justification": "PlaNet is used for learning a model of the environment within the Forecaster framework.",
          "quote": "The primitive world model PlaNet [5] which aims to learn a model of the environment."
        },
        "aliases": [
          "PlaNet"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning latent dynamics for planning from pixels",
          "justification": "The paper cites 'Learning latent dynamics for planning from pixels' as the basis for the PlaNet model.",
          "quote": "Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels, 2018."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2049,
    "prompt_tokens": 12314,
    "total_tokens": 14363
  }
}