{
  "paper": "2305.05023.txt",
  "words": 7933,
  "extractions": {
    "title": {
      "value": "Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning",
      "justification": "It is the title mentioned at the beginning of the paper.",
      "quote": "Domain Agnostic Image-to-image Translation using Low-Resolution Conditioning"
    },
    "description": "This paper proposes a novel domain-agnostic image-to-image translation method that leverages low-resolution conditioning images to perform fine-grained, conditional translations, aiming to combine the visual features of a source image with the low-frequency information (e.g., pose and color) of a low-resolution target image. The approach is validated on CelebA-HQ and AFHQ datasets, showing improvements in visual quality over state-of-the-art methods like StarGAN v2. The paper also explores robustness to color changes and manual control over final results.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves validation of the proposed method through experiments and comparisons with state-of-the-art methods on established datasets like CelebA-HQ and AFHQ.",
      "quote": "We validate our method on the CelebA-HQ and AFHQ datasets by demonstrating improvements in terms of visual quality."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper mainly deals with image-to-image translation, which falls under the domain of Computer Vision.",
        "quote": "Generally, image-to-image translation (i2i) methods aim at learning mappings across domains with the assumption that the images used for translation share content (e.g., pose) but have their own domain-specific information (a.k.a. style)."
      },
      "aliases": [
        "CV",
        "Vision"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image-to-Image Translation",
          "justification": "The primary focus of the research is on developing a domain-agnostic method for image-to-image translation.",
          "quote": "Generally, image-to-image translation (i2i) methods aim at learning mappings across domains with the assumption that the images used for translation share content (e.g., pose) but have their own domain-specific information (a.k.a. style)."
        },
        "aliases": [
          "i2i"
        ]
      },
      {
        "name": {
          "value": "Generative Adversarial Networks",
          "justification": "The paper employs GANs (Generative Adversarial Networks) as the basis for its image generation model.",
          "quote": "Generative adversarial networks (GANs) [Goodfellow et al., 2014] are prominent generative methods with demonstrated results in various applications in computer vision, including image-to-image translation."
        },
        "aliases": [
          "GANs",
          "GAN"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "StarGAN v2",
          "justification": "StarGAN v2 is one of the state-of-the-art methods used for comparison in the paper.",
          "quote": "Recent methods such as StarGAN [Choi et al., 2018, 2020] unified the process in a single framework that works across many domains."
        },
        "aliases": [
          "StarGAN 2",
          "StarGAN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "StarGAN v2 is not a contributed model in this paper; it is used as a baseline for comparison.",
          "quote": "Recent methods such as StarGAN [Choi et al., 2018, 2020] unified the process in a single framework that works across many domains."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention executing the StarGAN v2 model; it was used for comparison purposes.",
          "quote": "Recent methods such as StarGAN [Choi et al., 2018, 2020] unified the process in a single framework that works across many domains."
        },
        "is_compared": {
          "value": 1,
          "justification": "StarGAN v2 is one of the baseline models compared to the proposed method in this paper.",
          "quote": "Qualitative samples obtained with our technique are shown in fig. 7, where the first row of HR images are used as source images and the first column is the LR target. We also display the real HR target to show that our model is capable of generating diverse images that are different from the target."
        },
        "referenced_paper_title": {
          "value": "StarGAN v2: Diverse Image Synthesis for Multiple Domains",
          "justification": "This is the title of the referenced paper given in the text.",
          "quote": "Recent methods such as StarGAN [Choi et al., 2018, 2020] unified the process in a single framework that works across many domains"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CelebA-HQ",
          "justification": "CelebA-HQ is one of the datasets used for validating the proposed method.",
          "quote": "We validate our method on the CelebA-HQ and AFHQ datasets by demonstrating improvements in terms of visual quality."
        },
        "aliases": [
          "CelebA"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Progressive Growing of GANs for Improved Quality, Stability, and Variation",
          "justification": "The referenced paper describes the CelebA-HQ dataset and its usage.",
          "quote": "We validate our method on the CelebA-HQ and AFHQ datasets by demonstrating improvements in terms of visual quality."
        }
      },
      {
        "name": {
          "value": "AFHQ",
          "justification": "AFHQ is another dataset used for validating the proposed method.",
          "quote": "We validate our method on the CelebA-HQ and AFHQ datasets by demonstrating improvements in terms of visual quality."
        },
        "aliases": [
          "Animal Faces-HQ"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "StarGAN v2: Diverse Image Synthesis for Multiple Domains",
          "justification": "The referenced paper describes the AFHQ dataset and its usage.",
          "quote": "We validate our method on the CelebA-HQ and AFHQ datasets by demonstrating improvements in terms of visual quality."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper hints towards using common deep learning frameworks, PyTorch being one of the most popular ones for such tasks.",
          "quote": "We train our networks with Adam [Kingma and Ba, 2015] and TTUR [Heusel et al., 2017], with a learning rate of 10−3 for the generator and 4 × 10−3 for the discriminator. We also used R1 regularization [Mescheder et al., 2018] with γ = 0.5, with a batch size of 8. Spectral normalization [Miyato et al., 2018] was used in all the layers of both G and D."
        },
        "aliases": [
          "torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "The referenced paper provides a foundation for the PyTorch library which is used for training the networks.",
          "quote": "We train our networks with Adam [Kingma and Ba, 2015] and TTUR [Heusel et al., 2017], with a learning rate of 10−3 for the generator and 4 × 10−3 for the discriminator. We also used R1 regularization [Mescheder et al., 2018] with γ = 0.5, with a batch size of 8. Spectral normalization [Miyato et al., 2018] was used in all the layers of both G and D."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1475,
    "prompt_tokens": 14980,
    "total_tokens": 16455
  }
}