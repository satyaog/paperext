{
  "paper": "2403.12025.txt",
  "words": 28919,
  "extractions": {
    "title": {
      "value": "A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models",
      "justification": "It includes the main concept covered in the paper and the specific context of large language models.",
      "quote": "A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models"
    },
    "description": "The paper presents methodologies and resources to detect biases in large language models (LLMs) that affect health equity. This includes a human assessment framework, a collection of adversarial datasets known as EquityMedQA, and an empirical case study using Med-PaLM 2.",
    "type": {
      "value": "Empirical",
      "justification": "The paper involves practical application and evaluation of the proposed methodologies using real datasets and human assessments.",
      "quote": "In this work, we present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and then conduct an empirical case study with Med-PaLM 2."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The study focuses on methodologies for evaluating large language models, a subfield of Natural Language Processing.",
        "quote": "A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models"
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Healthcare",
          "justification": "The work specifically addresses health equity harms in the context of medical questions and healthcare applications.",
          "quote": "Equity in Health AI ... evaluating biases with potential to precipitate health equity-related harms in LLM-generated answers to medical questions."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Bias and Fairness",
          "justification": "The paper includes methodologies for the detection and evaluation of biases in AI systems.",
          "quote": "A critical step towards developing systems that promote health equity."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Med-PaLM 2",
          "justification": "The empirical case study uses Med-PaLM 2 to evaluate the proposed methodologies for detecting health equity-related biases.",
          "quote": "conduct an empirical case study with Med-PaLM 2"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Med-PaLM 2 is used as a case study model rather than being introduced or developed in this paper.",
          "quote": "we find that the use of a collection of datasets curated through a variety of methodologies, coupled with a thorough evaluation protocol that leverages multiple assessment rubric designs and diverse rater groups, surfaces biases that may be missed via narrower evaluation approaches."
        },
        "is_executed": {
          "value": 1,
          "justification": "Med-PaLM 2 was executed in the scope of the paper for the empirical study.",
          "quote": "Through our empirical study, we find that the use of a collection of datasets curated through a variety of methodologies... uncovers strengths and limitations of our approach."
        },
        "is_compared": {
          "value": 1,
          "justification": "Med-PaLM 2 was compared to other models (including Med-PaLM and physician-written answers) in the empirical study.",
          "quote": "We apply our three human rater assessments across answers from Med-PaLM and physicians to questions from the seven EquityMedQA datasets and three additional datasets."
        },
        "referenced_paper_title": {
          "value": "Towards Expert-Level Medical Question Answering with Large Language Models",
          "justification": "The referenced paper where Med-PaLM 2 is first introduced and studied must be included.",
          "quote": "Singhal et al. [2023] Towards Expert-Level Medical Question Answering with Large Language Models."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "EquityMedQA",
          "justification": "This is a new collection of datasets introduced in the paper for evaluating health equity-related biases.",
          "quote": "our contributions include ... EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries."
        },
        "aliases": [],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "None",
          "justification": "This dataset is introduced and developed in this paper.",
          "quote": "EquityMedQA, a collection of seven newly-released datasets comprising both manually-curated and LLM-generated questions enriched for adversarial queries."
        }
      },
      {
        "name": {
          "value": "MultiMedQA",
          "justification": "MultiMedQA is referenced in the paper and used in conjunction with other datasets.",
          "quote": "We use MultiMedQA to better understand how the adversarial datasets in EquityMedQA compare to more common consumer questions."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Large Language Models Encode Clinical Knowledge",
          "justification": "This referenced paper is where MultiMedQA was introduced and used initially.",
          "quote": "Singhal et al. [2023] demonstrated that tuning LLMs for medical question answering enabled improved comprehension, knowledge recall, and reasoning on a series of medical question answering benchmarks including medical exams, medical research, and consumer health search questions."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1238,
    "prompt_tokens": 48545,
    "total_tokens": 49783
  }
}