{
  "paper": "2302.12902.txt",
  "words": 12427,
  "extractions": {
    "title": {
      "value": "The Dormant Neuron Phenomenon in Deep Reinforcement Learning",
      "justification": "This title was directly extracted from the provided document.",
      "quote": "The Dormant Neuron Phenomenon in Deep Reinforcement Learning"
    },
    "description": "This paper identifies the 'dormant neuron phenomenon' in deep reinforcement learning, where an increasing number of inactive neurons negatively impacts network expressivity. The authors propose a solution called ReDo (Recycling Dormant neurons) to maintain network expressivity and improve performance. The paper demonstrates the presence of this phenomenon across various algorithms and environments, and showcases the effectiveness of ReDo in mitigating its adverse effects.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper presents empirical evidence of the dormant neuron phenomenon across different reinforcement learning algorithms and environments. It also includes experimental results demonstrating the effectiveness of the proposed ReDo method.",
      "quote": "We demonstrate the presence of the dormant neuron phenomenon across different algorithms and domains... Our experiments demonstrate that ReDo maintains the expressive power of networks..."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Reinforcement Learning",
        "justification": "The paper primarily focuses on deep reinforcement learning, specifically addressing issues related to neuron inactivity in neural networks used in reinforcement learning agents.",
        "quote": "In this work we identify the dormant neuron phenomenon in deep reinforcement learning..."
      },
      "aliases": [
        "Deep RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Network Expressivity",
          "justification": "The paper explores issues related to the expressivity of neural networks in deep reinforcement learning, specifically focusing on the dormant neuron phenomenon.",
          "quote": "...thereby affecting network expressivity."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ReDo (Recycling Dormant neurons)",
          "justification": "ReDo is the method proposed by the authors to address the dormant neuron phenomenon in deep reinforcement learning.",
          "quote": "To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training."
        },
        "aliases": [
          "ReDo"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "ReDo is a new method introduced by the authors to mitigate the dormant neuron phenomenon.",
          "quote": "To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experimental results where ReDo is implemented and tested across various deep reinforcement learning environments.",
          "quote": "Our experiments demonstrate that ReDo maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance."
        },
        "is_compared": {
          "value": 1,
          "justification": "ReDo is compared with other methods such as network resets and weight decay to demonstrate its effectiveness.",
          "quote": "Figure 1. Sample efficiency curves for DQN, with a replay ratio of 1, when using network resets (Nikishin et al., 2022), weight decay (WD), and our proposed ReDo."
        },
        "referenced_paper_title": {
          "value": "The Dormant Neuron Phenomenon in Deep Reinforcement Learning",
          "justification": "ReDo is introduced in the same paper where the experiments are conducted.",
          "quote": "In this work we identify the dormant neuron phenomenon in deep reinforcement learning... To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Arcade Learning Environment",
          "justification": "The paper mentions the use of the Arcade Learning Environment to demonstrate the dormant neuron phenomenon in the DQN and DrQ(Îµ) algorithms.",
          "quote": "We demonstrate the presence of the dormant neuron phenomenon across different algorithms and domains: in two value-based algorithms on the Arcade Learning Environment (Bellemare et al., 2013)..."
        },
        "aliases": [
          "ALE"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The arcade learning environment: An evaluation platform for general agents",
          "justification": "This is the seminal paper introducing the Arcade Learning Environment, which is cited in the research paper.",
          "quote": "We demonstrate the presence of the dormant neuron phenomenon across different algorithms and domains: in two value-based algorithms on the Arcade Learning Environment (Bellemare et al., 2013)..."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The paper uses the CIFAR-10 dataset to compare neuron activities in supervised learning settings as part of their experiments.",
          "quote": "We hypothesize that the non-stationarity of training deep RL agents is one of the causes for the dormant neuron phenomenon. To evaluate this hypothesis, we consider two supervised learning scenarios using the standard CIFAR-10 dataset (Krizhevsky et al., 2009)..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is the seminal paper introducing the CIFAR-10 dataset, which is cited in the research paper.",
          "quote": "We hypothesize that the non-stationarity of training deep RL agents is one of the causes for the dormant neuron phenomenon. To evaluate this hypothesis, we consider two supervised learning scenarios using the standard CIFAR-10 dataset (Krizhevsky et al., 2009)..."
        }
      },
      {
        "name": {
          "value": "MuJoCo",
          "justification": "The paper uses the MuJoCo suite to demonstrate the dormant neuron phenomenon in actor-critic methods like SAC.",
          "quote": "...and with an actor-critic method (SAC (Haarnoja et al., 2018)) evaluated on the MuJoCo suite (Todorov et al., 2012)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MuJoCo: A physics engine for model-based control",
          "justification": "This paper introduces the MuJoCo physics engine, which is referenced in the research paper.",
          "quote": "...and with an actor-critic method (SAC (Haarnoja et al., 2018)) evaluated on the MuJoCo suite (Todorov et al., 2012)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Dopamine",
          "justification": "The Dopamine framework is used for implementing and running the experiments involving discrete control tasks in the evaluation of ReDo.",
          "quote": "Code is available at https://github.com/google/dopamine/tree/master/dopamine/labs/redo"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Dopamine: A Research Framework for Deep Reinforcement Learning",
          "justification": "This paper introduces the Dopamine framework, which is being used in the research paper's experiments.",
          "quote": "Code is available at https://github.com/google/dopamine/tree/master/dopamine/labs/redo"
        }
      },
      {
        "name": {
          "value": "TF-Agents",
          "justification": "The TF-Agents library is utilized for implementing and running the continuous control experiments involving SAC in the evaluation of ReDo.",
          "quote": "For continuous control, we build on the SAC implementation in TF-Agents (Guadarrama et al., 2018) and the codebase of (Graesser et al., 2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TF-Agents: A library for reinforcement learning in tensorflow",
          "justification": "This paper introduces the TF-Agents library, which is used in the research paper's experiments.",
          "quote": "For continuous control, we build on the SAC implementation in TF-Agents (Guadarrama et al., 2018) and the codebase of (Graesser et al., 2022)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1537,
    "prompt_tokens": 23349,
    "total_tokens": 24886
  }
}