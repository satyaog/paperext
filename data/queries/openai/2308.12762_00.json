{
  "paper": "2308.12762.txt",
  "words": 20354,
  "extractions": {
    "title": {
      "value": "Reinforcement learning informed evolutionary search for autonomous systems testing",
      "justification": "It is the explicit title of the paper.",
      "quote": "Reinforcement learning informed evolutionary search for autonomous systems testing"
    },
    "description": "This paper proposes RIGAA, an approach that combines Reinforcement Learning (RL) and Evolutionary Algorithms (EA) to improve test scenario generation for autonomous robotic systems. By using an RL agent for initial population generation in the EA, RIGAA aims to enhance the efficiency and effectiveness of evolutionary search in discovering challenging and diverse test scenarios.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper evaluates the proposed approach RIGAA through empirical experiments involving two case studies: maze generation for an autonomous ‘Ant’ robot and road topology generation for an autonomous vehicle lane keeping assist system.",
      "quote": "We evaluate RIGAA on two case studies: maze generation for an autonomous ‘Ant’ robot and road topology generation for an autonomous vehicle lane keeping assist system."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper's main contribution is in integrating reinforcement learning to guide evolutionary search for test scenario generation.",
        "quote": "To improve the computational efficiency of the search-based testing, we propose augmenting the evolutionary search (ES) with a reinforcement learning (RL) agent trained using surrogate rewards derived from domain knowledge."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Evolutionary Algorithms",
          "justification": "The study primarily involves using evolutionary algorithms to evolve solutions and augmenting them with RL for better initialization.",
          "quote": "In our approach, known as RIGAA (Reinforcement learning Informed Genetic Algorithm for Autonomous systems testing), we first train an RL agent to learn useful constraints of the problem and then use it to produce a certain part of the initial population of the search algorithm."
        },
        "aliases": [
          "EA"
        ]
      },
      {
        "name": {
          "value": "Autonomous Systems",
          "justification": "The domain of the systems being tested—autonomous robots and vehicles—is clearly within the field of autonomous systems.",
          "quote": "We evaluate RIGAA on two case studies: maze generation for an autonomous ‘Ant’ robot and road topology generation for an autonomous vehicle lane keeping assist system."
        },
        "aliases": [
          "AS"
        ]
      },
      {
        "name": {
          "value": "Testing",
          "justification": "The primary focus is on generating test scenarios to evaluate autonomous systems.",
          "quote": "Evolutionary search-based techniques are commonly used for testing autonomous robotic systems."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Proximal Policy Optimization",
          "justification": "PPO is the RL algorithm used in multiple instances within the study to train the RL agent.",
          "quote": "We used the state-of-the-art Proximal Policy Optimization (PPO) algorithm to train the RL agent."
        },
        "aliases": [
          "PPO"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "PPO is not a contribution of the paper; it's an existing algorithm used in the study.",
          "quote": "We used the state-of-the-art Proximal Policy Optimization (PPO) algorithm to train the RL agent."
        },
        "is_executed": {
          "value": 1,
          "justification": "PPO was executed as part of the experiments.",
          "quote": "We used the state-of-the-art Proximal Policy Optimization (PPO) algorithm to train the RL agent."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not compare PPO with other RL algorithms.",
          "quote": "We used the state-of-the-art Proximal Policy Optimization (PPO) algorithm to train the RL agent."
        },
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "The referenced paper provides the original description of the PPO algorithm used in this study.",
          "quote": "One popular state-of-the-art RL algorithm is Proximal Policy Optimization (PPO) [63]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "D4RL Benchmark",
          "justification": "D4RL benchmark datasets are used for training the RL agents.",
          "quote": "We utilize the Ant robot pre-trained with an RL policy from the D4RL benchmark."
        },
        "aliases": [
          "D4RL"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
          "justification": "The referenced paper provides the original description of the D4RL benchmark datasets used in this study.",
          "quote": "We utilize the Ant robot pre-trained with an RL policy from the D4RL benchmark [26]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stable Baselines 3",
          "justification": "Stable Baselines 3 was utilized for implementing the RL algorithms.",
          "quote": "We used the PPO RL algorithm implementation provided by the Stable-baselines framework [56]."
        },
        "aliases": [
          "Stable-baselines3"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Stable Baselines",
          "justification": "The referenced paper provides the original description of the Stable Baselines 3 library used in this study.",
          "quote": "We used the PPO RL algorithm implementation provided by the Stable-baselines framework [56]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1148,
    "prompt_tokens": 34646,
    "total_tokens": 35794
  }
}