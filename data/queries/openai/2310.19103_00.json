{
  "paper": "2310.19103.txt",
  "words": 22797,
  "extractions": {
    "title": {
      "value": "Proving Linear Mode Connectivity of Neural Networks via Optimal Transport",
      "justification": "The title is explicit in stating the core topic of the paper, which is the proof of linear mode connectivity in neural networks using optimal transport theory.",
      "quote": "Proving Linear Mode Connectivity of Neural Networks\nvia Optimal Transport"
    },
    "description": "This paper provides a theoretical framework to explain the empirical observation of linear connectivity between different solutions in neural networks found through stochastic training. It uses convergence rates in Wasserstein distance of empirical measures to demonstrate that two two-layer wide neural networks trained with stochastic gradient descent (SGD) are linearly connected with high probability. The paper extends these results to multi-layer neural networks and empirically validates the approach on MNIST.",
    "type": {
      "value": "theoretical",
      "justification": "The paper aims at providing theoretical foundations and explanations for the linear mode connectivity phenomenon in neural networks.",
      "quote": "This paper aims at building theoretical foundations on the phenomenon of linear mode connectivity up to permutation."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning Theory",
        "justification": "The research primarily focuses on theoretical aspects of deep learning, specifically concerning neural network optimization and linear mode connectivity.",
        "quote": "This paper aims at building theoretical foundations on the phenomenon of linear mode connectivity up to permutation."
      },
      "aliases": [
        "DL Theory",
        "Theoretical Deep Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper deals with high-dimensional non-convex optimization problems and mode connectivity in the context of neural network training.",
          "quote": "The energy landscape of high-dimensional\nnon-convex optimization problems is crucial\nto understanding the effectiveness of modern deep neural network architectures."
        },
        "aliases": [
          "Non-Convex Optimization",
          "High-Dimensional Optimization"
        ]
      },
      {
        "name": {
          "value": "Neural Network Training",
          "justification": "The paper explores neural network training, specifically how different trained solutions are linearly connected in the parameter space.",
          "quote": "It suggests the existence of a continuous\nlow-loss path connecting all the local minima found by a given optimization procedure."
        },
        "aliases": [
          "NN Training"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "MLP",
          "justification": "The paper uses Multi-Layer Perceptrons as the primary model to demonstrate their theoretical results.",
          "quote": "We theoretically prove this phenomenon arises naturally\non multi-layer perceptrons (MLPs),"
        },
        "aliases": [
          "Multi-Layer Perceptron"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper does not introduce MLPs as a new model but utilizes them to validate the theoretical framework.",
          "quote": "We theoretically prove this phenomenon arises naturally\non multi-layer perceptrons (MLPs),"
        },
        "is_executed": {
          "value": 1,
          "justification": "The MLP model is empirically validated in the paper.",
          "quote": "Finally, we validate our theoretical framework\nby showing how the implicit dimension of the weight\ndistribution is correlated with linear mode connectivity for MLPs trained on MNIST with SGD and\npropose a new weight matching method."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper focuses on theoretical proofs and empirical validation rather than comparing MLPs with other models.",
          "quote": "Finally, we validate our theoretical framework\nby showing how the implicit dimension of the weight\ndistribution is correlated with linear mode connectivity for MLPs trained on MNIST with SGD and\npropose a new weight matching method."
        },
        "referenced_paper_title": {
          "value": "Neural Networks (Various Textbooks and Papers)",
          "justification": "MLPs are a standard model in the neural network literature.",
          "quote": "A Multi-Layer Perceptron (MLP) is a type of neural network architecture commonly referenced in numerous textbooks and papers."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is used to empirically validate the theoretical framework described in the paper.",
          "quote": "Finally, we validate our theoretical framework by showing how the implicit dimension of the weight\ndistribution is correlated with linear mode connectivity for MLPs trained on MNIST with SGD and\npropose a new weight matching method."
        },
        "aliases": [
          "MNIST Dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Modified National Institute of Standards and Technology database",
          "justification": "MNIST is a well-known dataset often referenced in the context of handwritten digit recognition.",
          "quote": "The MNIST database is a collection of handwritten digits commonly used for training various image processing systems."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "NumPy",
          "justification": "NumPy library is referenced for numerical operations and computations within the research.",
          "quote": "Our code is available at\nhttps://github.com/damienferbach/OT_LMC/tree/main."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The NumPy library: A Case Study in Numerical Computing",
          "justification": "NumPy is a standard library for numerical computations in Python.",
          "quote": "NumPy, a package for scientific computing with Python."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1258,
    "prompt_tokens": 42952,
    "total_tokens": 44210
  }
}