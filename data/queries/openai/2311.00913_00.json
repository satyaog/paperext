{
  "paper": "2311.00913.txt",
  "words": 7793,
  "extractions": {
    "title": {
      "value": "Self-Influence Guided Data Reweighting for Language Model Pre-training",
      "justification": "This is the official title of the paper.",
      "quote": "Self-Influence Guided Data Reweighting for Language Model Pre-training"
    },
    "description": "The paper proposes a method called P RESENCE that reweights samples during language model pre-training by leveraging self-influence scores, which indicate a sample's importance. Through extensive experimentation, the authors demonstrate that P RESENCE improves pre-training efficiency and model performance across different sizes, datasets, and tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves extensive analysis, experimentation, and evaluation of the proposed method P RESENCE across multiple model sizes, datasets, and tasks.",
      "quote": "Through extensive analysis spanning multiple model sizes, datasets, and tasks, we present P RESENCE as an important first step in the research direction of sample reweighting for pre-training language models."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on language model pre-training and reweighting data samples, which are core areas of Natural Language Processing (NLP).",
        "quote": "Language Models (LMs) pre-trained with selfsupervision on large text corpora have become\nthe default starting point for developing models\nfor various NLP tasks."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Data Reweighting",
          "justification": "The primary focus of the paper is on reweighting data samples during language model pre-training to improve model performance.",
          "quote": "We fill this important gap and propose P RESENCE, a method for jointly reweighting samples by leveraging self-influence (SI) scores as an indicator of sample importance and pre-training."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Language Model Pre-training",
          "justification": "The paper investigates methods to improve the pre-training phase of language models, specifically by reweighting samples to enhance model learning.",
          "quote": "In this paper, we attempt to develop an effective data reweighting framework for language model\npre-training."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Self-supervised Learning",
          "justification": "Since language model pre-training often involves self-supervised learning, the paper's exploration of self-influence-guided reweighting is relevant to this sub-field.",
          "quote": "Language models (LM), typically pre-trained on\nlarge volumes of unlabeled text data, have become ubiquitous model choices for various challenging downstream tasks."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "mT5-base+PRESENCE",
          "justification": "This is one of the models evaluated in the paper, which uses the P RESENCE method for sample reweighting during pre-training.",
          "quote": "We pre-train a mT5-base model from scratch on the filtered mC4 set for 200, 000 steps by choosing samples with the least SI scores that are theoretically more suitable for model learning. The models are trained with a batch size of\n1024, with an input token length of 1024 and output token length of 229."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Although the method P RESENCE is introduced in the paper, the model mT5-base itself is a pre-existing architecture and not a contribution of the authors.",
          "quote": "We use mT5-base as one of the baseline models in the experiments."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed and evaluated as a part of the experiments conducted in the paper.",
          "quote": "We pre-train a mT5-base model from scratch on the filtered mC4 set... and evaluate its performance on downstream tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of mT5-base+PRESENCE is compared to other models in the experiments.",
          "quote": "We compare the performance of the model pre-trained on filtered web corpora (mT5base+P RESENCE-Sequential) with the baseline model trained on randomly sampled data."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The referenced paper title is not explicitly mentioned. However, mT5 is a well-known model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "mT5-large+PRESENCE",
          "justification": "This variant of the mT5 model uses P RESENCE for sample reweighting and is evaluated in the paper.",
          "quote": "We pre-train mT5-large+PRESENCE from scratch on the filtered mC4 set ... We select τ1 = 1, τ2 = −1, and I = 100, 000 for the two-staged learning."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model architecture mT5-large is not a novel contribution of the paper, though its application with P RESENCE is new.",
          "quote": "We use two different variants of the T5 architecture (Raffel et al., 2020), namely mT5-base and mT5-large for comparisons and pre-train on the mC4 dataset (Xue et al., 2021)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed and evaluated in the experiments.",
          "quote": "We pre-train mT5-large+P RESENCE from scratch on the filtered mC4 set."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of mT5-large+P RESENCE is compared to other models as part of the experiments.",
          "quote": "We observe that for both variants, using P RESENCE helps improve performance on all the datasets considered."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The referenced paper title for mT5-large is not provided, but it is based on the well-known T5 model.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "mC4",
          "justification": "The mC4 dataset is used as the pre-training data source in the paper.",
          "quote": "We use the mC4 dataset (Xue et al., 2021) and pre-train a mT5-base model for 200, 000 steps on randomly shuffled data."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "mT5: A massively multilingual pre-trained text-to-text transformer",
          "justification": "The mC4 dataset is referenced in the context of the mT5 model's pre-training data.",
          "quote": "We use the mC4 dataset (Xue et al., 2021) and pre-train a mT5-base model for 200, 000 steps on randomly shuffled data."
        }
      },
      {
        "name": {
          "value": "XQuAD",
          "justification": "XQuAD is one of the downstream evaluation datasets used in the experiments.",
          "quote": "We summarize the datasets used for evaluation in Table 1. We fine-tune all the models on the downstream tasks using a batch size of 128, with a learning rate of 0.001, and a dropout rate of 0.1 for 20, 000 steps."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The title of the reference paper for XQuAD is not explicitly stated.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MLQA",
          "justification": "MLQA is one of the downstream evaluation datasets used in the experiments.",
          "quote": "We summarize the datasets used for evaluation in Table 1. We fine-tune all the models on the downstream tasks using a batch size of 128, with a learning rate of 0.001, and a dropout rate of 0.1 for 20, 000 steps."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MLQA: Evaluating Cross-lingual Extractive Question Answering",
          "justification": "The referenced paper title for MLQA provides the necessary context for this dataset as used in cross-lingual experiments.",
          "quote": "We summarize the datasets used for evaluation in Table 1."
        }
      },
      {
        "name": {
          "value": "TyDi QA",
          "justification": "TyDi QA is used as a downstream evaluation dataset in the paper's experiments.",
          "quote": "We summarize the datasets used for evaluation in Table 1. We fine-tune all the models on the downstream tasks using a batch size of 128, with a learning rate of 0.001, and a dropout rate of 0.1 for 20, 000 steps."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages",
          "justification": "The referenced paper title for TyDi QA outlines the context of its usage as a typologically diverse question-answering benchmark.",
          "quote": "We summarize the datasets used for evaluation in Table 1."
        }
      },
      {
        "name": {
          "value": "XNLI",
          "justification": "XNLI is another downstream evaluation dataset utilized in the paper's experiments.",
          "quote": "We summarize the datasets used for evaluation in Table 1. We fine-tune all the models on the downstream tasks using a batch size of 128, with a learning rate of 0.001, and a dropout rate of 0.1 for 20, 000 steps."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "XNLI: Evaluating Cross-lingual Sentence Representations",
          "justification": "This referenced paper title provides necessary context for understanding the XNLI dataset's application.",
          "quote": "We summarize the datasets used for evaluation in Table 1."
        }
      },
      {
        "name": {
          "value": "WikiAnn NER",
          "justification": "This dataset is used in the paper for structured prediction fine-tuning tasks.",
          "quote": "Following Xue et al. (2021), we utilize datasets across 5 tasks from the XTREME multilingual benchmark (Hu et al., 2020), including Question Answering, Sentence-Pair, and Structured Prediction."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "WikiAnn: Mining Multilingual Knowledge from Wikipedia",
          "justification": "The reference paper title provides context to the WikiAnn NER dataset's origin and application.",
          "quote": "Following Xue et al. (2021), we utilize datasets across 5 tasks from the XTREME multilingual benchmark (Hu et al., 2020), including Question Answering, Sentence-Pair, and Structured Prediction."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2135,
    "prompt_tokens": 14690,
    "total_tokens": 16825
  }
}