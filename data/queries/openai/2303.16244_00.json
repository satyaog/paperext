{
  "paper": "2303.16244.txt",
  "words": 9976,
  "extractions": {
    "title": {
      "value": "On Codex Prompt Engineering for OCL Generation: An Empirical Study",
      "justification": "This is the title of the given research paper.",
      "quote": "On Codex Prompt Engineering for OCL Generation: An Empirical Study"
    },
    "description": "This research paper investigates the reliability of Object Constraint Language (OCL) constraints generated by the Codex model from natural language specifications. The study involves engineering different types of prompts, providing UML model information, and using zero- and few-shot learning methods. The evaluation metrics include syntactic validity, execution accuracy, and cosine similarity compared to human-written constraints.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves conducting experiments and collecting data to analyze the impact of different prompting techniques on the reliability of OCL constraints generated by Codex.",
      "quote": "This empirical study presents statistical insights into the reliability of the OCL constraints generated by Codex and their similarity to the human-written constraints."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary research field is Natural Language Processing as the study focuses on generating code (OCL constraints) from natural language specifications using the Codex model.",
        "quote": "Large Language Models (LLMs) are deep neural networks that are pre-trained on massive amounts of text data to perform a wide variety of natural language processing (NLP) tasks... Codex, a descendant of GPT-3, is fine-tuned on publicly available, massive code repositories from GitHub."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Code Generation",
          "justification": "The paper is focused on generating Object Constraint Language (OCL) code from natural language specifications by using the Codex model.",
          "quote": "we evaluated Codex in generating OCL constraints, specifically for Python programming language, and compared its performance to its ascendant, GPT-3. Their results showed that Codex excels on code generation tasks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Prompt Engineering",
          "justification": "The paper extensively discusses the design and effectiveness of different types of prompts (basic, UML-enriched, zero-shot, and few-shot) for the Codex model in generating OCL constraints.",
          "quote": "Prompt Engineering is a systematic approach to creating prompt functions that effectively communicate the target downstream task to pre-trained language models."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Codex",
          "justification": "Codex is the primary model discussed and used to generate OCL constraints from natural language specifications in the study.",
          "quote": "Codex, developed by OpenAI, is a pre-trained large language model that has been fine-tuned on a vast corpus of publicly available code from GitHub repositories."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The Codex model is not a contribution of this paper but instead a tool used within the research.",
          "quote": "Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed to generate OCL constraints in different experimental setups.",
          "quote": "The experiments were carried out using the Codex engine 'code-davinci-002' with the default configurations."
        },
        "is_compared": {
          "value": true,
          "justification": "The Codex model's outputs (generated OCL constraints) were compared to human-written OCL constraints in terms of syntactic validity, execution accuracy, and cosine similarity.",
          "quote": "Our analysis of the OCL constraints generated by Codex shows that they are comparable and similar to the human-written constraints."
        },
        "referenced_paper_title": {
          "value": "Evaluating large language models trained on code",
          "justification": "This is the reference paper where the Codex model was introduced and evaluated.",
          "quote": "Chen et al., “Evaluating large language models trained on code,” arXiv preprint arXiv:2107.03374, 2021."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "OCL Constraints Dataset",
          "justification": "The dataset contains UML models and corresponding OCL constraints with natural language specifications, which forms the basis of the experiments conducted.",
          "quote": "we compiled a dataset of UML models and OCL constraints with their natural language specifications. This step enabled us to conduct experiments with different design settings and study the OCL constraints generated by Codex."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The dataset was compiled specifically for this research and does not have a referenced paper.",
          "quote": "To obtain a dataset for our analysis, we conducted manual searches across various resources, including educational courses on model-driven engineering, literature, and the GitHub repository."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "UML-based Specification Environment (USE)",
          "justification": "The USE modeling tool was used to compile and validate the syntactic correctness of the OCL constraints generated by Codex.",
          "quote": "To assess the generated OCL constraints, we first compile them using the UML-based Specification Environment (USE) modeling tool and report their syntactic validity."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The UML-based Specification Environment (USE) tool is referenced as part of the methodology for validation in the study.",
          "quote": "To evaluate the syntactic validity of the generated OCL constraint, we used the UML-based Specification Environment (USE) tool."
        }
      },
      {
        "name": {
          "value": "Transformer-based pre-trained language model",
          "justification": "This language model was used to compute sentence embeddings of the OCL constraints for cosine similarity analysis.",
          "quote": "To achieve this, we compute the sentence embeddings of both human and generated OCL constraints using a Transformer-based pre-trained language model and measure their cosine similarity."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
          "justification": "The paper references the Sentence-BERT model, which is a Transformer-based language model used for computing embeddings.",
          "quote": "we utilized a Transformer-based language model, specifically the 'All-MiniLM-L6-v2' mini model, to compute the embeddings of the OCL expressions. This model was chosen for its compact size and fast processing speed, making it suitable for computing the embeddings of both the generated OCL expressions and the ground truth."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1568,
    "prompt_tokens": 15157,
    "total_tokens": 16725
  }
}