{
  "paper": "2304.04858.txt",
  "words": 7575,
  "extractions": {
    "title": {
      "value": "Simulated Annealing in Early Layers Leads to Better Generalization",
      "justification": "This is the title of the research paper.",
      "quote": "Simulated Annealing in Early Layers Leads to Better Generalization"
    },
    "description": "This paper introduces Simulated annealing in EArly Layers (SEAL), a new iterative training method aimed at improving generalization in neural networks. SEAL applies gradient ascent to the early layers of the network during training, followed by gradient descent, without re-initializing the later layers. The paper demonstrates that SEAL outperforms the state-of-the-art later-layer-forgetting (LLF) method on tasks such as transfer learning and few-shot learning, using benchmarks like Tiny-ImageNet and several other datasets.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents extensive experiments and empirical results to demonstrate the effectiveness of the proposed SEAL method compared to existing methods such as LLF. It evaluates the performance on various datasets and tasks.",
      "quote": "Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The primary focus of the paper is on improving the generalization capabilities of neural networks, which is a core topic within the field of Machine Learning.",
        "quote": "Recently, a number of iterative learning methods have been introduced to improve generalization."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generalization in Deep Learning",
          "justification": "The paper is focused on methods to improve generalization in neural networks, specifically through a new iterative training approach called SEAL.",
          "quote": "Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers."
        },
        "aliases": [
          "Generalization"
        ]
      },
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The paper evaluates its method using transfer learning tasks to show improved performance compared to existing methods.",
          "quote": "We further show that, compared to normal training, LLF features, although improving on the target task, degrade the transfer learning performance across all datasets we explored."
        },
        "aliases": [
          "TL"
        ]
      },
      {
        "name": {
          "value": "Few-shot Learning",
          "justification": "The paper uses few-shot learning tasks to evaluate the proposed SEAL method, demonstrating better performance over state-of-the-art methods.",
          "quote": "Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin."
        },
        "aliases": [
          "FSL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SEAL (Simulated annealing in EArly Layers)",
          "justification": "SEAL is the primary model introduced in the paper as a new iterative training method aimed at improving generalization in neural networks.",
          "quote": "Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers."
        },
        "aliases": [
          "SEAL"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "SEAL is the novel contribution of the paper.",
          "quote": "Our principal innovation in this work is to use Simulated annealing in EArly Layers (SEAL) of the network in place of re-initialization of later layers."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes extensive experiments and results obtained using SEAL, indicating that the model was implemented and executed.",
          "quote": "Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin."
        },
        "is_compared": {
          "value": 1,
          "justification": "SEAL is compared to LLF and other standard training methods in terms of performance.",
          "quote": "Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "SEAL is a novel contribution of this paper, so there is no prior reference paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "LLF (Later-Layer-Forgetting)",
          "justification": "LLF is mentioned as a state-of-the-art method that SEAL aims to improve upon in the context of iterative training for better generalization.",
          "quote": "LLF (later-layer-forgetting) is a state-of-the-art method in this category."
        },
        "aliases": [
          "LLF"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "LLF is discussed in the paper but is not contributed by it. It is used as a comparison benchmark for SEAL.",
          "quote": "LLF (later-layer-forgetting) is a state-of-the-art method in this category."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not explicitly state that LLF was re-executed for the study, but it is used for comparison results.",
          "quote": "We further show that, compared to normal training, LLF features, although improving on the target task, degrade the transfer learning performance across all datasets we explored."
        },
        "is_compared": {
          "value": 1,
          "justification": "LLF is used as a comparison benchmark throughout the paper to evaluate the effectiveness of SEAL.",
          "quote": "Extensive experiments on the popular Tiny-ImageNet dataset benchmark and a series of transfer learning and few-shot learning tasks show that we outperform LLF by a significant margin."
        },
        "referenced_paper_title": {
          "value": "Fortuitous Forgetting in Connectionist Networks (Zhou et al., 2022)",
          "justification": "LLF is attributed to the work by Zhou et al., 2022, as a state-of-the-art method for iterative training.",
          "quote": "Zhou et al. [47] defined special masking strategies where weight masking and rewinding would lead to initialization that have much better-than-chance performance before retraining."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Tiny-ImageNet",
          "justification": "Tiny-ImageNet is one of the primary datasets used for evaluating the SEAL method in the paper.",
          "quote": "We have carried out extensive experiments on the Tiny-ImageNet [20] dataset."
        },
        "aliases": [
          "Tiny-ImageNet"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Tiny ImageNet Visual Recognition Challenge (Stanford CS231n)",
          "justification": "Tiny-ImageNet is used as one of the benchmark datasets for evaluation, with the referenced paper being the challenge hosted by Stanford CS231n.",
          "quote": "Tiny-ImageNet Visual Recognition Challenge (Ya Le and Xuan Yang, 2015)"
        }
      },
      {
        "name": {
          "value": "Flower102",
          "justification": "Flower102 is used as one of the transfer learning datasets to evaluate the performance of SEAL.",
          "quote": "For our transfer learning evaluations we use the natural image datasets Flower..."
        },
        "aliases": [
          "Flower102"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Automated Flower Classification over a Large Number of Classes (Nilsback and Zisserman, 2008)",
          "justification": "Flower102 is a standard dataset used for transfer learning evaluations, as referenced in the paper.",
          "quote": "Flower102 (Flower) [26] contains images from 102 flower categories"
        }
      },
      {
        "name": {
          "value": "CUB-200-2011 (CUB)",
          "justification": "CUB-200-2011 is used as one of the transfer learning datasets to evaluate the performance of SEAL.",
          "quote": "...CUB-200-2011 (CUB) [41] contains images of 200 wild bird species..."
        },
        "aliases": [
          "CUB"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Caltech-UCSD Birds-200-2011 Dataset (Wah et al., 2011)",
          "justification": "CUB-200-2011 is a standard dataset used for transfer learning evaluations, as referenced in the paper.",
          "quote": "...CUB-200-2011 (CUB) [41] contains images of 200 wild bird species..."
        }
      },
      {
        "name": {
          "value": "FGVC-Aircraft (Aircraft)",
          "justification": "FGVC-Aircraft is used as one of the transfer learning datasets to evaluate the performance of SEAL.",
          "quote": "...FGVC-Aircraft (Aircraft) [22] consists of 100 aircraft model variants..."
        },
        "aliases": [
          "Aircraft"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Fine-grained Visual Classification of Aircraft (Maji et al., 2013)",
          "justification": "FGVC-Aircraft is a standard dataset used for transfer learning evaluations, as referenced in the paper.",
          "quote": "...FGVC-Aircraft (Aircraft) [22] consists of 100 aircraft model variants..."
        }
      },
      {
        "name": {
          "value": "MIT Indoor 67 (MIT)",
          "justification": "MIT Indoor 67 is used as one of the transfer learning datasets to evaluate the performance of SEAL.",
          "quote": "...MIT Indoor 67 (MIT) [30] is an indoor scene recognition dataset that includes 67 different scene classes."
        },
        "aliases": [
          "MIT Indoor"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Recognizing Indoor Scenes (Quattoni and Torralba, 2009)",
          "justification": "MIT Indoor 67 is a standard dataset used for transfer learning evaluations, as referenced in the paper.",
          "quote": "... MIT Indoor 67 (MIT) [30] is an indoor scene recognition dataset that includes 67 different scene classes."
        }
      },
      {
        "name": {
          "value": "Stanford Dogs",
          "justification": "Stanford Dogs is used as one of the transfer learning datasets to evaluate the performance of SEAL.",
          "quote": "...Stanford Dogs dataset contains images of 120 breeds of dogs for fine-grained classification."
        },
        "aliases": [
          "Dogs"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet Dogs (Stanford Dogs Dataset) (Khosla et al., 2011)",
          "justification": "Stanford Dogs is a standard dataset used for transfer learning evaluations, as referenced in the paper.",
          "quote": "...Stanford Dogs dataset contains images of 120 breeds of dogs for fine-grained classification."
        }
      },
      {
        "name": {
          "value": "Cross-Domain Few-Shot Learning (CD-FSL)",
          "justification": "CD-FSL is used as a benchmark for evaluating the few-shot learning performance of SEAL.",
          "quote": "Cross-Domain Few-Shot Learning Benchmark (CD-FSL) [11] was selected as the dataset for the task, which includes data from four different data sets, namely CropDiseases [24], EuroSAT [13], ISIC2018 [3, 38], and ChestX [42]."
        },
        "aliases": [
          "CD-FSL"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "A Broader Study of Cross-Domain Few-Shot Learning (Guo et al., 2020)",
          "justification": "CD-FSL is a benchmark dataset specifically designed for cross-domain few-shot learning tasks, as referenced in the paper.",
          "quote": "Cross-Domain Few-Shot Learning Benchmark (CD-FSL) [11] was selected as the dataset for the task, which includes data from four different data sets, namely CropDiseases [24], EuroSAT [13], ISIC2018 [3, 38], and ChestX [42]."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2348,
    "prompt_tokens": 14745,
    "total_tokens": 17093
  }
}