{
  "paper": "2306.03522.txt",
  "words": 9791,
  "extractions": {
    "title": {
      "value": "A Functional Data Perspective and Baseline On Multi-Layer Out-of-Distribution Detection",
      "justification": "The title is taken directly from the paper",
      "quote": "A Functional Data Perspective and Baseline On Multi-Layer Out-of-Distribution Detection"
    },
    "description": "This research introduces a novel approach for Out-of-Distribution (OOD) detection using deep neural networks. It leverages the statistical trajectories of input samples through the network layers, contrasting them against the training set's typical behavior. The method offers unsupervised OOD detection by relying on functional data perspectives and has been validated on established computer vision benchmarks, showing improved performance.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves extensive experimentation and validation of the proposed method against state-of-the-art baselines.",
      "quote": "We validate our method and empirically demonstrate its effectiveness in OOD detection compared to strong state-of-the-art baselines on computer vision benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Out-of-Distribution Detection",
        "justification": "The main focus of the paper is on detecting samples that are out-of-distribution (OOD) using a novel functional data perspective.",
        "quote": "In this paper, we introduce a significant change of perspective. Instead of looking at each layer score independently, we cast the scores into a sequential representation that captures the statistical trajectory of an input sample through the various layers of a multi-layer neural network."
      },
      "aliases": [
        "OOD Detection"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The validation and experiments are conducted on computer vision benchmarks like CIFAR-10 and ImageNet.",
          "quote": "We validate our method and empirically demonstrate its effectiveness in OOD detection compared to strong state-of-the-art baselines on computer vision benchmarks."
        },
        "aliases": [
          "CV"
        ]
      },
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The paper focuses on using deep neural networks for OOD detection, which is a sub-field of Deep Learning.",
          "quote": "A key feature of out-of-distribution (OOD) detection is to exploit a trained neural network by extracting statistical patterns and relationships through the multi-layer classifier to detect shifts in the expected input data distribution."
        },
        "aliases": [
          "DL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DenseNet-121",
          "justification": "The DenseNet-121 model is used in the experiments and is mentioned multiple times in the paper.",
          "quote": "A DenseNet-121 [30] pre-trained on ILSVRC-2012 with 8M parameters and test set top-1 accuracy of 74.43%."
        },
        "aliases": [
          "DenseNet"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The DenseNet-121 model is used in experiments but was not contributed by this paper.",
          "quote": "A DenseNet-121 [30] pre-trained on ILSVRC-2012 with 8M parameters and test set top-1 accuracy of 74.43%."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments with DenseNet-121 were carried out as part of the research.",
          "quote": "A DenseNet-121 [30] pre-trained on ILSVRC-2012 with 8M parameters and test set top-1 accuracy of 74.43%."
        },
        "is_compared": {
          "value": 1,
          "justification": "DenseNet-121 is compared against other state-of-the-art models in the results.",
          "quote": "For DenseNet-121 (see Appendix A.2), we improved on ReAct by 16% and 3.9% in TNR and ROC, respectively."
        },
        "referenced_paper_title": {
          "value": "Densely Connected Convolutional Networks",
          "justification": "This is the seminal paper on DenseNet-121, listed in the references and referred to in the text.",
          "quote": "A DenseNet-121 [30] pre-trained on ILSVRC-2012 with 8M parameters and test set top-1 accuracy of 74.43%."
        }
      },
      {
        "name": {
          "value": "ResNet-50",
          "justification": "ResNet-50 is used in the experiments and is explicitly mentioned in the paper.",
          "quote": "A ResNet-50 model with top-1 test set accuracy of 75.85% and 25M parameters."
        },
        "aliases": [
          "ResNet"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The ResNet-50 model is used but not contributed by this paper.",
          "quote": "A ResNet-50 model with top-1 test set accuracy of 75.85% and 25M parameters."
        },
        "is_executed": {
          "value": 1,
          "justification": "ResNet-50 was used in the research experiments.",
          "quote": "A ResNet-50 model with top-1 test set accuracy of 75.85% and 25M parameters."
        },
        "is_compared": {
          "value": 1,
          "justification": "ResNet-50 is compared against other models in the study.",
          "quote": "On ResNet-50, we achieve a gain of 6.7% in TNR and 1.5% in ROC compared to ReAct."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "This is the seminal paper on ResNet-50, listed in the references and referred to in the text.",
          "quote": "A ResNet-50 model with top-1 test set accuracy of 75.85% and 25M parameters."
        }
      },
      {
        "name": {
          "value": "BiT-S-101",
          "justification": "BiT-S-101 is explicitly mentioned and used in the experiments in the paper.",
          "quote": "A BiT-S-101 [35] model based on a ResNetv2-101 architecture with top-1 test set accuracy of 77.41% and 44M parameters."
        },
        "aliases": [
          "BiT ResNet"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The BiT-S-101 model is used but not contributed by this paper.",
          "quote": "A BiT-S-101 [35] model based on a ResNetv2-101 architecture with top-1 test set accuracy of 77.41% and 44M parameters."
        },
        "is_executed": {
          "value": 1,
          "justification": "BiT-S-101 was used in research experiments.",
          "quote": "A BiT-S-101 [35] model based on a ResNetv2-101 architecture with top-1 test set accuracy of 77.41% and 44M parameters."
        },
        "is_compared": {
          "value": 1,
          "justification": "BiT-S-101 is compared against other models in the study.",
          "quote": "For BiT-S-101, we outperform GradNorm by 18.9% TNR and 5.4% ROC."
        },
        "referenced_paper_title": {
          "value": "Big Transfer (BiT): General Visual Representation Learning",
          "justification": "This is the seminal paper on BiT-S-101, listed in the references and referred to in the text.",
          "quote": "A BiT-S-101 [35] model based on a ResNetv2-101 architecture with top-1 test set accuracy of 77.41% and 44M parameters."
        }
      },
      {
        "name": {
          "value": "Vision Transformer (ViT-B-16)",
          "justification": "ViT-B-16 is explicitly mentioned as one of the models used in the experiments.",
          "quote": "And a Vision Transformer (ViT-B-16; 15), which is trained on the ILSVRC2012 dataset with 82.64% top-1 test accuracy and 70M parameters."
        },
        "aliases": [
          "ViT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The ViT-B-16 model is used but not contributed by this paper.",
          "quote": "And a Vision Transformer (ViT-B-16; 15), which is trained on the ILSVRC2012 dataset with 82.64% top-1 test accuracy and 70M parameters."
        },
        "is_executed": {
          "value": 1,
          "justification": "ViT-B-16 was used in research experiments.",
          "quote": "And a Vision Transformer (ViT-B-16; 15), which is trained on the ILSVRC2012 dataset with 82.64% top-1 test accuracy and 70M parameters."
        },
        "is_compared": {
          "value": 1,
          "justification": "ViT-B-16 is compared against other models in the study.",
          "quote": "For the ViT-B-16, the gap between methods is small and our method exhibits a comparable TNR and ROC to previous state-of-the-art."
        },
        "referenced_paper_title": {
          "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "justification": "This is the seminal paper on ViT-B-16, listed in the references and referred to in the text.",
          "quote": "And a Vision Transformer (ViT-B-16; 15), which is trained on the ILSVRC2012 dataset with 82.64% top-1 test accuracy and 70M parameters."
        }
      },
      {
        "name": {
          "value": "MobileNetV3 Large",
          "justification": "MobileNetV3 Large is explicitly mentioned and used in the experiments in the paper.",
          "quote": "And a MobileNetV3 large, with accuracy of 74.6% and around 5M parameters."
        },
        "aliases": [
          "MobileNet"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The MobileNetV3 Large model is used but not contributed by this paper.",
          "quote": "And a MobileNetV3 large, with accuracy of 74.6% and around 5M parameters."
        },
        "is_executed": {
          "value": 1,
          "justification": "MobileNetV3 Large was used in research experiments.",
          "quote": "And a MobileNetV3 large, with accuracy of 74.6% and around 5M parameters."
        },
        "is_compared": {
          "value": 1,
          "justification": "MobileNetV3 Large is compared against other models in the study.",
          "quote": "Finally, on MobileNet-V3 Large, we registered gains of around 20% TNR and 9.2% ROC."
        },
        "referenced_paper_title": {
          "value": "Searching for MobileNetV3",
          "justification": "This is the seminal paper on MobileNetV3 Large, listed in the references and referred to in the text.",
          "quote": "And a MobileNetV3 large, with accuracy of 74.6% and around 5M parameters."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is used as the primary in-distribution dataset for the experiments.",
          "quote": "We set as in-distribution dataset ImageNet-1K (= ILSVRC2012; 13) for our main experiments, which is a challenging mid-size and realistic dataset."
        },
        "aliases": [
          "ImageNet-1K",
          "ILSVRC2012"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "This is the seminal paper on ImageNet, listed in the references and referred to in the text.",
          "quote": "We set as in-distribution dataset ImageNet-1K (= ILSVRC2012; 13) for our main experiments, which is a challenging mid-size and realistic dataset."
        }
      },
      {
        "name": {
          "value": "iNaturalist",
          "justification": "iNaturalist is used as one of the out-of-distribution datasets for evaluating the methods.",
          "quote": "The iNaturalist [28] split with 10,000 test samples with concepts from 110 classes different from the in-distribution ones."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The iNaturalist Challenge 2017 Dataset",
          "justification": "This is the reference paper for the iNaturalist dataset, listed in the references and referred to in the text.",
          "quote": "The iNaturalist [28] split with 10,000 test samples with concepts from 110 classes different from the in-distribution ones."
        }
      },
      {
        "name": {
          "value": "Sun",
          "justification": "Sun is used as one of the out-of-distribution datasets for evaluating the methods.",
          "quote": "The Sun [70] dataset with a split with 10,000 randomly sampled test examples belonging to 50 categories."
        },
        "aliases": [
          "SUN"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "SUN Database: Large-Scale Scene Recognition from Abbey to Zoo",
          "justification": "This is the reference paper for the SUN dataset, listed in the references and referred to in the text.",
          "quote": "The Sun [70] dataset with a split with 10,000 randomly sampled test examples belonging to 50 categories."
        }
      },
      {
        "name": {
          "value": "Places365",
          "justification": "Places365 is used as one of the out-of-distribution datasets for evaluating the methods.",
          "quote": "The Places365 [73] dataset with 10,000 samples from 50 disjoint categories."
        },
        "aliases": [
          "Places"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Places: A 10 million Image Database for Scene Recognition",
          "justification": "This is the reference paper for the Places365 dataset, listed in the references and referred to in the text.",
          "quote": "The Places365 [73] dataset with 10,000 samples from 50 disjoint categories."
        }
      },
      {
        "name": {
          "value": "Textures",
          "justification": "Textures is used as one of the out-of-distribution datasets for evaluating the methods.",
          "quote": "For the DTD or Textures [10] dataset, we considered all of the 5,640 available test samples."
        },
        "aliases": [
          "DTD"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Describing Textures in the Wild",
          "justification": "This is the reference paper for the Textures dataset, listed in the references and referred to in the text.",
          "quote": "For the DTD or Textures [10] dataset, we considered all of the 5,640 available test samples."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is another in-distribution dataset used in a special case study to demonstrate the method's applicability on a smaller dataset.",
          "quote": "We ran experiments with a ResNet-18 model trained on CIFAR-10 [36]."
        },
        "aliases": [
          "CIFAR"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the reference paper for the CIFAR-10 dataset, listed in the references and referred to in the text.",
          "quote": "We ran experiments with a ResNet-18 model trained on CIFAR-10 [36]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions that all model weights were downloaded from PyTorch hub.",
          "quote": "We download all the checkpoints weights from PyTorch [47] hub."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the reference paper for PyTorch, listed in the references and referred to in the text.",
          "quote": "We download all the checkpoints weights from PyTorch [47] hub."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3110,
    "prompt_tokens": 20186,
    "total_tokens": 23296
  }
}