{
  "paper": "2402.08530.txt",
  "words": 13733,
  "extractions": {
    "title": {
      "value": "A Distributional Analogue to the Successor Representation",
      "justification": "The title is taken directly from the paper.",
      "quote": "A Distributional Analogue to the Successor Representation"
    },
    "description": "This paper presents a new method for distributional reinforcement learning (DRL) that separates the transition structure and reward in the learning process. The method introduces the Distributional Successor Measure (DSM), which describes the distributional consequences of behavior. The paper provides theoretical connections between DSM and distributional/model-based RL, and proposes an algorithm for learning DSM from data using a two-level maximum mean discrepancy approach. It also presents an application in zero-shot risk-sensitive policy evaluation.",
    "type": {
      "value": "Theoretical",
      "justification": "The paper primarily introduces new theoretical constructs and algorithms related to distributional reinforcement learning.",
      "quote": "We formulate the distributional SM as a distribution over distributions and provide theory connecting it with distributional and model-based reinforcement learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper's primary focus is on advancing techniques in reinforcement learning, specifically distributional reinforcement learning.",
        "quote": "A Distributional Analogue to the Successor Representation"
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Distributional Reinforcement Learning",
          "justification": "The paper introduces a novel method in the specific area of distributional reinforcement learning.",
          "quote": "This paper contributes a new approach for distributional reinforcement learning which elucidates a clean separation of transition structure and reward in the learning process."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Zero-Shot Learning",
          "justification": "The paper proposes methods that allow for zero-shot evaluation of novel reward functions without requiring further learning.",
          "quote": "we show that it enables zero-shot risk-sensitive policy evaluation in a way that was not previously possible."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "δ-model",
          "justification": "The δ-model is proposed in the paper as a key algorithmic contribution for approximating the distributional successor measure.",
          "quote": "Our primary algorithmic contribution is the δ-model, a tractable approximation to the distributional successor measure based on ensembles of diverse generative models, along with practical implementation techniques that are crucial for success."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The δ-model is introduced as a new method by the authors.",
          "quote": "Our primary algorithmic contribution is the δ-model, a tractable approximation to the distributional successor measure based on ensembles of diverse generative models, along with practical implementation techniques that are crucial for success."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not specify the hardware used for executing the δ-model.",
          "quote": ""
        },
        "is_compared": {
          "value": 1,
          "justification": "The δ-model is compared against other methods such as γ-models and traditional return distribution estimations.",
          "quote": "As a baseline, we compare our method to an ensemble of γ-models (Janner et al., 2020), which is almost equivalent to a δ-model, with the difference being that the individual γ-models of the ensemble are trained independently rather than coupled through the model MMD loss."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The δ-model is introduced as a new method and does not directly reference a prior work as its basis.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "γ-model",
          "justification": "γ-models are used as a baseline comparison in the experiments.",
          "quote": "As a baseline, we compare our method to an ensemble of γ-models (Janner et al., 2020), which is almost equivalent to a δ-model, with the difference being that the individual γ-models of the ensemble are trained independently rather than coupled through the model MMD loss."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The γ-models are not contributed by this paper but are used for comparison purposes.",
          "quote": "As a baseline, we compare our method to an ensemble of γ-models (Janner et al., 2020), which is almost equivalent to a δ-model, with the difference being that the individual γ-models of the ensemble are trained independently rather than coupled through the model MMD loss."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not specify the hardware used for executing the γ-models.",
          "quote": ""
        },
        "is_compared": {
          "value": 1,
          "justification": "The γ-models are compared to the δ-model as a baseline.",
          "quote": "As a baseline, we compare our method to an ensemble of γ-models (Janner et al., 2020), which is almost equivalent to a δ-model, with the difference being that the individual γ-models of the ensemble are trained independently rather than coupled through the model MMD loss."
        },
        "referenced_paper_title": {
          "value": "Gamma-Models: Generative Temporal Difference Learning for Infinite-Horizon Prediction",
          "justification": "The reference paper for γ-models is cited in the experimental comparison.",
          "quote": "As a baseline, we compare our method to an ensemble of γ-models (Janner et al., 2020), which is almost equivalent to a δ-model."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1299,
    "prompt_tokens": 24003,
    "total_tokens": 25302
  }
}