{
  "paper": "2210.03022.txt",
  "words": 11499,
  "extractions": {
    "title": {
      "value": "Stateful Active Facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning",
      "justification": "Title of the paper",
      "quote": "Stateful Active Facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning"
    },
    "description": "This paper addresses cooperative multi-agent reinforcement learning (MARL) by introducing the concepts of coordination level and environmental heterogeneity level. It proposes a novel centralized training decentralized execution learning approach called Stateful Active Facilitator (SAF) to handle varying degrees of coordination and heterogeneity. The paper also introduces HECOGrid, a suite of MARL environments to evaluate different approaches under different conditions.",
    "type": {
      "value": "empirical study",
      "justification": "The paper provides empirical evaluations of the proposed SAF approach and compares it with baselines like IPPO and MAPPO across different tasks using the HECOGrid environments.",
      "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid. Our results show that SAF consistently outperforms the baselines across different tasks and different heterogeneity and coordination levels."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary focus of the paper is on cooperative multi-agent reinforcement learning (MARL).",
        "quote": "In cooperative multi-agent reinforcement learning, a team of agents works together to achieve a common goal."
      },
      "aliases": [
        "RL",
        "MARL",
        "Reinforcement Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Agent Systems",
          "justification": "The study also focuses on the behaviors and coordination between multiple agents, a key aspect of Multi-Agent Systems.",
          "quote": "In many settings, MARL manifests itself in the form of cooperative tasks in which all the agents work together in order to achieve a common goal."
        },
        "aliases": [
          "MAS",
          "Multi-Agent Reinforcement Learning"
        ]
      },
      {
        "name": {
          "value": "Centralized Training Decentralized Execution (CTDE)",
          "justification": "The proposed methodology, Stateful Active Facilitator (SAF), falls under centralized training decentralized execution architectures.",
          "quote": "To enable efficient training of MARL agents in high coordination and heterogeneity, we introduce a novel approach called Stateful Active Facilitator (SAF)."
        },
        "aliases": [
          "CTDE"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Stateful Active Facilitator (SAF)",
          "justification": "The main model introduced by the paper is the Stateful Active Facilitator (SAF).",
          "quote": "we propose a Centralized Training Decentralized Execution learning approach called Stateful Active Facilitator (SAF) that enables agents to work efficiently in high-coordination and high-heterogeneity environments through a differentiable and shared knowledge source used during training and dynamic selection from a shared pool of policies."
        },
        "aliases": [
          "SAF"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is introduced for the first time in this paper as a contribution.",
          "quote": "we propose a Centralized Training Decentralized Execution learning approach called Stateful Active Facilitator (SAF)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is trained and evaluated empirically as stated in the paper.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "is_compared": {
          "value": 1,
          "justification": "SAF is evaluated and compared against IPPO and MAPPO within the paper.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "As the contribution of the paper, there is no referenced paper for SAF.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Independent Proximal Policy Optimization (IPPO)",
          "justification": "This model is used as a baseline for comparison with the proposed model.",
          "quote": "we propose a Centralized Training Decentralized Execution learning approach called Stateful Active Facilitator (SAF)... We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "aliases": [
          "IPPO"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not introduced by this paper but used for comparison.",
          "quote": "de Witt et al. (2020) recently showed that PPO, when used for independent learning in multi-agent settings (called Independent PPO or IPPO) is in fact capable of beating several state-of-the-art approaches in MARL on competitive benchmarks such as StarCraft (Samvelyan et al., 2019)."
        },
        "is_executed": {
          "value": 1,
          "justification": "IPPO is executed within the paper for comparison.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "is_compared": {
          "value": 1,
          "justification": "IPPO is numerically compared to SAF and MAPPO in the study.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "referenced_paper_title": {
          "value": "Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?",
          "justification": "The referenced paper title for IPPO is provided in the citations.",
          "quote": "de Witt et al. (2020) recently showed that PPO, when used for independent learning in multi-agent settings (called Independent PPO or IPPO) is in fact capable of beating several state-of-the-art approaches in MARL on competitive benchmarks such as StarCraft (Samvelyan et al., 2019)."
        }
      },
      {
        "name": {
          "value": "Multi-Agent Proximal Policy Optimization (MAPPO)",
          "justification": "This model is used as a baseline for comparison with the proposed model.",
          "quote": "we propose a Centralized Training Decentralized Execution learning approach called Stateful Active Facilitator (SAF)... We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "aliases": [
          "MAPPO"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not introduced by this paper but used for comparison.",
          "quote": "This usually consists of a centralized critic during training which has access to the observations of every agent and guides the policies of each agent. In many settings... MAPPO (Yu et al., 2021), ... was developed."
        },
        "is_executed": {
          "value": 1,
          "justification": "MAPPO is executed within the paper for comparison.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "is_compared": {
          "value": 1,
          "justification": "MAPPO is numerically compared to SAF and IPPO in the study.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "referenced_paper_title": {
          "value": "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games",
          "justification": "The referenced paper title for MAPPO is provided in the citations.",
          "quote": "Yu et al. (2021) proposes the extension PPO (Schulman et al., 2017) to a multi-agent framework in a similar manner."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "HECOGrid",
          "justification": "The introduced HECOGrid environment is a dataset of multi-agent RL environments designed for evaluating different approaches under varying levels of coordination and environmental heterogeneity.",
          "quote": "we propose HECOGrid, a suite of multi-agent RL environments that facilitates empirical evaluation of different MARL approaches across different levels of coordination and environmental heterogeneity by providing a quantitative control over coordination and heterogeneity levels of the environment."
        },
        "aliases": [
          "N/A"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "As a contribution of the paper, there is no referenced paper for HECOGrid.",
          "quote": "N/A"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used for implementing and training the models in the study.",
          "quote": "All models were implemented and trained using the PyTorch library."
        },
        "aliases": [
          "N/A"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The paper does not provide a specific reference title for PyTorch.",
          "quote": "N/A"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1666,
    "prompt_tokens": 20038,
    "total_tokens": 21704
  }
}