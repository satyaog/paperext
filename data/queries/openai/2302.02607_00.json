{
  "paper": "2302.02607.txt",
  "words": 21535,
  "extractions": {
    "title": {
      "value": "Target-based Surrogates for Stochastic Optimization",
      "justification": "The title is explicitly provided at the beginning of the paper.",
      "quote": "Target-based Surrogates for Stochastic Optimization"
    },
    "description": "This paper addresses the optimization of functions with expensive gradient computations by proposing a framework that uses these expensive gradient computations to construct surrogate functions in a target space. These surrogates can be minimized efficiently, allowing for multiple parameter updates and amortizing the computational cost. The paper introduces both deterministic and stochastic target smoothness surrogates with theoretical guarantees and evaluates their framework on supervised and imitation learning tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper provides theoretical frameworks and evaluates the proposed optimization algorithms on various supervised and imitation learning tasks.",
      "quote": "To evaluate our framework, we consider a suite of supervised learning and imitation learning problems. Our experiments indicate the benefits of target optimization and the effectiveness of SSO."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization Methods",
        "justification": "The paper is centered on improving optimization techniques, particularly optimizing expensive-to-evaluate functions using stochastic and deterministic surrogates.",
        "quote": "consider the setting where we have access to an expensive stochastic gradient oracle that returns a noisy, but unbiased estimate of the true gradient."
      },
      "aliases": [
        "Optimization",
        "Optimization Techniques"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper discusses the application of their optimization methods to reinforcement learning problems, specifically mentioning policy optimization in online imitation learning as a key application area.",
          "quote": "In our applications of interest, computing ∇z ℓ(z) requires accessing the expensive gradient oracle, but ∇θ f (θ) can be computed efficiently... For example, in OIL, computing the cumulative loss ℓ (and the corresponding gradient ∇z ℓ(z)) for a policy involves evaluating it in the environment."
        },
        "aliases": [
          "RL"
        ]
      },
      {
        "name": {
          "value": "Supervised Learning",
          "justification": "The optimization algorithms are evaluated on standard supervised learning problems to demonstrate their effectiveness.",
          "quote": "We use the the rcv1 dataset from libsvm (Chang and Lin, 2011) across four different batch sizes under a logistic-loss. We include additional experiments over other data-sets, and optimization algorithms in Appendix E."
        },
        "aliases": [
          "SL",
          "Supervised ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Soft Actor-Critic",
          "justification": "Soft Actor-Critic is mentioned as the algorithm used to train the expert policy in the imitation learning experiments.",
          "quote": "The expert policy, defined by a normal distribution and parameterized by a two-layer MLP is trained using the Soft-Actor-Critic Algorithm (Haarnoja et al., 2018)."
        },
        "aliases": [
          "SAC"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used as a reference model for training the expert policy and is not introduced in this paper.",
          "quote": "The expert policy, defined by a normal distribution and parameterized by a two-layer MLP is trained using the Soft-Actor-Critic Algorithm (Haarnoja et al., 2018)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The SAC algorithm is applied in the experiments to train the expert policy for the imitation learning tasks.",
          "quote": "The expert policy, defined by a normal distribution and parameterized by a two-layer MLP is trained using the Soft-Actor-Critic Algorithm (Haarnoja et al., 2018)."
        },
        "is_compared": {
          "value": 0,
          "justification": "The SAC model is not compared with other models in this work; it is used as part of the experimental setup.",
          "quote": "The expert policy, defined by a normal distribution and parameterized by a two-layer MLP is trained using the Soft-Actor-Critic Algorithm (Haarnoja et al., 2018)."
        },
        "referenced_paper_title": {
          "value": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
          "justification": "The paper cites the original work on SAC by Haarnoja et al., 2018.",
          "quote": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "rcv1",
          "justification": "The rcv1 dataset is explicitly mentioned as one of the datasets used to evaluate the framework.",
          "quote": "We use the the rcv1 dataset from libsvm (Chang and Lin, 2011) across four different batch sizes under a logistic-loss."
        },
        "aliases": [
          "Reuters Corpus Volume 1"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Libsvm: A library for support vector machines",
          "justification": "The dataset is sourced from the LIBSVM repository, which is a well-known library for machine learning datasets.",
          "quote": "We use the the rcv1 dataset from libsvm (Chang and Lin, 2011) across four different batch sizes under a logistic-loss."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Pytorch",
          "justification": "Pytorch is mentioned as the library used for implementing the neural network models used in the experiments.",
          "quote": "As before, in all settings, algorithms use either their theoretical step-size when available, or the default as defined by (Paszke et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pytorch: An imperative style, high-performance deep learning library",
          "justification": "The paper cites the original work on Pytorch by Paszke et al., 2019.",
          "quote": "As before, in all settings, algorithms use either their theoretical step-size when available, or the default as defined by (Paszke et al., 2019)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1174,
    "prompt_tokens": 41771,
    "total_tokens": 42945
  }
}