{
  "paper": "2206.12839.txt",
  "words": 14004,
  "extractions": {
    "title": {
      "value": "Repository-Level Prompt Generation for Large Language Models of Code",
      "justification": "The title at the beginning of the paper.",
      "quote": "Repository-Level Prompt Generation for Large Language Models of Code"
    },
    "description": "The paper proposes the Repo-Level Prompt Generator (RLPG), which enhances prompt generation for Large Language Models (LLMs) in the context of code completion. RLPG leverages the structure of code repositories and derives context from multiple files within the repository to generate example-specific prompts without requiring access to the weights of the LLMs. Extensive experiments show significant performance improvements over Codex and other baselines in single-line code auto-completion tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts extensive experiments with models and datasets to show the effectiveness of the proposed method.",
      "quote": "We conduct experiments on the task of single-line code auto-completion..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on generating prompts for LLMs, which is a prominent task in Natural Language Processing.",
        "quote": "techniques for introducing domain-specific knowledge in the prompt design process become important..."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Code Completion",
          "justification": "The primary application domain discussed in the paper is code auto-completion using LLMs.",
          "quote": "We conduct experiments on the task of single-line code auto-completion using code repositories taken from Google Code archives."
        },
        "aliases": [
          ""
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "CodeBERT",
          "justification": "The pretrained CodeBERT model is used in the RLPG framework for obtaining context representations.",
          "quote": "We used CodeBERT (Feng et al., 2020) as our pretrained model FÏ• to obtain the representation of hole window."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses the existing CodeBERT model for context representation.",
          "quote": "We used CodeBERT (Feng et al., 2020) as our pretrained model."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not state that CodeBERT was executed on GPU or CPU.",
          "quote": "We used CodeBERT (Feng et al., 2020) as our pretrained model."
        },
        "is_compared": {
          "value": 0,
          "justification": "CodeBERT is used as a part of the RLPG framework and is not directly compared.",
          "quote": "We used CodeBERT (Feng et al., 2020) as our pretrained model."
        },
        "referenced_paper_title": {
          "value": "CodeBERT: A pre-trained model for programming and natural languages",
          "justification": "This is the referenced paper title for CodeBERT.",
          "quote": "Feng et al., 2020"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Google Code archives",
          "justification": "The dataset used in the experiments is taken from Google Code archives.",
          "quote": "We conduct experiments on the task of single-line code auto-completion using code repositories taken from Google Code archives."
        },
        "aliases": [
          ""
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The original paper for this dataset is not explicitly mentioned.",
          "quote": "We conduct experiments on the task of single-line code auto-completion using code repositories taken from Google Code archives."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "OpenAI Codex",
          "justification": "Codex is used for generating completions and is a part of the evaluation in the experiments.",
          "quote": "We used the OpenAI Codex Completions API for generating the predicted hole from the Codex model."
        },
        "aliases": [
          "Codex"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Evaluating large language models trained on code",
          "justification": "This is the reference paper for Codex as mentioned in the text.",
          "quote": "Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021."
        }
      },
      {
        "name": {
          "value": "tree-sitter API for Java",
          "justification": "Tree-sitter API is used to parse Java files to obtain Abstract Syntax Trees (ASTs).",
          "quote": "We used the tree-sitter API for Java that enables us to get the AST of a file and query it."
        },
        "aliases": [
          "tree-sitter"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The original paper for this API is not explicitly mentioned.",
          "quote": "We used the tree-sitter API for Java that enables us to get the AST of a file and query it."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 987,
    "prompt_tokens": 23503,
    "total_tokens": 24490
  }
}