{
  "paper": "2310.01765.txt",
  "words": 35581,
  "extractions": {
    "title": {
      "value": "Data Cleaning and Machine Learning: A Systematic Literature Review",
      "justification": "Title of the research paper as provided by the user.",
      "quote": "Data Cleaning and Machine Learning: A Systematic Literature Review"
    },
    "description": "This paper conducts a systematic literature review to explore the relationship between data cleaning and machine learning, summarizing recent approaches, providing future research recommendations, and identifying open challenges. The review encompasses various data cleaning activities such as feature cleaning, label cleaning, entity matching, outlier detection, imputation, and holistic data cleaning.",
    "type": {
      "value": "Empirical",
      "justification": "The paper conducts a systematic literature review, collecting and analyzing data from various published studies to summarize the state of the art and provide recommendations.",
      "quote": "In this study, we summarize the latest approaches in Data Cleaning for ML (DC4ML) and ML for Data Cleaning (ML4DC). To reach that goal, we adopt the Systematic Literature Review (SLR) approach, which differs from traditional literature reviews by its strict and rigorous methodology."
    },
    "primary_research_field": {
      "name": {
        "value": "Data Cleaning in Machine Learning",
        "justification": "The primary focus of the paper is on data cleaning methods and their relationship with machine learning, explored through a systematic literature review.",
        "quote": "In this study, we summarize the latest approaches in Data Cleaning for ML (DC4ML) and ML for Data Cleaning (ML4DC)."
      },
      "aliases": [
        "DC4ML",
        "ML4DC"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Feature Cleaning",
          "justification": "The paper elaborates on different techniques for cleaning feature data biases and errors.",
          "quote": "In total, we summarize the content of 101 papers and provide 24 future work directions. Our review can serve as a basis for researchers and practitioners to understand state-of-the-art DC&ML approaches and to contribute to the field."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Label Cleaning",
          "justification": "The paper presents methods for detecting and correcting label errors in datasets.",
          "quote": "In total, we summarize the content of 101 papers and provide 24 future work directions. Our review can serve as a basis for researchers and practitioners to understand state-of-the-art DC&ML approaches and to contribute to the field."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Entity Matching",
          "justification": "Entity matching techniques are covered in detail, focusing on how machine learning can be used for detecting duplicates and matching records.",
          "quote": "In total, we summarize the content of 101 papers and provide 24 future work directions. Our review can serve as a basis for researchers and practitioners to understand state-of-the-art DC&ML approaches and to contribute to the field."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Outlier Detection",
          "justification": "It discusses outlier detection techniques with a focus on their application in data cleaning for machine learning.",
          "quote": "In total, we summarize the content of 101 papers and provide 24 future work directions. Our review can serve as a basis for researchers and practitioners to understand state-of-the-art DC&ML approaches and to contribute to the field."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Imputation",
          "justification": "The paper reviews methods for imputing missing values in datasets as part of the data cleaning process.",
          "quote": "In total, we summarize the content of 101 papers and provide 24 future work directions. Our review can serve as a basis for researchers and practitioners to understand state-of-the-art DC&ML approaches and to contribute to the field."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Holistic Data Cleaning",
          "justification": "Holistic approaches that attempt to clean multiple types of data errors simultaneously are also reviewed.",
          "quote": "In total, we summarize the content of 101 papers and provide 24 future work directions. Our review can serve as a basis for researchers and practitioners to understand state-of-the-art DC&ML approaches and to contribute to the field."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CuratorNet",
          "justification": "The paper describes CuratorNet, a deep-learning-based model used for estimating and correcting radial blur in images.",
          "quote": "The authors designed a convolutional neural network (CNN) named CuratorNet, to estimate the point spread function (PSF) of an image with a precision of up to a second decimal point."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CuratorNet is mentioned as a model used within the scope of the paper but is not depicted as a contribution specifically from this paper.",
          "quote": "The authors designed a convolutional neural network (CNN) named CuratorNet, to estimate the point spread function (PSF) of an image with a precision of up to a second decimal point."
        },
        "is_executed": {
          "value": 1,
          "justification": "CuratorNet was described as being used to estimate the point spread function (PSF) of an image, implying that it was executed to perform its designated task.",
          "quote": "The authors designed a convolutional neural network (CNN) named CuratorNet, to estimate the point spread function (PSF) of an image with a precision of up to a second decimal point."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not mention any direct comparison of CuratorNet to other models.",
          "quote": "The authors designed a convolutional neural network (CNN) named CuratorNet, to estimate the point spread function (PSF) of an image with a precision of up to a second decimal point."
        },
        "referenced_paper_title": {
          "value": "Deep learning based radial blur estimation and image enhancement",
          "justification": "Reference to CuratorNet found in this paper links back to another research work.",
          "quote": "The authors designed a convolutional neural network (CNN) named CuratorNet, to estimate the point spread function (PSF) of an image with a precision of up to a second decimal point."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is cited as an example of a large public dataset that has been used in data cleaning studies.",
          "quote": "However, as reported by a previous study (Northcutt et al., 2019), large public datasets considered for a long time to be error-free such as ImageNet (Russakovsky et al., 2015) contain erroneous labels."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "Referenced as an example of a dataset that has endured erroneous labels.",
          "quote": "However, as reported by a previous study (Northcutt et al., 2019), large public datasets considered for a long time to be error-free such as ImageNet (Russakovsky et al., 2015) contain erroneous labels."
        }
      },
      {
        "name": {
          "value": "Hospital dataset",
          "justification": "The Hospital dataset is listed as one of the commonly used datasets for evaluating data cleaning methods.",
          "quote": "To evaluate a data cleaning approach or train an ML model to be used in a data cleaning approach, one needs a labeled dataset. As building a labeled dataset is a laborious process (Gauen et al., 2017), researchers are interested in using the existing public datasets. Additionally, using public datasets provides a baseline for researchers to compare their approaches. The release of large public datasets has been known to propel research in ML (Gauen et al., 2017). For instance, the release of ImageNet datasets (Deng et al., 2009) led to the birth of new neural architectures such as AlexNet (Krizhevsky et al., 2017) and VGG (Simonyan and Zisserman, 2014). For data cleaning tasks, there exist popular datasets used across many papers such as the Hospital dataset..."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "The data civilizer system",
          "justification": "This reference mentions the use of the Hospital dataset, making it clear that it is not a new contribution of the current paper but a dataset used in other research.",
          "quote": "The release of large public datasets has been known to propel research in ML... For data cleaning tasks, there exist popular datasets used across many papers such as the Hospital dataset."
        }
      },
      {
        "name": {
          "value": "Flights dataset",
          "justification": "The Flights dataset is another example of a dataset frequently utilized in data cleaning papers.",
          "quote": "For data cleaning tasks, there exist popular datasets used across many papers such as the Hospital dataset, the Flights dataset, and the Beers dataset (all of which are described in Mahdavi et al. (2019))."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Raha: A Configuration-Free Error Detection System",
          "justification": "Referenced as a commonly used dataset in the field of data cleaning.",
          "quote": "For data cleaning tasks, there exist popular datasets used across many papers such as the Hospital dataset, the Flights dataset, and the Beers dataset (all of which are described in Mahdavi et al. (2019))."
        }
      },
      {
        "name": {
          "value": "Beers dataset",
          "justification": "The Beers dataset is mentioned as a frequent dataset used in data cleaning research.",
          "quote": "For data cleaning tasks, there exist popular datasets used across many papers such as the Hospital dataset, the Flights dataset, and the Beers dataset (all of which are described in Mahdavi et al. (2019))."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Raha: A Configuration-Free Error Detection System",
          "justification": "Referenced as a dataset described within the cited reference related to data cleaning.",
          "quote": "For data cleaning tasks, there exist popular datasets used across many papers such as the Hospital dataset, the Flights dataset, and the Beers dataset (all of which are described in Mahdavi et al. (2019))."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Monte Carlo Dropout (MCD)",
          "justification": "Monte Carlo Dropout is used in the study to approximate Bayesian Neural Networks for uncertainty estimation.",
          "quote": "To address this problem, Ponzio et al. (2021) approximates BNNs with Monte Carlo Dropout (MCD). MCD provides confidence measures that are numerically tractable and easier to implement than BNNs."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning",
          "justification": "This library is referenced in another paper that this study cites.",
          "quote": "To address this problem, Ponzio et al. (2021) approximates BNNs with Monte Carlo Dropout (MCD)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2174,
    "prompt_tokens": 55770,
    "total_tokens": 57944
  }
}