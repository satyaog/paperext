{
  "paper": "2310.13265.txt",
  "words": 9122,
  "extractions": {
    "title": {
      "value": "M OQAGPT: Zero-Shot Multi-modal Open-domain Question Answering with Large Language Models",
      "justification": "This is the exact title of the paper as given.",
      "quote": "M OQAGPT: Zero-Shot Multi-modal Open-domain Question Answering\nwith Large Language Models"
    },
    "description": "This paper introduces M OQAGPT, a framework designed to handle multi-modal open-domain question answering in a zero-shot manner using Large Language Models (LLMs). The framework utilizes a divide-and-conquer strategy to retrieve and extract answers from various modalities and then uses LLMs to synthesize this information and provide a final answer. The study validates the framework's effectiveness through extensive experiments and comparisons with existing methods.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results and comparisons to validate the effectiveness of the proposed framework, indicating it is an empirical study.",
      "quote": "We corroborate these advantages by presenting experimental results on two multi-modal open-domain question answering (MMOQA) datasets: MMCoQA (Li et al., 2022) and MultModalQA (Talmor et al., 2021)."
    },
    "primary_research_field": {
      "name": {
        "value": "Question Answering",
        "justification": "The study primarily focuses on question answering tasks, particularly in a multi-modal, open-domain context.",
        "quote": "To enable LLMs to tackle the task in a zero-shot manner, we introduce M OQAGPT, a straightforward and flexible framework."
      },
      "aliases": [
        "QA"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "Question Answering is a subfield of Natural Language Processing, which the paper significantly addresses.",
          "quote": "LLM-based Modular Systems ... The emergence of LLMs has renewed interest in this area."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper also involves processing non-textual inputs like images, which is part of the Computer Vision field.",
          "quote": "Multi-modal open-domain question answering typically requires evidence retrieval from databases across diverse modalities, such as images, tables, passages, etc."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "M OQAGPT",
          "justification": "The main model proposed in the paper.",
          "quote": "we introduce M OQAGPT, a straightforward and flexible framework."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "M OQAGPT is the main contribution of the paper.",
          "quote": "we introduce M OQAGPT, a straightforward and flexible framework."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model framework is executed and validated through extensive experiments.",
          "quote": "We corroborate these advantages by presenting experimental results on two multi-modal open-domain question answering (MMOQA) datasets."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of M OQAGPT is compared with other baselines and methods.",
          "quote": "Our outputs are less prone to hallucination, making them more trustworthy. Lastly, we examined several success and failure cases of our method."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "M OQAGPT is a novel contribution of this paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "ChatGPT",
          "justification": "ChatGPT is mentioned as one of the LLMs used for comparison.",
          "quote": "Large Language Models (LLMs) including ChatGPT (OpenAI, 2022b),"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ChatGPT is not contributed by this paper; it is used for comparison.",
          "quote": "Large Language Models (LLMs) including ChatGPT (OpenAI, 2022b),"
        },
        "is_executed": {
          "value": 1,
          "justification": "ChatGPT was used in the experiments.",
          "quote": "We conducted experiments using several of the latest models and demonstrated that our method is effective across all of them, highlighting our framework’s flexibility."
        },
        "is_compared": {
          "value": 1,
          "justification": "ChatGPT was compared numerically to M OQAGPT.",
          "quote": "We benchmark our method against other LLMs including ChatGPT (OpenAI, 2022b),"
        },
        "referenced_paper_title": {
          "value": "ChatGPT",
          "justification": "Title of the reference paper for ChatGPT.",
          "quote": "Large Language Models (LLMs) including ChatGPT (OpenAI, 2022b),"
        }
      },
      {
        "name": {
          "value": "LLaMA",
          "justification": "LLaMA is mentioned as one of the LLMs used for comparison.",
          "quote": "Large Language Models (LLMs) including ... LLaMA (Touvron et al., 2023)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "LLaMA is not contributed by this paper; it is used for comparison.",
          "quote": "Large Language Models (LLMs) including ... LLaMA (Touvron et al., 2023)"
        },
        "is_executed": {
          "value": 1,
          "justification": "LLaMA was used in the experiments.",
          "quote": "We conducted experiments using several of the latest models and demonstrated that our method is effective across all of them, highlighting our framework’s flexibility."
        },
        "is_compared": {
          "value": 1,
          "justification": "LLaMA was compared numerically to M OQAGPT.",
          "quote": "We benchmark our method against other LLMs including ... LLaMA (Touvron et al., 2023)"
        },
        "referenced_paper_title": {
          "value": "LLaMA: Open and Efficient Foundation Language Models",
          "justification": "Title of the reference paper for LLaMA.",
          "quote": "Large Language Models (LLMs) including ... LLaMA (Touvron et al., 2023)"
        }
      },
      {
        "name": {
          "value": "PaLM2",
          "justification": "PaLM2 is mentioned as one of the LLMs used for comparison.",
          "quote": "Large Language Models (LLMs) including ... PaLM2 (Anil et al., 2023)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "PaLM2 is not contributed by this paper; it is used for comparison.",
          "quote": "Large Language Models (LLMs) including ... PaLM2 (Anil et al., 2023)"
        },
        "is_executed": {
          "value": 1,
          "justification": "PaLM2 was used in the experiments.",
          "quote": "We conducted experiments using several of the latest models and demonstrated that our method is effective across all of them, highlighting our framework’s flexibility."
        },
        "is_compared": {
          "value": 1,
          "justification": "PaLM2 was compared numerically to M OQAGPT.",
          "quote": "We benchmark our method against other LLMs including ... PaLM2 (Anil et al., 2023)"
        },
        "referenced_paper_title": {
          "value": "PaLM 2 Technical Report",
          "justification": "Title of the reference paper for PaLM2.",
          "quote": "Large Language Models (LLMs) including ... PaLM2 (Anil et al., 2023)"
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is mentioned as one of the LLMs used for comparison.",
          "quote": "Large Language Models (LLMs) including ... GPT4 (OpenAI, 2022c)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-4 is not contributed by this paper; it is used for comparison.",
          "quote": "Large Language Models (LLMs) including ... GPT-4 (OpenAI, 2022c)"
        },
        "is_executed": {
          "value": 1,
          "justification": "GPT-4 was used in the experiments.",
          "quote": "We conducted experiments using several of the latest models and demonstrated that our method is effective across all of them, highlighting our framework’s flexibility."
        },
        "is_compared": {
          "value": 1,
          "justification": "GPT-4 was compared numerically to M OQAGPT.",
          "quote": "We benchmark our method against other LLMs including ... GPT4 (OpenAI, 2022c)"
        },
        "referenced_paper_title": {
          "value": "GPT-4",
          "justification": "Title of the reference paper for GPT-4.",
          "quote": "Large Language Models (LLMs) including ... GPT4 (OpenAI, 2022c)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MMCoQA",
          "justification": "MMCoQA is explicitly named as a dataset used in the experiments.",
          "quote": "We corroborate these advantages by presenting experimental results on two multi-modal open-domain question answering (MMOQA) datasets: MMCoQA (Li et al., 2022)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MMCoQA: Conversational Question Answering over Text, Tables, and Images",
          "justification": "Title of the reference paper for MMCoQA.",
          "quote": "MMCoQA (Li et al., 2022)"
        }
      },
      {
        "name": {
          "value": "MultiModalQA",
          "justification": "MultiModalQA is explicitly named as a dataset used in the experiments.",
          "quote": "We corroborate these advantages by presenting experimental results on two multi-modal open-domain question answering (MMOQA) datasets: ... MultiModalQA (Talmor et al., 2021)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MultiModalQA: Complex Question Answering over Text, Tables and Images",
          "justification": "Title of the reference paper for MultiModalQA.",
          "quote": "MultiModalQA (Talmor et al., 2021)"
        }
      },
      {
        "name": {
          "value": "ManyModalQA",
          "justification": "ManyModalQA is mentioned in the related work section as one of the datasets used to benchmark multi-modal QA models.",
          "quote": "In light of this, several datasets have been introduced to benchmark the development of solutions in this area, such as ManyModalQA (Hannan et al., 2020)"
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "ManyModalQA: Modality Disambiguation and QA Over Diverse Inputs",
          "justification": "Title of the reference paper for ManyModalQA.",
          "quote": "In light of this, several datasets have been introduced to benchmark the development of solutions in this area, such as ManyModalQA (Hannan et al., 2020)"
        }
      },
      {
        "name": {
          "value": "HYBRIDQA",
          "justification": "HYBRIDQA is mentioned in the related work section as one of the datasets used to benchmark multi-modal QA models.",
          "quote": "benchmark the development of solutions in this area, such as ManyModalQA (Hannan et al., 2020), HYBRIDQA (Chen et al., 2020)"
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "HybridQA: A Dataset of Multi-hop Question Answering over Tabular and Textual Data",
          "justification": "Title of the reference paper for HYBRIDQA.",
          "quote": "benchmark the development of solutions in this area, such as ManyModalQA (Hannan et al., 2020), HYBRIDQA (Chen et al., 2020)"
        }
      },
      {
        "name": {
          "value": "WebQA",
          "justification": "WebQA is mentioned in the related work section as one of the datasets used to benchmark multi-modal QA models.",
          "quote": "In light of this, several datasets have been introduced to benchmark the development of solutions in this area, such as ... WebQA (Chang et al., 2022)"
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "WebQA: Multihop and Multimodal QA",
          "justification": "Title of the reference paper for WebQA.",
          "quote": "In light of this, several datasets have been introduced to benchmark the development of solutions in this area, such as ... WebQA (Chang et al., 2022)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3048,
    "prompt_tokens": 17547,
    "total_tokens": 20595
  }
}