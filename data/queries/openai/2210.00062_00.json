{
  "paper": "2210.00062.txt",
  "words": 10069,
  "extractions": {
    "title": {
      "value": "Learning Robust Kernel Ensembles with Kernel Average Pooling",
      "justification": "The exact title of the paper mentioned at the beginning.",
      "quote": "Learning Robust Kernel Ensembles with Kernel Average\nPooling"
    },
    "description": "The paper introduces Kernel Average Pooling (KAP), a neural network building block to improve the robustness of convolutional neural networks against input perturbations and adversarial attacks. Empirical evaluations on multiple datasets demonstrated significant improvements in robustness without training on adversarial examples.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper includes empirical evaluations on multiple datasets such as CIFAR10, CIFAR100, TinyImagenet, and Imagenet to demonstrate the effectiveness of the proposed method.",
      "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on improving neural network robustness, particularly Convolutional Neural Networks (CNNs), which are a core aspect of computer vision tasks.",
        "quote": "We introduce Kernel Average Pooling (KAP), a neural network building\nblock that applies the mean filter along the kernel dimension of the layer activation tensor."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Adversarial Attacks and Defenses",
          "justification": "The paper specifically addresses improving robustness against adversarial attacks through the introduction of Kernel Average Pooling (KAP).",
          "quote": "when trained on inputs perturbed with additive Gaussian noise, KAP models are remarkably robust against various forms\nof adversarial attacks."
        },
        "aliases": [
          "Adversarial Robustness"
        ]
      },
      {
        "name": {
          "value": "Neural Network Training",
          "justification": "The introduction of Kernel Average Pooling (KAP) focuses on modifying the training process of convolutional neural networks to improve robustness.",
          "quote": "We introduce Kernel Average Pooling (KAP), a neural network building\nblock that applies the mean filter along the kernel dimension of the layer activation tensor."
        },
        "aliases": [
          "Deep Neural Network Training"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Kernel Average Pooling (KAP)",
          "justification": "Kernel Average Pooling (KAP) is introduced as a novel neural network building block in this paper to improve robustness against adversarial attacks.",
          "quote": "We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor."
        },
        "aliases": [
          "KAP"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The method is the primary contribution of the paper.",
          "quote": "We introduce Kernel Average Pooling (KAP), a neural network building block that applies the mean filter along the kernel dimension of the layer activation tensor."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was empirically evaluated on several datasets using convolutional neural networks, which typically require GPU for efficiency.",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness."
        },
        "is_compared": {
          "value": 1,
          "justification": "The new KAP method is compared with baseline models such as standard convolutional neural networks and adversarially trained models.",
          "quote": "We found that adding KAP to the model significantly improves the robustness against all attacks. The amount of improvement was larger for networks trained on larger noise\n(i.e. larger Ïƒ), however, the higher robustness came at the cost of lower clean accuracy."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "KAP is a novel contribution from the authors of this paper and not referenced from another source.",
          "quote": "We introduce Kernel Average Pooling (KAP)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR10",
          "justification": "One of the datasets used for empirical evaluations to validate the effectiveness of the proposed Kernel Average Pooling (KAP).",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
        },
        "aliases": [
          "CIFAR-10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The CIFAR10 dataset is commonly referenced in deep learning research.",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
        }
      },
      {
        "name": {
          "value": "CIFAR100",
          "justification": "One of the datasets used for empirical evaluations to validate the effectiveness of the proposed Kernel Average Pooling (KAP).",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
        },
        "aliases": [
          "CIFAR-100"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The CIFAR100 dataset is commonly referenced in deep learning research.",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
        }
      },
      {
        "name": {
          "value": "TinyImagenet",
          "justification": "One of the datasets used for empirical evaluations to validate the effectiveness of the proposed Kernel Average Pooling (KAP).",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
        },
        "aliases": [
          "Tiny-ImageNet"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Tiny ImageNet Visual Recognition Challenge",
          "justification": "The TinyImagenet dataset is commonly referenced in deep learning research.",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "One of the datasets used for empirical evaluations to validate the effectiveness of the proposed Kernel Average Pooling (KAP).",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
        },
        "aliases": [
          "ILSVRC2012"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "The ImageNet dataset is commonly referenced in deep learning research.",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and\nImagenet datasets show substantial improvements in robustness against strong adversarial\nattacks such as AutoAttack without training on any adversarial examples."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions empirical evaluations and experiments that likely involved the use of a popular deep learning library like PyTorch for implementation.",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial improvements in robustness."
        },
        "aliases": [
          "Torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "Heavy computations and neural network training as described in the experiments section are commonly done using PyTorch.",
          "quote": "Empirical evaluations on CIFAR10, CIFAR100, TinyImagenet, and Imagenet datasets show substantial improvements in robustness."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1633,
    "prompt_tokens": 18513,
    "total_tokens": 20146
  }
}