{
  "paper": "2307.01163.txt",
  "words": 9790,
  "extractions": {
    "title": {
      "value": "Improving Language Plasticity via Pretraining with Active Forgetting",
      "justification": "The title of the paper is 'Improving Language Plasticity via Pretraining with Active Forgetting'.",
      "quote": "Improving Language Plasticity via Pretraining with Active Forgetting"
    },
    "description": "This paper proposes an active forgetting mechanism during the pretraining of language models to improve their adaptability to new languages. By regularly resetting the embedding layer during pretraining, the authors encourage the model to better learn new embeddings, similar to a meta-learning effect. Experiments demonstrate faster convergence and better performance in low-data scenarios, especially for languages distant from English.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experiments and empirical results to validate the proposed active forgetting mechanism.",
      "quote": "Experiments demonstrate faster convergence and better performance in low-data scenarios, especially for languages distant from English."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on pretrained language models and their adaptability, which are central topics in NLP.",
        "quote": "Pretrained language models (PLMs) are today the primary model for natural language processing."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Cross-lingual Transfer",
          "justification": "The paper addresses the adaptability of language models to new languages, which falls under cross-lingual transfer.",
          "quote": "Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline by large margins."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "RoBERTa-base",
          "justification": "The paper mentions using RoBERTa-base as the primary model for their experiments.",
          "quote": "we choose to pretrain RoBERTa-base Liu et al. [2019], a 12-layer transformer-based model, on English CC100 [Conneau et al., 2020]."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CC100",
          "justification": "The model was pretrained on the CC100 dataset.",
          "quote": "we choose to pretrain RoBERTa-base Liu et al. [2019], a 12-layer transformer-based model, on English CC100 [Conneau et al., 2020]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "XNLI",
          "justification": "The XNLI dataset was used for zero-shot evaluations.",
          "quote": "Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline by large margins: average gains of +21.2% on XNLI."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MLQA",
          "justification": "The MLQA dataset was used for zero-shot evaluations.",
          "quote": "Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline by large margins: average gains of  +33.8% on MLQA."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "XQuAD",
          "justification": "The XQuAD dataset was used for zero-shot evaluations.",
          "quote": "Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline by large margins: average gains of..60.9% on XQuAD."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MultiNLI",
          "justification": "The MultiNLI dataset was used for task adaptation in English.",
          "quote": "In the task adapt stage, both models were finetuned for 10 epochs on the English task data, specifically MultiNLI [Williams et al., 2018] for the NLI task."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SQuAD",
          "justification": "The SQuAD dataset was used for task adaptation in English.",
          "quote": "In the task adapt stage, both models were finetuned for 10 epochs on the English task data, specifically ... SQUAD Rajpurkar et al. [2016] for the QA task."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "fairseq",
          "justification": "The experiments were implemented using the fairseq library.",
          "quote": "Our experiments were implemented using fairseq [Ott et al., 2019]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1247,
    "prompt_tokens": 17437,
    "total_tokens": 18684
  }
}