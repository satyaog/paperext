{
  "paper": "2211.03942.txt",
  "words": 10590,
  "extractions": {
    "title": {
      "value": "Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design",
      "justification": "This is the title mentioned at the beginning of the provided paper.",
      "quote": "Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design"
    },
    "description": "The paper introduces the Interpolated Minimum Variance Unbiased (I-MVU) mechanism, a new privacy-aware compression technique for Federated Learning. It focuses on balancing privacy, communication efficiency, and model accuracy by using a numerical interpolation method to improve scalability and privacy-utility trade-offs.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents empirical results comparing the proposed I-MVU mechanism with various baselines on datasets like MNIST and CIFAR-10.",
      "quote": "Experimentally, we find that under both client-level and sample-level DP settings and across various benchmark datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "Federated Learning is the main focus as the paper discusses improving privacy and communication efficiency in FL settings.",
        "quote": "In private federated learning (FL), a server aggregates differentially private updates."
      },
      "aliases": [
        "FL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Differential Privacy",
          "justification": "The paper extensively discusses differential privacy mechanisms and their application in FL.",
          "quote": "In this work, we propose the interpolated MVU (I-MVU) mechanism, which extends MVU using an interpolation procedure that relaxes the unbiasedness assumption."
        },
        "aliases": [
          "DP"
        ]
      },
      {
        "name": {
          "value": "Communication Efficiency",
          "justification": "The core of the paper is about reducing the communication cost between clients and the server in Federated Learning.",
          "quote": "a major challenge in this space is ensuring a good privacy-accuracy-communication trade-off."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Numerical Optimization",
          "justification": "The proposed mechanism (I-MVU) relies on numerical optimization techniques for its design.",
          "quote": "a privacy-aware compression mechanism, called the minimum variance unbiased (MVU) mechanism, that numerically solves an optimization problem."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Interpolated Minimum Variance Unbiased Mechanism",
          "justification": "I-MVU is the main focus of the paper and is presented as a new mechanism.",
          "quote": "In this work, we propose the interpolated MVU (I-MVU) mechanism."
        },
        "aliases": [
          "I-MVU"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The I-MVU mechanism is introduced as a new method within this paper.",
          "quote": "The result is the new Interpolated MVU mechanism."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper contains empirical experiments where the I-MVU mechanism is applied.",
          "quote": "Experimentally, we find that under both client-level and sample-level DP settings and across various benchmark datasets, the I-MVU mechanism provides a better privacy-utility trade-off."
        },
        "is_compared": {
          "value": 1,
          "justification": "The I-MVU mechanism is compared against other models such as SignSGD and MVU.",
          "quote": "Experimentally, we find that under both client-level and sample-level DP settings and across various benchmark datasets, the I-MVU mechanism provides a better privacy-utility trade-off than SignSGD (Jin et al., 2020) and MVU (Chaudhuri et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design",
          "justification": "This paper itself is the reference for the I-MVU mechanism.",
          "quote": "In this work, we propose the interpolated MVU (I-MVU) mechanism."
        }
      },
      {
        "name": {
          "value": "Minimum Variance Unbiased Mechanism",
          "justification": "The paper builds upon the MVU mechanism and compares it with the proposed I-MVU mechanism.",
          "quote": "a privacy-aware compression mechanism, called the minimum variance unbiased (MVU) mechanism."
        },
        "aliases": [
          "MVU"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "MVU is used as a baseline method and is cited as a previous work.",
          "quote": "Prior work has achieved a good trade-off by designing a privacy-aware compression mechanism, called the minimum variance unbiased (MVU) mechanism."
        },
        "is_executed": {
          "value": 1,
          "justification": "The MVU mechanism was used for experimental comparison.",
          "quote": "the I-MVU mechanism provides a better privacy-utility trade-off than ... MVU (Chaudhuri et al., 2022)"
        },
        "is_compared": {
          "value": 1,
          "justification": "The I-MVU mechanism is numerically compared against the MVU mechanism.",
          "quote": "provides a better privacy-utility trade-off than ... MVU (Chaudhuri et al., 2022)"
        },
        "referenced_paper_title": {
          "value": "Privacy-aware compression for federated data analysis",
          "justification": "This is the cited reference for the MVU mechanism.",
          "quote": "(Chaudhuri et al., 2022) introduces the minimum variance unbiased (MVU) mechanism."
        }
      },
      {
        "name": {
          "value": "SignSGD",
          "justification": "SignSGD is used for comparison in the experiments.",
          "quote": "SignSGD (Jin et al., 2020) can perform remarkably well."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "SignSGD is a previously known method and is cited as related work.",
          "quote": "SignSGD (Jin et al., 2020) is a strong simple baseline that first applies a non-compressed DP mechanism to the gradient and then transmits its sign coordinate-wise."
        },
        "is_executed": {
          "value": 1,
          "justification": "SignSGD was used for experimental comparison.",
          "quote": "Moreover, I-MVU achieves close to the same performance as the standard non-compressed Laplace and Gaussian mechanisms (Abadi et al., 2016) for similar levels of (ϵ, δ)-DP, leading to new state-of-the-art results for private communication-efficient FL."
        },
        "is_compared": {
          "value": 1,
          "justification": "The I-MVU mechanism is numerically compared against SignSGD.",
          "quote": "provides a better privacy-utility trade-off than SignSGD (Jin et al., 2020)"
        },
        "referenced_paper_title": {
          "value": "Stochastic sign SGD for federated learning with theoretical guarantees",
          "justification": "This is the cited reference for the SignSGD mechanism.",
          "quote": "SignSGD (Jin et al., 2020) is a strong simple baseline that first applies a non-compressed DP mechanism to the gradient and then transmits its sign coordinate-wise."
        }
      },
      {
        "name": {
          "value": "Skellam Mechanism",
          "justification": "The Skellam mechanism is used as a baseline for comparison in the experiments.",
          "quote": "We evaluate the performance of I-MVU on MNIST and CIFAR-10 using Skellam Mechanism (Agarwal et al., 2021)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The Skellam mechanism is a known baseline and is cited as related work.",
          "quote": "Skellam mechanism (Agarwal et al., 2021), which discretizes the input using randomized dithering and adds noise from the Skellam distribution to achieve privacy-aware compression of L2 -bounded client updates."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Skellam mechanism was used for experimental comparison.",
          "quote": "provides a better privacy-utility trade-off than SignSGD (Jin et al., 2020) and MVU (Chaudhuri et al., 2022) at an extremely low communication budget of one bit per gradient dimension."
        },
        "is_compared": {
          "value": 1,
          "justification": "The I-MVU mechanism is numerically compared against the Skellam mechanism.",
          "quote": "We evaluate the performance of I-MVU on MNIST and CIFAR-10 using Skellam Mechanism (Agarwal et al., 2021)."
        },
        "referenced_paper_title": {
          "value": "The Skellam Mechanism for Differentially Private Federated Learning",
          "justification": "This is the cited reference for the Skellam mechanism.",
          "quote": "Skellam mechanism (Agarwal et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The paper uses MNIST as one of the benchmark datasets for evaluating the proposed I-MVU mechanism.",
          "quote": "leading to new state-of-the-art results for private communication-efficient FL on a variety of datasets (MNIST and CIFAR-10)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MNIST: A Large-Scale Hierarchical Image Database",
          "justification": "This is the standard reference for the MNIST dataset.",
          "quote": "a variety of datasets (MNIST and CIFAR-10)"
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The paper uses CIFAR-10 as one of the benchmark datasets for evaluating the proposed I-MVU mechanism.",
          "quote": "provides a better privacy-utility trade-off than SignSGD (Jin et al., 2020) and MVU (Chaudhuri et al., 2022) at an extremely low communication budget of one bit per gradient dimension."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the standard reference for the CIFAR-10 dataset.",
          "quote": "a variety of datasets (MNIST and CIFAR-10)."
        }
      },
      {
        "name": {
          "value": "FEMNIST",
          "justification": "The paper evaluates the I-MVU mechanism under the sample-level DP setting using the FEMNIST dataset.",
          "quote": "We evaluate under the sample-level DP setting on the FEMNIST dataset (Caldas et al., 2018) for classifying written characters into 62 distinct classes."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "LEAF: A Benchmark for Federated Settings",
          "justification": "This is the standard reference for the FEMNIST dataset.",
          "quote": "We evaluate under the sample-level DP setting on the FEMNIST dataset (Caldas et al., 2018) for classifying written characters into 62 distinct classes."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "The paper uses ImageNet for pre-training the model before fine-tuning on CIFAR-10.",
          "quote": "The model is pre-trained on down-scaled 32 × 32 resolution ImageNet (Deng et al., 2009) samples and then finetuned on CIFAR-10 using either the Gaussian mechanism, SignSGD or I-MVU."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "This is the standard reference for the ImageNet dataset.",
          "quote": "The model is pre-trained on down-scaled 32 × 32 resolution ImageNet (Deng et al., 2009) samples and then finetuned on CIFAR-10 using either the Gaussian mechanism, SignSGD or I-MVU."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2836,
    "prompt_tokens": 20289,
    "total_tokens": 23125
  }
}