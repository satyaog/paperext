{
  "paper": "2310.07096.txt",
  "words": 7330,
  "extractions": {
    "title": {
      "value": "Sparse Universal Transformer",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "Sparse Universal Transformer"
    },
    "description": "This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) and a new stick-breaking-based dynamic halting mechanism to enhance the Universal Transformer (UT) by reducing its computation complexity while retaining parameter efficiency and generalization ability.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments on datasets like WMT'14 English-German translation, CFQ, and Logical Inference to validate the effectiveness of the proposed Sparse Universal Transformer (SUT).",
      "quote": "Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT’14 and strong generalization results on formal language tasks (Logical inference and CFQ)."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary applications and experiments in the paper focus on tasks related to Natural Language Processing including machine translation and logical inference.",
        "quote": "Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT’14 and strong generalization results on formal language tasks (Logical inference and CFQ)."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Translation",
          "justification": "The experiments on the WMT'14 English-German translation dataset explicitly indicate that one of the sub-fields is Machine Translation.",
          "quote": "To demonstrate effective scaling, we perform experiments on WMT’14 English to German translation, showing that an SUT can achieve better performance for the same parameter count, while incurring less computation cost than an equivalent dense UT."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Compositional Generalization",
          "justification": "The paper investigates the compositional generalization properties of the SUT using the CFQ dataset.",
          "quote": "Since the UT setting is a specific case of SUT, we show on the Compositional Freebase Questions (CFQ; Keysers et al. 2019) tasks that UTs have better compositional generalization properties, improving upon CFQ results from Csordás et al. (2021)."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Logical Inference",
          "justification": "One of the tasks used for evaluating the SUT is the Logical Inference task.",
          "quote": "Using the Logical Inference task (Bowman et al., 2015), we analyse the behaviour of our UT on length and compositional generalization."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Sparse Universal Transformer (SUT)",
          "justification": "The paper proposes and focuses on the Sparse Universal Transformer (SUT).",
          "quote": "This paper proposes the Sparse Universal Transformer (SUT)"
        },
        "aliases": [
          "SUT"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The Sparse Universal Transformer (SUT) is proposed in this paper as an improvement over the Universal Transformer (UT).",
          "quote": "This paper proposes the Sparse Universal Transformer (SUT)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions that the experiments were conducted using GPU memory, implying that SUT was executed on GPU.",
          "quote": "According to Takase and Kiyono (2021), UT requires two times the training time and far more GPU memory than VT in WMT English-German translation task."
        },
        "is_compared": {
          "value": 1,
          "justification": "The SUT is numerically compared to other models such as Vanilla Transformers (VTs) and Universal Transformers (UTs) in the experiments.",
          "quote": "Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT’14 and strong generalization results on formal language tasks (Logical inference and CFQ)."
        },
        "referenced_paper_title": {
          "value": "Universal Transformers",
          "justification": "The Sparse Universal Transformer (SUT) is proposed as an improvement over the Universal Transformer (UT), which is a model referenced and improved upon in this paper.",
          "quote": "Universal Transformers (UTs; Dehghani et al. 2018) are Transformers that share parameters at every layer of the architecture."
        }
      },
      {
        "name": {
          "value": "Vanilla Transformers (VT)",
          "justification": "The Vanilla Transformer (VT) model is used as a baseline in the paper.",
          "quote": "Vanilla Transformers (VTs)"
        },
        "aliases": [
          "VT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The Vanilla Transformer (VT) is not a new model introduced by this paper, but rather used as a baseline for comparisons.",
          "quote": "Vanilla Transformers (VTs)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper discusses the computational complexities and comparisons involving VTs, indicating execution.",
          "quote": "Recent theoretical work has pointed out that finite-depth Transformers have an issue of expressibility that will result in failure to generalize (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Liu et al., 2022)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of Universal Transformers (UTs) and Sparse Universal Transformers (SUTs) against Vanilla Transformers (VTs).",
          "quote": "Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT’14 and strong generalization results on formal language tasks (Logical inference and CFQ)."
        },
        "referenced_paper_title": {
          "value": "Attention is All You Need",
          "justification": "Vanilla Transformers (VTs) refer to the original Transformer architecture introduced in the 'Attention is All You Need' paper.",
          "quote": "Recent theoretical work has pointed out that finite-depth Transformers have an issue of expressibility that will result in failure to generalize (Hahn, 2020; Hao et al., 2022; Merrill et al., 2022; Liu et al., 2022)."
        }
      },
      {
        "name": {
          "value": "Universal Transformer (UT)",
          "justification": "The Universal Transformer (UT) is central to the study, acting as a foundation for the Sparse Universal Transformer (SUT).",
          "quote": "Universal Transformers (UTs; Dehghani et al. 2018) are Transformers that share parameters at every layer of the architecture."
        },
        "aliases": [
          "UT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The Universal Transformer (UT) is not introduced by this paper but by Dehghani et al. in 2018. It is however, a foundational model the paper aims to improve.",
          "quote": "Universal Transformers (UTs; Dehghani et al. 2018) are Transformers that share parameters at every layer of the architecture."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Universal Transformer (UT) is executed as part of the empirical evaluations in the paper.",
          "quote": "According to Takase and Kiyono (2021), UT requires two times the training time and far more GPU memory than VT in WMT English-German translation task."
        },
        "is_compared": {
          "value": 1,
          "justification": "The UT model is numerically compared to the SUT and VT models in the paper.",
          "quote": "Experiments show that SUT achieves the same performance as strong baseline models while only using half computation and parameters on WMT’14 and strong generalization results on formal language tasks (Logical inference and CFQ)."
        },
        "referenced_paper_title": {
          "value": "Universal Transformers",
          "justification": "The Universal Transformer model is introduced in the 'Universal Transformers' paper by Dehghani et al. in 2018.",
          "quote": "Universal Transformers (UTs; Dehghani et al. 2018) are Transformers that share parameters at every layer of the architecture."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "WMT'14 English-German",
          "justification": "The WMT’14 English-German dataset is used to demonstrate the translation capabilities of the SUT.",
          "quote": "To demonstrate effective scaling, we perform experiments on WMT’14 English to German translation, showing that an SUT can achieve better performance for the same parameter count, while incurring less computation cost than an equivalent dense UT."
        },
        "aliases": [
          "WMT'14 En-De"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Findings of the 2014 Workshop on Statistical Machine Translation",
          "justification": "The dataset is discussed in the context of machine translation benchmarks and is well-cited.",
          "quote": "We perform experiments on the WMT’14 English-German translation dataset (Bojar et al., 2014)."
        }
      },
      {
        "name": {
          "value": "Compositional Freebase Questions (CFQ)",
          "justification": "The CFQ dataset is used to test the compositional generalization properties of the SUT.",
          "quote": "Since the UT setting is a specific case of SUT, we show on the Compositional Freebase Questions (CFQ; Keysers et al. 2019) tasks that UTs have better compositional generalization properties, improving upon CFQ results from Csordás et al. (2021)."
        },
        "aliases": [
          "CFQ"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Measuring compositional generalization: A comprehensive method on realistic data",
          "justification": "The Compositional Freebase Questions (CFQ) dataset is a known dataset for measuring compositional generalization, introduced in a paper by Keysers et al. in 2019.",
          "quote": "Since the UT setting is a specific case of SUT, we show on the Compositional Freebase Questions (CFQ; Keysers et al. 2019) tasks that UTs have better compositional generalization properties, improving upon CFQ results from Csordás et al. (2021)."
        }
      },
      {
        "name": {
          "value": "Logical Inference Task",
          "justification": "The Logical Inference Task dataset is used to analyze the behavior of the model in logical reasoning.",
          "quote": "Using the Logical Inference task (Bowman et al., 2015), we analyse the behaviour of our UT on length and compositional generalization."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Tree-structured composition in neural networks without tree-structured architectures",
          "justification": "The Logical Inference Task dataset is referenced to an influential paper by Bowman et al. in 2015, which discusses this particular task.",
          "quote": "Using the Logical Inference task (Bowman et al., 2015), we analyse the behaviour of our UT on length and compositional generalization."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Fairseq",
          "justification": "The paper mentions implementing their experiments within the Fairseq framework.",
          "quote": "All experiments were implemented within the Fairseq framework (Ott et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "fairseq: A fast, extensible toolkit for sequence modeling",
          "justification": "The library used is explicitly mentioned in the context of implementing the experiments.",
          "quote": "All experiments were implemented within the Fairseq framework (Ott et al., 2019)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2303,
    "prompt_tokens": 13775,
    "total_tokens": 16078
  }
}