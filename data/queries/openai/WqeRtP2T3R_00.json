{
  "paper": "WqeRtP2T3R.txt",
  "words": 9944,
  "extractions": {
    "title": {
      "value": "Embracing Diversity: Zero Shot Classification Beyond One Vector Per Class",
      "justification": "Title is stated explicitly at the beginning of the paper.",
      "quote": "Embracing Diversity: Zero Shot Classification Beyond One Vector Per Class"
    },
    "description": "The paper proposes a method to improve zero-shot classification by accounting for diversity within a class using inferred attributes without retraining. The method outperforms standard zero-shot classification on various datasets.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes empirical evaluations and comparisons of the proposed method against existing methods across several datasets.",
      "quote": "We find our method consistently outperforms standard zero-shot classification over a large suite of datasets encompassing hierarchies, diverse object states, and real-world geographic diversity."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary focus of the paper is on vision-language models and their performance on visual classification tasks.",
        "quote": "Vision-language models for the first time enable open-world classification of objects without the need for any retraining."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Zero-Shot Learning",
          "justification": "The paper focuses on improving zero-shot classification methods.",
          "quote": "While this zero-shot paradigm marks a significant advance, even today’s best models exhibit skewed performance when objects are dissimilar from their typical depiction."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is frequently mentioned as a model used for the evaluations and comparisons in the paper.",
          "quote": "For example, Richards et al. (2023) show models such as CLIP have exacerbated the gap in performance between regions such as Africa and Europe (as well as the gap across income levels)."
        },
        "aliases": [
          "CLIP"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "CLIP was not contributed by this paper but used for evaluation.",
          "quote": "We measure performance of zero-shot classifiers using the popular CLIP ViT-B/16 and BLIP-2 VLMs (Radford et al., 2021; Li et al., 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "CLIP was executed as part of the experiments to evaluate the proposed method.",
          "quote": "We measure performance of zero-shot classifiers using the popular CLIP ViT-B/16 and BLIP-2 VLMs (Radford et al., 2021; Li et al., 2023)."
        },
        "is_compared": {
          "value": 1,
          "justification": "CLIP was used as a baseline model for comparison purposes.",
          "quote": "We find similar biases arise when an object is visually dissimilar from its typical depiction. For example, Figure 1 (left) shows CLIP’s 97.3% accuracy on typical pears drops dramatically when a pear is peeled (45.2%) or puréed (30.3%)."
        },
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models from Natural Language Supervision",
          "justification": "This is the title of the original paper where CLIP was introduced.",
          "quote": "CLIP, a seminal VLM, achieves this via joint contrastive training of image and text encoders on 400 million image-caption pairs."
        }
      },
      {
        "name": {
          "value": "BLIP-2",
          "justification": "BLIP-2 is frequently mentioned as a model used for the evaluations and comparisons in the paper.",
          "quote": "Recent models such as BLIP-2 (Li et al., 2023) bootstrap the training of more powerful VLMs by taking larger pretrained vision and language backbones and fusing their outputs to a single space."
        },
        "aliases": [
          "BLIP-2"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "BLIP-2 was not contributed by this paper but used for evaluation.",
          "quote": "We measure performance of zero-shot classifiers using the popular CLIP ViT-B/16 and BLIP-2 VLMs (Radford et al., 2021; Li et al., 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "BLIP-2 was executed as part of the experiments to evaluate the proposed method.",
          "quote": "We measure performance of zero-shot classifiers using the popular CLIP ViT-B/16 and BLIP-2 VLMs (Radford et al., 2021; Li et al., 2023)."
        },
        "is_compared": {
          "value": 1,
          "justification": "BLIP-2 was used as a baseline model for comparison purposes.",
          "quote": "We measure performance of zero-shot classifiers using the popular CLIP ViT-B/16 and BLIP-2 VLMs (Radford et al., 2021; Li et al., 2023)."
        },
        "referenced_paper_title": {
          "value": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
          "justification": "This is the title of the original paper where BLIP-2 was introduced.",
          "quote": "Recent models such as BLIP-2 (Li et al., 2023) bootstrap the training of more powerful VLMs by taking larger pretrained vision and language backbones and fusing their outputs to a single space."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is used for evaluating classification performance and diversity of classes.",
          "quote": "To systematically quantify this tension, both for VLMs and for the one vector per class paradigm generally, we examine class accuracies on ImageNet (Deng et al., 2009) relative to the diversity of each class across several models with varying levels of supervision."
        },
        "aliases": [
          "ImageNet"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "This is the title of the original paper where ImageNet was introduced.",
          "quote": "We examine class accuracies on ImageNet (Deng et al., 2009) relative to the diversity of each class across several models with varying levels of supervision."
        }
      },
      {
        "name": {
          "value": "Breeds",
          "justification": "Breeds datasets are used to study hierarchical classification tasks.",
          "quote": "We use the four Breeds datasets (Santurkar et al., 2020) for their hierarchical label sets, as used in the CHiLS paper; indeed, these datasets were amongst those where CHiLS worked best."
        },
        "aliases": [
          "Breeds"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Breeds: Benchmarks for subpopulation shift",
          "justification": "This is the title of the original paper where Breeds was introduced.",
          "quote": "We use the four Breeds datasets (Santurkar et al., 2020) for their hierarchical label sets, as used in the CHiLS paper; indeed, these datasets were amongst those where CHiLS worked best."
        }
      },
      {
        "name": {
          "value": "MIT States",
          "justification": "MIT States is used to analyze performance over labeled states (e.g., sliced or diced for pear).",
          "quote": "Next, we devise two classification tasks (coarse and fine grained) from the MIT States dataset (Isola et al., 2015) to track performance over labeled states (e.g., sliced or diced for pear)."
        },
        "aliases": [
          "MIT States"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Discovering states and transformations in image collections",
          "justification": "This is the title of the original paper where MIT States was introduced.",
          "quote": "Next, we devise two classification tasks (coarse and fine grained) from the MIT States dataset (Isola et al., 2015) to track performance over labeled states (e.g., sliced or diced for pear)."
        }
      },
      {
        "name": {
          "value": "DollarStreet",
          "justification": "DollarStreet is used to evaluate performance across real-world geographic and income-level diversity.",
          "quote": "As the diversity in these datasets is naturally occurring diversity, they can encompass many axes of variation, as opposed to our other datasets that only vary along known axes, like object state or kind."
        },
        "aliases": [
          "DollarStreet"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Dollar Street Dataset: Images representing the geographic and socioeconomic diversity of the world",
          "justification": "This is the title of the original paper where DollarStreet was introduced.",
          "quote": "We use the Dollar Street dataset (Rojas et al., 2022), which contain images from varied geographic regions and income levels."
        }
      },
      {
        "name": {
          "value": "GeoDE",
          "justification": "GeoDE is used to evaluate performance across real-world geographic and income-level diversity.",
          "quote": "Importantly, we also include the datasets Dollarstreet (Rojas et al., 2022) and GeoDE (Ramaswamy et al., 2023), which contain images from varied geographic regions and income levels."
        },
        "aliases": [
          "GeoDE"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Beyond Web-Scraping: Crowd-Sourcing a Geo-diverse Dataset",
          "justification": "This is the title of the original paper where GeoDE was introduced.",
          "quote": "Importantly, we also include the datasets Dollarstreet (Rojas et al., 2022) and GeoDE (Ramaswamy et al., 2023), which contain images from varied geographic regions and income levels."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Vicuna",
          "justification": "Vicuna is used to generate diverse per-class attributes for the method proposed in the paper.",
          "quote": "To infer attributes, we utilize the open source Vicuna-13b-v1.5 language model (Chiang et al., 2023), which notably is already contained in the BLIP-2 model we use."
        },
        "aliases": [
          "Vicuna"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality",
          "justification": "This is the title of the original paper where Vicuna was introduced.",
          "quote": "To infer attributes, we utilize the open source Vicuna-13b-v1.5 language model (Chiang et al., 2023), which notably is already contained in the BLIP-2 model we use."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 4205,
    "prompt_tokens": 37732,
    "total_tokens": 41937
  }
}