{
  "paper": "2302.02792.txt",
  "words": 14004,
  "extractions": {
    "title": {
      "value": "Dealing with Non-Stationarity in Decentralized Cooperative Multi-Agent Deep Reinforcement Learning via Multi-Timescale Learning",
      "justification": "The title clearly specifies the main aspects of the research which are handling non-stationarity in decentralized cooperative multi-agent deep reinforcement learning and the proposed method of multi-timescale learning.",
      "quote": "Decentralized cooperative multi-agent deep reinforcement learning via multi-timescale learning."
    },
    "description": "The paper addresses the challenge of non-stationarity in decentralized cooperative multi-agent deep reinforcement learning (MARL). The authors propose a multi-timescale learning algorithm where agents learn at different rates to mitigate non-stationarity. The proposed multi-timescale learning method is evaluated against state-of-the-art decentralized learning methods using the epymarl benchmark, showing notable improvements in performance.",
    "type": {
      "value": "empirical study",
      "justification": "The paper involves the implementation and evaluation of a proposed algorithm on benchmark datasets and compares its performance against existing methods.",
      "quote": "We evaluate our hypothesis that agents learning at different timescales improve decentralized cooperative deep MARL compared to agents learning independently at one timescale through rigorous experiments."
    },
    "primary_research_field": {
      "name": {
        "value": "Multi-Agent Reinforcement Learning",
        "justification": "The research focuses on improving learning outcomes in cooperative multi-agent settings using reinforcement learning techniques.",
        "quote": "Decentralized cooperative multi-agent deep reinforcement learning (MARL) can be a versatile learning framework..."
      },
      "aliases": [
        "MARL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Decentralized Learning",
          "justification": "The paper specifically targets decentralized training scenarios where agents learn independently.",
          "quote": "An alternative that does not suffer from the limitations of CTDE is decentralized training, which is the focus of this work."
        },
        "aliases": [
          "Decentralized Training"
        ]
      },
      {
        "name": {
          "value": "Cooperative Learning",
          "justification": "The research's focus is on cooperative environments where agents work together to achieve a common objective.",
          "quote": "In this paper, we focus on cooperative multi-agent environments."
        },
        "aliases": [
          "Cooperative Multi-Agent Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Multi-timescale PPO (MTPPO)",
          "justification": "The paper proposes and evaluates MTPPO as part of its empirical study.",
          "quote": "We propose Multi-timescale Proximal Policy Optimization (MTPPO) which is based on the IPPO algorithm."
        },
        "aliases": [
          "MTPPO"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is introduced as a new approach in the paper.",
          "quote": "We propose Multi-timescale PPO (MTPPO) as multi-timescale versions of the two commonly used decentralized deep MARL algorithms."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and tested using computational resources.",
          "quote": "MTPPO outperforms both independent and sequential deep MARL algorithms in most of the tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "MTPPO is compared against other models such as IPPO and IQL.",
          "quote": "We compare the performance of MTPPO and MTQL with those of IPPO and IQL."
        },
        "referenced_paper_title": {
          "value": "Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?",
          "justification": "The IPPO (Independent PPO) method is cited as a baseline for comparison.",
          "quote": "We propose Multi-timescale PPO (MTPPO) which is based on the IPPO algorithm."
        }
      },
      {
        "name": {
          "value": "Multi-timescale Q-learning (MTQL)",
          "justification": "The paper proposes and evaluates MTQL as part of its empirical study.",
          "quote": "We propose Multi-timescale Q-Learning (MTQL) which is based on the IQL algorithm."
        },
        "aliases": [
          "MTQL"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is introduced as a new approach in the paper.",
          "quote": "We propose Multi-timescale Q-Learning (MTQL) as multi-timescale versions of the two commonly used decentralized deep MARL algorithms."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and tested using computational resources.",
          "quote": "MTQL clearly performs better than IQL in all tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "MTQL is compared against other models such as IPPO and IQL.",
          "quote": "We compare the performance of MTPPO and MTQL with those of IPPO and IQL."
        },
        "referenced_paper_title": {
          "value": "Multiagent Cooperation and Competition with Deep Reinforcement Learning",
          "justification": "The IQL (Independent Q-Learning) method is cited as a baseline for comparison.",
          "quote": "We propose Multi-timescale Q-Learning (MTQL) which is based on the IQL algorithm."
        }
      },
      {
        "name": {
          "value": "Independent Proximal Policy Optimization (IPPO)",
          "justification": "The paper uses IPPO as a baseline model for comparison.",
          "quote": "We propose Multi-timescale Proximal Policy Optimization (MTPPO) as multi-timescale versions of the two commonly used decentralized deep MARL algorithms: Independent PPO (IPPO)."
        },
        "aliases": [
          "IPPO"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model was not proposed by the authors but used as a baseline for comparison.",
          "quote": "Examples of applying independent learning to MARL include independent PPO (IPPO)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and compared within the study.",
          "quote": "In all our multi-timescale experiments, we have two timescales with learning rates lr0 and lr1. In the case of two agents, each agent learns with the respective learning rate."
        },
        "is_compared": {
          "value": 1,
          "justification": "IPPO is used as a benchmark for comparison with the proposed models.",
          "quote": "We compare the performance of MTPPO and MTQL with those of IPPO and IQL."
        },
        "referenced_paper_title": {
          "value": "Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?",
          "justification": "IPPO is referenced as an existing method in the related work.",
          "quote": "Examples of applying independent learning to MARL include independent PPO (IPPO) (de Witt et al., 2020)."
        }
      },
      {
        "name": {
          "value": "Independent Q-learning (IQL)",
          "justification": "The paper uses IQL as a baseline model for comparison.",
          "quote": "Examples of applying independent learning to MARL include independent Q-learning (IQL)."
        },
        "aliases": [
          "IQL"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model was not proposed by the authors but used as a baseline for comparison.",
          "quote": "Examples of applying independent learning to MARL include independent Q-learning (IQL)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and compared within the study.",
          "quote": "In all our multi-timescale experiments, we have two timescales with learning rates lr0 and lr1. In the case of two agents, each agent learns with the respective learning rate."
        },
        "is_compared": {
          "value": 1,
          "justification": "IQL is used as a benchmark for comparison with the proposed models.",
          "quote": "We compare the performance of MTPPO and MTQL with those of IPPO and IQL."
        },
        "referenced_paper_title": {
          "value": "Multiagent Cooperation and Competition with Deep Reinforcement Learning",
          "justification": "IQL is referenced as an existing method in the related work.",
          "quote": "Examples of applying independent learning to MARL include independent Q-learning (IQL) (Tampuu et al., 2017)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Multi-Agent Particle Environment (MPE)",
          "justification": "MPE is one of the environments used to benchmark the proposed multi-timescale learning algorithms.",
          "quote": "We include three tasks from the MPE environment: Speaker-Listener, Adversary, and Tag."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
          "justification": "The MPE environment is taken from a previously published work.",
          "quote": "Multi-Agent Particle Environment (MPE) (Lowe et al., 2017)..."
        }
      },
      {
        "name": {
          "value": "Level-Based Foraging (LBF)",
          "justification": "LBF is one of the environments used to benchmark the proposed multi-timescale learning algorithms.",
          "quote": "We include three tasks from LBF environment: 8×8-2p-2f-c, 10×10-3p-3f and 15×15-4p-3f with varying world-size, number of agents and food items."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc Coordination in Multiagent Systems",
          "justification": "The LBF environment is taken from a previously published work.",
          "quote": "Level-Based Foraging (LBF) (Albrecht & Ramamoorthy, 2015)."
        }
      },
      {
        "name": {
          "value": "Multi-Robot Warehouse (RWARE)",
          "justification": "RWARE is one of the environments used to benchmark the proposed multi-timescale learning algorithms.",
          "quote": "We include three partially observable tasks from RWARE environment: tiny-4ag, tiny-2ag and small-4ag."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning",
          "justification": "The RWARE environment is taken from a previously published work.",
          "quote": "Multi-Robot Warehouse (RWARE) (Christianos et al., 2020)..."
        }
      },
      {
        "name": {
          "value": "StarCraft Multi-Agent Challenge (SMAC)",
          "justification": "SMAC is one of the environments used to benchmark the proposed multi-timescale learning algorithms.",
          "quote": "We include three tasks from SMAC environment: MMM2 (10 agents), 3s5z (8 agents) and 3s_vs_5z (3 agents) with a different number of agents and levels of difficulty."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The StarCraft Multi-Agent Challenge",
          "justification": "The SMAC environment is taken from a previously published work.",
          "quote": "StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al., 2019)..."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2623,
    "prompt_tokens": 30963,
    "total_tokens": 33586
  }
}