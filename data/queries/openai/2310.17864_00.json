{
  "paper": "2310.17864.txt",
  "words": 6422,
  "extractions": {
    "title": {
      "value": "TORCHAUDIO 2.1: ADVANCING SPEECH RECOGNITION, SELF-SUPERVISED LEARNING, AND AUDIO PROCESSING COMPONENTS FOR PYTORCH",
      "justification": "Title found at the top of the paper.",
      "quote": "TORCHAUDIO 2.1: ADVANCING SPEECH RECOGNITION, SELF-SUPERVISED LEARNING, AND AUDIO PROCESSING COMPONENTS FOR PYTORCH"
    },
    "description": "TorchAudio is an open-source audio and speech processing library built for PyTorch. This paper surveys its design principles and contents, highlighting key features of its latest version (2.1), including self-supervised learning pre-trained pipelines and training recipes, high-performance CTC decoders, speech recognition models, advanced media I/O capabilities, and tools for forced alignment, multi-channel speech enhancement, and reference-less speech assessment.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper presents empirical results to demonstrate the efficacy of the new features in TorchAudio 2.1.",
      "quote": "For a selection of these features, through empirical studies, we demonstrate their efficacy and show that they achieve competitive or state-of-the-art performance."
    },
    "primary_research_field": {
      "name": {
        "value": "Audio and Speech Processing",
        "justification": "The paper focuses on audio and speech processing technologies, specifically enhancements in TorchAudio 2.1 for PyTorch.",
        "quote": "TorchAudio supplements PyTorch with easy-to-use and performant components for developing audio and speech machine learning models."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper includes contributions to self-supervised learning through pre-trained pipelines and training recipes.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for Wav2Vec 2.0 [4], HuBERT [5], XLS-R [6], and WavLM [7]."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Speech Recognition",
          "justification": "The paper presents new features and models for speech recognition, including high-performance CTC decoders and speech recognition models.",
          "quote": "high-performance CTC decoders, speech recognition models and training recipes"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Wav2Vec 2.0",
          "justification": "Mentioned as one of the self-supervised learning models provided in TorchAudio 2.1.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for Wav2Vec 2.0 [4]"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model's contribution is attributed to a referenced paper, not this one.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for Wav2Vec 2.0 [4]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is provided with pre-trained pipelines for use in TorchAudio 2.1.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for Wav2Vec 2.0"
        },
        "is_compared": {
          "value": 0,
          "justification": "The model is mentioned as part of the library's features, not as part of a numerical comparison.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for Wav2Vec 2.0"
        },
        "referenced_paper_title": {
          "value": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
          "justification": "This is the referenced paper for the Wav2Vec 2.0 model.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for Wav2Vec 2.0 [4]"
        }
      },
      {
        "name": {
          "value": "HuBERT",
          "justification": "Mentioned as one of the self-supervised learning models provided in TorchAudio 2.1.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... HuBERT [5]."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model's contribution is attributed to a referenced paper, not this one.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... HuBERT [5]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is provided with pre-trained pipelines for use in TorchAudio 2.1.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... HuBERT [5]."
        },
        "is_compared": {
          "value": 0,
          "justification": "The model is mentioned as part of the library's features, not as part of a numerical comparison.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... HuBERT [5]."
        },
        "referenced_paper_title": {
          "value": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
          "justification": "This is the referenced paper for the HuBERT model.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... HuBERT [5]."
        }
      },
      {
        "name": {
          "value": "Conformer",
          "justification": "Discussed as an important model for speech recognition included in TorchAudio 2.1.",
          "quote": "We have developed a PyTorch-based implementation of Conformer and published an RNN-Transducer ASR training recipe that uses it."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The paper discusses its own PyTorch-based implementation of the Conformer model.",
          "quote": "We have developed a PyTorch-based implementation of Conformer and published an RNN-Transducer ASR training recipe that uses it."
        },
        "is_executed": {
          "value": 1,
          "justification": "This implementation is part of the empirical studies conducted in the paper.",
          "quote": "Using the recipe, we produced a model that achieves word-error-rate (WER) parity with comparable open-source implementations"
        },
        "is_compared": {
          "value": 1,
          "justification": "The Conformer model is compared numerically to other models in terms of word-error-rate (WER).",
          "quote": "Using the recipe, we produced a model that achieves word-error-rate (WER) parity with comparable open-source implementations"
        },
        "referenced_paper_title": {
          "value": "Conformer: Convolution-augmented Transformer for Speech Recognition",
          "justification": "This is the primary reference for the Conformer model architecture.",
          "quote": "Conformer is a transformer-based acoustic model architecture that has achieved state-of-the-art results for ASR [9, 22]."
        }
      },
      {
        "name": {
          "value": "Emformer",
          "justification": "Discussed as a streaming-capable model for speech recognition included in TorchAudio 2.1.",
          "quote": "We have introduced an implementation of Emformer matching that described in [10] along with an Emformer transducer ASR training recipe and pre-trained inference pipeline."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The paper describes its own implementation of the Emformer acoustic model.",
          "quote": "We have introduced an implementation of Emformer matching that described in [10] along with an Emformer transducer ASR training recipe and pre-trained inference pipeline."
        },
        "is_executed": {
          "value": 1,
          "justification": "This implementation is part of the empirical studies conducted in the paper.",
          "quote": "We have introduced an implementation of Emformer matching that described in [10] along with an Emformer transducer ASR training recipe and pre-trained inference pipeline."
        },
        "is_compared": {
          "value": 0,
          "justification": "Though evaluated for performance, it is not explicitly numerically compared to other models in the text.",
          "quote": "We have introduced an implementation of Emformer matching that described in [10] along with an Emformer transducer ASR training recipe and pre-trained inference pipeline."
        },
        "referenced_paper_title": {
          "value": "Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition",
          "justification": "This is the primary reference for the Emformer model architecture.",
          "quote": "Emformer is a streaming-capable efficient memory transformer-based acoustic model [10]."
        }
      },
      {
        "name": {
          "value": "AV-ASR (Audio-Visual Automatic Speech Recognition)",
          "justification": "Mentioned as a novel feature for real-time audio-visual speech recognition in TorchAudio 2.1.",
          "quote": "We extend Auto-AVSR to real-time AV-ASR and provide an example Emformer transducer training pipeline that incorporates audio-visual input."
        },
        "aliases": [
          "Audio-Visual ASR"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper introduces its own implementation of real-time AV-ASR.",
          "quote": "We extend Auto-AVSR to real-time AV-ASR and provide an example Emformer transducer training pipeline that incorporates audio-visual input."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is evaluated for performance under real-time constraints.",
          "quote": "Our audio-visual model with an algorithmic latency [10]\nof 800 ms (160 ms + 1280 ms × 0.5) yields a WER of 1.3%, which is\non par with those achieved by state-of-the-art offline models such as\nAV-HuBERT, RAVEn, and Auto-AVSR."
        },
        "is_compared": {
          "value": 1,
          "justification": "The AV-ASR model's performance is compared numerically in the context of word-error-rate (WER).",
          "quote": "Our audio-visual model with an algorithmic latency [10]\nof 800 ms (160 ms + 1280 ms × 0.5) yields a WER of 1.3%, which is\non par with those achieved by state-of-the-art offline models such as\nAV-HuBERT, RAVEn, and Auto-AVSR."
        },
        "referenced_paper_title": {
          "value": "Auto-AVSR: Audio-visual speech recognition with automatic labels",
          "justification": "This is the primary reference for the Auto-AVSR model, which was extended in this paper.",
          "quote": "We extend Auto-AVSR to real-time AV-ASR and provide an example Emformer transducer training pipeline that incorporates audio-visual input."
        }
      },
      {
        "name": {
          "value": "XLS-R",
          "justification": "Mentioned as one of the self-supervised learning models provided in TorchAudio 2.1.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... XLS-R [6]."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model's contribution is attributed to a referenced paper, not this one.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... XLS-R [6]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is provided with pre-trained pipelines for use in TorchAudio 2.1.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... XLS-R [6]."
        },
        "is_compared": {
          "value": 0,
          "justification": "The model is mentioned as part of the library's features, not as part of a numerical comparison.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... XLS-R [6]."
        },
        "referenced_paper_title": {
          "value": "Unsupervised cross-lingual representation learning for speech recognition",
          "justification": "This is the referenced paper for the XLS-R model.",
          "quote": "TorchAudio now provides models and pre-trained pipelines for... XLS-R [6]."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "TorchAudio",
          "justification": "The paper is centered around the advancements within TorchAudio 2.1.",
          "quote": "TorchAudio supplements PyTorch with easy-to-use and performant components for developing audio and speech machine learning models."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Torchaudio: Building blocks for audio and speech processing",
          "justification": "This is the primary reference for the TorchAudio library, mentioned multiple times in the context of this paper.",
          "quote": "several design principles, which we distill from [3] and clarify."
        }
      },
      {
        "name": {
          "value": "PyTorch",
          "justification": "TorchAudio is built as an extension of PyTorch, inheriting its design principles.",
          "quote": "TorchAudio firmly adheres to several design principles, which we distill from [3] and clarify. Extend PyTorch to audio."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pytorch: An imperative style, high-performance deep learning library",
          "justification": "This is the primary reference for the PyTorch library, on which TorchAudio is built.",
          "quote": "With the rapid advancement and increasing pervasiveness of machine learning technologies, usage of open-source toolkits such as Tensorflow [1] and PyTorch [2] for developing machine learning applications has grown significantly."
        }
      },
      {
        "name": {
          "value": "ESPnet",
          "justification": "Mentioned as a toolkit that provides ready-to-use models and training recipes for speech recognition.",
          "quote": "Many higher-level audio and speech machine learning toolkits exist in the PyTorch ecosystem, e.g. ESPnet [14]."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "ESPnet: End-to-end speech processing toolkit",
          "justification": "This is the primary reference for the ESPnet toolkit, mentioned multiple times in the context of this paper.",
          "quote": "Many higher-level audio and speech machine learning toolkits exist in the PyTorch ecosystem, e.g. ESPnet [14]."
        }
      },
      {
        "name": {
          "value": "Icefall",
          "justification": "Mentioned as a toolkit that includes implementations of the Emformer model based on TorchAudio's introduction.",
          "quote": "Our implementation is the first to be publicly available, and it has\nbeen adopted and extended by icefall"
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "icefall documentation",
          "justification": "This is the primary reference for the Icefall toolkit mentioned in the context of this paper.",
          "quote": "Our implementation is the first to be publicly available, and it has\nbeen adopted and extended by icefall"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2830,
    "prompt_tokens": 12873,
    "total_tokens": 15703
  }
}