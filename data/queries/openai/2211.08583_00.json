{
  "paper": "2211.08583.txt",
  "words": 26259,
  "extractions": {
    "title": {
      "value": "Empirical Study on Optimizer Selection for Out-of-Distribution Generalization",
      "justification": "The provided text specifies the title at the beginning.",
      "quote": "Empirical Study on Optimizer Selection for Out-of-Distribution Generalization"
    },
    "description": "The paper investigates the impact of different optimizers on out-of-distribution (OOD) generalization performance in deep learning models. It examines the performance of popular first-order optimizers for various types of distributional shifts in image and text classification tasks. The study uses a wide range of hyperparameters and evaluates over 20,000 models.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments and evaluates the performance of different optimizers on OOD generalization, making it an empirical study.",
      "quote": "We examine the performance of popular first-order optimizers for different classes of distributional shift under empirical risk minimization and invariant risk minimization. We... examine classification accuracy (in-distribution and out-of-distribution) for over 20,000 models."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper focuses on optimizing the performance of different machine learning models and their ability to generalize to out-of-distribution data, which is a core aspect of machine learning.",
        "quote": "We examine the performance of popular first-order optimizers for different classes of distributional shift..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The study involves text classification tasks, a primary application of NLP.",
          "quote": "We evaluate the OOD generalization performance of these optimizers on 10 different benchmarks... and CivilComments-WILDS (Koh et al., 2021)."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The study involves image classification tasks, a primary application of Computer Vision.",
          "quote": "We evaluate the OOD generalization performance of these optimizers on 10 different benchmarks: DomainBed (which includes seven image datasets) (Gulrajani & Lopez-Paz, 2021), the Backgrounds Challenge dataset (Xiao et al., 2021), and CivilComments-WILDS (Koh et al., 2021)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet-50",
          "justification": "The study uses ResNet-50 for various image classification tasks.",
          "quote": "ResNet-50"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet-50 is a well-established model not introduced by this paper.",
          "quote": "ResNet-50"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments mention training models using ResNet-50.",
          "quote": "we trained it on ResNet-50"
        },
        "is_compared": {
          "value": 1,
          "justification": "The results compare ResNet-50 with other models.",
          "quote": "we provide the comparison"
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "Reference for the ResNet-50 model.",
          "quote": "He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); p. 770-778."
        }
      },
      {
        "name": {
          "value": "ViT",
          "justification": "Vision Transformer (ViT) was used in additional experiments to compare with ResNet-50.",
          "quote": "Vision Transformer"
        },
        "aliases": [
          "Vision Transformer"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ViT is an established model and not introduced in this paper.",
          "quote": "ViT"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments mention training models using ViT.",
          "quote": "we trained it on ViT"
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares ViT's performance with other models.",
          "quote": "we provide the comparison"
        },
        "referenced_paper_title": {
          "value": "An image is worth 16x16 words: Transformers for image recognition at scale",
          "justification": "Reference for the Vision Transformer (ViT) model.",
          "quote": "Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929."
        }
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "DistilBERT is used for various text classification tasks.",
          "quote": "DistilBERT"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "DistilBERT is a well-known model, not introduced in this paper.",
          "quote": "DistilBERT"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments mention training models using DistilBERT.",
          "quote": "we trained it on DistilBERT"
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares DistilBERT's OOD performance with other models.",
          "quote": "we provide the comparison"
        },
        "referenced_paper_title": {
          "value": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "justification": "Reference for the DistilBERT model.",
          "quote": "Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108."
        }
      },
      {
        "name": {
          "value": "ResNet-20",
          "justification": "The study uses ResNet-20 for ColoredMNIST and some additional experiments.",
          "quote": "ResNet-20"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet-20 is an established model not introduced in this paper.",
          "quote": "ResNet-20"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments mention training models using ResNet-20.",
          "quote": "we trained it on ResNet-20"
        },
        "is_compared": {
          "value": 1,
          "justification": "The results compare ResNet-20 with other models.",
          "quote": "we provide the comparison"
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "Reference for the ResNet-20 model.",
          "quote": "He K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR); p. 770-778."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "DomainBed",
          "justification": "The study uses the DomainBed dataset suite for various image classification tasks.",
          "quote": "DomainBed"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "In Search of Lost Domain Generalization",
          "justification": "The reference is for the DomainBed benchmark.",
          "quote": "Gulrajani I, Lopez-Paz D. In search of lost domain generalization. arXiv preprint arXiv:2104.00051."
        }
      },
      {
        "name": {
          "value": "WILDS",
          "justification": "The study uses the WILDS dataset suite for various text and image classification tasks that deal with distribution shifts.",
          "quote": "WILDS"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
          "justification": "The reference is for the WILDS benchmark.",
          "quote": "Koh PW, Sagawa S, Marklund H, Xie SM, Zhang M, Balsubramani A, et al. WILDS: A benchmark of in-the-wild distribution shifts. In: International Conference on Machine Learning. PMLR; 2021. p. 5637-5664."
        }
      },
      {
        "name": {
          "value": "Backgrounds Challenge",
          "justification": "The study uses the Backgrounds Challenge dataset for robustness evaluation.",
          "quote": "Backgrounds Challenge"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Noise or signal: The role of image backgrounds in object recognition",
          "justification": "The reference is for the Backgrounds Challenge dataset.",
          "quote": "Xiao K, Engstrom L, Ilyas A, Madry A. Noise or signal: The role of image backgrounds in object recognition. In: International Conference on Learning Representations; 2021."
        }
      },
      {
        "name": {
          "value": "CivilComments-WILDS",
          "justification": "The study uses CivilComments-WILDS dataset for text classification tasks.",
          "quote": "CivilComments-WILDS"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
          "justification": "The reference is for the WILDS benchmark, which includes CivilComments-WILDS.",
          "quote": "Koh PW, Sagawa S, Marklund H, Xie SM, Zhang M, Balsubramani A, et al. WILDS: A benchmark of in-the-wild distribution shifts. In: International Conference on Machine Learning. PMLR; 2021. p. 5637-5664."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly used for implementing and running the models and experiments in the study.",
          "quote": "PyTorch."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The reference is for the PyTorch library.",
          "quote": "Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: An imperative style, high-performance deep learning library. In: Advances in Neural Information Processing Systems; 2019. p. 8024-8035."
        }
      },
      {
        "name": {
          "value": "Weights & Biases",
          "justification": "Weights & Biases is used for hyperparameter tuning and experiment tracking in the study.",
          "quote": "Weights & Biases"
        },
        "aliases": [
          "wandb"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Weights and Biases: Machine Learning Experiment Tracking",
          "justification": "The reference is for the Weights & Biases library.",
          "quote": "W&B – Developer tools for ML"
        }
      },
      {
        "name": {
          "value": "scikit-learn",
          "justification": "scikit-learn is used for certain machine learning utilities and preprocessing tasks.",
          "quote": "scikit-learn"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Scikit-learn: Machine Learning in Python",
          "justification": "The reference is for the scikit-learn library.",
          "quote": "Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, et al. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research. 2011; 12(Oct):2825-2830."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2271,
    "prompt_tokens": 55138,
    "total_tokens": 57409
  }
}