{
  "paper": "2303.09387.txt",
  "words": 14143,
  "extractions": {
    "title": {
      "value": "Characterizing Manipulation from AI Systems",
      "justification": "The title is clear and directly obtained from the provided text.",
      "quote": "Characterizing Manipulation from AI Systems"
    },
    "description": "This paper characterizes the ways in which AI systems may engage in manipulative behaviors without explicit human intent, and proposes four axes to understand such manipulation: incentives, intent, harm, and covertness. It discusses applications of these characterizations in recommender systems and language models, and highlights the regulatory challenges and necessary precautions.",
    "type": {
      "value": "Empirical",
      "justification": "The paper discusses concrete examples of manipulative behavior in AI systems and presents initial work on measuring each axis of manipulation. It further delves into the operationalization of these concepts in real-world applications like recommender systems and language models.",
      "quote": "In this article, we characterize key components of manipulation from AI systems and clarify ongoing challenges. Firstly, by connecting to the existing literature, we characterize manipulation in AI systems through four axes: incentives, intent, harm, and covertness. We discuss recent work to measure each axis as well as remaining gaps."
    },
    "primary_research_field": {
      "name": {
        "value": "AI Ethics",
        "justification": "The primary field deals with ethical concerns surrounding AI systems' ability to manipulate users, which falls under AI Ethics.",
        "quote": "Manipulation could pose a significant threat to human autonomy and precautionary actions to mitigate it are likely warranted."
      },
      "aliases": [
        "Artificial Intelligence Ethics",
        "Ethical AI"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Recommender Systems",
          "justification": "The paper discusses how manipulation can occur through recommender systems by altering user preferences and engagement.",
          "quote": "We then analyze how our characterization of manipulation applies to recommender systems and language models, and give a brief overview of the regulation of manipulation in other domains."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Language Models",
          "justification": "The paper analyzes the potential manipulative behavior of language models and how these systems can influence human behavior.",
          "quote": "We then analyze how our characterization of manipulation applies to recommender systems and language models, and give a brief overview of the regulation of manipulation in other domains."
        },
        "aliases": [
          "LLMs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Horizon",
          "justification": "The Horizon model is mentioned as Facebook's open source applied reinforcement learning platform used for long-horizon recommender systems, which could be prone to manipulative behavior.",
          "quote": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is discussed as an example and is not a contribution of this paper.",
          "quote": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform"
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper mentions the Horizon model as an example; it does not indicate that the model was executed as part of their research.",
          "quote": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform"
        },
        "is_compared": {
          "value": 0,
          "justification": "The model is discussed in the context of incentives for manipulation but is not compared with other models numerically in this paper.",
          "quote": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform"
        },
        "referenced_paper_title": {
          "value": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform",
          "justification": "The referenced paper title is explicitly mentioned in the text.",
          "quote": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform"
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "Horizon",
          "justification": "Horizon is used as an example of a reinforcement learning platform, rather than being actively used to generate results in the paper.",
          "quote": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform"
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform",
          "justification": "The referenced paper title is explicitly mentioned in the text.",
          "quote": "Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 820,
    "prompt_tokens": 27484,
    "total_tokens": 28304
  }
}