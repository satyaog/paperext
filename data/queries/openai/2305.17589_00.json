{
  "paper": "2305.17589.txt",
  "words": 10495,
  "extractions": {
    "title": {
      "value": "Graph Inductive Biases in Transformers without Message Passing",
      "justification": "The title aptly encapsulates the main focus and contributions of the paper.",
      "quote": "Graph Inductive Biases in Transformers without Message Passing"
    },
    "description": "The paper introduces GRIT, a Graph Transformer incorporating graph inductive biases without using message passing. The model utilizes learned relative positional encodings based on random walk probabilities, a flexible attention mechanism that updates node and node-pair representations, and degree information injection in each layer.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper provides extensive empirical results demonstrating the effectiveness of the proposed GRIT model across various benchmarks.",
      "quote": "we provide ample empirical evidence to demonstrate the effectiveness of our design choices. GRIT achieves state-of-the-art empirical performance across a variety of graph learning benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on developing new deep learning architectures, specifically Graph Transformers, to incorporate inductive biases.",
        "quote": "To bridge this gap, we propose the Graph Inductive bias Transformer (GRIT) — a new Graph Transformer that incorporates graph inductive biases without using message passing."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Graph Neural Networks",
          "justification": "The study addresses the limitations of Message-Passing Graph Neural Networks (MPNNs) and proposes a new Graph Transformer architecture.",
          "quote": "Transformers for graph data are increasingly widely studied and successful in numerous learning tasks. Graph inductive biases are crucial for Graph Transformers."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GRIT",
          "justification": "GRIT is the main model introduced and validated in the paper.",
          "quote": "we propose the Graph Inductive bias Transformer (GRIT) — a new Graph Transformer that incorporates graph inductive biases without using message passing."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ZINC",
          "justification": "ZINC is one of the benchmarks used to evaluate the performance of the GRIT model.",
          "quote": "On the small ZINC dataset (12,000 graphs) (Dwivedi et al., 2022a), GNNs that rely on message-passing take up the top spots on the leaderboards."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PCQM4Mv2",
          "justification": "PCQM4Mv2 is used as a benchmark in the experiments to demonstrate GRIT's performance.",
          "quote": "For the large PCQM4MV2 dataset (about 3,700,000 graphs) (Hu et al., 2021), Graph Transformers take up the top spots."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 730,
    "prompt_tokens": 19152,
    "total_tokens": 19882
  }
}