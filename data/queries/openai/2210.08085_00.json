{
  "paper": "2210.08085.txt",
  "words": 10769,
  "extractions": {
    "title": {
      "value": "Adaptive Patch Foraging in Deep Reinforcement Learning Agents",
      "justification": "Title of the paper",
      "quote": "Adaptive patch foraging in deep\nreinforcement learning agents"
    },
    "description": "This paper investigates the ability of deep reinforcement learning (DRL) agents to perform adaptive patch foraging in an ecological environment, comparing their behavior to that of biological foragers and the optimal patch foraging behavior. The study reveals that DRL agents can learn adaptive patch foraging patterns similar to biological agents and approach the optimal solution when accounting for temporal discounting. The paper also examines the internal dynamics of these agents, finding similarities with the neural recordings from foraging non-human primates.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments involving deep reinforcement learning agents in a simulated ecological environment and evaluates their performance against theoretical and biological benchmarks.",
      "quote": "Here, we investigate deep\nreinforcement learning agents in an ecological patch foraging task. For the first time, we\nshow that machine learning agents can learn to patch forage adaptively in patterns similar\nto biological foragers, and approach optimal patch foraging behavior when accounting for\ntemporal discounting."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on using deep reinforcement learning agents to solve the patch foraging problem.",
        "quote": "Here, we investigate deep\nreinforcement learning agents in an ecological patch foraging task. For the first time, we\nshow that machine learning agents can learn to patch forage adaptively in patterns similar\nto biological foragers, and approach optimal patch foraging behavior when accounting for\ntemporal discounting."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Biologically-Inspired Reinforcement Learning",
          "justification": "The study draws comparisons between the foraging behavior of DRL agents and biological agents, aiming to understand intelligence in both domains.",
          "quote": "In this paper we demonstrate agents capable of patch foraging in a complex environment using a continuous\naction space. Future experiments may build on the ecological complexity of the environment and interesting\nagent behavior may arise in environments where the assumptions and predictions of the MVT start to break\ndown (Davidson & El Hady, 2019; Stephens & Krebs, 2019)."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Decision Making",
          "justification": "The focus on adaptive patch foraging revolves around decision-making processes regarding when to leave a patch for a richer one.",
          "quote": "The MVT dictates a comparison between the long-run average reward\nrate of the environment with the instantaneous reward rate of the current patch. This value comparison\nrequires multiscale temporal resolutions, both local and global, which can be difficult to represent using\nmodel-free reinforcement learning, but are seen in animal cognition and neural dynamics (Badman et al.,\n2020)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Maximum a Posteriori Policy Optimization (MPO)",
          "justification": "The paper employs the MPO algorithm to train the deep reinforcement learning agents for the patch foraging task.",
          "quote": "We use a state-of-the-art continuous control deep reinforcement learning (RL) algorithm for training our\nagents: maximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018)."
        },
        "aliases": [
          "MPO"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The MPO algorithm is not a contribution of this paper; it is used as a tool for training agents.",
          "quote": "We use a state-of-the-art continuous control deep reinforcement learning (RL) algorithm for training our\nagents: maximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018)."
        },
        "is_executed": {
          "value": true,
          "justification": "The MPO algorithm is utilized to train the agents on the patch foraging task, indicating that it was indeed executed.",
          "quote": "We use a state-of-the-art continuous control deep reinforcement learning (RL) algorithm for training our\nagents: maximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018)."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not compare the MPO algorithm to other algorithms; it focuses on the outcomes of using MPO.",
          "quote": "We use a state-of-the-art continuous control deep reinforcement learning (RL) algorithm for training our\nagents: maximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018)."
        },
        "referenced_paper_title": {
          "value": "Maximum a posteriori policy optimisation",
          "justification": "This referenced paper is cited as the original source of the MPO algorithm used in this study.",
          "quote": "We use a state-of-the-art continuous control deep reinforcement learning (RL) algorithm for training our\nagents: maximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018)."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "TensorFlow",
          "justification": "TensorFlow is mentioned as one of the libraries used for implementing the models in the paper.",
          "quote": "TensorFlow is used for implementing the neural network models to train the agents in the patch foraging environment."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1088,
    "prompt_tokens": 18270,
    "total_tokens": 19358
  }
}