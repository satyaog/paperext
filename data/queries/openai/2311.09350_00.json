{
  "paper": "2311.09350.txt",
  "words": 6561,
  "extractions": {
    "title": {
      "value": "Generalizable Imitation Learning Through Pre-Trained Representations",
      "justification": "This is the title provided at the beginning of the document",
      "quote": "Generalizable Imitation Learning Through Pre-Trained Representations"
    },
    "description": "This paper introduces BC-ViT, an imitation learning algorithm that utilizes pre-trained DINO Vision Transformer (ViT) features to improve generalization in robotic manipulation tasks. The study includes creating a benchmark based on the Google Scanned Objects dataset and demonstrates improved policy transfer to unseen objects compared to state-of-the-art approaches.",
    "type": {
      "value": "empirical study",
      "justification": "The paper presents experimental results comparing their proposed BC-ViT model against several baselines, involving collecting datasets and conducting rollouts to evaluate performance.",
      "quote": "Our experimental results show that our method outperforms both of these approaches."
    },
    "primary_research_field": {
      "name": {
        "value": "Robotics",
        "justification": "The research focuses on imitation learning for robotic manipulation tasks.",
        "quote": "Imitation Learning (IL) is a proven method for training complex robot behaviours from demonstrations."
      },
      "aliases": [
        "robotic manipulation",
        "robot learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Imitation Learning",
          "justification": "The paper specifically addresses imitation learning techniques as a method for robotic manipulation.",
          "quote": "Imitation Learning (IL) is a proven method for training complex robot behaviours from demonstrations."
        },
        "aliases": [
          "IL"
        ]
      },
      {
        "name": {
          "value": "Vision Transformers",
          "justification": "The paper leverages Vision Transformer (ViT) models pre-trained with DINO to enhance generalization in imitation learning.",
          "quote": "We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings."
        },
        "aliases": [
          "ViT"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BC-ViT",
          "justification": "This model is introduced and evaluated as part of the contributions of the paper.",
          "quote": "We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The main focus of the paper is the introduction of the BC-ViT model.",
          "quote": "We introduce BC-ViT, an imitation learning algorithm that leverages rich DINO pre-trained Visual Transformer (ViT) patch-level embeddings."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper discusses evaluation and rollouts, indicating execution of the model.",
          "quote": "Our evaluation then consists of 50 rollouts for each test environment, rotating the test object in the environment to cover 360 degrees."
        },
        "is_compared": {
          "value": 1,
          "justification": "BC-ViT is compared against several baseline models in various experiments.",
          "quote": "Our experimental results show that our method outperforms both of these approaches."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "BC-ViT is presented as a novel contribution in this paper.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DINO Vision Transformers (ViTs)",
          "justification": "This model is used as a pre-trained representation for improving generalization in the BC-ViT model.",
          "quote": "In particular, self-supervised approaches such as DINO exhibit zero-shot emerging abilities such as background-foreground segmentation and encode semantic object-part information in its embeddings."
        },
        "aliases": [
          "DINO ViT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The DINO Vision Transformers are used as a pre-trained model and were not developed within this paper.",
          "quote": "self-supervised approaches such as DINO exhibit zero-shot emerging abilities."
        },
        "is_executed": {
          "value": 1,
          "justification": "DINO ViTs are executed as a part of the embeddings and feature extraction process.",
          "quote": "In particular, self-supervised approaches such as DINO exhibit zero-shot emerging abilities."
        },
        "is_compared": {
          "value": 1,
          "justification": "The DINO ViT is used as a part of the BC-ViT model and is instrumental in its comparison against other methods.",
          "quote": "In particular, self-supervised approaches such as DINO exhibit zero-shot emerging abilities."
        },
        "referenced_paper_title": {
          "value": "Emerging Properties in Self-Supervised Vision Transformers",
          "justification": "DINO is a known model detailed in a separate referenced paper.",
          "quote": "In particular, self-supervised approaches such as DINO exhibit zero-shot emerging abilities."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Google Scanned Objects dataset",
          "justification": "The dataset is mentioned explicitly several times and used for evaluating the robot policies.",
          "quote": "We develop a benchmark based on a subset of 24 objects from the Google Scanned Objects Dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items",
          "justification": "The dataset is referenced and used for the experiments within the paper.",
          "quote": "We develop a benchmark based on a subset of 24 objects from the Google Scanned Objects Dataset."
        }
      },
      {
        "name": {
          "value": "expert demonstrations through teleoperation",
          "justification": "The dataset of expert demonstrations is collected specifically for training and evaluating the BC-ViT model.",
          "quote": "To train the IL agents, we collect a dataset of expert demonstrations through teleoperation."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "This dataset is specifically collected and contributed for the experiments in this paper.",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Robosuite",
          "justification": "Robosuite is used as the simulation environment for evaluating the robot policies.",
          "quote": "All tasks are simulated in Robosuite."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "robosuite: A Modular Simulation Framework and Benchmark for Robot Learning",
          "justification": "Robosuite is a known library and its reference is provided in the paper.",
          "quote": "All tasks are simulated in Robosuite."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1585,
    "prompt_tokens": 13427,
    "total_tokens": 15012
  }
}