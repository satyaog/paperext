{
  "paper": "2302.08531.txt",
  "words": 8333,
  "extractions": {
    "title": {
      "value": "Learning with Rejection for Abstractive Text Summarization",
      "justification": "The paper is titled 'Learning with Rejection for Abstractive Text Summarization.'",
      "quote": "Learning with Rejection for Abstractive Text Summarization"
    },
    "description": "This paper proposes a new training objective and decoding objective for abstractive text summarization to improve the factuality of generated summaries. The training objective involves rejection learning to allow the model to reject unsupported text spans, and the decoding objective penalizes non-factual summaries using a regularization term. The proposed method is compared with five baseline models and shows significant improvement in both automatic and human evaluations.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper primarily involves experimental comparisons and evaluations of different models and training objectives for abstractive text summarization.",
      "quote": "We show that our method considerably improves the factuality of generated summaries in automatic and human evaluations when compared to five baseline models."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus of the paper is on abstractive text summarization, a sub-field of Natural Language Processing.",
        "quote": "State-of-the-art abstractive summarization systems frequently hallucinate content that is not supported by the source document, mainly due to noise in the training dataset."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Abstractive Text Summarization",
          "justification": "The paper specifically addresses issues related to abstractive text summarization.",
          "quote": "In this work, we propose a training objective for abstractive summarization based on rejection learning, in which the model learns whether or not to reject potentially noisy tokens."
        },
        "aliases": [
          "Text Summarization"
        ]
      },
      {
        "name": {
          "value": "Text Generation",
          "justification": "The research involves generating text summaries based on input documents.",
          "quote": "Given a source document x = (x1 , ..., xL ) with L tokens, the task is to learn a probabilistic model p̂θ (y|x) that generates a summary y = (y1 , ..., y|y| ), where yi comes from a pre-defined vocabulary V."
        },
        "aliases": [
          "Natural Language Generation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BART",
          "justification": "The BART model is used as a baseline and also with the rejection learning and regularized decoding.",
          "quote": "We use pre-trained BART-LARGE as the backbone for our model."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "BART is a pre-existing model used as a baseline in the study.",
          "quote": "For MLE, we use the BART (Lewis et al., 2020) model."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments were conducted using BART executed on GPUs.",
          "quote": "All experiments are conducted on 4 Tesla V100 GPUs with 32GB of memory."
        },
        "is_compared": {
          "value": 1,
          "justification": "BART is one of the baseline models against which the proposed method is compared.",
          "quote": "Our main contributions in this work are ... Our method significantly improves the factuality of generated summaries in automatic and human evaluations when compared to five baseline models."
        },
        "referenced_paper_title": {
          "value": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "justification": "This is the referenced paper for the BART model.",
          "quote": "Lewis et al., 2020"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "XSUM",
          "justification": "The XSUM dataset is used for training and evaluating the models.",
          "quote": "We evaluated our method on the XSUM (Narayan et al., 2018) dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
          "justification": "This is the referenced paper for the XSUM dataset.",
          "quote": "We evaluated our method on the XSUM (Narayan et al., 2018) dataset."
        }
      },
      {
        "name": {
          "value": "XENT",
          "justification": "The XENT dataset is used for fine-grained entity-level factuality analysis.",
          "quote": "For fine-grained entity-level factuality analysis (see Section 5.1), we use the XENT dataset created by Cao et al. (2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Hallucinated but Factual! Inspecting the Factuality of Hallucinations in Abstractive Summarization",
          "justification": "This is the referenced paper for the XENT dataset.",
          "quote": "For fine-grained entity-level factuality analysis (see Section 5.1), we use the XENT dataset created by Cao et al. (2022)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions using pre-trained BART models, which are commonly implemented using PyTorch.",
          "quote": "We use pre-trained BART-LARGE as the backbone for our model."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "PyTorch is the library used for implementing BART models.",
          "quote": "We use pre-trained BART-LARGE as the backbone for our model."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1101,
    "prompt_tokens": 14921,
    "total_tokens": 16022
  }
}