{
  "paper": "2303.09032.txt",
  "words": 10733,
  "extractions": {
    "title": {
      "value": "Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning",
      "justification": "This is the title of the paper.",
      "quote": "Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning"
    },
    "description": "The paper proposes an exploration method called Conditionally Optimistic Exploration (COE) for cooperative deep Multi-Agent Reinforcement Learning. COE introduces sequential action-computation with an action-conditioned optimistic bonus to encourage cooperative strategies. Experimental results show that COE outperforms current state-of-the-art methods on hard exploration tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper proposes a method and evaluates it experimentally across various benchmarks, demonstrating its empirical performance.",
      "quote": "Experiments across various cooperative MARL benchmarks show that COE outperforms current state-of-the-art exploration methods on hard-exploration tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on Multi-Agent Reinforcement Learning (MARL).",
        "quote": "Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL)."
      },
      "aliases": [
        "MARL",
        "Multi-Agent RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Agent Systems",
          "justification": "The paper deals with reinforcement learning in the context of multiple interacting agents.",
          "quote": "Efficient exploration is critical in cooperative deep Multi-Agent Reinforcement Learning (MARL)."
        },
        "aliases": [
          "Multi-Agent"
        ]
      },
      {
        "name": {
          "value": "Cooperative Exploration",
          "justification": "The method proposed involves exploring cooperative strategies in a multi-agent setting.",
          "quote": "In this work, we propose an exploration method that effectively encourages cooperative exploration based on the idea of sequential action-computation scheme."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Conditionally Optimistic Exploration (COE)",
          "justification": "The proposed method developed in the paper is called COE.",
          "quote": "Inspired by the theoretically justified tree search algorithm UCT (Upper Confidence bounds applied to Trees), we develop a method called Conditionally Optimistic Exploration (COE)."
        },
        "aliases": [
          "COE"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "COE is an exploration method proposed by the authors in this paper.",
          "quote": "we develop a method called Conditionally Optimistic Exploration (COE)."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments are conducted to evaluate the performance of COE.",
          "quote": "Experiments across various cooperative MARL benchmarks show that COE outperforms current state-of-the-art exploration methods on hard-exploration tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "COE is compared against other state-of-the-art exploration methods in experiments.",
          "quote": "Experiments across various cooperative MARL benchmarks show that COE outperforms current state-of-the-art exploration methods on hard-exploration tasks."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no referenced paper title for COE as it is introduced in this paper.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "QMIX",
          "justification": "QMIX is used as a baseline model for comparisons in the experiments.",
          "quote": "For methods that use intrinsic reward — i.e. COE, EMC, and MAVEN — we only test constant intrinsic reward scales. For COE, the hyperparameter combination with cact = crew = cboot = 0 is ignored as this setting refers to the greedy-action QMIX."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "QMIX is a baseline used for comparison, not a contribution of this paper.",
          "quote": "For methods that use intrinsic reward — i.e. COE, EMC, and MAVEN — we only test constant intrinsic reward scales. For COE, the hyperparameter combination with cact = crew = cboot = 0 is ignored as this setting refers to the greedy-action QMIX."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments include running QMIX as a baseline for comparison.",
          "quote": "For methods that use intrinsic reward — i.e. COE, EMC, and MAVEN — we only test constant intrinsic reward scales. For COE, the hyperparameter combination with cact = crew = cboot = 0 is ignored as this setting refers to the greedy-action QMIX."
        },
        "is_compared": {
          "value": 1,
          "justification": "QMIX is used as a baseline and its performance is compared against COE.",
          "quote": "For methods that use intrinsic reward — i.e. COE, EMC, and MAVEN — we only test constant intrinsic reward scales. For COE, the hyperparameter combination with cact = crew = cboot = 0 is ignored as this setting refers to the greedy-action QMIX."
        },
        "referenced_paper_title": {
          "value": "QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning",
          "justification": "QMIX is a well-known method, and its reference paper is cited for comparison.",
          "quote": "[25] Rashid, T., Samvelyan, M., Schroeder de Witt, C., Farquhar, G., Foerster, J. and Whiteson, S. (2018). QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning."
        }
      },
      {
        "name": {
          "value": "EMC",
          "justification": "EMC is used as a baseline model for comparisons in the experiments.",
          "quote": "We evaluate COE and compare it with the following state-of-the-art baselines in the experiments: (ii) EMC [Zheng et al., 2021]"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "EMC is a baseline used for comparison, not a contribution of this paper.",
          "quote": "We evaluate COE and compare it with the following state-of-the-art baselines in the experiments: (ii) EMC [Zheng et al., 2021]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments include running EMC as a baseline for comparison.",
          "quote": "We evaluate COE and compare it with the following state-of-the-art baselines in the experiments: (ii) EMC [Zheng et al., 2021]"
        },
        "is_compared": {
          "value": 1,
          "justification": "EMC is used as a baseline and its performance is compared against COE.",
          "quote": "We evaluate COE and compare it with the following state-of-the-art baselines in the experiments: (ii) EMC [Zheng et al., 2021]"
        },
        "referenced_paper_title": {
          "value": "Episodic multi-agent reinforcement learning with curiosity-driven exploration",
          "justification": "EMC is a well-known method, and its reference paper is cited for comparison.",
          "quote": "[43] Zheng, L., Chen, J., Wang, J., He, J., Hu, Y., Chen, Y., Fan, C., Gao, Y. and Zhang, C. (2021). Episodic multi-agent reinforcement learning with curiosity-driven exploration."
        }
      },
      {
        "name": {
          "value": "MAVEN",
          "justification": "MAVEN is used as a baseline model for comparisons in the experiments.",
          "quote": "We evaluate COE and compare it with the following state-of-the-art baselines in the experiments: (iii) MAVEN [Mahajan et al., 2019]"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "MAVEN is a baseline used for comparison, not a contribution of this paper.",
          "quote": "We evaluate COE and compare it with the following state-of-the-art baselines in the experiments: (iii) MAVEN [Mahajan et al., 2019]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments include running MAVEN as a baseline for comparison.",
          "quote": "We evaluate COE and compare it with the following state-of-the-art baselines in the experiments: (iii) MAVEN [Mahajan et al., 2019]"
        },
        "is_compared": {
          "value": 1,
          "justification": "MAVEN is used as a baseline and its performance is compared against COE.",
          "quote": "We evaluate COE and compare it with the following state-of-the-art baselines in the experiments: (iii) MAVEN [Mahajan et al., 2019]"
        },
        "referenced_paper_title": {
          "value": "Maven: Multi-agent variational exploration",
          "justification": "MAVEN is a well-known method, and its reference paper is cited for comparison.",
          "quote": "[36] Mahajan, A., Rashid, T., Samvelyan, M. and Whiteson, S. (2019). MAVEN: Multi-Agent Variational Exploration."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2150,
    "prompt_tokens": 20301,
    "total_tokens": 22451
  }
}