{
  "paper": "2306.00452.txt",
  "words": 5186,
  "extractions": {
    "title": {
      "value": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?",
      "justification": "Title of the research paper.",
      "quote": "Speech Self-Supervised Representation Benchmarking: Are We Doing it Right?"
    },
    "description": "This paper investigates the robustness of benchmarking results for self-supervised learning (SSL) models in the context of speech tasks. It evaluates how changes in decoding architectures affect the performance and ranking of SSL models.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves conducting experiments and benchmarking different SSL models and decoding architectures on various speech tasks.",
      "quote": "The contributions are three-fold: i) We benchmark a set of published state-of-the-art SSL models on various speech tasks, varying the downstream decoders (Section 2); ii) We show that, except for ASR on LibriSpeech, the rankings and relative performances of the models are severely impacted by a change of decoder (Section 3); iii) We release the code1 for the benchmark, implemented in SpeechBrain [15], for replication and further research on SSL models and downstream tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Self-Supervised Learning",
        "justification": "The paper focuses on evaluating self-supervised learning models for speech tasks.",
        "quote": "Self-supervised learning (SSL) has recently allowed leveraging large datasets of unlabeled speech signals to reach impressive performance on speech tasks using only small amounts of annotated data."
      },
      "aliases": [
        "SSL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Benchmarking",
          "justification": "The study evaluates the robustness of SSL models through benchmarking across different tasks and architectures.",
          "quote": "This work investigates the robustness of such benchmarking results to changes in the decoder architecture."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Speech Recognition",
          "justification": "The paper benchmarks SSL models specifically on speech recognition tasks among others.",
          "quote": "Evaluating the performance on a set of downstream tasks exploring various aspects of the speech signal."
        },
        "aliases": [
          "ASR"
        ]
      },
      {
        "name": {
          "value": "Automatic Speaker Verification",
          "justification": "One of the tasks evaluated in the paper is automatic speaker verification, which falls under this sub-field.",
          "quote": "Automatic Speaker Verification (ASV) involves a binary classification process to determine if the speakers in a pair of utterances are the same."
        },
        "aliases": [
          "ASV"
        ]
      },
      {
        "name": {
          "value": "Emotion Recognition",
          "justification": "The experiments also involve benchmarking models on the task of emotion recognition.",
          "quote": "Emotion Recognition (ER). IEMOCAP [29], a dataset containing 10, 039 from 10 speakers, is used for Emotion Recognition."
        },
        "aliases": [
          "ER"
        ]
      },
      {
        "name": {
          "value": "Intent Classification",
          "justification": "The paper also includes intent classification as one of the downstream tasks for benchmarking.",
          "quote": "Intent Classification (IC). Instead of Speech Commands (SC) [30], which is the dataset used in the SUPERB benchmarks for IC, we use the SLURP dataset [31], as the error rates with SC are already very low and SLURP is a more challenging task."
        },
        "aliases": [
          "IC"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Wav2vec 2.0",
          "justification": "Wav2vec 2.0 is one of the models benchmarked in the study.",
          "quote": "Hence, 9 models acting directly on the raw waveform have been selected: Wav2vec 2.0 [1], HuBERT [18], WavLM2 [19], and Data2vec [20] both in their Base and Large versions."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not contributed by the paper but is evaluated.",
          "quote": "Hence, 9 models acting directly on the raw waveform have been selected: Wav2vec 2.0 [1]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the benchmarking tasks.",
          "quote": "The contributions are three-fold: i) We benchmark a set of published state-of-the-art SSL models on various speech tasks, varying the downstream decoders (Section 2);"
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was compared to other models in the benchmarking study.",
          "quote": "On most downstream tasks, rankings and relative performances are severely impacted by the change of the downstream architecture."
        },
        "referenced_paper_title": {
          "value": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
          "justification": "Title of the referenced paper for Wav2vec 2.0.",
          "quote": "Wav2vec 2.0 [1]"
        }
      },
      {
        "name": {
          "value": "HuBERT",
          "justification": "HuBERT is one of the models benchmarked in the study.",
          "quote": "Hence, 9 models acting directly on the raw waveform have been selected: Wav2vec 2.0 [1], HuBERT [18], WavLM2 [19], and Data2vec [20] both in their Base and Large versions."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not contributed by the paper but is evaluated.",
          "quote": "Hence, 9 models acting directly on the raw waveform have been selected: ... HuBERT [18]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the benchmarking tasks.",
          "quote": "The contributions are three-fold: i) We benchmark a set of published state-of-the-art SSL models on various speech tasks, varying the downstream decoders (Section 2);"
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was compared to other models in the benchmarking study.",
          "quote": "On most downstream tasks, rankings and relative performances are severely impacted by the change of the downstream architecture."
        },
        "referenced_paper_title": {
          "value": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
          "justification": "Title of the referenced paper for HuBERT.",
          "quote": "HuBERT [18]"
        }
      },
      {
        "name": {
          "value": "WavLM",
          "justification": "WavLM is one of the models benchmarked in the study.",
          "quote": "Hence, 9 models acting directly on the raw waveform have been selected: Wav2vec 2.0 [1], HuBERT [18], WavLM2 [19], and Data2vec [20] both in their Base and Large versions."
        },
        "aliases": [
          "WavLM2"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not contributed by the paper but is evaluated.",
          "quote": "Hence, 9 models acting directly on the raw waveform have been selected: ... WavLM2 [19]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the benchmarking tasks.",
          "quote": "The contributions are three-fold: i) We benchmark a set of published state-of-the-art SSL models on various speech tasks, varying the downstream decoders (Section 2);"
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was compared to other models in the benchmarking study.",
          "quote": "On most downstream tasks, rankings and relative performances are severely impacted by the change of the downstream architecture."
        },
        "referenced_paper_title": {
          "value": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
          "justification": "Title of the referenced paper for WavLM.",
          "quote": "WavLM [19]"
        }
      },
      {
        "name": {
          "value": "Data2vec",
          "justification": "Data2vec is one of the models benchmarked in the study.",
          "quote": "Hence, 9 models acting directly on the raw waveform have been selected: Wav2vec 2.0 [1], HuBERT [18], WavLM2 [19], and Data2vec [20] both in their Base and Large versions."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not contributed by the paper but is evaluated.",
          "quote": "Hence, 9 models acting directly on the raw waveform have been selected: ... Data2vec [20]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the benchmarking tasks.",
          "quote": "The contributions are three-fold: i) We benchmark a set of published state-of-the-art SSL models on various speech tasks, varying the downstream decoders (Section 2);"
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was compared to other models in the benchmarking study.",
          "quote": "On most downstream tasks, rankings and relative performances are severely impacted by the change of the downstream architecture."
        },
        "referenced_paper_title": {
          "value": "Data2vec: A general framework for self-supervised learning in speech, vision and language",
          "justification": "Title of the referenced paper for Data2vec.",
          "quote": "Data2vec [20]"
        }
      },
      {
        "name": {
          "value": "DistilHuBERT",
          "justification": "DistilHuBERT is one of the models benchmarked in the study.",
          "quote": "DistilHuBERT [21], a distilled version of Hubert Base with 4 times less transformer layers, is also added to the list."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not contributed by the paper but is evaluated.",
          "quote": "DistilHuBERT [21], a distilled version of Hubert Base with 4 times less transformer layers, is also added to the list."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the benchmarking tasks.",
          "quote": "The contributions are three-fold: i) We benchmark a set of published state-of-the-art SSL models on various speech tasks, varying the downstream decoders (Section 2);"
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was compared to other models in the benchmarking study.",
          "quote": "On most downstream tasks, rankings and relative performances are severely impacted by the change of the downstream architecture."
        },
        "referenced_paper_title": {
          "value": "DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT",
          "justification": "Title of the referenced paper for DistilHuBERT.",
          "quote": "DistilHuBERT [21]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "LibriSpeech",
          "justification": "LibriSpeech is one of the datasets used for evaluating the SSL models in the study.",
          "quote": "LibriSpeech960 [17] have been popular choices in the literature"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Librispeech: An ASR corpus based on public domain audio books",
          "justification": "Title of the referenced paper for the LibriSpeech dataset.",
          "quote": "LibriSpeech960 [17]"
        }
      },
      {
        "name": {
          "value": "VoxCeleb1",
          "justification": "VoxCeleb1 is one of the datasets used for benchmarking automatic speaker verification in the study.",
          "quote": "As in the SUPERB benchmark, we use VoxCeleb1 [28] train and test splits for this task."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "VoxCeleb: a large-scale speaker identification dataset",
          "justification": "Title of the referenced paper for the VoxCeleb1 dataset.",
          "quote": "VoxCeleb1 [28]"
        }
      },
      {
        "name": {
          "value": "IEMOCAP",
          "justification": "IEMOCAP is one of the datasets used for benchmarking emotion recognition in the paper.",
          "quote": "IEMOCAP [29], a dataset containing 10, 039 from 10 speakers, is used for Emotion Recognition."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Iemocap: Interactive emotional dyadic motion capture database",
          "justification": "Title of the referenced paper for the IEMOCAP dataset.",
          "quote": "IEMOCAP [29]"
        }
      },
      {
        "name": {
          "value": "SLURP",
          "justification": "SLURP dataset is used for intent classification benchmarking in the study.",
          "quote": "Instead of Speech Commands (SC) [30], which is the dataset used in the SUPERB benchmarks for IC, we use the SLURP dataset [31], as the error rates with SC are already very low and SLURP is a more challenging task."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Slurp: A spoken language understanding resource package",
          "justification": "Title of the referenced paper for the SLURP dataset.",
          "quote": "SLURP dataset [31]"
        }
      },
      {
        "name": {
          "value": "CommonVoice 11.0",
          "justification": "CommonVoice 11.0 dataset is used for evaluating low-resource language tasks in the study.",
          "quote": "...two low-resource languages tasks, extracted from the CommonVoice 11.0 [26] release, are considered: Welsh (Cymraeg) and Basque (Euskera)."
        },
        "aliases": [
          "Common Voice"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Common voice: A massively-multilingual speech corpus",
          "justification": "Title of the referenced paper for the CommonVoice 11.0 dataset.",
          "quote": "CommonVoice 11.0 [26]"
        }
      },
      {
        "name": {
          "value": "Libri-light",
          "justification": "Libri-light is a dataset mentioned as being used for training the SSL models.",
          "quote": "Libri-light [16] and LibriSpeech960 [17] have been popular choices in the literature"
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Libri-light: A benchmark for ASR with limited or no supervision",
          "justification": "Title of the referenced paper for the Libri-light dataset.",
          "quote": "Libri-light [16]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "SpeechBrain is the primary library used for implementing the benchmark in the study.",
          "quote": "We release the code for the benchmark, implemented in SpeechBrain [15], for replication and further research on SSL models and downstream tasks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A general-purpose speech toolkit",
          "justification": "Title of the referenced paper for SpeechBrain.",
          "quote": "SpeechBrain [15]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2925,
    "prompt_tokens": 10887,
    "total_tokens": 13812
  }
}