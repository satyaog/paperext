{
  "paper": "2301.05709.txt",
  "words": 7715,
  "extractions": {
    "title": {
      "value": "Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss",
      "justification": "This is the title of the paper as provided by the user and in the paper itself.",
      "quote": "Self-Supervised Image-to-Point Distillation via Semantically Tolerant Contrastive Loss"
    },
    "description": "This paper introduces a novel framework for learning 3D representations for perception tasks, specifically focusing on autonomous driving datasets. The proposed method, Semantically Tolerant SLidR (ST-SLidR), alleviates the issues of self-similarity and class imbalance commonly found in such datasets through a semantically tolerant contrastive loss and a class-agnostic balanced loss. The framework effectively utilizes 2D self-supervised image features to enhance 3D point cloud representations, improving performance in 3D semantic segmentation.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper conducts experiments and presents empirical results demonstrating the performance improvements of the proposed method over existing state-of-the-art methods.",
      "quote": "We also show that our proposed semantically-tolerant loss improves 3D semantic segmentation performance across a wide range of 2D self-supervised pretrained image features, consistently outperforming state-of-the-art 2D-to-3D representation learning frameworks."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on learning 3D representations and improving 3D semantic segmentation for autonomous driving datasets, which fall under the domain of Computer Vision.",
        "quote": "Our method consistently outperforms state-of-the-art 2D-to-3D representation learning frameworks across a wide range of 2D self-supervised pretrained models."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "3D Representation Learning",
          "justification": "The primary focus of the paper is on learning 3D representations from 2D self-supervised image features to improve 3D semantic segmentation.",
          "quote": "We demonstrate that our semantically-tolerant contrastive loss with class balancing improves state-of-the-art 2D-to-3D representation learning in all evaluation settings on 3D semantic segmentation."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Autonomous Driving",
          "justification": "The paper specifically addresses the challenges of 3D representation learning in the context of autonomous driving datasets, such as nuScenes.",
          "quote": "However, image-to-point representation learning for autonomous driving datasets faces two main challenges: 1) the abundance of self-similarity ... and 2) severe class imbalance."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper employs self-supervised learning techniques for pretraining 2D image encoders, which are then used to guide the learning of 3D point cloud representations.",
          "quote": "An effective framework for learning 3D representations for perception tasks is distilling rich self-supervised image features via contrastive learning."
        },
        "aliases": [
          "SSL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ST-SLidR",
          "justification": "The paper contributes a new model called Semantically Tolerant SLidR (ST-SLidR) which addresses self-similarity and class imbalance issues in 3D representation learning.",
          "quote": "We summarize our approach with two main contributions, which we present below. Semantically-Tolerant Loss. To address the similarity of samples in 2D-to-3D representation learning frameworks, we propose a novel contrastive loss..."
        },
        "aliases": [
          "Semantically Tolerant SLidR"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper contributes this new model to the research field.",
          "quote": "We summarize our approach with two main contributions, which we present below. Semantically-Tolerant Loss. To address the similarity of samples in 2D-to-3D representation learning frameworks, we propose a novel contrastive loss..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments conducted in the paper utilized ST-SLidR, indicating that it was executed during the study.",
          "quote": "For all experiments except 2D SSL frameworks, the 2D backbone for PPKT, SLidR and ST-SLidR is initialized using MoCoV2."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of ST-SLidR was compared with other state-of-the-art models in the experiments section.",
          "quote": "We demonstrate that our semantically-tolerant contrastive loss with class balancing improves state-of-the-art 2D-to-3D representation learning in all evaluation settings on 3D semantic segmentation."
        },
        "referenced_paper_title": {
          "value": "Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data",
          "justification": "The new approach builds upon prior work in this area, specifically citing the SLidR model, which is referenced in the paper.",
          "quote": "By extending the state-of-the-art 2D-to-3D representation learning frameworks using our proposed semantically-tolerant contrastive loss with class balancing, we show that we can improve their performance."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "nuScenes",
          "justification": "The nuScenes dataset is used for pre-training and evaluating the 3D representations learned by the proposed ST-SLidR model.",
          "quote": "We use the nuScenes dataset, which contains 700 training scenes. Following SLidR, we further split the 700 scenes into 600 for pre-training and 100 scenes for selecting the optimal hyper-parameters."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "nuScenes: A multimodal dataset for autonomous driving",
          "justification": "This dataset is referenced specifically for its role and significance in autonomous driving research.",
          "quote": "In this work, we simultaneously address the challenge of contrasting semantically similar point and image regions and the challenge of learning 3D representations from highly imbalanced autonomous driving datasets...For pre-training, we use the nuScenes dataset, which contains 700 training scenes"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "MoCoV2",
          "justification": "MoCoV2 is one of the self-supervised pretraining frameworks used in this paper for the 2D image encoder.",
          "quote": "For all experiments except 2D SSL frameworks, the 2D backbone for PPKT, SLidR and ST-SLidR is initialized using MoCoV2."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Improved baselines with momentum contrastive learning",
          "justification": "The reference paper for MoCoV2 provides the baseline methodology for the self-supervised training of the 2D image encoder.",
          "quote": "For all experiments except 2D SSL frameworks, the 2D backbone for PPKT, SLidR and ST-SLidR is initialized using MoCoV2."
        }
      },
      {
        "name": {
          "value": "SwAV",
          "justification": "SwAV is another self-supervised learning framework used in experiments to initialize the 2D image encoder.",
          "quote": "We also show that our proposed semantically-tolerant loss improves 3D semantic segmentation performance across a wide range of 2D self-supervised pretrained image features...In Table 2, we present results for experiments using weights pre-trained with different 2D SSL frameworks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Unsupervised learning of visual features by contrasting cluster assignments",
          "justification": "The reference paper for SwAV provides the baseline methodology for the self-supervised training of the 2D image encoder.",
          "quote": "In Table 2, we present results for experiments using weights pre-trained with different 2D SSL frameworks."
        }
      },
      {
        "name": {
          "value": "DINO",
          "justification": "DINO is used as one of the self-supervised pretraining frameworks for the 2D image encoder in the experiments.",
          "quote": "We also show that our proposed semantically-tolerant loss improves 3D semantic segmentation performance across a wide range of 2D self-supervised pretrained image features...In Table 2, we present results for experiments using weights pre-trained with different 2D SSL frameworks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Emerging properties in self-supervised vision transformers",
          "justification": "The reference paper for DINO explains the self-supervised pretraining method used for 2D image encoders in the experiments.",
          "quote": "In Table 2, we present results for experiments using weights pre-trained with different 2D SSL frameworks."
        }
      },
      {
        "name": {
          "value": "OBoW",
          "justification": "OBoW is also used in the experiments for pretraining the 2D image encoder before ST-SLidR is applied.",
          "quote": "We also show that our proposed semantically-tolerant loss improves 3D semantic segmentation performance across a wide range of 2D self-supervised pretrained image features...In Table 2, we present results for experiments using weights pre-trained with different 2D SSL frameworks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Obow: Online bag-of-visual-words generation for self-supervised learning",
          "justification": "The reference paper for OBoW details the self-supervised pretraining approach used for the 2D image encoder in some experiments.",
          "quote": "In Table 2, we present results for experiments using weights pre-trained with different 2D SSL frameworks."
        }
      },
      {
        "name": {
          "value": "DenseCL",
          "justification": "DenseCL is another self-supervised learning framework used in the experiments to initialize the 2D image encoder.",
          "quote": "We also show that our proposed semantically-tolerant loss improves 3D semantic segmentation performance across a wide range of 2D self-supervised pretrained image features...In Table 2, we present results for experiments using weights pre-trained with different 2D SSL frameworks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Dense contrastive learning for self-supervised visual pre-training",
          "justification": "The reference paper for DenseCL provides insights into the self-supervised training method used for the 2D image encoder.",
          "quote": "In Table 2, we present results for experiments using weights pre-trained with different 2D SSL frameworks."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2109,
    "prompt_tokens": 14508,
    "total_tokens": 16617
  }
}