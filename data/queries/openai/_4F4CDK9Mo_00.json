{
  "paper": "_4F4CDK9Mo.txt",
  "words": 17205,
  "extractions": {
    "title": {
      "value": "RAINPROOF: AN UMBRELLA TO SHIELD TEXT GENERATORS FROM OUT-OF-DISTRIBUTION DATA",
      "justification": "The title is prominently mentioned in the beginning of the paper and multiple other places throughout the text.",
      "quote": "RAINPROOF: AN UMBRELLA TO SHIELD TEXT GENERATORS FROM OUT-OF-DISTRIBUTION DATA"
    },
    "description": "This paper addresses out-of-distribution (OOD) detection specifically for machine translation and dialog generation. It introduces RAINPROOF, a novel framework for OOD detection based on information projection measures. The paper also presents a new benchmark for evaluating OOD detectors in operational settings, named LOFTER.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes extensive experiments and evaluations of the proposed OOD detection framework, RAINPROOF, against several datasets and metrics.",
      "quote": "Our extensive experiments on LOFTER show that OOD detectors may filter out samples..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on OOD detection for text generation systems such as machine translation and dialog generation, which are core areas of Natural Language Processing.",
        "quote": "This paper addresses the problem of OOD detection for machine translation and dialog generation from an operational perspective."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Translation",
          "justification": "One of the primary applications discussed in the paper is machine translation, for which the effectiveness of OOD detection is critical.",
          "quote": "First, it introduces more realistic data shifts that go beyond English Fan et al. (2021): language shifts induced by closely related language pairs (e.g., Spanish and Catalan or Dutch and Afrikaans)"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Dialog Systems",
          "justification": "Another significant application discussed is dialog generation systems, which are crucial for conversational AI.",
          "quote": "We study the shifts induced by Catalan-Spanish, Portugese-Spanish and Afrikaans-Dutch. Domain shifts, which occur when the model is exposed to a specific topic that was not seen during training, can also affect the quality of the translation (see Tab. 4)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "RAINPROOF",
          "justification": "RAINPROOF is the primary framework presented in this paper for OOD detection in text generation models.",
          "quote": "We present RAINPROOF: a Relative informAItioN Projection Out OF distribution detector. RAINPROOF is fully unsupervised."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The paper introduces RAINPROOF as its main contribution to the field of OOD detection for text generators.",
          "quote": "We present RAINPROOF: a Relative informAItioN Projection Out OF distribution detector. RAINPROOF is fully unsupervised."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments in the paper demonstrate the execution of RAINPROOF across multiple datasets.",
          "quote": "Our extensive experiments on LOFTER show that OOD detectors may filter out samples"
        },
        "is_compared": {
          "value": 1,
          "justification": "RAINPROOF is compared against several baseline methods for OOD detection.",
          "quote": "Our results show that RAINPROOF breaks this curse and achieve good results in OOD detection while increasing performance."
        },
        "referenced_paper_title": {
          "value": "RAINPROOF: AN UMBRELLA TO SHIELD TEXT GENERATORS FROM OUT-OF-DISTRIBUTION DATA",
          "justification": "The title is consistently referred to as RAINPROOF in the paper itself.",
          "quote": "We present RAINPROOF: a Relative informAItioN Projection Out OF distribution detector. RAINPROOF is fully unsupervised."
        }
      },
      {
        "name": {
          "value": "GPT",
          "justification": "The paper mentions the use of existing powerful text generators like GPT in the context of OOD detection.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years with the development of powerful generic (e.g., GPT (Radford et al., 2018; 2019; Brown et al., 2020))"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT is used as a point of reference but not contributed by this paper.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years with the development of powerful generic (e.g., GPT (Radford et al., 2018; 2019; Brown et al., 2020))"
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention specific execution details related to GPT within its experimental setup.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years with the development of powerful generic (e.g., GPT (Radford et al., 2018; 2019; Brown et al., 2020))"
        },
        "is_compared": {
          "value": 0,
          "justification": "GPT is not directly compared numerically; it's referenced more for context.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years with the development of powerful generic (e.g., GPT (Radford et al., 2018; 2019; Brown et al., 2020))"
        },
        "referenced_paper_title": {
          "value": "Language Models are Unsupervised Multitask Learners",
          "justification": "The referenced paper titles for GPT are mentioned in the citations (Radford et al., 2018; 2019; Brown et al., 2020).",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years with the development of powerful generic (e.g., GPT (Radford et al., 2018; 2019; Brown et al., 2020))"
        }
      },
      {
        "name": {
          "value": "Grover",
          "justification": "Grover is cited in the paper as one of the task-specific text generators.",
          "quote": "and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Grover is not contributed by this paper; it is cited as a reference.",
          "quote": "and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not provide details on the execution of Grover.",
          "quote": "and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "is_compared": {
          "value": 0,
          "justification": "Grover is cited for context but not numerically compared.",
          "quote": "and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "referenced_paper_title": {
          "value": "Defending Against Neural Fake News",
          "justification": "The referenced paper title for Grover is mentioned in the citation (Zellers et al., 2019).",
          "quote": "and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        }
      },
      {
        "name": {
          "value": "Pegasus",
          "justification": "The paper cites Pegasus as another task-specific text generator used as a reference.",
          "quote": "and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Pegasus is not a contribution of this paper.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "is_executed": {
          "value": 0,
          "justification": "Pegasus is mentioned but not executed in the context of this paper.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "is_compared": {
          "value": 0,
          "justification": "Pegasus is cited for context but not numerically compared in this study.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "referenced_paper_title": {
          "value": "PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization",
          "justification": "The referenced paper title for Pegasus is explicitly mentioned in the citation (Zhang et al., 2020).",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        }
      },
      {
        "name": {
          "value": "DialogGPT",
          "justification": "DialogGPT is another task-specific text generator referenced in the paper.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "DialogGPT is cited in the context of existing text generation methods.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "is_executed": {
          "value": 0,
          "justification": "DialogGPT is not executed as part of the experiments in this paper.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "is_compared": {
          "value": 0,
          "justification": "DialogGPT is mentioned but not numerically compared within the scope of this paper.",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        },
        "referenced_paper_title": {
          "value": "Dialogpt: Large-scale generative pre-training for conversational response generation",
          "justification": "The referenced paper title for DialogGPT is cited as (Zhang et al., 2019).",
          "quote": "Significant progress have been made in Natural Language Generation (NLG) in recent years... and task-specific (e.g., Grover (Zellers et al., 2019), Pegasus (Zhang et al., 2020) and DialogGPT (Zhang et al., 2019)) text generators."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Tatoeba",
          "justification": "The Tatoeba dataset is mentioned several times, especially in the context of language shifts.",
          "quote": "For language shifts, we focus on closely related language pairs coming from the Tatoeba dataset (Tiedemann, 2012b)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Parallel data, tools and interfaces in opus",
          "justification": "The referenced paper for Tatoeba is cited as Tiedemann (2012b).",
          "quote": "For language shifts, we focus on closely related language pairs coming from the Tatoeba dataset (Tiedemann, 2012b)"
        }
      },
      {
        "name": {
          "value": "LOFTER",
          "justification": "LOFTER is a novel benchmark introduced in the paper to evaluate OOD detection in operational settings.",
          "quote": "We present LOFTER the Language Out oF disTribution pErformance benchmaRk."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "RAINPROOF: AN UMBRELLA TO SHIELD TEXT GENERATORS FROM OUT-OF-DISTRIBUTION DATA",
          "justification": "The paper introduces LOFTER as a contribution and directly refers to its title RAINPROOF.",
          "quote": "We present LOFTER the Language Out oF disTribution pErformance benchmaRk."
        }
      },
      {
        "name": {
          "value": "Multimodal EmotionLines Dataset (MELD)",
          "justification": "MELD is one of the datasets used for evaluating dialog shifts.",
          "quote": "We rely on the Multi WOZ dataset (Zang et al., 2020), a human to human dataset collected in the Wizard-of-Oz set-up (Kelley, 1984), for IN distribution data. This choice is mostly motivated by the availability of pretrained models on Multi WOZ."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MELD: A multimodal multi-party dataset for emotion recognition in conversations.",
          "justification": "The referenced paper for MELD is cited as Poria et al., 2018.",
          "quote": "(i.e., MRDA (Shriberg et al., 2004) and Multimodal EmotionLines Dataset MELD (Poria et al., 2018))"
        }
      },
      {
        "name": {
          "value": "DailyDialog",
          "justification": "DailyDialog is used as one of the datasets for evaluating dialog systems under different distributions.",
          "quote": "(i.e., DailyDialog DyDA Li et al. (2017)), and scripted scenarii (i.e., IEMOCAP Tripathi et al. (2018))"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "DailyDialog: A manually labelled multi-turn dialogue dataset",
          "justification": "The referenced paper for DailyDialog is cited as Li et al. (2017).",
          "quote": "(i.e., DailyDialog DyDA Li et al. (2017)), and scripted scenarii (i.e., IEMOCAP Tripathi et al. (2018))"
        }
      },
      {
        "name": {
          "value": "EuroParl dataset",
          "justification": "The EuroParl dataset is used in the context of domain shifts for translation tasks.",
          "quote": "We study the shifts induced by Catalan-Spanish, Portugese-Spanish and Afrikaans-Dutch. Domain shifts, which occur when the model is exposed to a specific topic that was not seen during training, can also affect the quality of the translation."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Creating a Multilingual Parallel Corpus Using a Parallel Corpus",
          "justification": "The referenced paper for the EuroParl dataset is cited as Tiedemann (2012a).",
          "quote": "The shifts are induced by the EuroParl dataset (Tiedemann, 2012a) and EMEA (Tiedemann, 2012b) dataset."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3990,
    "prompt_tokens": 40671,
    "total_tokens": 44661
  }
}