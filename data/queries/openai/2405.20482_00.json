{
  "paper": "2405.20482.txt",
  "words": 9269,
  "extractions": {
    "title": {
      "value": "Sparsity regularization via tree-structured environments for disentangled representations",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "Sparsity regularization via tree-structured environments for disentangled representations"
    },
    "description": "The paper introduces Tree-Based Regularization (TBR), an approach for disentangled representation learning leveraging sparsity and hierarchical relationships across environments. It models sparse changes in the mapping from latent variables to target variables across related environments, particularly suitable for biological settings. Theoretical and empirical analyses demonstrate that TBR improves recovery of latent causal variables and enhances predictive performance.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes theoretical contributions, but it emphasizes empirical evaluation through simulations and application on ground-truth gene expression data.",
      "quote": "We evaluate the theory empirically with both simulations and ground-truth gene expression data."
    },
    "primary_research_field": {
      "name": {
        "value": "Representation Learning",
        "justification": "The focus of the paper is on developing methods for learning disentangled representations from data.",
        "quote": "Causal representation learning—the task of correctly mapping low-level observations to latent causal variables—could advance scientific understanding by enabling inference of latent variables such as pathway activation."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Causal Inference",
          "justification": "The paper focuses on causal representation learning and discusses inferring causal variables and estimating causal effects.",
          "quote": "To that end, this paper proposes Tree-Based Regularization (TBR), a new disentangled representation learning method that is particularly well-suited to biological settings."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Bioinformatics",
          "justification": "The proposed methods are applied to biological data such as gene expression measurements.",
          "quote": "As a running example, we consider the task of predicting a phenotype from gene expression, where we often collect data from multiple cell types or organisms that are related in known ways."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Tree-Based Regularization (TBR)",
          "justification": "The primary model introduced and evaluated in the paper is Tree-Based Regularization (TBR).",
          "quote": "To model sparse changes, we introduce Tree-Based Regularization (TBR), an objective that minimizes both prediction error and regularizes closely related environments to learn similar predictors."
        },
        "aliases": [
          "TBR"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is introduced and thoroughly evaluated within this paper.",
          "quote": "To this end, this paper proposes Tree-Based Regularization (TBR), a new disentangled representation learning method..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is evaluated empirically using simulations and real data.",
          "quote": "We evaluate the theory empirically with both simulations and ground-truth gene expression data."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of TBR against baseline methods.",
          "quote": "We compare the representations produced by TBR to those from a baseline method..."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "This is the original paper introducing the model.",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions using PyTorch for implementing the models.",
          "quote": "All models were implemented in PyTorch Paszke et al. [2019]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the standard reference for PyTorch.",
          "quote": "All models were implemented in PyTorch Paszke et al. [2019]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 942,
    "prompt_tokens": 16393,
    "total_tokens": 17335
  }
}