{
  "paper": "2304.03094.txt",
  "words": 14301,
  "extractions": {
    "title": {
      "value": "PopulAtion Parameter Averaging (PAPA)",
      "justification": "The title of the paper is explicitly mentioned.",
      "quote": "PopulAtion Parameter Averaging (PAPA)"
    },
    "description": "The paper proposes a new method called PopulAtion Parameter Averaging (PAPA) that combines multiple neural networks into one by averaging their weights, improving efficiency without sacrificing performance compared to traditional ensembling methods. PAPA leverages a population of diverse models and their weights are slowly pushed toward the population average during training.",
    "type": {
      "value": "Empirical",
      "justification": "The paper introduces a new method and tests its effectiveness through extensive experiments.",
      "quote": "With this goal in mind, we make the following contributions: ... We demonstrate in Section 4 that PAPA and its variants lead to substantial performance gains ..."
    },
    "primary_research_field": {
      "name": {
        "value": "Ensemble Learning",
        "justification": "The paper focuses on techniques to improve performance by combining multiple neural networks into one, which is a central theme in ensemble learning.",
        "quote": "Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time. To avoid these costs, multiple neural networks can be combined into one by averaging their weights."
      },
      "aliases": [
        "Ensemble Methods"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Averaging",
          "justification": "The paper discusses various methods to improve performance by averaging the weights of multiple models.",
          "quote": "To explore the case where we replace the weights by the population average every few epochs, we consider two ways of averaging the models: 1) averaging all models to make a single average model that replaces every member of the population (PAPA-all), and 2) every member of the population is replaced by averages of different random pairs of models (PAPA-2)."
        },
        "aliases": [
          "Weight Averaging"
        ]
      },
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper discusses optimization techniques related to combining and averaging models' weights to achieve better performance.",
          "quote": "Given the recent developments of techniques to improve the performance of weight averaging, we explore the idea of weight averaging to get the benefits of ensembling in a single model."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "PAPA",
          "justification": "PAPA is introduced as the main method proposed in the paper.",
          "quote": "Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging."
        },
        "aliases": [
          "PopulAtion Parameter Averaging"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "PAPA is the primary contribution of the paper.",
          "quote": "Based on this idea, we propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging."
        },
        "is_executed": {
          "value": 1,
          "justification": "The method was empirically tested, and results were provided in the paper.",
          "quote": "We demonstrate in Section 4 that PAPA and its variants lead to substantial performance gains when training small network populations (2-10 networks) from scratch with low compute (1 GPU)."
        },
        "is_compared": {
          "value": 1,
          "justification": "PAPA was numerically compared to other models, including baseline models and other PAPA variants.",
          "quote": "For the experiments, we compare PAPA variants to baseline models trained independently with no averaging during training on two different tasks: image classification and satellite image segmentation."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "PAPA is a novel contribution introduced in this paper, so there is no reference paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "PAPA-all",
          "justification": "PAPA-all is a variant of the PAPA method proposed in the paper.",
          "quote": "We propose PAPA variants that are more amenable to parallelization, where the weights of each model are rarely replaced every few epochs by i) the average weights of all models (PAPA-all) or ii) the average weights of two randomly selected models (PAPA-2)."
        },
        "aliases": [
          "PAPA-all"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "PAPA-all is a variant introduced and tested in the paper.",
          "quote": "We propose PAPA variants (PAPA-all, and PAPA-2) that average weights rarely rather than continuously; all methods increase generalization, but PAPA tends to perform best."
        },
        "is_executed": {
          "value": 1,
          "justification": "PAPA-all was empirically tested, and results were provided in the paper.",
          "quote": "We propose PAPA variants that are more amenable to parallelization, where the weights of each model are rarely replaced every few epochs by i) the average weights of all models (PAPA-all) or ii) the average weights of two randomly selected models (PAPA-2)."
        },
        "is_compared": {
          "value": 1,
          "justification": "PAPA-all was numerically compared to both PAPA and other models in the paper.",
          "quote": "In Section 4, we demonstrate that PAPA and its variants lead to substantial performance gains..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "PAPA-all is a variant introduced in this paper, so there is no reference paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "PAPA-2",
          "justification": "PAPA-2 is another variant of the PAPA method proposed in the paper.",
          "quote": "We propose PAPA variants that are more amenable to parallelization, where the weights of each model are rarely replaced every few epochs by i) the average weights of all models (PAPA-all) or ii) the average weights of two randomly selected models (PAPA-2)."
        },
        "aliases": [
          "PAPA-2"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "PAPA-2 is a variant introduced and tested in the paper.",
          "quote": "We propose PAPA variants (PAPA-all, and PAPA-2) that average weights rarely rather than continuously; all methods increase generalization, but PAPA tends to perform best."
        },
        "is_executed": {
          "value": 1,
          "justification": "PAPA-2 was empirically tested, and results were provided in the paper.",
          "quote": "We propose PAPA variants that are more amenable to parallelization, where the weights of each model are rarely replaced every few epochs by i) the average weights of all models (PAPA-all) or ii) the average weights of two randomly selected models (PAPA-2)."
        },
        "is_compared": {
          "value": 1,
          "justification": "PAPA-2 was numerically compared to both PAPA and other models in the paper.",
          "quote": "In Section 4, we demonstrate that PAPA and its variants lead to substantial performance gains..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "PAPA-2 is a variant introduced in this paper, so there is no reference paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is explicitly mentioned as one of the datasets used to evaluate the PAPA model and its variants.",
          "quote": "We also propose PAPA variants (PAPA-all, and PAPA-2) that average weights rarely rather than continuously; all methods increase generalization, but PAPA tends to perform best. PAPA reduces the performance gap between averaging and ensembling, increasing the average accuracy of a population of models by up to 0.8% on CIFAR-10..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "CIFAR-10 is a standard dataset widely used within the research community, originating from a famous paper by Alex Krizhevsky et al.",
          "quote": "For training-from-scratch on CIFAR-10 and CIFAR-100, training is done over 300 epochs with a cosine learning rate (1e-1 to 1e-4) (Loshchilov and Hutter, 2016) using SGD with a weight decay of 1e-4. Batch size is 64 and REPAIR uses 5 forward-passes."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is explicitly mentioned as one of the datasets used to evaluate the PAPA model and its variants.",
          "quote": "PAPA reduces the performance gap between averaging and ensembling, increasing the average accuracy of a population of models by ... 1.9% on CIFAR-100"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "CIFAR-100 is a standard dataset widely used within the research community, originating from a famous paper by Alex Krizhevsky et al.",
          "quote": "For training-from-scratch on CIFAR-10 and CIFAR-100, training is done over 300 epochs with a cosine learning rate (1e-1 to 1e-4) (Loshchilov and Hutter, 2016) using SGD with a weight decay of 1e-4. Batch size is 64 and REPAIR uses 5 forward-passes."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is explicitly mentioned as one of the datasets used to evaluate the PAPA model and its variants.",
          "quote": "PAPA reduces the performance gap between averaging and ensembling, ... by up to ... 1.6% on ImageNet"
        },
        "aliases": [
          "ILSVRC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "ImageNet is a standard dataset widely used within the research community, originating from a famous paper by Jia Deng et al.",
          "quote": "For training-from-scratch on ImageNet, training is done over 90 epochs with a cosine learning rate (1e-1 to 1e-4) (Loshchilov and Hutter, 2016) using SGD with a weight decay of 1e-4."
        }
      },
      {
        "name": {
          "value": "ISPRS Vaihingen",
          "justification": "The ISPRS Vaihingen dataset is explicitly mentioned as one of the datasets used to evaluate the PAPA model and its variants for image segmentation tasks.",
          "quote": "For image segmentation, we train models from scratch on ISPRS Vaihingen (Rottensteiner et al., 2012)."
        },
        "aliases": [
          "ISPRS Vaihingen 2D Semantic Labeling Dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The ISPRS benchmark on urban object classification and 3D building reconstruction",
          "justification": "The ISPRS Vaihingen dataset is well-known in the field of remote sensing and urban object classification, originating from a famous paper by Franz Rottensteiner et al.",
          "quote": "For image segmentation, we train models from scratch on ISPRS Vaihingen (Rottensteiner et al., 2012)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SGD",
          "justification": "Stochastic Gradient Descent (SGD) is explicitly mentioned as an optimization method used in training the models.",
          "quote": "For training-from-scratch on CIFAR-10 and CIFAR-100, training is done over 300 epochs with a cosine learning rate (1e-1 to 1e-4) (Loshchilov and Hutter, 2016) using SGD with a weight decay of 1e-4."
        },
        "aliases": [
          "Stochastic Gradient Descent"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "A stochastic approximation method",
          "justification": "The method originates from a famous paper by Robbins and Monro (1951).",
          "quote": "For training-from-scratch on CIFAR-10 and CIFAR-100, training is done over 300 epochs with a cosine learning rate (1e-1 to 1e-4) (Loshchilov and Hutter, 2016) using SGD with a weight decay of 1e-4."
        }
      },
      {
        "name": {
          "value": "AdamW",
          "justification": "AdamW is explicitly mentioned as an optimization method used in the fine-tuning phase.",
          "quote": "For fine-tuning, training is done over 150 epochs with a cosine learning rate (1e-4 to 1e-6) with and without restarts (every 25 epochs) (Loshchilov and Hutter, 2016) using AdamW (Kingma and Ba, 2014; Loshchilov and Hutter, 2017) with a weight decay of 1e-4."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A method for stochastic optimization",
          "justification": "The method originates from the famous paper by Diederik P. Kingma and Jimmy Ba.",
          "quote": "For fine-tuning, training is done over 150 epochs with a cosine learning rate (1e-4 to 1e-6) with and without restarts (every 25 epochs) (Loshchilov and Hutter, 2016) using AdamW (Kingma and Ba, 2014; Loshchilov and Hutter, 2017) with a weight decay of 1e-4."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2754,
    "prompt_tokens": 26979,
    "total_tokens": 29733
  }
}