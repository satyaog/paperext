{
  "paper": "Nq45xeghcL.txt",
  "words": 8878,
  "extractions": {
    "title": {
      "value": "INTELLIGENT SWITCHING IN RESET-FREE RL",
      "justification": "The title is stated clearly at the beginning of the paper.",
      "quote": "INTELLIGENT SWITCHING IN RESET-FREE RL"
    },
    "description": "The paper introduces Reset Free RL with Intelligently Switching Controller (RISC), a novel reinforcement learning algorithm designed for environments where automatic resets are not available. The paper demonstrates that RISC offers state-of-the-art performance on several challenging reset-free RL environments.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results to validate the effectiveness of the proposed RISC algorithm across various environments.",
      "quote": "In this section, we empirically analyze the performance of RISC. Specifically, we: (1) Investigate whether reverse curriculums are the best approach for reset-free RL; (2) Compare the performance of RISC to other reset-free methods on the EARL benchmark; (3) Evaluate the necessity of both; timeout-nonterminal bootstrapping and early switching for RISC with an ablation study."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on improving reinforcement learning algorithms for environments where episode resets are not feasible.",
        "quote": "Our current algorithms are designed around the ability to reset the environment, and do not transfer to settings without resets (Co-Reyes et al., 2020)."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reset-Free Reinforcement Learning",
          "justification": "The research addresses challenges in reinforcement learning where automatic environment resets are unavailable.",
          "quote": "Recent works have started to explore learning in environments where automatic resets are not available in a setting known as reset-free or autonomous RL."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Reset Free RL with Intelligently Switching Controller (RISC)",
          "justification": "RISC is the primary model introduced and evaluated in the paper.",
          "quote": "To that end, we introduce Reset Free RL with Intelligently Switching Controller (RISC)."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "RISC is a new algorithm proposed by the authors.",
          "quote": "To that end, we introduce Reset Free RL with Intelligently Switching Controller (RISC)."
        },
        "is_executed": {
          "value": true,
          "justification": "RISC is evaluated experimentally in various environments.",
          "quote": "In this section, we empirically analyze the performance of RISC."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of RISC is compared to several other reset-free RL methods.",
          "quote": "Figures 4 and 5 show the results of RISC on the EARL benchmark environments compared to previously published results."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "RISC is a novel contribution of the paper and does not reference an existing paper for its definition.",
          "quote": "To that end, we introduce Reset Free RL with Intelligently Switching Controller (RISC)."
        }
      },
      {
        "name": {
          "value": "Forward Backward RL (FBRL)",
          "justification": "FBRL is mentioned and compared against RISC as a baseline model.",
          "quote": "A simple approach to learn in this setting is Forward Backward RL (FBRL), which alternates between a forward controller that tries to accomplish the task and a reset controller that tries to recover the initial state distribution (Eysenbach et al., 2017; Han et al., 2015)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is not a contribution of the paper but is used for comparison.",
          "quote": "A simple approach to learn in this setting is Forward Backward RL (FBRL), which alternates between a forward controller that tries to accomplish the task and a reset controller that tries to recover the initial state distribution (Eysenbach et al., 2017; Han et al., 2015)."
        },
        "is_executed": {
          "value": false,
          "justification": "The model is not executed, only referenced for comparison.",
          "quote": "A simple approach to learn in this setting is Forward Backward RL (FBRL), which alternates between a forward controller that tries to accomplish the task and a reset controller that tries to recover the initial state distribution (Eysenbach et al., 2017; Han et al., 2015)."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of FBRL is compared to the RISC algorithm.",
          "quote": "We compare against the following baselines and methods: (1) Forward Backward RL (FBRL)"
        },
        "referenced_paper_title": {
          "value": "Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning",
          "justification": "The referenced paper for FBRL is cited in the article.",
          "quote": "A simple approach to learn in this setting is Forward Backward RL (FBRL), which alternates between a forward controller that tries to accomplish the task and a reset controller that tries to recover the initial state distribution (Eysenbach et al., 2017; Han et al., 2015)."
        }
      },
      {
        "name": {
          "value": "R3L",
          "justification": "R3L is used as a comparison baseline to evaluate RISC's performance.",
          "quote": "Other works explore different reset strategies, such as R3L where the reset controller tries to reach novel states (Zhu et al., 2020)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "R3L is not a contribution of the paper but is used for comparison.",
          "quote": "Other works explore different reset strategies, such as R3L where the reset controller tries to reach novel states (Zhu et al., 2020)."
        },
        "is_executed": {
          "value": false,
          "justification": "R3L is mentioned and not executed in the paper.",
          "quote": "Other works explore different reset strategies, such as R3L where the reset controller tries to reach novel states (Zhu et al., 2020)."
        },
        "is_compared": {
          "value": true,
          "justification": "R3L's performance is compared to evaluate the effectiveness of the RISC model.",
          "quote": "(2) R3L (Zhu et al., 2020), which uses a novelty based reset controller;"
        },
        "referenced_paper_title": {
          "value": "The Ingredients of Real-World Robotic Reinforcement Learning",
          "justification": "R3L's referenced paper is cited in the document.",
          "quote": "Other works explore different reset strategies, such as R3L where the reset controller tries to reach novel states (Zhu et al., 2020)."
        }
      },
      {
        "name": {
          "value": "MEDAL",
          "justification": "MEDAL is one of the methods to which RISC is compared in the experimental analysis.",
          "quote": "The performances for these methods were either sourced from publicly available numbers (Sharma et al., 2021b) or recreated with public implementations... (4) MEDAL (Sharma et al., 2022), where the reset controller learns to reset to states in the demonstration data;"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "MEDAL is not a contribution of the paper but is used for comparison.",
          "quote": "The performances for these methods were either sourced from publicly available numbers (Sharma et al., 2021b) or recreated with public implementations... (4) MEDAL (Sharma et al., 2022), where the reset controller learns to reset to states in the demonstration data;"
        },
        "is_executed": {
          "value": false,
          "justification": "MEDAL is mentioned as a comparative model but is not executed in the paper.",
          "quote": "The performances for these methods were either sourced from publicly available numbers (Sharma et al., 2021b) or recreated with public implementations... (4) MEDAL (Sharma et al., 2022), where the reset controller learns to reset to states in the demonstration data;"
        },
        "is_compared": {
          "value": true,
          "justification": "MEDAL's performance is compared against that of RISC.",
          "quote": "The performances for these methods were either sourced from publicly available numbers (Sharma et al., 2021b) or recreated with public implementations... (4) MEDAL (Sharma et al., 2022), where the reset controller learns to reset to states in the demonstration data;"
        },
        "referenced_paper_title": {
          "value": "A State-Distribution Matching Approach to Non-Episodic Reinforcement Learning",
          "justification": "The paper cites MEDAL's reference in the document.",
          "quote": "The performances for these methods were either sourced from publicly available numbers (Sharma et al., 2021b) or recreated with public implementations... (4) MEDAL (Sharma et al., 2022), where the reset controller learns to reset to states in the demonstration data;"
        }
      },
      {
        "name": {
          "value": "VapRL",
          "justification": "VapRL is another baseline model used for performance comparison with RISC.",
          "quote": "VapRL uses demonstration data to make a reset controller that builds a reverse curriculum for the forward controller (Sharma et al., 2021a)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The VapRL model is not contributed in this paper but used for comparative purposes.",
          "quote": "VapRL uses demonstration data to make a reset controller that builds a reverse curriculum for the forward controller (Sharma et al., 2021a)."
        },
        "is_executed": {
          "value": false,
          "justification": "The VapRL model is referenced but not executed in this paper.",
          "quote": "VapRL uses demonstration data to make a reset controller that builds a reverse curriculum for the forward controller (Sharma et al., 2021a)."
        },
        "is_compared": {
          "value": true,
          "justification": "VapRL is compared against the proposed RISC algorithm to demonstrate RISC's performance improvement.",
          "quote": "We compare against the following baselines and methods:... (4) VapRL (Sharma et al., 2021a)."
        },
        "referenced_paper_title": {
          "value": "Autonomous Reinforcement Learning via Subgoal Curricula",
          "justification": "The VapRL model is referenced to its corresponding paper in the document.",
          "quote": "VapRL uses demonstration data to make a reset controller that builds a reverse curriculum for the forward controller (Sharma et al., 2021a)."
        }
      },
      {
        "name": {
          "value": "IBC",
          "justification": "IBC is mentioned as a concurrent work exploring similar reset-free RL challenges.",
          "quote": "IBC, a concurrent work, uses optimal transport to create a curriculum for both the forward and reset agents without demonstrations (Kim et al., 2023)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "IBC is not a contribution of this paper and is mentioned as concurrent work.",
          "quote": "IBC, a concurrent work, uses optimal transport to create a curriculum for both the forward and reset agents without demonstrations (Kim et al., 2023)."
        },
        "is_executed": {
          "value": false,
          "justification": "IBC is mentioned, not executed.",
          "quote": "IBC, a concurrent work, uses optimal transport to create a curriculum for both the forward and reset agents without demonstrations (Kim et al., 2023)."
        },
        "is_compared": {
          "value": false,
          "justification": "IBC is mentioned as a concurrent work and not compared numerically.",
          "quote": "IBC, a concurrent work, uses optimal transport to create a curriculum for both the forward and reset agents without demonstrations (Kim et al., 2023)."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "IBC is identified as a concurrent work, so no reference paper is provided.",
          "quote": "IBC, a concurrent work, uses optimal transport to create a curriculum for both the forward and reset agents without demonstrations (Kim et al., 2023)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "EARL benchmark",
          "justification": "The EARL benchmark is used to evaluate the performance of the RISC algorithm.",
          "quote": "We evaluate our algorithm’s performance on the recently proposed EARL benchmark (Sharma et al., 2021b)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Autonomous Reinforcement Learning: Formalism and Benchmarking",
          "justification": "The paper cites the EARL benchmark reference in the document.",
          "quote": "We evaluate our algorithm’s performance on the recently proposed EARL benchmark (Sharma et al., 2021b)."
        }
      },
      {
        "name": {
          "value": "Tabletop Manipulation",
          "justification": "This environment is used as one of the test cases in the EARL benchmark.",
          "quote": "the Tabletop Manipulation environment (Sharma et al., 2021a) involves moving a mug to one of four locations with a gripper;"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Autonomous Reinforcement Learning via Subgoal Curricula",
          "justification": "The paper uses the Tabletop Manipulation environment which is detailed in a cited reference.",
          "quote": "the Tabletop Manipulation environment (Sharma et al., 2021a) involves moving a mug to one of four locations with a gripper;"
        }
      },
      {
        "name": {
          "value": "Sawyer Door",
          "justification": "This environment is part of the EARL benchmark and is used to test the proposed method.",
          "quote": "the Sawyer Door environment (Yu et al., 2019) has a sawyer robot learn to close a door;"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
          "justification": "The paper references the Sawyer Door environment which is part of the Meta-World benchmark described in a cited work.",
          "quote": "the Sawyer Door environment (Yu et al., 2019) has a sawyer robot learn to close a door;"
        }
      },
      {
        "name": {
          "value": "Sawyer Peg",
          "justification": "This environment is used as a test case for the RISC algorithm.",
          "quote": "the Sawyer Peg environment (Yu et al., 2019) has a sawyer robot learning to insert a peg into a goal location;"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
          "justification": "The paper references the Sawyer Peg environment which is part of the Meta-World benchmark described in a cited work.",
          "quote": "the Sawyer Peg environment (Yu et al., 2019) has a sawyer robot learning to insert a peg into a goal location;"
        }
      },
      {
        "name": {
          "value": "Minitaur",
          "justification": "It is one of the environments used in the EARL benchmark to evaluate the proposed method.",
          "quote": "the Minitaur environment (Coumans & Bai, 2016) is a locomotion task where a minitaur robot learns to navigate to a set of goal locations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyBullet, a Python module for physics simulation for games, robotics and machine learning",
          "justification": "The referenced paper for Minitaur is cited in the document.",
          "quote": "the Minitaur environment (Coumans & Bai, 2016) is a locomotion task where a minitaur robot learns to navigate to a set of goal locations."
        }
      },
      {
        "name": {
          "value": "4 Rooms",
          "justification": "This environment is used in the experiments to assess curriculum learning strategies.",
          "quote": "The experiments in Section 5.1 use a 4 rooms gridworld where the agent needs to go from one corner to the opposite corner."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Minimalistic Gridworld Environment for Gymnasium",
          "justification": "The paper uses the 4 Rooms environment which is detailed in a cited reference.",
          "quote": "The experiments in Section 5.1 use a 4 rooms gridworld where the agent needs to go from one corner to the opposite corner."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyBullet",
          "justification": "PyBullet is mentioned as part of the Minitaur environment setup used for evaluation.",
          "quote": "the Minitaur environment (Coumans & Bai, 2016) is a locomotion task where a minitaur robot learns to navigate to a set of goal locations."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "PyBullet, a Python module for physics simulation for games, robotics and machine learning",
          "justification": "PyBullet's referenced paper is cited in the document.",
          "quote": "the Minitaur environment (Coumans & Bai, 2016) is a locomotion task where a minitaur robot learns to navigate to a set of goal locations."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 5024,
    "prompt_tokens": 34952,
    "total_tokens": 39976
  }
}