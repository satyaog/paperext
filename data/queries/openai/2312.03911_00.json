{
  "paper": "2312.03911.txt",
  "words": 13016,
  "extractions": {
    "title": {
      "value": "IMPROVING GRADIENT-GUIDED NESTED SAMPLING FOR POSTERIOR INFERENCE",
      "justification": "The document header clearly states it.",
      "quote": "IMPROVING GRADIENT-GUIDED NESTED SAMPLING FOR POSTERIOR INFERENCE"
    },
    "description": "The paper presents a performant, general-purpose gradient-guided nested sampling algorithm (GGNS) that combines differentiable programming, Hamiltonian slice sampling, clustering, mode separation, dynamic nested sampling, and parallelization. It demonstrates the algorithm's efficacy and scalability on a variety of synthetic and real-world problems, including the capability of combining nested sampling with generative flow networks to produce high-quality samples from the posterior distribution.",
    "type": {
      "value": "empirical",
      "justification": "The paper demonstrates algorithms' performance on various tasks and uses experimental results to support its claims.",
      "quote": "We show that the GGNS method presented in this work can be used to perform inference in a wide range of problems and that it can be used to improve the performance of existing nested sampling algorithms. Furthermore, we compare our method to existing algorithms for posterior inference and show that it outperforms them, particularly when dealing with highly multimodal distributions."
    },
    "primary_research_field": {
      "name": {
        "value": "Bayesian Inference",
        "justification": "The main focus of the paper is on improving Bayesian parameter estimation and model comparison through nested sampling.",
        "quote": "From the perspective of differential programming, less attention has been paid in recent years to nested sampling (Skilling, 2006; Buchner, 2021; Ashton et al., 2022), which is a widely used algorithm for Bayesian parameter inference and model comparison."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Differentiable Programming",
          "justification": "The paper integrates differentiable programming techniques to compute gradients efficiently for the nested sampling process.",
          "quote": "From the perspective of differential programming, less attention has been paid in recent years to nested sampling."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Hamiltonian Monte Carlo",
          "justification": "The paper uses Hamiltonian slice sampling as a key component.",
          "quote": "Previous works, such as Betancourt (2011); Speagle (2020) have shown the potential of reflective slice sampling (Neal, 2003), also known as Galilean Monte Carlo (GMC) (Feroz & Skilling, 2013) or Hamiltonian slice sampling (HSS) (Zhang et al., 2016; Bloem-Reddy & Cunningham, 2016), for general-purpose sampling."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Gradient-Guided Nested Sampling",
          "justification": "The algorithm GGNS is described and developed within the paper.",
          "quote": "In this work, we combine ideas from across the nested sampling literature and learning-based samplers and create a new gradient-guided nested sampling (GGNS) algorithm."
        },
        "aliases": [
          "GGNS"
        ],
        "is_contributed": {
          "value": true,
          "justification": "GGNS is introduced as a new algorithm in this paper.",
          "quote": "In this work, we combine ideas from across the nested sampling literature and learning-based samplers and create a new gradient-guided nested sampling (GGNS) algorithm."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper experiments involve executing the GGNS algorithm.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "is_compared": {
          "value": true,
          "justification": "GGNS is compared with other nested sampling algorithms like PolyChord and dynesty.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No reference paper mentioned for GGNS since it is introduced in this paper.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PolyChord",
          "justification": "PolyChord is used as a comparative model in the paper.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "PolyChord is not developed in this paper but is used for comparison.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "is_executed": {
          "value": true,
          "justification": "PolyChord is executed to compare its performance.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "is_compared": {
          "value": true,
          "justification": "PolyChord is one of the models used for comparison with GGNS.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "referenced_paper_title": {
          "value": "POLYCHORD: next-generation nested sampling",
          "justification": "The title is found in the references section.",
          "quote": "PolyChord: nested sampling for cosmology. Monthly Notices of the Royal Astronomical Society: Letters, 450(1):L61–L65, 2015a."
        }
      },
      {
        "name": {
          "value": "dynesty",
          "justification": "dynesty is used as a comparative model in the paper.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "dynesty is not developed in this paper but is used for comparison.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "is_executed": {
          "value": true,
          "justification": "dynesty is executed to compare its performance.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "is_compared": {
          "value": true,
          "justification": "dynesty is one of the models used for comparison with GGNS.",
          "quote": "We compare the performance of gradient-guided nested sampling with two popular nested sampling algorithms, already introduced in Section 2: PolyChord and dynesty."
        },
        "referenced_paper_title": {
          "value": "dynesty: a dynamic nested sampling package for estimating Bayesian posteriors and evidences",
          "justification": "The title is found in the references section.",
          "quote": "dynesty: a dynamic nested sampling package for estimating Bayesian posteriors and evidences. Monthly Notices of the Royal Astronomical Society, 493(3):3132–3158, 2020."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Gaussian Mixture",
          "justification": "The Gaussian mixture model is used as a dataset for evaluation.",
          "quote": "We compare these methods with GGNS in two tasks, already introduced in (Hoffman et al., 2014; 2019; Zhang & Chen, 2022; Lahlou et al., 2023): The first one is the funnel distribution, which is a 10D distribution with a funnel shape. The second one is a Gaussian mixture in 2-dimension, which consists of a mixture of 9 mode-separated Gaussians."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The references given are related to descriptions of the tasks but do not specify prior papers for Gaussian Mixture.",
          "quote": "We compare these methods with GGNS in two tasks, already introduced in (Hoffman et al., 2014; 2019; Zhang & Chen, 2022; Lahlou et al., 2023): The first one is the funnel distribution, which is a 10D distribution with a funnel shape. The second one is a Gaussian mixture in 2-dimension, which consists of a mixture of 9 mode-separated Gaussians."
        }
      },
      {
        "name": {
          "value": "Funnel Distribution",
          "justification": "The funnel distribution is used as a dataset for evaluation.",
          "quote": "We compare these methods with GGNS in two tasks, already introduced in (Hoffman et al., 2014; 2019; Zhang & Chen, 2022; Lahlou et al., 2023): The first one is the funnel distribution, which is a 10D distribution with a funnel shape. The second one is a Gaussian mixture in 2-dimension, which consists of a mixture of 9 mode-separated Gaussians."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The references given are related to descriptions of the tasks but do not specify prior papers for Funnel Distribution.",
          "quote": "We compare these methods with GGNS in two tasks, already introduced in (Hoffman et al., 2014; 2019; Zhang & Chen, 2022; Lahlou et al., 2023): The first one is the funnel distribution, which is a 10D distribution with a funnel shape. The second one is a Gaussian mixture in 2-dimension, which consists of a mixture of 9 mode-separated Gaussians."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The GGNS implementation is built in PyTorch.",
          "quote": "An implementation of GGNS in PyTorch (Paszke et al., 2019), along with notebooks to reproduce the results from the experiments, is available at https://github.com/Pablo-Lemos/GGNS."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pytorch: An imperative style, high-performance deep learning library",
          "justification": "This reference is given in the paper.",
          "quote": "PyTorch (Paszke et al., 2019)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2473,
    "prompt_tokens": 24267,
    "total_tokens": 26740
  }
}