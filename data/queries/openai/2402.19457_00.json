{
  "paper": "2402.19457.txt",
  "words": 10568,
  "extractions": {
    "title": {
      "value": "COSMIC: Mutual Information for Task-Agnostic Summarization Evaluation",
      "justification": "This is the title of the paper as provided.",
      "quote": "COSMIC: Mutual Information for Task-Agnostic Summarization Evaluation"
    },
    "description": "This paper proposes COSMIC, a task-oriented evaluation approach for assessing summarizers based on their ability to generate summaries that are useful for downstream tasks. It introduces a metric based on mutual information and demonstrates its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper introduces a novel metric and evaluates it through experiments demonstrating its correlation with human judgment-based metrics and effectiveness in predicting downstream task performance.",
      "quote": "We evaluate the quality of our approach in two ways. We show that summarizers that induce a summary distribution with higher MI with the source textsâ€™ distribution are higher quality ... We examine how well MI predicts the performance of downstream tasks ..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper deals with the evaluation of text summarization models, a prominent task in Natural Language Processing.",
        "quote": "In this work, we introduce two modifications... we propose to evaluate the probability distribution of the summary induced by a summarizer."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Summarization",
          "justification": "The main focus of the paper is on evaluating summarizers and improving the methods to assess the quality of summaries generated for downstream tasks.",
          "quote": "Assessing the quality of summarizers in different settings, tasks, and datasets is critical for better understanding these models and for studying their strengths and weaknesses."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "MT5",
          "justification": "The MT5 model is mentioned as one of the models evaluated in the experiments.",
          "quote": "For instance, Clark et al. (2023) introduced six new learnt metrics by finetuning a pretrained MT5 model (Xue et al., 2021) to predict human judgement along different axes."
        },
        "aliases": [
          "MT5"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "MT5 is not introduced as a new model but is used as a part of the evaluation.",
          "quote": "For instance, Clark et al. (2023) introduced six new learnt metrics by finetuning a pretrained MT5 model (Xue et al., 2021) to predict human judgement along different axes."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no specific mention of executing MT5 in this paper.",
          "quote": "For instance, Clark et al. (2023) introduced six new learnt metrics by finetuning a pretrained MT5 model (Xue et al., 2021) to predict human judgement along different axes."
        },
        "is_compared": {
          "value": 0,
          "justification": "MT5 is mentioned as part of a referenced work, not directly compared in this paper.",
          "quote": "For instance, Clark et al. (2023) introduced six new learnt metrics by finetuning a pretrained MT5 model (Xue et al., 2021) to predict human judgement along different axes."
        },
        "referenced_paper_title": {
          "value": "MT5: A massively multilingual pre-trained text-to-text transformer",
          "justification": "The paper in which MT5 was introduced.",
          "quote": "MT5 model (Xue et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CNN/DailyMail",
          "justification": "This dataset is mentioned as one of the datasets used for evaluation in the experiments.",
          "quote": "We select three well-known summarization datasets for the English language: CNN/DailyMail (See et al., 2017; Hermann et al., 2015), XSum (Narayan et al., 2018) and MultiNews (Fabbri et al., 2019)..."
        },
        "aliases": [
          "CNN/DailyMail"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Teaching machines to read and comprehend",
          "justification": "This is the reference paper for the CNN/DailyMail dataset.",
          "quote": "CNN/DailyMail (See et al., 2017; Hermann et al., 2015)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face Transformers",
          "justification": "This is cited in the context of using pre-trained models available on the HuggingFace hub.",
          "quote": "We evaluate numerous summarizers from the HuggingFace hub, relying on different backbones, pretraining methods and finetuned on different datasets."
        },
        "aliases": [
          "HF Transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Hugging Face's Transformers: State-of-the-art Natural Language Processing",
          "justification": "This is the reference paper for the Hugging Face library cited.",
          "quote": "We evaluate numerous summarizers from the HuggingFace hub, relying on different backbones, pretraining methods and finetuned on different datasets."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1010,
    "prompt_tokens": 23969,
    "total_tokens": 24979
  }
}