{
  "paper": "cPDVjsOytS.txt",
  "words": 14296,
  "extractions": {
    "title": {
      "value": "PopulAtion Parameter Averaging (PAPA)",
      "justification": "Derived directly from the title and abstract of the paper.",
      "quote": "Title: PopulAtion Parameter Averaging (PAPA)\nAuthors: Alexia Jolicoeur-Martineau, Emy Gervais, Kilian Fatras, Yan Zhang, Simon Lacoste-Julien"
    },
    "description": "The paper introduces PopulAtion Parameter Averaging (PAPA), a method combining the efficiency of weight averaging with the generality of ensembling to improve the performance of neural networks.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental results and performance evaluations on multiple datasets, indicating it is an empirical study.",
      "quote": "We propose PAPA variants (PAPA-all, and PAPA-2) that average weights rarely rather than continuously; all methods increase generalization, but PAPA tends to perform best. PAPA reduces the performance gap between averaging and ensembling, increasing the average accuracy of a population of models by up to 0.8% on CIFAR-10, 1.9% on CIFAR-100, and 1.6% on ImageNet when compared to training independent (non-averaged) models."
    },
    "primary_research_field": {
      "name": {
        "value": "Ensemble Methods",
        "justification": "The paper primarily focuses on techniques to combine multiple models to improve performance.",
        "quote": "Ensemble methods combine the predictions of multiple models to improve performance, but they require significantly higher computation costs at inference time."
      },
      "aliases": [
        "Ensembling",
        "Model Ensembling"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper includes empirical results and performance improvements on popular computer vision datasets such as CIFAR-10, CIFAR-100, and ImageNet.",
          "quote": "We demonstrate in Section 4 that PAPA and its variants lead to substantial performance gains when training small network populations (2-10 networks) from scratch with low compute (1 GPU). Our method increases the average accuracy of the population by up to 0.8% on CIFAR-10 (5-10 networks), 1.9% on CIFAR-100 (5-10 networks), and 1.6% on ImageNet (2-3 networks)."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "PAPA",
          "justification": "The primary focus of the paper is on the PAPA model and its variants.",
          "quote": "In particular, we propose PopulAtion Parameter Averaging (PAPA) to combine the benefits of ensembling and weight averaging."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "PAPA is the primary contribution of this paper.",
          "quote": "We propose PopulAtion Parameter Averaging (PAPA): a method that combines the generality of ensembling with the efficiency of weight averaging."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes empirical results implying that the model was executed for evaluation.",
          "quote": "We demonstrate in Section 4 that PAPA and its variants lead to substantial performance gains when training small network populations (2-10 networks) from scratch with low compute (1 GPU)."
        },
        "is_compared": {
          "value": true,
          "justification": "PAPA was compared to baseline models and other methods to demonstrate performance improvements.",
          "quote": "Our method increases the average accuracy of the population by up to 0.8% on CIFAR-10, 1.9% on CIFAR-100, and 1.6% on ImageNet when compared to training independent (non-averaged) models."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no single referenced paper since PAPA is an original contribution of the current paper.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The paper uses CIFAR-10 for evaluating the performance of the proposed PAPA method.",
          "quote": "Our method increases the average accuracy of the population by up to 0.8% on CIFAR-10 (5-10 networks)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-10 is from a public dataset commonly used, typically known from Alex Krizhevsky's work.",
          "quote": "For training-from-scratch on CIFAR-10 and CIFAR-100, training is done over 300 epochs with a cosine learning rate (1e-1 to 1e-4) (Loshchilov and Hutter, 2016) using SGD with a weight decay of 1e-4. Batch size is 64 and REPAIR uses 5 forward-passes."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "The paper uses CIFAR-100 for evaluating the performance of the proposed PAPA method.",
          "quote": "Our method increases the average accuracy of the population by up to 1.9% on CIFAR-100 (5-10 networks)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-100 is from a public dataset commonly used, typically known from Alex Krizhevsky's work.",
          "quote": "For training-from-scratch on CIFAR-10 and CIFAR-100, training is done over 300 epochs with a cosine learning rate (1e-1 to 1e-4) (Loshchilov and Hutter, 2016) using SGD with a weight decay of 1e-4. Batch size is 64 and REPAIR uses 5 forward-passes."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "The paper uses ImageNet for evaluating the performance of the proposed PAPA method.",
          "quote": "Our method increases the average accuracy of the population by up to 1.6% on ImageNet (2-3 networks)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The dataset is commonly used and well-known in computer vision.",
          "quote": "For training-from-scratch on ImageNet, training is done over 90 epochs with a cosine learning rate (1e-1 to 1e-4) (Loshchilov and Hutter, 2016) using SGD with a weight decay of 1e-4. Batch size is 256 and REPAIR is not used."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1595,
    "prompt_tokens": 26161,
    "total_tokens": 27756
  }
}