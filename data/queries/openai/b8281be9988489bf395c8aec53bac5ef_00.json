{
  "paper": "b8281be9988489bf395c8aec53bac5ef.txt",
  "words": 16057,
  "extractions": {
    "title": {
      "value": "Acceleration in Policy Optimization",
      "justification": "This is the title given at the beginning of the text and the header",
      "quote": "Acceleration in Policy Optimization"
    },
    "description": "The paper proposes a unifying paradigm for accelerating policy optimization methods in reinforcement learning by integrating foresight in the policy improvement step via optimistic and adaptive updates. It introduces an optimistic policy gradient algorithm that is adaptive via meta-gradient learning and empirically investigates several design choices for acceleration.",
    "type": {
      "value": "empirical study",
      "justification": "The paper includes empirical analysis and experiments on the proposed methods.",
      "quote": "illustrative task."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The study focuses on accelerating policy optimization methods in reinforcement learning.",
        "quote": "We work towards a unifying paradigm for accelerating policy optimization methods in reinforcement learning (RL) by integrating foresight in the policy improvement step via optimistic and adaptive updates."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Policy Optimization",
          "justification": "The main focus of the paper is on improving policy optimization methods.",
          "quote": "We work towards a unifying paradigm for accelerating policy optimization methods"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Meta-gradient Learning",
          "justification": "The paper introduces an adaptive optimistic policy gradient algorithm that leverages meta-gradient learning.",
          "quote": "Leveraging theoretical insights from Sec. 3, in Sec. 3.2, we introduce an optimistic policy gradient algorithm that is adaptive via meta-gradient learning."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "STACX",
          "justification": "STACX is mentioned as an optimistic variant discussed in the empirical analysis.",
          "quote": "For example, STACX [Zahavy et al., 2020] represents an optimistic variant of Impala [Espeholt et al., 2018] and achieves a doubling of Impala’s performance on the Atari-57 suite."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "STACX is mentioned as an example of a well-known algorithm under the new paradigm proposed by the paper.",
          "quote": "For example, STACX [Zahavy et al., 2020] represents an optimistic variant of Impala [Espeholt et al., 2018] and achieves a doubling of Impala’s performance on the Atari-57 suite."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that the authors executed STACX in the scope of this paper.",
          "quote": "For example, STACX [Zahavy et al., 2020] represents an optimistic variant of Impala [Espeholt et al., 2018] and achieves a doubling of Impala’s performance on the Atari-57 suite."
        },
        "is_compared": {
          "value": 0,
          "justification": "STACX is used as an example, not as a comparison model in the experiments conducted.",
          "quote": "For example, STACX [Zahavy et al., 2020] represents an optimistic variant of Impala [Espeholt et al., 2018] and achieves a doubling of Impala’s performance on the Atari-57 suite."
        },
        "referenced_paper_title": {
          "value": "A Self-Tuning Actor-Critic Algorithm",
          "justification": "This reference provides details about the STACX algorithm.",
          "quote": "For example, STACX [Zahavy et al., 2020] represents an optimistic variant."
        }
      },
      {
        "name": {
          "value": "BMG",
          "justification": "BMG is another example of an algorithm discussed as an application of the optimistic policy gradient approach.",
          "quote": "Similarly, adding further optimistic steps in BMG [Flennerhag et al., 2021] yields another doubling of the performance relative to that of STACX."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "BMG is discussed as an existing algorithm that is further improved by the optimistic steps proposed.",
          "quote": "Similarly, adding further optimistic steps in BMG [Flennerhag et al., 2021] yields another doubling of the performance relative to that of STACX."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that BMG was executed in the experiments in this paper.",
          "quote": "Similarly, adding further optimistic steps in BMG [Flennerhag et al., 2021] yields another doubling of the performance relative to that of STACX."
        },
        "is_compared": {
          "value": 0,
          "justification": "BMG is mentioned for its empirical success, not as a numerical comparison.",
          "quote": "Similarly, adding further optimistic steps in BMG [Flennerhag et al., 2021] yields another doubling of the performance relative to that of STACX."
        },
        "referenced_paper_title": {
          "value": "Bootstrapped Meta-learning",
          "justification": "This is the reference providing details about the BMG algorithm.",
          "quote": "Similarly, adding further optimistic steps in BMG [Flennerhag et al., 2021] yields another doubling of the performance relative to that of STACX."
        }
      },
      {
        "name": {
          "value": "Impala",
          "justification": "The paper references Impala as a well-known reinforcement learning algorithm.",
          "quote": "Impala [Espeholt et al., 2018]"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Impala is a referenced algorithm and not a contribution of this paper.",
          "quote": "Impala [Espeholt et al., 2018]"
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that Impala was executed in this paper.",
          "quote": "Impala [Espeholt et al., 2018]"
        },
        "is_compared": {
          "value": 0,
          "justification": "Impala serves as a reference model, not as a comparison in the experiments.",
          "quote": "Impala [Espeholt et al., 2018]"
        },
        "referenced_paper_title": {
          "value": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
          "justification": "This is the reference providing details about the Impala algorithm.",
          "quote": "Impala [Espeholt et al., 2018]"
        }
      },
      {
        "name": {
          "value": "AlphaZero",
          "justification": "AlphaZero is mentioned as a successful application of model-based reinforcement learning.",
          "quote": "In model-based RL, algorithms with extra steps of planning, e.g., the AlphaZero family of algorithms [Silver et al., 2016a, 2017], with perfect simulators, also enjoy huge success in challenging domains"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "AlphaZero is used as an example of existing successful algorithms.",
          "quote": "In model-based RL, algorithms with extra steps of planning, e.g., the AlphaZero family of algorithms [Silver et al., 2016a, 2017], with perfect simulators, also enjoy huge success in challenging domains"
        },
        "is_executed": {
          "value": 0,
          "justification": "AlphaZero is not executed in the experiments in this paper.",
          "quote": "In model-based RL, algorithms with extra steps of planning, e.g., the AlphaZero family of algorithms [Silver et al., 2016a, 2017], with perfect simulators, also enjoy huge success in challenging domains"
        },
        "is_compared": {
          "value": 0,
          "justification": "AlphaZero is not used for numerical comparison in the paper.",
          "quote": "In model-based RL, algorithms with extra steps of planning, e.g., the AlphaZero family of algorithms [Silver et al., 2016a, 2017], with perfect simulators, also enjoy huge success in challenging domains"
        },
        "referenced_paper_title": {
          "value": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
          "justification": "This reference provides detailed information on the AlphaZero algorithm.",
          "quote": "the AlphaZero family of algorithms [Silver et al., 2016a, 2017]"
        }
      },
      {
        "name": {
          "value": "MuZero",
          "justification": "MuZero is highlighted as an advanced model-based RL algorithm that achieves superhuman performance.",
          "quote": "MuZero [Schrittwieser et al., 2019], with an adaptive model, achieves superhuman performance in challenging and visually complex domains."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "MuZero is discussed as a state-of-the-art model-based RL algorithm.",
          "quote": "MuZero [Schrittwieser et al., 2019], with an adaptive model, achieves superhuman performance in challenging and visually complex domains."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that MuZero was executed within the scope of this paper.",
          "quote": "MuZero [Schrittwieser et al., 2019], with an adaptive model, achieves superhuman performance in challenging and visually complex domains."
        },
        "is_compared": {
          "value": 0,
          "justification": "MuZero is mentioned for its properties and empirical success, not as a comparison.",
          "quote": "MuZero [Schrittwieser et al., 2019], with an adaptive model, achieves superhuman performance in challenging and visually complex domains."
        },
        "referenced_paper_title": {
          "value": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
          "justification": "This reference provides details about the MuZero algorithm.",
          "quote": "MuZero [Schrittwieser et al., 2019]"
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "JAX is used for implementation and experimentation in the empirical analysis section.",
          "quote": "The experiments were written using JAX, Haiku, and Optax [Bradbury et al., 2018, Babuschkin et al., 2020, Hennigan et al., 2020]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "JAX: composable transformations of python+ numpy programs",
          "justification": "This is the reference providing details about JAX.",
          "quote": "The experiments were written using JAX, Haiku, and Optax [Bradbury et al., 2018, Babuschkin et al., 2020, Hennigan et al., 2020]."
        }
      },
      {
        "name": {
          "value": "Haiku",
          "justification": "Haiku is mentioned as one of the libraries used in the experiments.",
          "quote": "The experiments were written using JAX, Haiku, and Optax [Bradbury et al., 2018, Babuschkin et al., 2020, Hennigan et al., 2020]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Haiku: Sonnet for JAX",
          "justification": "This is the reference providing details about Haiku.",
          "quote": "The experiments were written using JAX, Haiku, and Optax [Bradbury et al., 2018, Babuschkin et al., 2020, Hennigan et al., 2020]."
        }
      },
      {
        "name": {
          "value": "Optax",
          "justification": "Optax is used along with JAX and Haiku in the experiments.",
          "quote": "The experiments were written using JAX, Haiku, and Optax [Bradbury et al., 2018, Babuschkin et al., 2020, Hennigan et al., 2020]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Optax: composable gradient processing and optimization in JAX",
          "justification": "This is the reference providing details about Optax.",
          "quote": "The experiments were written using JAX, Haiku, and Optax [Bradbury et al., 2018, Babuschkin et al., 2020, Hennigan et al., 2020]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2974,
    "prompt_tokens": 32127,
    "total_tokens": 35101
  }
}