{
  "paper": "2306.07908.txt",
  "words": 7874,
  "extractions": {
    "title": {
      "value": "Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision",
      "justification": "This is the full title of the paper as provided at the beginning.",
      "quote": "Best-Case Retrieval Evaluation: Improving the Sensitivity of\nReciprocal Rank with Lexicographic Precision"
    },
    "description": "The paper addresses the brittleness of reciprocal rank in discriminating between high-performing systems by introducing a new evaluation method called lexicographic precision, which generalizes reciprocal rank and improves its sensitivity and robustness.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper contributes to the theoretical understanding of evaluation metrics and introduces a new preference-based evaluation method called lexicographic precision.",
      "quote": "In this paper, we contribute to the theoretical understanding\nof evaluation through a detailed study of RL1 metrics, best-case\nretrieval evaluation, and lexiprecision."
    },
    "primary_research_field": {
      "name": {
        "value": "Information Retrieval",
        "justification": "The paper primarily deals with methods and metrics for evaluating ranking systems in information retrieval.",
        "quote": "Evaluating ranking systems for users seeking exactly one relevant\nitem has a long history in information retrieval."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Evaluation Metrics",
          "justification": "The paper focuses on new evaluation metrics for ranking systems, especially in the context of information retrieval.",
          "quote": "This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to\ndistinguish. We address the lack of sensitivity of reciprocal rank by\nintroducing and connecting it to the concept of best-case retrieval,\nan evaluation method..."
        },
        "aliases": []
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "Deep Learning 2021 Track Data",
          "justification": "The paper mentions using the 2021 TREC Deep Learning track data for empirical analysis.",
          "quote": "The 2021 TREC Deep Learning track data in particular\ndemonstrates higher skew than others, confirming observations\npreviously made about saturation at top rank positions [21]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Too Many Relevants: Whither Cranfield Test Collections?",
          "justification": "The dataset is referenced in the context of observations made by Voorhees et al., 2022, which is titled 'Too Many Relevants: Whither Cranfield Test Collections?'.",
          "quote": "The 2021 TREC Deep Learning track data in particular\ndemonstrates higher skew than others, confirming observations\npreviously made about saturation at top rank positions [21]."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 537,
    "prompt_tokens": 16414,
    "total_tokens": 16951
  }
}