{
  "paper": "UdaTyy0BNB.txt",
  "words": 19817,
  "extractions": {
    "title": {
      "value": "Double Gumbel Q-Learning",
      "justification": "The paper provides a detailed outline of the Double Gumbel Q-Learning algorithm, making it the obvious title.",
      "quote": "Double Gumbel Q-Learning"
    },
    "description": "The paper proposes Double Gumbel Q-Learning, a Deep Q-Learning algorithm designed to handle both discrete and continuous control problems by accounting for heteroscedastic Gumbel noise sources. The algorithm introduces a loss function and a hyperparameter to adjust pessimism in Q-Learning, outperforming several existing methods across multiple tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents a novel algorithm and supports its effectiveness with empirical results and benchmarks against existing methods.",
      "quote": "We present a default value for our pessimism hyperparameter that enables DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D"
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on a Q-Learning algorithm applicable to reinforcement learning tasks.",
        "quote": "We show that Deep Neural Networks introduce two heteroscedastic Gumbel noise sources into Q-Learning. To account for these noise sources, we propose Double Gumbel Q-Learning, a Deep Q-Learning algorithm applicable for both discrete and continuous control."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Continuous Control",
          "justification": "The paper mentions that the Double Gumbel Q-Learning algorithm is applicable to continuous control tasks.",
          "quote": "Double Gumbel Q-Learning, a Deep Q-Learning algorithm applicable for both discrete and continuous control."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Discrete Control",
          "justification": "The paper mentions that the Double Gumbel Q-Learning algorithm is applicable to discrete control tasks.",
          "quote": "Double Gumbel Q-Learning, a Deep Q-Learning algorithm applicable for both discrete and continuous control."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Double Gumbel Q-Learning",
          "justification": "The focus of the paper is the Double Gumbel Q-Learning algorithm.",
          "quote": "Double Gumbel Q-Learning, a Deep Q-Learning algorithm"
        },
        "aliases": [
          "DoubleGum"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The Double Gumbel Q-Learning algorithm is a contribution of this paper.",
          "quote": "We propose Double Gumbel Q-Learning, a Deep Q-Learning algorithm."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results, implying that the model was executed.",
          "quote": "We present a default value for our pessimism hyperparameter that enables DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D and show that tuning this hyperparameter may further improve sample efficiency."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper benchmarks Double Gumbel Q-Learning against several other algorithms.",
          "quote": "DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D"
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The model is introduced in this paper and does not reference a previous paper for its introduction.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "DeepMind Control",
          "justification": "The dataset is used for benchmarking the Double Gumbel Q-Learning algorithm.",
          "quote": "DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D"
        },
        "aliases": [
          "DMC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepMind Control Suite",
          "justification": "The DeepMind Control dataset is part of the DeepMind Control Suite.",
          "quote": "DeepMind Control Suite"
        }
      },
      {
        "name": {
          "value": "MuJoCo",
          "justification": "The dataset is used for benchmarking the Double Gumbel Q-Learning algorithm.",
          "quote": "DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MuJoCo: A physics engine for model-based control",
          "justification": "The MuJoCo dataset is from the paper titled 'MuJoCo: A physics engine for model-based control'.",
          "quote": "Todorov, E., Erez, T., and Tassa, Y. (2012). Mujoco: A physics engine for model-based control."
        }
      },
      {
        "name": {
          "value": "MetaWorld",
          "justification": "The dataset is used for benchmarking the Double Gumbel Q-Learning algorithm.",
          "quote": "DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
          "justification": "The MetaWorld dataset is from the paper titled 'Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning'.",
          "quote": "Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., and Levine, S. (2020). Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning."
        }
      },
      {
        "name": {
          "value": "Box2D",
          "justification": "The dataset is used for benchmarking the Double Gumbel Q-Learning algorithm.",
          "quote": "DoubleGum to outperform DDPG, TD3, SAC, XQL, quantile regression, and Mixture-of-Gaussian Critics in aggregate over 33 tasks from DeepMind Control, MuJoCo, MetaWorld, and Box2D"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "OpenAI Gym",
          "justification": "The Box2D dataset is part of OpenAI Gym, a toolkit for developing and comparing reinforcement learning algorithms.",
          "quote": "Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. (2016). Openai gym."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "JAX is explicitly mentioned as one of the libraries used for implementing the algorithms in the research.",
          "quote": "All algorithms and code were implemented in JAX"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "JAX: composable transformations of Python+NumPy programs",
          "justification": "The reference paper for JAX is 'JAX: composable transformations of Python+NumPy programs'.",
          "quote": "Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., and Zhang, Q. (2018). JAX: composable transformations of Python+NumPy programs."
        }
      },
      {
        "name": {
          "value": "Haiku",
          "justification": "Haiku is explicitly mentioned as one of the libraries used for implementing the algorithms in the research.",
          "quote": "All algorithms and code were implemented in Haiku"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Haiku: Sonnet for JAX",
          "justification": "The reference paper for Haiku is 'Haiku: Sonnet for JAX'.",
          "quote": "Hennigan, T., Cai, T., Norman, T., and Babuschkin, I. (2020). Haiku: Sonnet for JAX."
        }
      },
      {
        "name": {
          "value": "cpprb",
          "justification": "cpprb is explicitly mentioned as one of the libraries used for running reinforcement learning experiments.",
          "quote": "Helper functions for running RL experiments were taken from Kostrikov (2021); Wu (2021); Hoffman et al. (2020) and Agarwal et al. (2021)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "cpprb",
          "justification": "The reference paper for cpprb is titled 'cpprb'.",
          "quote": "Yamada, H. (2019). cpprb."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1873,
    "prompt_tokens": 42364,
    "total_tokens": 44237
  }
}