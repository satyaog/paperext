{
  "paper": "2308.05260.txt",
  "words": 2274,
  "extractions": {
    "title": {
      "value": "AI4GCC - Track 3: Consumption and the Challenges of Multi-Agent RL",
      "justification": "This is the title of the paper as provided by the user and confirmed by the meta information.",
      "quote": "AI4GCC - Track 3: Consumption and the Challenges of Multi-Agent RL"
    },
    "description": "The paper discusses the potential areas for improvement in the AI4GCC competition, particularly focusing on evaluating consumption and addressing challenges in Multi-Agent Reinforcement Learning (MARL) to better simulate negotiation protocols.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves evaluating and suggesting improvements for the AI4GCC competition based on empirical evidence and experiments.",
      "quote": "we include in Table 1 the key metrics for the no negotiation baseline (before and after 10k episodes of training)"
    },
    "primary_research_field": {
      "name": {
        "value": "Multi-Agent Reinforcement Learning (MARL)",
        "justification": "The paper deals extensively with challenges and improvements related to Multi-Agent Reinforcement Learning.",
        "quote": "the lack of research on the performance of multi-agent RL (MARL)"
      },
      "aliases": [
        "MARL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Economic Policy Simulation",
          "justification": "The paper discusses integrating machine learning with traditional economic policy analysis, which falls under this sub-field.",
          "quote": "integrating machine learning with traditional economic policy analysis"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Game Theory",
          "justification": "The paper delves into game theoretic properties of outcomes from proposed negotiation protocols.",
          "quote": "the game theoretic properties of outcomes from proposed negotiation protocols"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "A2C agent",
          "justification": "The A2C agent is mentioned as part of the replication studies in the paper.",
          "quote": "using an A2C agent setup as in [Zhang et al., 2022a]."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The A2C agent is not a novel contribution of this paper but is used within it.",
          "quote": "using an A2C agent setup as in [Zhang et al., 2022a]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The A2C agent was executed within the experiments conducted in the study.",
          "quote": "using an A2C agent setup as in [Zhang et al., 2022a]."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was evaluated and possibly compared against other RL agents or baselines.",
          "quote": "using an A2C agent setup as in [Zhang et al., 2022a]."
        },
        "referenced_paper_title": {
          "value": "Ai for global climate cooperation: Modeling global climate negotiations, agreements, and long-term cooperation in rice-n",
          "justification": "This is the paper cited for the A2C agent setup.",
          "quote": "using an A2C agent setup as in [Zhang et al., 2022a]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AI Economist",
          "justification": "The AI Economist dataset was referenced for improving equality and productivity with AI-driven tax policies.",
          "quote": "S. Zheng, A. Trott, S. Srinivasa, N. Naik, M. Gruesbeck, D. C. Parkes, and R. Socher. The ai economist: Improving equality and productivity with ai-driven tax policies."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "The ai economist: Improving equality and productivity with ai-driven tax policies",
          "justification": "This is the referenced paper title for the AI Economist dataset.",
          "quote": "S. Zheng, A. Trott, S. Srinivasa, N. Naik, M. Gruesbeck, D. C. Parkes, and R. Socher. The ai economist: Improving equality and productivity with ai-driven tax policies."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "RLLib",
          "justification": "RLLib is mentioned in connection with implementing multi-agent reinforcement learning setups.",
          "quote": "As such, potentially viable negotiation protocols might be skipped over as RL agents fail to learn cooperative equilibria they induce."
        },
        "aliases": [
          "RL Library"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning with opponent-learning awareness",
          "justification": "This is one of the reference papers cited related to reinforcement learning libraries.",
          "quote": "J. N. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with opponent-learning awareness. arXiv preprint arXiv:1709.04326, 2017."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 965,
    "prompt_tokens": 4627,
    "total_tokens": 5592
  }
}