{
  "paper": "2306.12991.txt",
  "words": 5148,
  "extractions": {
    "title": {
      "value": "SPEECH EMOTION DIARIZATION: WHICH EMOTION APPEARS WHEN?",
      "justification": "This is the exact title of the paper.",
      "quote": "SPEECH EMOTION DIARIZATION: WHICH EMOTION APPEARS WHEN?"
    },
    "description": "This paper introduces the concept of Speech Emotion Diarization (SED), aiming to identify both the emotions conveyed in speech and their temporal boundaries. It proposes a new evaluation metric, the Emotion Diarization Error Rate (EDER), and introduces the Zaion Emotion Dataset (ZED), designed for fine-grained emotion recognition.",
    "type": {
      "value": "empirical",
      "justification": "The paper proposes a new task (Speech Emotion Diarization), a new dataset (Zaion Emotion Dataset), and provides evaluation results using baseline models on this task.",
      "quote": "In summary, our work brings the following contributions to the field of fine-grained speech emotion recognition..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on speech emotion recognition, which falls under Natural Language Processing.",
        "quote": "Speech Emotion Recognition (SER) typically relies on utterance-level solutions."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Speech Emotion Recognition",
          "justification": "The primary focus of the paper is on recognizing emotions in speech.",
          "quote": "Speech Emotion Recognition (SER) typically relies on utterance-level solutions."
        },
        "aliases": [
          "SER"
        ]
      },
      {
        "name": {
          "value": "Speech Processing",
          "justification": "The paper involves processing speech to detect and diarize emotions, which falls under Speech Processing.",
          "quote": "...fine-grained nature of speech emotions and to unify various fine-grained methods under a single objective..."
        },
        "aliases": [
          ""
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Wav2vec 2.0",
          "justification": "The model is used as an emotional encoder in the experiments.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model was used in the scope of the paper but was not introduced by it.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments."
        },
        "is_executed": {
          "value": true,
          "justification": "The Wav2vec 2.0 model was executed in the scope of the paper's experiments.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance was evaluated and compared with other models.",
          "quote": "The performance of the proposed baseline models is shown in Table 2. The baselines were trained with a 90/10 train-validation split of the constructed training set and were evaluated on the proposed ZED dataset."
        },
        "referenced_paper_title": {
          "value": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
          "justification": "This is the reference paper for the Wav2vec 2.0 model.",
          "quote": "[30] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” in NeurIPS, 2020."
        }
      },
      {
        "name": {
          "value": "HuBERT",
          "justification": "The model is used as an emotional encoder in the experiments.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments"
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model was used in the scope of the paper but was not introduced by it.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments"
        },
        "is_executed": {
          "value": true,
          "justification": "The HuBERT model was executed in the scope of the paper's experiments.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments"
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance was evaluated and compared with other models.",
          "quote": "The performance of the proposed baseline models is shown in Table 2. The baselines were trained with a 90/10 train-validation split of the constructed training set and were evaluated on the proposed ZED dataset."
        },
        "referenced_paper_title": {
          "value": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
          "justification": "This is the reference paper for the HuBERT model.",
          "quote": "[31] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021."
        }
      },
      {
        "name": {
          "value": "WavLM",
          "justification": "The model is used as an emotional encoder in the experiments.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model was used in the scope of the paper but was not introduced by it.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments."
        },
        "is_executed": {
          "value": true,
          "justification": "The WavLM model was executed in the scope of the paper's experiments.",
          "quote": "Large models (with about 317 million parameters) of pre-trained Wav2vec 2.0/HuBERT/WavLM were employed as emotional encoders in our experiments."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance was evaluated and compared with other models.",
          "quote": "The performance of the proposed baseline models is shown in Table 2."
        },
        "referenced_paper_title": {
          "value": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
          "justification": "This is the referenced paper for the WavLM model.",
          "quote": "[32] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Hui Wang, and Furu Wei, “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Zaion Emotion Dataset (ZED)",
          "justification": "The dataset is introduced and used for evaluation in the paper.",
          "quote": "To facilitate the evaluation of the performance and establish a common benchmark, we introduce the Zaion Emotion Dataset (ZED), an openly accessible speech emotion dataset that includes non-acted emotions recorded in real-life conditions, along with manually annotated boundaries of emotion segments within the utterance."
        },
        "aliases": [
          "ZED"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "The Zaion Emotion Dataset is a novel contribution of this paper, not referenced from another paper.",
          "quote": "The ZED dataset includes discrete categorical labels for emotional segments within each utterance, as well as manually annotated boundaries for each emotion segment."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "The paper mentions the use of SpeechBrain toolkit for open-sourcing the code and pre-trained models.",
          "quote": "We open-source the code and the pre-trained models 2 on the popular SpeechBrain toolkit [19]."
        },
        "aliases": [
          ""
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A general-purpose speech toolkit",
          "justification": "This is the referenced paper for the SpeechBrain toolkit used in the paper.",
          "quote": "[19] Mirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, et al., “Speechbrain: A general-purpose speech toolkit,” arXiv preprint arXiv:2106.04624, 2021."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1903,
    "prompt_tokens": 9509,
    "total_tokens": 11412
  }
}