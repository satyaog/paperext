{
  "paper": "2211.15457.txt",
  "words": 10446,
  "extractions": {
    "title": {
      "value": "Hypernetworks for Zero-shot Transfer in Reinforcement Learning",
      "justification": "The title is clear from the initial segment of the paper.",
      "quote": "Hypernetworks for Zero-shot Transfer in Reinforcement Learning"
    },
    "description": "This paper explains a novel approach using hypernetworks to achieve zero-shot transfer in reinforcement learning (RL). The authors propose a TD-based training objective and use data from near-optimal RL solutions for training. The method focuses on achieving strong zero-shot performance by viewing each RL algorithm as a mapping from MDP specifics to optimal value functions and policies. The researchers validate their approach empirically on continuous control tasks from DeepMind Control Suite, demonstrating significant improvements over existing methods.",
    "type": {
      "value": "Empirical",
      "justification": "The paper provides empirical validation of their proposed method on various continuous control environments, demonstrating improvements over existing methods.",
      "quote": "We empirically evaluate the effectiveness of our method for zero-shot transfer to new reward and transition dynamics on a series of continuous control tasks from DeepMind Control Suite."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper primarily deals with approaches and improvements in Reinforcement Learning by introducing hypernetworks for achieving zero-shot transfer.",
        "quote": "Our technical approach is based upon viewing each RL algorithm as a mapping from the MDP specifics to the near-optimal value function and policy and seek to approximate it with a hypernetwork that can generate near-optimal value functions and policies, given the parameters of the MDP."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Meta Learning",
          "justification": "The work relates to meta RL (Reinforcement Learning), which is a subset of Meta Learning.",
          "quote": "This work relates to meta RL, contextual RL, and transfer learning, with a particular focus on zero-shot performance at test time."
        },
        "aliases": [
          "Meta RL",
          "Meta-Reinforcement Learning"
        ]
      },
      {
        "name": {
          "value": "Contextual Reinforcement Learning",
          "justification": "The work utilizes contextual task parameters to achieve zero-shot transfer in RL.",
          "quote": "This work relates to meta RL, contextual RL, and transfer learning, with a particular focus on zero-shot performance at test time, enabled by knowledge of the task parameters (also known as context)."
        },
        "aliases": [
          "Contextual RL"
        ]
      },
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The paper emphasizes improving transfer learning by achieving zero-shot transfer to new tasks.",
          "quote": "This work relates to meta RL, contextual RL, and transfer learning, with a particular focus on zero-shot performance at test time."
        },
        "aliases": [
          "Transfer Learning"
        ]
      },
      {
        "name": {
          "value": "Continuous Control",
          "justification": "The empirical validation of the paper's approach is conducted in continuous control tasks.",
          "quote": "We perform experimental validation using several families of continuous control environments where we have parameterized the physical dynamics, the task reward, or both."
        },
        "aliases": [
          "Continuous Control"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "HyperZero",
          "justification": "HyperZero is the name given to the proposed method involving hypernetworks for zero-shot transfer in RL.",
          "quote": "We evaluate our proposed method, referred to as HyperZero (hypernetworks for zero-shot transfer) on a series of challenging continuous control tasks from DeepMind Control Suite."
        },
        "aliases": [
          "HyperZero"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The main contribution of the paper is the HyperZero method.",
          "quote": "We evaluate our proposed method, referred to as HyperZero (hypernetworks for zero-shot transfer) on a series of challenging continuous control tasks from DeepMind Control Suite."
        },
        "is_executed": {
          "value": 1,
          "justification": "The HyperZero method is executed in several continuous control environments for empirical validation.",
          "quote": "We perform experimental validation using several families of continuous control environments where we have parameterized the physical dynamics, the task reward, or both to evaluate learners."
        },
        "is_compared": {
          "value": 1,
          "justification": "HyperZero is compared against multiple baseline methods including context-conditioned policies and meta policies.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no referenced paper specifically for HyperZero as it is the method proposed in this paper.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Conditional Policy",
          "justification": "Conditional Policy is one of the baselines used in the experiments.",
          "quote": "1. Context-conditioned policy; trained to predict actions, similarly to imitation learning methods."
        },
        "aliases": [
          "Context-conditioned Policy"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The Conditional Policy is a baseline method.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning: 1. Context-conditioned policy; trained to predict actions, similarly to imitation learning methods."
        },
        "is_executed": {
          "value": 1,
          "justification": "Conditional Policy is executed and used for comparison in experimental validation.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning, including context-conditioned policy."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Conditional Policy is one of the baseline methods used for comparison.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning, including context-conditioned policy."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper for the Conditional Policy; it is a described baseline.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PEARL",
          "justification": "PEARL is another baseline method used in the experiments.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning: 4. PEARL (Rakelly et al. 2019) policy; trained to predict actions."
        },
        "aliases": [
          "PEARL"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "PEARL is a baseline method referenced from other work.",
          "quote": "4. PEARL (Rakelly et al. 2019) policy; trained to predict actions."
        },
        "is_executed": {
          "value": 1,
          "justification": "PEARL is executed and used for comparison in experimental validation.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning, including PEARL."
        },
        "is_compared": {
          "value": 1,
          "justification": "PEARL is one of the baseline methods used for comparison.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning, including PEARL."
        },
        "referenced_paper_title": {
          "value": "Efficient off-policy meta-reinforcement learning via probabilistic context variables",
          "justification": "The PEARL method references the paper by Rakelly et al. 2019.",
          "quote": "4. PEARL (Rakelly et al. 2019) policy; trained to predict actions."
        }
      },
      {
        "name": {
          "value": "TD3",
          "justification": "TD3 is used to obtain the near-optimal RL solutions for generating the dataset.",
          "quote": "We use TD3 (Fujimoto, Hoof, and Meger 2018) as the RL algorithm that is to be approximated."
        },
        "aliases": [
          "TD3"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "TD3 is an existing RL algorithm used for generating data in this paper, not a new contribution.",
          "quote": "We use TD3 (Fujimoto, Hoof, and Meger 2018) as the RL algorithm that is to be approximated."
        },
        "is_executed": {
          "value": 1,
          "justification": "TD3 is executed to train RL agents for generating datasets.",
          "quote": "We use TD3 (Fujimoto, Hoof, and Meger 2018) as the RL algorithm that is to be approximated."
        },
        "is_compared": {
          "value": 0,
          "justification": "TD3 is not a baseline for comparison but is used to obtain the near-optimal solutions.",
          "quote": "We use TD3 (Fujimoto, Hoof, and Meger 2018) as the RL algorithm that is to be approximated."
        },
        "referenced_paper_title": {
          "value": "Addressing function approximation error in actor-critic methods",
          "justification": "The TD3 algorithm is referenced from the paper by Fujimoto et al. 2018.",
          "quote": "We use TD3 (Fujimoto, Hoof, and Meger 2018) as the RL algorithm that is to be approximated."
        }
      },
      {
        "name": {
          "value": "Meta Policy",
          "justification": "Meta Policy is another baseline method used in the experiments.",
          "quote": "2. Context-conditioned meta policy; trained with MAML (Finn, Abbeel, and Levine 2017) to predict actions and evaluated for both zero-shot and few-shot transfer."
        },
        "aliases": [
          "Meta Policy"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Meta Policy is a baseline method used for empirical comparison.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning: 2. Context-conditioned meta policy; trained with MAML (Finn, Abbeel, and Levine 2017) to predict actions and evaluated for both zero-shot and few-shot transfer."
        },
        "is_executed": {
          "value": 1,
          "justification": "Meta Policy is executed and used for comparison in experimental validation.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning, including Meta Policy."
        },
        "is_compared": {
          "value": 1,
          "justification": "Meta Policy is one of the baseline methods used for comparison.",
          "quote": "We compare HyperZero against common baselines for multitask and meta learning, including Meta Policy."
        },
        "referenced_paper_title": {
          "value": "Model-agnostic meta-learning for fast adaptation of deep networks",
          "justification": "The Meta Policy is referenced from the MAML method by Finn, Abbeel, and Levine 2017.",
          "quote": "2. Context-conditioned meta policy; trained with MAML (Finn, Abbeel, and Levine 2017) to predict actions and evaluated for both zero-shot and few-shot transfer."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Generated Datasets from DeepMind Control Suite",
          "justification": "The datasets used are generated by rolling out trained policies on various DeepMind Control Suite environments.",
          "quote": "Our learning code, generated datasets, and custom continuous control environments, which are built upon DeepMind Control Suite, are publicly available at: https://sites.google.com/view/hyperzero-rl"
        },
        "aliases": [
          "DeepMind Control Suite Datasets"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "DeepMind Control Suite",
          "justification": "The datasets mentioned are derived from the DeepMind Control Suite",
          "quote": "We empirically evaluate the effectiveness of our method for zero-shot transfer to new reward and transition dynamics on a series of continuous control tasks from DeepMind Control Suite."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is likely used, as it is one of the most common deep learning frameworks for RL research.",
          "quote": "Our HyperZero implementation, as well as the full learning pipeline for zero-shot transfer learning by approximating RL solutions, will be made publicly available by the time of publication. We implemented our method in PyTorch."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper does not reference a specific paper for PyTorch.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Mujoco",
          "justification": "Mujoco is used for simulating the continuous control environments in the DeepMind Control Suite.",
          "quote": "Results were obtained using Python v3.9.12, PyTorch 1.10.1, CUDA 11.1, and Mujoco 2.1.1 (Todorov, Erez, and Tassa 2012) on Nvidia RTX A6000 GPUs."
        },
        "aliases": [
          "Mujoco"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MuJoCo: A physics engine for model-based control",
          "justification": "The referenced paper for Mujoco is its original paper by Todorov, Erez, and Tassa, 2012.",
          "quote": "Results were obtained using Python v3.9.12, PyTorch 1.10.1, CUDA 11.1, and Mujoco 2.1.1 (Todorov, Erez, and Tassa 2012) on Nvidia RTX A6000 GPUs."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3109,
    "prompt_tokens": 19967,
    "total_tokens": 23076
  }
}