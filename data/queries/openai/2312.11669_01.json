{
  "paper": "2312.11669.txt",
  "words": 22802,
  "extractions": {
    "title": {
      "value": "Prediction and Control in Continual Reinforcement Learning",
      "justification": "The title directly reflects the main focus of the paper.",
      "quote": "Prediction and Control in Continual Reinforcement Learning"
    },
    "description": "This paper proposes a novel approach for value function estimation in continual reinforcement learning by decomposing the value function into two components: a permanent value function, and a transient value function. This decomposition allows for better handling of the stability-plasticity dilemma, improving the agent's ability to retain long-term knowledge while quickly adapting to new information.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes empirical case studies and experiments validating the proposed methods' effectiveness.",
      "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31], and MinAtar environments [51]."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper addresses methodologies in reinforcement learning, which is a subset of deep learning.",
        "quote": "Deep reinforcement learning (RL) has achieved remarkable successes in complex tasks, e.g., Go"
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Continual Reinforcement Learning",
          "justification": "The paper specifically focuses on continual learning in reinforcement learning environments.",
          "quote": "Let S be the set of possible states and A the set of actions. At each timestep t, the agent takes action At ∈ A in state St according to its (stochastic) policy π... This quantity can be estimated with a function approximator parameterized by w, for example using TD learning."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "PT-TD learning",
          "justification": "The paper introduces PT-TD learning as a part of their continual reinforcement learning approach.",
          "quote": "PT-TD learning (Prediction)"
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PT-Q-learning",
          "justification": "The paper introduces PT-Q-learning as a part of their continual reinforcement learning approach.",
          "quote": "PT-Q-learning (Control)"
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Minigrid",
          "justification": "Minigrid was used to empirically validate the new reinforcement learning approaches proposed in the paper.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "JellyBeanWorld",
          "justification": "JBW was used to empirically validate the new reinforcement learning approaches proposed in the paper.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31]"
        },
        "aliases": [
          "JBW"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MinAtar",
          "justification": "MinAtar was used to empirically validate the new reinforcement learning approaches proposed in the paper.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31], and MinAtar environments [51]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SGD optimizer",
          "justification": "SGD optimizer is mentioned as one of the optimization methods used in the paper.",
          "quote": "Our approach performs fast and slow interplay at various levels...The transient value function is updated using a larger learning rate (fast) [3]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Adam optimizer",
          "justification": "Adam optimizer is mentioned as one of the optimization methods used in the paper.",
          "quote": "All the results are reported using Adam optimizer to update weights."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 837,
    "prompt_tokens": 43093,
    "total_tokens": 43930
  }
}