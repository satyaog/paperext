{
  "paper": "2306.10171.txt",
  "words": 15846,
  "extractions": {
    "title": {
      "value": "Bootstrapped Representations in Reinforcement Learning",
      "justification": "Title of the paper is stated at the beginning of the manuscript.",
      "quote": "Bootstrapped Representations in Reinforcement Learning"
    },
    "description": "This paper explores the construction of state representations in reinforcement learning (RL) through the use of auxiliary objectives, specifically bootstrapping methods. The authors provide a theoretical characterization of the state representation learned by temporal difference (TD) learning and contrast it with Monte Carlo and residual gradient algorithms. New auxiliary learning rules are designed based on the theoretical analysis, and an empirical comparison of these learning rules is conducted using classic domains such as the Four Rooms and Mountain Car. The study aims to address the gap in understanding which features these bootstrapping algorithms capture.",
    "type": {
      "value": "theoretical",
      "justification": "The focus of the paper is on providing a theoretical characterization of state representations and analyzing the effectiveness of different learning rules using theoretical insights and mathematical proofs.",
      "quote": "We describe the efficacy of these representations for policy evaluation, and use our theoretical analysis to design new auxiliary learning rules."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper's main focus is on the construction of state representations in reinforcement learning (RL) and the usage of auxiliary objectives to improve learning processes.",
        "quote": "In reinforcement learning (RL), state representations are key to dealing with large or continuous state spaces."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper deeply explores the construction and improvement of state representations, which is a central aspect of representation learning in the context of RL.",
          "quote": "To construct representations supporting these characteristics, different kinds of auxiliary tasks have thus been incorporated into the learning process."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Auxiliary Objectives",
          "justification": "The paper discusses the role and impact of auxiliary objectives in shaping the state representation learned by RL agents.",
          "quote": "To mitigate this issue, auxiliary objectives are often incorporated into the learning process and help shape the learnt state representation."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Temporal Difference Learning",
          "justification": "The paper investigates the state representations learned by temporal difference (TD) learning and contrasts them with those from other methods.",
          "quote": "We study the representations learnt by TD learning when training auxiliary tasks consisting in predicting the expected return of a fixed policy for several cumulant functions (Section 3)."
        },
        "aliases": [
          "TD Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Temporal Difference (TD) Learning",
          "justification": "The paper provides a theoretical characterization of state representations learned by TD learning and compares these to other methods.",
          "quote": "We study the representations learnt by TD learning when training auxiliary tasks consisting in predicting the expected return of a fixed policy for several cumulant functions (Section 3)."
        },
        "aliases": [
          "TD Learning"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Temporal Difference (TD) Learning is a well-established algorithm in reinforcement learning and is not introduced as a new model in this paper. The paper builds on this pre-existing model to study its theoretical aspects.",
          "quote": "In this paper, we consider one-step temporal difference learning..."
        },
        "is_executed": {
          "value": true,
          "justification": "Empirical evaluation of TD learning is mentioned in the paper, indicating that it was executed in the research.",
          "quote": "We present an empirical evaluation that supports our theoretical characterizations and show the importance of the choice of a learning rule to learn the value function in Section 5."
        },
        "is_compared": {
          "value": true,
          "justification": "TD learning is compared with Monte Carlo and residual gradient algorithms in this study.",
          "quote": "Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting."
        },
        "referenced_paper_title": {
          "value": "Learning to predict by the methods of temporal differences",
          "justification": "This is the fundamental paper introducing Temporal Difference Learning, which is referenced in the given research paper.",
          "quote": "Temporal difference (TD; Sutton, 1988) is the method of choice for these auxiliary predictions."
        }
      },
      {
        "name": {
          "value": "Monte Carlo Learning",
          "justification": "The paper contrasts state representations learned by Monte Carlo methods with those learned by TD learning.",
          "quote": "Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting."
        },
        "aliases": [
          "MC Learning"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Monte Carlo Learning is not a contribution of this paper but a well-known method in reinforcement learning.",
          "quote": "Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting."
        },
        "is_executed": {
          "value": true,
          "justification": "The empirical evaluation uses Monte Carlo methods as a comparison point.",
          "quote": "We present an empirical evaluation that supports our theoretical characterizations and show the importance of the choice of a learning rule to learn the value function in Section 5."
        },
        "is_compared": {
          "value": true,
          "justification": "Monte Carlo Learning is compared to TD Learning and residual gradient algorithms.",
          "quote": "Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting."
        },
        "referenced_paper_title": {
          "value": "Learning to predict by the methods of temporal differences",
          "justification": "This is a significant reference point for Monte Carlo methods in the context of reinforcement learning, showing its foundational presence alongside temporal difference learning.",
          "quote": "Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting."
        }
      },
      {
        "name": {
          "value": "Residual Gradient Algorithms",
          "justification": "Residual gradient algorithms are explored and compared to TD and Monte Carlo methods in this research paper.",
          "quote": "Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Residual Gradient Algorithms are not a specific contribution of this paper but part of existing reinforcement learning methods.",
          "quote": "Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting."
        },
        "is_executed": {
          "value": true,
          "justification": "The empirical evaluation includes residual gradient algorithms as a comparison.",
          "quote": "We present an empirical evaluation that supports our theoretical characterizations and show the importance of the choice of a learning rule to learn the value function in Section 5."
        },
        "is_compared": {
          "value": true,
          "justification": "Residual Gradient Algorithms are compared to TD Learning and Monte Carlo methods in this research paper.",
          "quote": "Surprisingly, we find that this representation differs from the features learned by Monte Carlo and residual gradient algorithms for most transition structures of the environment in the policy evaluation setting."
        },
        "referenced_paper_title": {
          "value": "Residual algorithms: Reinforcement learning with function approximation",
          "justification": "This is a significant reference for understanding residual gradient algorithms within reinforcement learning.",
          "quote": "Residual gradient algorithms (Baird, 1995)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Four Rooms",
          "justification": "The Four Rooms domain is explicitly mentioned as a classic domain used for empirical comparison in the paper.",
          "quote": "We complement our theoretical results with an empirical comparison of these learning rules for different cumulant functions on classic domains such as the four-room domain (Sutton et al., 1999) and Mountain Car (Moore, 1990)."
        },
        "aliases": [
          "Four-Room Domain"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
          "justification": "This is the original paper by Sutton et al. (1999) introducing the Four-Room Domain.",
          "quote": "four-room domain (Sutton et al., 1999)"
        }
      },
      {
        "name": {
          "value": "Mountain Car",
          "justification": "The Mountain Car domain is explicitly mentioned as a classic domain used for empirical comparison in the paper.",
          "quote": "We complement our theoretical results with an empirical comparison of these learning rules for different cumulant functions on classic domains such as the four-room domain (Sutton et al., 1999) and Mountain Car (Moore, 1990)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Efficient memory-based learning for robot control",
          "justification": "This is the original paper by Moore (1990) introducing the Mountain Car domain.",
          "quote": "Mountain Car (Moore, 1990)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is a popular deep learning library utilized in various reinforcement learning research, and its mention here indicates its use.",
          "quote": "The idea is to reduce the mean squared error between the approximant ψ̂ and the target values by stochastic gradient descent (SGD). Taking the gradient of L with respect to Φ and W, we obtain the semi-gradient update rule... (Paszke et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the reference paper for PyTorch, a widely used deep learning library.",
          "quote": "Paszke et al., 2019"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1949,
    "prompt_tokens": 28417,
    "total_tokens": 30366
  }
}