{
  "paper": "2307.11865.txt",
  "words": 5655,
  "extractions": {
    "title": {
      "value": "CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots",
      "justification": "This is the title as listed in the provided paper.",
      "quote": "CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots"
    },
    "description": "This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation. The research focuses on enabling robots to follow complex, conversational instructions rather than simple imperative commands. The method, CARTIER, leverages large language models to interpret user queries and directs robots accordingly, demonstrating improved performance in interpreting such instructions compared to existing methods.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results and evaluations to demonstrate the effectiveness of the proposed method, CARTIER.",
      "quote": "We then evaluate multiple approaches, of varying complexity, to construct and query the spatial language index."
    },
    "primary_research_field": {
      "name": {
        "value": "Robotic Planning and Navigation",
        "justification": "The main focus of the paper is on using language models for improving robotic navigation and planning based on conversational instructions.",
        "quote": "This paper explores the extent to which natural interaction is possible between human and robot in the context of a navigation task."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The research intersects with NLP as it involves using large language models to interpret natural, conversational queries.",
          "quote": "We focus on following complex instructions that are more akin to natural conversation than traditional explicit procedural directives typically seen in robotics."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper utilizes visual language models and object detection, highlighting its relevance to the field of Computer Vision.",
          "quote": "We make use of the visual language model CLIP ... and use an off-the-shelf object detector (EVA [30]) trained on the LVIS dataset [31]."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CARTIER",
          "justification": "CARTIER is introduced as the primary method developed and tested in this paper.",
          "quote": "We then introduce CARTIER (Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots), a pipeline that first uses an LLM to infer which object the user is referring to in the query, then uses a “spatial language index” to associate that object with a location in the scene."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The model is the main contribution of the paper.",
          "quote": "The introduction of our method, CARTIER, that can handle all three query types."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and tested in experiments as part of the research.",
          "quote": "We then evaluate multiple approaches, of varying complexity, to construct and query the spatial language index."
        },
        "is_compared": {
          "value": 1,
          "justification": "CARTIER was compared to other models in the evaluation section of the paper.",
          "quote": "We compare against two recently published baselines. Details are provided below."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This is the primary model contributed by this paper and does not reference another paper directly for its development.",
          "quote": "We then introduce CARTIER (Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AI2Thor",
          "justification": "The paper leverages the AI2Thor simulation environment for creating scenarios and testing the CARTIER model.",
          "quote": "We leverage the 3D simulator AI2Thor to create household query scenarios at scale, and augment it by adding complex language queries for 40 object types."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "AI2-THOR: An Interactive 3D Environment for Visual AI",
          "justification": "This is the paper referenced for the AI2Thor dataset.",
          "quote": "The AI2Thor simulation environment contains 189 different object types."
        }
      },
      {
        "name": {
          "value": "LVIS",
          "justification": "The paper uses an off-the-shelf object detector trained on the LVIS dataset.",
          "quote": "We make use of the visual language model CLIP [4] and use an off-the-shelf object detector (EVA [30]) trained on the LVIS dataset [31]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "LVIS: A dataset for large vocabulary instance segmentation",
          "justification": "This is the paper referenced for the LVIS dataset.",
          "quote": "a dataset for large vocabulary instance segmentation"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "ChatGPT",
          "justification": "ChatGPT is explicitly used as one of the large language models in the experiments.",
          "quote": "We query the LLM with temperature set to 0 in order to sample maximum likelihood tokens and minimize the stochasticity of our results. In our experiments, we use ChatGPT and GPT-4."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "This is the commonly referenced paper for GPT-based models including ChatGPT.",
          "quote": "In our experiments, we use ChatGPT and GPT-4 [29]."
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is explicitly used as one of the large language models in the experiments.",
          "quote": "In our experiments, we use ChatGPT and GPT-4."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "This is the commonly referenced paper for GPT-based models including GPT-4.",
          "quote": "In our experiments, we use ChatGPT and GPT-4 [29]."
        }
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "The CLIP visual language model is used for vision-based tasks in the experiments.",
          "quote": "To exploit the association between images and language, we make use of the visual language model CLIP [4]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models From Natural Language Supervision",
          "justification": "This is the paper referenced for the CLIP model.",
          "quote": "To exploit the association between images and language, we make use of the visual language model CLIP [4]."
        }
      },
      {
        "name": {
          "value": "EVA",
          "justification": "The paper uses an off-the-shelf object detector named EVA trained on the LVIS dataset.",
          "quote": "We use an off-the-shelf object detector (EVA [30]) trained on the LVIS dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale",
          "justification": "This is the paper referenced for the EVA model.",
          "quote": "We use an off-the-shelf object detector (EVA [30]) trained on the LVIS dataset."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1421,
    "prompt_tokens": 10107,
    "total_tokens": 11528
  }
}