{
  "paper": "2301.03652.txt",
  "words": 6368,
  "extractions": {
    "title": {
      "value": "On The Fragility of Learned Reward Functions",
      "justification": "This is the actual title of the paper.",
      "quote": "On The Fragility of Learned Reward Functions"
    },
    "description": "This paper focuses on the challenges of reward learning, particularly the difficulty in specifying reward functions for complex tasks. It demonstrates the potential failures in current reward learning algorithms by examining how changes in reward model design and trajectory dataset composition can affect the training of new policies. The authors propose more retraining-based evaluations to improve the reliability of learned reward functions.",
    "type": {
      "value": "empirical study",
      "justification": "The paper provides empirical evidence through experiments in different environments to study the behavior of learned reward functions.",
      "quote": "Inspired by this work, our paper empirically examines the relearner performance for learned reward functions."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper extensively discusses issues related to reward functions and reinforcement learning algorithms.",
        "quote": "Preference-based reward learning Our primary focus is on methods that learn from preference comparisons between two trajectories [2, 36, 29, 8]."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reward Learning",
          "justification": "The main topic of the paper is the learning of reward functions from human feedback and preferences.",
          "quote": "Reward learning approaches attempt to infer reward functions from human feedback and preferences."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The paper mentions the usage of algorithms such as Soft Actor-Critic (SAC) from Stable-Baselines3.",
          "quote": "Deep RL from Human Preferences We follow the framework of learning a preference model r̂φ from trajectory segment comparisons."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Soft Actor-Critic (SAC)",
          "justification": "The paper mentions the use of soft actor-critic (SAC) algorithms for continuous control settings.",
          "quote": "In the continuous control setting, we use soft actor-critic (SAC) [14] from Stable-Baselines3 [26] and the learned reward networks receive the observation, action and next observation as input."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not a contribution of the paper; it is used for performing experiments.",
          "quote": "In the continuous control setting, we use soft actor-critic (SAC) [14] from Stable-Baselines3 [26]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed in the context of the experiments discussed in the paper.",
          "quote": "In the continuous control setting, we use soft actor-critic (SAC) [14] from Stable-Baselines3 [26]"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the model is discussed and compared in the context of relearning failures.",
          "quote": "Figure 1 shows the learning curves of the sampler and relearner experiments in HalfCheetah."
        },
        "referenced_paper_title": {
          "value": "Soft actor-critic algorithms and applications",
          "justification": "This is the reference paper for Soft Actor-Critic as mentioned in the citation.",
          "quote": "In the continuous control settings, we use soft actor-critic (SAC) [14] from Stable-Baselines3 [26]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "HalfCheetah-v3",
          "justification": "The paper uses the HalfCheetah environment for experiments in the continuous control domain.",
          "quote": "First, we investigate the occurrence of relearning failures in the continuous control domain. We use HalfCheetah environment as our test bed since it has been used in past works on preference-based reward learning [8]."
        },
        "aliases": [
          "HalfCheetah"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "OpenAI Gym",
          "justification": "This is the origin of the HalfCheetah environment used in this paper.",
          "quote": "First, we investigate the occurrence of relearning failures in the continuous control domain. We use HalfCheetah environment as our test bed since it has been used in past works on preference-based reward learning [8]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stable-Baselines3",
          "justification": "The paper uses Stable-Baselines3 for implementing SAC in continuous control experiments.",
          "quote": "We use soft actor-critic (SAC) [14] from Stable-Baselines3 [26] in the locomotion control tasks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Stable-Baselines3: Reliable reinforcement learning implementations",
          "justification": "This is the source of the Stable-Baselines3 library used in the experiments.",
          "quote": "We use soft actor-critic (SAC) [14] from Stable-Baselines3 [26] in the locomotion control tasks."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1769,
    "prompt_tokens": 25580,
    "total_tokens": 27349
  }
}