{
  "paper": "2306.03831.txt",
  "words": 11137,
  "extractions": {
    "title": {
      "value": "GEO-Bench: Toward Foundation Models for Earth Monitoring",
      "justification": "It appears in the header of the paper and throughout the text.",
      "quote": "GEO-Bench: Toward Foundation Models for Earth Monitoring"
    },
    "description": "The paper proposes a benchmark called GEO-Bench aimed at developing foundation models for Earth monitoring. It includes six classification and six segmentation tasks covering various modalities and datasets. The work seeks to improve model generalization for climate-related remote sensing tasks.",
    "type": {
      "value": "empirical study",
      "justification": "The paper describes experiments and presents results for several models and discusses practical applications and performance evaluations.",
      "quote": "Finally, we present an extensive set of experiments, showing the performance of 20 state-of-the-art models on the benchmark to lay down reference points and to gain valuable information on existing pre-trained models"
    },
    "primary_research_field": {
      "name": {
        "value": "Remote Sensing and Earth Monitoring",
        "justification": "The paper focuses on developing models and benchmarks specifically for Earth observation and remote sensing tasks.",
        "quote": "To stimulate the development of foundation models for Earth monitoring, we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation."
      },
      "aliases": [
        "Earth observation",
        "Environmental monitoring"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Classification",
          "justification": "Image classification is included as one of the primary tasks in the benchmark.",
          "quote": "Our proposed benchmark, GEO-Bench1, is composed of six image classification... tasks"
        },
        "aliases": [
          "Image Recognition"
        ]
      },
      {
        "name": {
          "value": "Semantic Segmentation",
          "justification": "Semantic segmentation tasks are also a key component of the benchmark.",
          "quote": "Our proposed benchmark, GEO-Bench1, is composed of ... six semantic segmentation tasks"
        },
        "aliases": [
          "Image Segmentation",
          "Scene Parsing"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet",
          "justification": "ResNet is mentioned as being used for comparison in multiple sections of the paper.",
          "quote": "Across many of these applications, pre-trained models (e.g., a ResNet trained on ImageNet) have been used to increase generalisation performance."
        },
        "aliases": [
          "ResNet18",
          "ResNet50"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper indicates ResNet is used for comparisons, not contributed.",
          "quote": "pre-trained models (e.g., a ResNet trained on ImageNet) have been used to increase generalisation performance"
        },
        "is_executed": {
          "value": 1,
          "justification": "The ResNet models were tested and reported in the experiments.",
          "quote": "Finally, we present an extensive set of experiments, showing the performance of 20 state-of-the-art models on the benchmark to lay down reference points and to gain valuable information on existing pre-trained models."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper includes comprehensive comparison results of the ResNet models against other models.",
          "quote": "Finally, we present an extensive set of experiments, showing the performance of 20 state-of-the-art models on the benchmark to lay down reference points and to gain valuable information on existing pre-trained models."
        },
        "referenced_paper_title": {
          "value": "Deep residual learning for image recognition",
          "justification": "The foundational paper for ResNet is referenced to give credit to the origin of the model.",
          "quote": "He et al. [2016]"
        }
      },
      {
        "name": {
          "value": "ConvNeXt",
          "justification": "ConvNeXt is presented as one of the models being evaluated in the benchmark for performance.",
          "quote": "We denote the outstanding performance of ConvNext and SwinV2 compared to other models."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ConvNeXt is used for benchmarking and not contributed in this paper.",
          "quote": "In this first experiment, all models can only see the RGB channels. These results offer valuable information across 10 common baselines in the literature."
        },
        "is_executed": {
          "value": 1,
          "justification": "ConvNeXt was executed as part of the experiments.",
          "quote": "We denote the outstanding performance of ConvNext and SwinV2 compared to other models."
        },
        "is_compared": {
          "value": 1,
          "justification": "Performance of ConvNeXt is compared with other models in the benchmark.",
          "quote": "These results offer valuable information across 10 common baselines in the literature."
        },
        "referenced_paper_title": {
          "value": "A ConvNet for the 2020s",
          "justification": "It is essential to cite the original paper for ConvNeXt to provide context.",
          "quote": "Liu et al. [2022b]"
        }
      },
      {
        "name": {
          "value": "Vision Transformer (ViT)",
          "justification": "The ViT model is used for comparing different architectures in the benchmark.",
          "quote": "ViT architectures ... were used in the benchmark evaluation."
        },
        "aliases": [
          "ViT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The ViT model is used but not contributed by this paper.",
          "quote": "ViT-S: ViT architectures Dosovitskiy et al. [2020] of size tiny and small respectively;"
        },
        "is_executed": {
          "value": 1,
          "justification": "ViT models were executed as part of the experiments in the paper.",
          "quote": "ViT including the tiny and small versions were used during the experiments."
        },
        "is_compared": {
          "value": 1,
          "justification": "ViT models are compared against other architectures in the benchmark.",
          "quote": "We denote the outstanding performance of ConvNext and SwinV2 compared to other models."
        },
        "referenced_paper_title": {
          "value": "An image is worth 16x16 words: Transformers for image recognition at scale",
          "justification": "The original paper of Vision Transformer is referenced for context and credit.",
          "quote": "Dosovitskiy et al. [2020]"
        }
      },
      {
        "name": {
          "value": "Swin Transformer V2",
          "justification": "The Swin Transformer V2 model is mentioned as used in the experiments and shown to have high performance.",
          "quote": "We denote the outstanding performance of ConvNext and SwinV2 compared to other models."
        },
        "aliases": [
          "SwinV2"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses but does not introduce Swin Transformer V2.",
          "quote": "In this first experiment, all models can only see the RGB channels. These results offer valuable information across 10 common baselines in the literature."
        },
        "is_executed": {
          "value": 1,
          "justification": "Swin Transformer V2 models were executed in experiments.",
          "quote": "We denote the outstanding performance of ConvNext and SwinV2 compared to other models."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of Swin Transformer V2 with other models.",
          "quote": "In this first experiment, all models can only see the RGB channels. These results offer valuable information across 10 common baselines in the literature."
        },
        "referenced_paper_title": {
          "value": "Swin Transformer V2: Scaling Up Capacity and Resolution",
          "justification": "The foundational paper for Swin Transformer V2 is referenced as a source.",
          "quote": "Liu et al. [2022a]"
        }
      },
      {
        "name": {
          "value": "U-Net",
          "justification": "U-Net is one of the models used for semantic segmentation tasks in the experiments.",
          "quote": "ResNet18 U-Net - ResNet101 U-Net ResNet augmented with the U-Net architecture... with pre-trained weights from the timm library."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "U-Net is used for comparison in the experiments but not contributed.",
          "quote": "ResNet18 U-Net - ResNet101 U-Net ResNet augmented with the U-Net architecture Ronneberger et al. [2015] with pre-trained weights from the timm library."
        },
        "is_executed": {
          "value": 1,
          "justification": "U-Net models were executed for the segmentation tasks in the experiments.",
          "quote": "ResNet18 U-Net - ResNet101 U-Net ResNet augmented with the U-Net architecture... with pre-trained weights from the timm library."
        },
        "is_compared": {
          "value": 1,
          "justification": "U-Net models are compared with DeepLabV3 for the segmentation tasks.",
          "quote": "In this section, we report the bootstrapped IQM of the normalized Intersection over Union (IoU) for the 6 segmentation datasets. In Figure, we report the seeds for the 10 experiments."
        },
        "referenced_paper_title": {
          "value": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
          "justification": "The reference provides the origin of U-Net and is cited appropriately.",
          "quote": "Ronneberger et al. [2015]"
        }
      },
      {
        "name": {
          "value": "DeepLabV3",
          "justification": "DeepLabV3 is used for semantic segmentation tasks in the experiments and its performance is discussed.",
          "quote": "ResNet50 DeepLabV3 ResNet augmented with the DeepLabV3 architecture... with pre-trained weights from the timm library."
        },
        "aliases": [
          "DeepLab",
          "DeepLabV3+"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is utilized for comparisons but not introduced by this paper.",
          "quote": "ResNet50 DeepLabV3 ResNet augmented with the DeepLabV3 architecture... with pre-trained weights from the timm library."
        },
        "is_executed": {
          "value": 1,
          "justification": "DeepLabV3 models are executed in the various segmentation experiments.",
          "quote": "ResNet50 DeepLabV3 ResNet augmented with the DeepLabV3 architecture... with pre-trained weights from the timm library."
        },
        "is_compared": {
          "value": 1,
          "justification": "DeepLabV3 is evaluated and compared with U-Net for the segmentation tasks.",
          "quote": "In Figure, we report the bootstrapped IQM of the normalized Intersection over Union (IoU) for the 6 segmentation datasets."
        },
        "referenced_paper_title": {
          "value": "Rethinking Atrous Convolution for Semantic Image Segmentation",
          "justification": "The source paper for DeepLabV3 is appropriately cited for background.",
          "quote": "Chen et al. [2017]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "BigEarthNet",
          "justification": "BigEarthNet dataset is mentioned multiple times as one of the datasets used in the GEO-Bench benchmark.",
          "quote": "Satellite data sources such as Sentinel-2 Drusch et al. [2012], ESA [2021] and BigEarthNet Sumbul et al. [2021] provide images in multiple spectral bands with periodic revisits."
        },
        "aliases": [
          "m-bigearthnet"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "BigEarthNet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding",
          "justification": "The original paper of BigEarthNet dataset is cited.",
          "quote": "Sumbul et al. [2021]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions the use of PyTorch for various experimental setups and model training.",
          "quote": "We also provide tools based on PyTorch-Lightning Falcon and The PyTorch Lightning team [2019] to facilitate model training."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The original paper for PyTorch is referenced to give credit.",
          "quote": "Paszke et al. [2019]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2318,
    "prompt_tokens": 20325,
    "total_tokens": 22643
  }
}