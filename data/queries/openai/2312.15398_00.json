{
  "paper": "2312.15398.txt",
  "words": 9857,
  "extractions": {
    "title": {
      "value": "Fairness-Aware Structured Pruning in Transformers",
      "justification": "The title is present on the first page of the paper.",
      "quote": "Fairness-Aware Structured Pruning in Transformers"
    },
    "description": "The paper proposes a new pruning technique for transformer-based language models that aims to consider both fairness and performance. It introduces Fairness-Aware Structured Pruning (FASP), which selectively removes attention heads that negatively impact fairness while retaining those critical for language modeling performance. The findings demonstrate reduced bias across various social groups with minimal performance loss.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper reports experimental results from applying the proposed pruning method (FASP) to various language models, presenting quantitative findings on fairness and performance.",
      "quote": "This section presents an overview of our bias assessment prompts, baselines, evaluation metrics, and models used in our experiments."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The study focuses on transformer-based language models, which are a significant aspect of NLP.",
        "quote": "The increasing size of large language models (LLMs) has introduced challenges in their training and inference."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Fairness in AI",
          "justification": "The paper's primary motivation is to improve the fairness of language models by reducing bias through a novel pruning technique.",
          "quote": "We propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance, i.e. language modeling capabilities."
        },
        "aliases": [
          "Ethics in AI"
        ]
      },
      {
        "name": {
          "value": "Model Pruning",
          "justification": "The proposed method involves structured pruning of transformer model components (attention heads) to achieve the dual objectives of fairness and performance.",
          "quote": "In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models. We then propose a novel method to prune the attention heads that negatively impact fairness while retaining the heads critical for performance."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Transformer Models",
          "justification": "The research is specifically aimed at improving transformer-based models by selectively pruning their attention heads.",
          "quote": "In this work, first, we investigate how attention heads impact fairness and performance in pre-trained transformer-based language models."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "DistilGPT-2",
          "justification": "The paper includes experimental results using the DistilGPT-2 model.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not introduced in this paper but used to validate the proposed pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was used to run experiments to test the pruning method proposed in the paper.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance and bias metrics of this model are compared after applying the pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The model is a well-known pre-trained model and there is no specific paper reference here.",
          "quote": "Not applicable"
        }
      },
      {
        "name": {
          "value": "GPT-2",
          "justification": "The paper includes experimental results using the GPT-2 model.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not introduced in this paper but used to validate the proposed pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was used to run experiments to test the pruning method proposed in the paper.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance and bias metrics of this model are compared after applying the pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "referenced_paper_title": {
          "value": "Language Models are Unsupervised Multitask Learners",
          "justification": "The title of the reference paper found in the text.",
          "quote": "(Radford et al. 2019)"
        }
      },
      {
        "name": {
          "value": "GPT-Neo",
          "justification": "The paper includes experimental results using the GPT-Neo model.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not introduced in this paper but used to validate the proposed pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was used to run experiments to test the pruning method proposed in the paper.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance and bias metrics of this model are compared after applying the pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "referenced_paper_title": {
          "value": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
          "justification": "The title of the reference paper found in the text.",
          "quote": "(Black et al. 2021)"
        }
      },
      {
        "name": {
          "value": "GPT-J",
          "justification": "The paper includes experimental results using the GPT-J model.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not introduced in this paper but used to validate the proposed pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was used to run experiments to test the pruning method proposed in the paper.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance and bias metrics of this model are compared after applying the pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "referenced_paper_title": {
          "value": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
          "justification": "The title of the reference paper found in the text.",
          "quote": "(Wang and Komatsuzaki 2021)"
        }
      },
      {
        "name": {
          "value": "Llama 2",
          "justification": "The paper includes experimental results using the Llama 2 model.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not introduced in this paper but used to validate the proposed pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was used to run experiments to test the pruning method proposed in the paper.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance and bias metrics of this model are compared after applying the pruning method.",
          "quote": "Our findings demonstrate a reduction in gender bias by 19%, 19.5%, 39.5%, 34.7%, 23%, and 8% for DistilGPT-2, GPT-2, GPT-Neo of two different sizes, GPT-J, and Llama 2 models, respectively, in comparison to the biased model, with only a slight decrease in performance."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open foundation and fine-tuned chat models",
          "justification": "The title of the reference paper found in the text.",
          "quote": "(Touvron et al. 2023)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Holistic Bias Dataset",
          "justification": "The dataset is used for evaluating bias across different social groups in the study.",
          "quote": "Each head h's impact on gender bias is defined as zbias (h, S) and measured using the Holistic Bias dataset (Smith et al. 2022a)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "“I’m sorry to hear that”: Finding New Biases in Language Models with a Holistic Descriptor Dataset",
          "justification": "The title of the reference paper found in the text.",
          "quote": "(Smith et al. 2022a)"
        }
      },
      {
        "name": {
          "value": "WikiText-2",
          "justification": "The dataset is used for evaluating the language modeling performance of the models in the study.",
          "quote": "Using the effect of removal of a model component as a proxy of its influence on the model’s output has been employed in previous studies (Rotman, Feder, and Reichart 2021). However, it is important to note that the effect of removing multiple heads is not equivalent to the sum of the effects of each head removed individually due to the non-linearity of the model. Notwithstanding, our experimental results indicate that such simplification is a practical and effective way of estimating the impact of attention heads."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pointer Sentinel Mixture Models",
          "justification": "The title of the reference paper found in the text.",
          "quote": "(Merity et al. 2017)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3738,
    "prompt_tokens": 17620,
    "total_tokens": 21358
  }
}