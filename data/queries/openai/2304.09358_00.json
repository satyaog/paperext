{
  "paper": "2304.09358.txt",
  "words": 7790,
  "extractions": {
    "title": {
      "value": "Investigating the Nature of 3D Generalization in Deep Neural Networks",
      "justification": "The title is explicitly mentioned at the beginning of the research paper",
      "quote": "Investigating the Nature of 3D Generalization in Deep Neural Networks"
    },
    "description": "This paper characterizes the ability of common deep learning architectures to generalize to novel 3D views of objects. It formulates this as a supervised classification task where labels correspond to unique 3D objects, and examples correspond to 2D views of these objects at different orientations. The paper presents new synthetic datasets and investigates mechanisms of 3D generalization in popular deep learning models like ResNets, VGG, and ViTs.",
    "type": {
      "value": "Empirical Study",
      "justification": "This work primarily involves experiments to observe the generalization capabilities of various deep learning models and introduces new datasets to support these experiments.",
      "quote": "In this paper, we aim to understand the mechanisms responsible for generalization in deep networks by evaluating how far a model can generalize by learning on a limited number of training views for a given 3D object. 2D views of objects are generated via rotations around different axes."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary focus of this study is on visual object recognition and generalization to novel 3D views, which are key aspects of computer vision.",
        "quote": "Modern deep learning architectures for object recognition generalize well to novel views, but the mechanisms are not well understood."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Object Recognition",
          "justification": "The paper focuses on understanding how object recognition systems can generalize from 2D training views to novel 3D views.",
          "quote": "Visual object recognition systems need to generalize from a set of 2D training views to novel views."
        },
        "aliases": [
          ""
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet",
          "justification": "The study uses ResNet architecture to evaluate 3D generalization capabilities.",
          "quote": "We always train on rotations along the y-axis and evaluate on rotations along all three axes, including simultaneous rotations along two axes. We default to using ResNet-18 for our experiments unless mentioned otherwise."
        },
        "aliases": [
          "ResNet-18"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet is a pre-existing model used in the experiments but not contributed by this paper.",
          "quote": "We default to using ResNet-18 for our experiments unless mentioned otherwise."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed in the experimental validations of the paper.",
          "quote": "We always train on rotations along the y-axis and evaluate on rotations along all three axes, including simultaneous rotations along two axes."
        },
        "is_compared": {
          "value": 1,
          "justification": "ResNet was compared with other models numerically in the experiments.",
          "quote": "We show that these results are consistent across different input representations, architectures (ResNets, VGG, and ViTs) as well as real-world 3D objects (3D models of chairs [6]), highlighting that these results are not an artifact of the chosen representation, 3D object or architecture."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "The referenced paper is the original paper describing the ResNet architecture.",
          "quote": "We default to using ResNet-18 for our experiments unless mentioned otherwise."
        }
      },
      {
        "name": {
          "value": "VGG-11",
          "justification": "The VGG-11 architecture was used to evaluate 3D generalization capabilities.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        },
        "aliases": [
          "VGG-11 with BN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "VGG-11 is a pre-existing model utilized in the research but not contributed by the paper.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments to analyze its performance.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        },
        "is_compared": {
          "value": 1,
          "justification": "VGG-11 was compared numerically with other models like ResNet and ViT.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        },
        "referenced_paper_title": {
          "value": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
          "justification": "The referenced paper is the original paper describing the VGG architecture.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        }
      },
      {
        "name": {
          "value": "ViT",
          "justification": "The Vision Transformer (ViT) model was included to analyze its 3D generalization capabilities.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        },
        "aliases": [
          "ViT-B/16"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ViT is an existing model employed in the study but not developed within it.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        },
        "is_executed": {
          "value": 1,
          "justification": "ViT was executed as part of the research experiments to evaluate performance.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        },
        "is_compared": {
          "value": 1,
          "justification": "The ViT model's performance was compared numerically to other models like ResNet and VGG.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        },
        "referenced_paper_title": {
          "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "justification": "The referenced paper is the original work outlining the Vision Transformer model.",
          "quote": "In Fig. 7, we additionally show results for VGG-11 (w/ BN) [22] and ViT-B/16 [10]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Paperclip Dataset",
          "justification": "The dataset was created for the experiments in this paper and is used extensively to analyze the 3D generalization capabilities of models.",
          "quote": "The paperclip dataset is comprised of 10,000 synthetically generated 3D paperclip models."
        },
        "aliases": [
          ""
        ],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The paperclip dataset was generated specifically for this research and was not based on a pre-existing paper.",
          "quote": "The paperclip dataset is comprised of 10,000 synthetically generated 3D paperclip models."
        }
      },
      {
        "name": {
          "value": "3D Chairs Dataset from ShapeNet",
          "justification": "ShapeNet's 3D chair models were used in the experiments to study 3D object generalization. The dataset was adapted for this study.",
          "quote": "We use the same generation protocol as for the paperclips dataset but use the 3D models of chairs from ShapeNet [6] instead of synthetic Paperclips."
        },
        "aliases": [
          ""
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ShapeNet: An Information-Rich 3D Model Repository",
          "justification": "The referenced paper is the original work describing the ShapeNet dataset used in the experiments.",
          "quote": "We use the same generation protocol as for the paperclips dataset but use the 3D models of chairs from ShapeNet [6] instead of synthetic Paperclips."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1692,
    "prompt_tokens": 13560,
    "total_tokens": 15252
  }
}