{
  "paper": "2304.13765.txt",
  "words": 3864,
  "extractions": {
    "title": {
      "value": "Towards Ethical Multimodal Systems",
      "justification": "The title is explicitly mentioned at the start of the document.",
      "quote": "Towards Ethical Multimodal Systems"
    },
    "description": "This paper addresses the ethics of multimodal AI systems, especially those incorporating both text and images. It discusses creating an ethical multimodal dataset, evaluating a specific model (MAGMA), and developing algorithms to automate the ethical assessment of AI system responses.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves the creation of a dataset and the empirical evaluation of algorithms to assess ethicality in multimodal AI systems.",
      "quote": "Then, using this database, we develop algorithms, including a RoBERTalarge classifier and a multilayer perceptron, to automatically assess the ethicality of system responses."
    },
    "primary_research_field": {
      "name": {
        "value": "AI Ethics",
        "justification": "The paper focuses on evaluating the ethics of AI systems, particularly multimodal ones.",
        "quote": "This paper focuses on evaluating the ethics of multimodal AI systems involving both text and images."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "Part of the research involves text-based AI models and combines them with image processing for a multimodal approach.",
          "quote": "Our focus is on multimodal input, combining text and images."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The research involves image processing as part of the multimodal systems.",
          "quote": "Our focus is on multimodal input, combining text and images."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "MAGMA",
          "justification": "The paper evaluates the MAGMA model explicitly for ethical considerations.",
          "quote": "In this paper, we present our results for the MAGMA model by Eichenberg et al. [2022]."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model was not introduced in this paper but used for evaluation.",
          "quote": "In this paper, we present our results for the MAGMA model by Eichenberg et al. [2022]."
        },
        "is_executed": {
          "value": true,
          "justification": "The MAGMA model was executed as part of the evaluation in this study.",
          "quote": "In this paper, we present our results for the MAGMA model by Eichenberg et al. [2022]."
        },
        "is_compared": {
          "value": true,
          "justification": "The MAGMA model’s responses are compared to ethical standards in the study.",
          "quote": "MAGMA can sometimes generate responses not well-aligned with common ethical standard, thus serving as a motivation for our line of work."
        },
        "referenced_paper_title": {
          "value": "\"magma – multimodal augmentation of generative models through adapter-based finetuning\"",
          "justification": "It is the referenced paper for the MAGMA model, clearly cited in the document.",
          "quote": "MAGMA model by Eichenberg et al. [2022]."
        }
      },
      {
        "name": {
          "value": "GPT-J",
          "justification": "The paper mentions GPT-J as part of the MAGMA architecture.",
          "quote": "MAGMA is based on the CLIP visual encoder (Radford et al. [2021]) and the GPT-J language model (Ben Wang and Aran Komatsuzaki [2021])."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is used as part of the MAGMA architecture, not an original contribution.",
          "quote": "MAGMA is based on the CLIP visual encoder (Radford et al. [2021]) and the GPT-J language model (Ben Wang and Aran Komatsuzaki [2021])."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed as part of the MAGMA architecture in the study.",
          "quote": "MAGMA is based on the CLIP visual encoder (Radford et al. [2021]) and the GPT-J language model (Ben Wang and Aran Komatsuzaki [2021])."
        },
        "is_compared": {
          "value": false,
          "justification": "The model itself is not compared but used within MAGMA for evaluation.",
          "quote": "MAGMA is based on the CLIP visual encoder (Radford et al. [2021]) and the GPT-J language model (Ben Wang and Aran Komatsuzaki [2021])."
        },
        "referenced_paper_title": {
          "value": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
          "justification": "It is the referenced paper for the GPT-J model, clearly cited in the document.",
          "quote": "MAGMA is based on the CLIP visual encoder (Radford et al. [2021]) and the GPT-J language model (Ben Wang and Aran Komatsuzaki [2021])."
        }
      },
      {
        "name": {
          "value": "RoBERTa-large classifier",
          "justification": "RoBERTa-large classifier is evaluated as an ethicality classifier in the study.",
          "quote": "To start, we utilized the RoBERTa-large common-sense classifier introduced in Hendrycks et al. [2020] as it exhibited promising performance in their research, achieving a 63.4% accuracy on the hard test set."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is used for evaluation and not introduced in this paper.",
          "quote": "To start, we utilized the RoBERTa-large common-sense classifier introduced in Hendrycks et al. [2020] as it exhibited promising performance in their research."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed to classify ethical responses in this study.",
          "quote": "Given that this classifier exclusively processes text, we concatenated the question and answer from our database into a prompt. The classifier assigns each prompt a score between 0 and 1, reflecting its confidence in the statement’s morality."
        },
        "is_compared": {
          "value": true,
          "justification": "The RoBERTa-large classifier's performance is compared with other approaches.",
          "quote": "Using this, our classification accuracy, when compared to the users' most voted response, was 52%."
        },
        "referenced_paper_title": {
          "value": "Aligning AI With Shared Human Values",
          "justification": "It is the referenced paper for RoBERTa-large classifier, cited in the document.",
          "quote": "To start, we utilized the RoBERTa-large common-sense classifier introduced in Hendrycks et al. [2020]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Multimodal Ethical Dataset",
          "justification": "It is explicitly mentioned as a contribution of the paper.",
          "quote": "Our first contribution is a dataset consisting of 789 question and image pairs covering various ethical fields, including economics, medicine, society and discrimination."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "None",
          "justification": "The dataset is original to this study and not based on any previously referenced paper.",
          "quote": "Our first contribution is a dataset consisting of 789 question and image pairs covering various ethical fields, including economics, medicine, society and discrimination."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Discord API",
          "justification": "The paper discusses using Discord as part of their crowd-sourcing evaluation pipeline.",
          "quote": "The biggest platform with no ban on algorithmically generated content is Discord. Hence, this is the platform that was chosen."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "None",
          "justification": "The paper does not reference any particular paper for the Discord API.",
          "quote": "The biggest platform with no ban on algorithmically generated content is Discord. Hence, this is the platform that was chosen."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1556,
    "prompt_tokens": 7308,
    "total_tokens": 8864
  }
}