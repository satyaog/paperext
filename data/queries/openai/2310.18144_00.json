{
  "paper": "2310.18144.txt",
  "words": 8886,
  "extractions": {
    "title": {
      "value": "IMPROVING INTRINSIC EXPLORATION BY CREATING STATIONARY OBJECTIVES",
      "justification": "The title is clearly stated at the top of the paper.",
      "quote": "IMPROVING INTRINSIC EXPLORATION BY CREATING STATIONARY OBJECTIVES"
    },
    "description": "This paper presents the SOFE (Stationary Objectives for Exploration) framework designed to transform non-stationary intrinsic exploration rewards into stationary ones in reinforcement learning. The paper empirically shows that SOFE improves exploration in various challenging tasks and environments by using augmented state representations.",
    "type": {
      "value": "empirical study",
      "justification": "The paper includes experimental evaluations to demonstrate the efficacy of the proposed framework in various environments and tasks.",
      "quote": "We evaluate the empirical performance of SOFE in different exploration modalities and show that SOFE enables learning better exploration policies."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary focus of the research is on intrinsic exploration within reinforcement learning tasks and environments.",
        "quote": "Exploration bonuses in reinforcement learning guide long-horizon exploration by defining custom intrinsic objectives."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Exploration in Reinforcement Learning",
          "justification": "The research mainly deals with improving exploration strategies in reinforcement learning tasks.",
          "quote": "Exploration in RL Exploration is a central challenge in RL."
        },
        "aliases": [
          "Exploration in RL"
        ]
      },
      {
        "name": {
          "value": "Optimization of Intrinsic Rewards",
          "justification": "The paper discusses the complexity and optimization of non-stationary intrinsic rewards in reinforcement learning.",
          "quote": "The non-stationarity of count-based rewards induces a partially observable MDP (POMDP), as the dynamics of the reward distribution are unobserved by the agent."
        },
        "aliases": [
          "Intrinsic Rewards Optimization"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SOFE (Stationary Objectives for Exploration)",
          "justification": "The SOFE framework is the key contribution of the paper and is prominently discussed and evaluated.",
          "quote": "For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework."
        },
        "aliases": [
          "SOFE"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The SOFE framework is introduced and evaluated as a novel contribution in the paper.",
          "quote": "For this purpose, we introduce the Stationary Objectives For Exploration (SOFE) framework."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results obtained from executing the SOFE framework in various tasks and environments.",
          "quote": "We evaluate the empirical performance of SOFE in different exploration modalities and show that SOFE enables learning better exploration policies."
        },
        "is_compared": {
          "value": 1,
          "justification": "The SOFE framework's performance is compared with other exploration methods like count-based methods and E3B (Elliptical Bonuses) in the paper.",
          "quote": "We show that SOFE improves the performance of several exploration objectives, including count-based bonuses, pseudo-counts, and state-entropy maximization."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "SOFE is a novel framework introduced in the current paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MiniHack-MultiRoom-N6-v0",
          "justification": "The paper evaluates the performance of SOFE using the MiniHack-MultiRoom-N6-v0 environment, which provides pixel and natural language observations.",
          "quote": "We use the Multiroom-N6 task from the Minihack suite (Samvelyan et al., 2021) to evaluate the performance of E3B and our proposed augmentation, as originally used in Henaff et al. (2022)."
        },
        "aliases": [
          "MiniHack"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research",
          "justification": "The dataset/environment is referenced to a paper that introduces it.",
          "quote": "We use the Multiroom-N6 task from the Minihack suite (Samvelyan et al., 2021) to evaluate the performance of E3B and our proposed augmentation, as originally used in Henaff et al. (2022)."
        }
      },
      {
        "name": {
          "value": "Procgen-Maze",
          "justification": "The Procgen-Maze environment is used to evaluate the performance of SOFE in procedurally generated mazes.",
          "quote": "We use the Procgen-Maze task from the Procgen benchmark (Cobbe et al., 2020) to evaluate the performance of E3B and our proposed augmentation."
        },
        "aliases": [
          "Procgen"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Leveraging Procedural Generation to Benchmark Reinforcement Learning",
          "justification": "The dataset/environment is referenced to a paper that introduces it.",
          "quote": "We use the Procgen-Maze task from the Procgen benchmark (Cobbe et al., 2020) to evaluate the performance of E3B and our proposed augmentation."
        }
      },
      {
        "name": {
          "value": "DeepSea",
          "justification": "DeepSea is used as an environment to evaluate the performance of SOFE in hard-exploration tasks with sparse rewards.",
          "quote": "We evaluate whether SOFE enables better optimization of the joint objective of intrinsic and task rewards. Furthermore, we use the DeepSea sparse-reward hard-exploration task from the DeepMind suite (Osband et al., 2019)."
        },
        "aliases": [
          "DeepSea Environment"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Behaviour Suite for Reinforcement Learning",
          "justification": "The dataset/environment is referenced to a paper that introduces it.",
          "quote": "Furthermore, we use the DeepSea sparse-reward hard-exploration task from the DeepMind suite (Osband et al., 2019)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stable-Baselines3",
          "justification": "Stable-Baselines3 is mentioned in the context of running the experiments for various reinforcement learning algorithms.",
          "quote": "We use Stable-Baselines3 (Raffin et al., 2021) to run our experiments in the mazes, Godot maps, and DeepSea."
        },
        "aliases": [
          "SB3"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Stable-Baselines3: Reliable Reinforcement Learning Implementations",
          "justification": "The library is referenced to a paper that introduces it.",
          "quote": "We use Stable-Baselines3 (Raffin et al., 2021) to run our experiments in the mazes, Godot maps, and DeepSea."
        }
      },
      {
        "name": {
          "value": "Godot Engine",
          "justification": "The Godot game engine is utilized to create the 3D world used in the experiments.",
          "quote": "We use the Godot game engine to design the 3D world used in Section 5.1, which we open-source together with the code."
        },
        "aliases": [
          "Godot"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Godot Engine is a widely known game engine and not typically referenced to a specific academic paper.",
          "quote": "We use the Godot game engine to design the 3D world used in Section 5.1, which we open-source together with the code."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1469,
    "prompt_tokens": 16335,
    "total_tokens": 17804
  }
}