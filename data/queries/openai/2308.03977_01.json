{
  "paper": "2308.03977.txt",
  "words": 17241,
  "extractions": {
    "title": {
      "value": "PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning",
      "justification": "The title is clearly stated on the first page of the provided document.",
      "quote": "PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning"
    },
    "description": "The paper introduces a new generation of photorealistic synthetic datasets called PUG (Photorealistic Unreal Graphics), designed for representation learning research. Leveraging the Unreal Engine, these datasets provide granular control over various factors like pose, background, size, texture, and lighting, aiming to bridge the gap between synthetic and real image data. The paper discusses the creation of four key datasets—PUG: Animals, PUG: ImageNet, PUG: SPAR, and PUG: AR4T—demonstrating their utility in evaluating and fine-tuning vision-language models, as well as assessing model robustness and out-of-distribution generalization.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper extensively discusses the implementation of synthetic datasets and their applications in various experimental settings, demonstrating empirical results.",
      "quote": "In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on the creation and utilization of synthetic image datasets for representation learning in vision models.",
        "quote": "We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Synthetic Data for Representation Learning",
          "justification": "The paper specifically addresses the use of synthetic datasets to enhance representation learning, focusing on aspects such as photorealism and controllability.",
          "quote": "In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and realism."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet50",
          "justification": "ResNet50 is clearly mentioned in the context of experiments evaluating model robustness to various factors.",
          "quote": "In Figure 3, we present our results training a ResNet50 with different held out factors."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "The paper discusses leveraging CLIP for evaluating vision-language models.",
          "quote": "We feed images and their corresponding captions to 9 pretrained vision-language models including multiple CLIP models."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DINOv2",
          "justification": "DINOv2 is mentioned in the context of assessing model robustness across various factors.",
          "quote": "We also provide a collection of objects with mappings to classes in the popular ImageNet dataset, enabling researchers to probe the robustness of SoTA vision models without retraining, such as DINOv2."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BLIP",
          "justification": "BLIP is mentioned in the context of evaluating vision-language models similar to other models like CLIP.",
          "quote": "First, we feed images and their corresponding captions to 9 pretrained vision-language models including multiple CLIP models Radford et al. [2021], NegCLIP Yuksekgonul et al. [2023] Flava Singh et al. [2022], BLIP Li et al. [2022b] and X-VLM Zeng et al. [2021] and collect their embeddings of PUG: Animals images and created captions."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Swin-B",
          "justification": "Swin-B is discussed in the context of robustness evaluations on the PUG: ImageNet dataset.",
          "quote": "For example, the pretrained ViT-B32 trained on ImageNet-21k is better on the ImageNet validation set compared to a Swin-B, but offers worse robustness across all factors."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "PUG: Animals",
          "justification": "PUG: Animals is one of the primary datasets introduced and used for various experiments in the paper.",
          "quote": "We present PUG: Animals for research on out-of-distribution (OOD) generalization and to study the representational space of foundation models."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PUG: ImageNet",
          "justification": "The paper introduces PUG: ImageNet specifically as an additional test set for robustness evaluations.",
          "quote": "We introduce PUG: ImageNet as an additional robustness test set to ImageNet, containing a rich set of factor changes such as pose, background, size, texture, and lighting."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PUG: SPAR",
          "justification": "PUG: SPAR is introduced for evaluating vision-language models.",
          "quote": "We introduce PUG: SPAR for evaluating vision-language models. We use it to demonstrate how synthetic data can be utilized to address known benchmark limitations."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PUG: AR4T",
          "justification": "The paper discusses PUG: AR4T as a dataset for fine-tuning vision-language models.",
          "quote": "In addition, we introduce PUG: AR4T for fine-tuning vision-language models and use it to demonstrate the reliability of PUG: SPAR in contrast to other benchmarks."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is frequently mentioned as a standard dataset for training and evaluating models.",
          "quote": "The main purpose of this dataset is to provide a novel useful benchmark, paralleling ImageNet, but for fine-grained evaluation of the robustness of image classifiers, along several factors of variation."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TorchMultiverse",
          "justification": "TorchMultiverse is introduced as a Python library used for creating and manipulating the PUG datasets.",
          "quote": "In addition to pre-rendered static image datasets, we also introduce the TorchMultiverse python library, which offers a simple python interface to enable easily controlled dataset creation from any given PUG environment."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1514,
    "prompt_tokens": 31238,
    "total_tokens": 32752
  }
}