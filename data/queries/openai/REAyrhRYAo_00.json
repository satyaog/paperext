{
  "paper": "REAyrhRYAo.txt",
  "words": 11028,
  "extractions": {
    "title": {
      "value": "Gradient Masked Averaging for Federated Learning",
      "justification": "The title is clearly mentioned at the beginning and the header of each page of the paper.",
      "quote": "Gradient Masked Averaging for Federated Learning"
    },
    "description": "This paper presents \"gradient masked averaging\" (GMA) as a new aggregation technique in Federated Learning (FL). The goal of GMA is to address the issue of heterogeneous client data in FL by focusing on the invariant mechanisms across clients and ignoring spurious mechanisms, thereby improving the generalization of the global model. The technique can replace the standard averaging method in existing FL algorithms. Extensive experiments show that GMA consistently improves the performance of FL approaches.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts extensive experiments on multiple FL algorithms with various datasets, demonstrating the practical advantages of the proposed GMA technique.",
      "quote": "We perform extensive experiments on multiple FL algorithms with in-distribution, real-world, feature-skewed out-of-distribution, and quantity imbalanced datasets and show that it provides consistent improvements, particularly in the case of heterogeneous clients."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The research focuses on improving the aggregation techniques in Federated Learning, aiming to better handle data heterogeneity across clients.",
        "quote": "Federated Learning (FL) is an emerging paradigm that permits a large number of clients with heterogeneous data to coordinate learning of a unified global model without the need to share data amongst each other."
      },
      "aliases": [
        "FL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Out-Of-Distribution Generalization",
          "justification": "The paper draws inspiration from the field of Out-Of-Distribution generalization to propose the GMA technique.",
          "quote": "Inspired from recent works in Outof-Distribution generalization, we propose a gradient masked averaging approach for FL as an alternative to the standard averaging of client updates."
        },
        "aliases": [
          "OOD Generalization"
        ]
      },
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper discusses various optimization approaches in FL and compares them to the proposed GMA technique.",
          "quote": "Many existing FL works attempt to tackle this problem through the lens of optimization, proposing constrained gradient optimization based approaches Karimireddy et al. (2021); Wang et al. (2019); Li et al. (2019) which attempt to maintain a solution close to the global optimum across the federated data."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "LeNet",
          "justification": "LeNet is used in the experiments for datasets like MNIST and FMNIST as mentioned in the text and tables of the paper.",
          "quote": "An SGD optimizer with a momentum (ρ = 0.9) and cross-entropy loss was used to train each client before aggregation at the server in all our experiments. The momentum parameters of adaptive federated optimizers are fixed... The code is available at https://github.com/arvi797/FL. Details of the datasets explored and the respective skews induced is summarised in Table 1...LeNet."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "LeNet is a well-known model and not a contribution of this paper.",
          "quote": "LeNet."
        },
        "is_executed": {
          "value": true,
          "justification": "Since extensive experiments are carried out on a variety of datasets using LeNet, it is clear that the model was executed in the study.",
          "quote": "LeNet."
        },
        "is_compared": {
          "value": true,
          "justification": "LeNet's performance is compared within experimental results using various federated learning methods, including GMA and other standard techniques.",
          "quote": "Average best test performance of the algorithms and their GMA versions are reported below."
        },
        "referenced_paper_title": {
          "value": "Gradient-Based Learning Applied to Document Recognition",
          "justification": "The title of the reference paper for LeNet is provided here as it is the original paper where LeNet is introduced.",
          "quote": "Gradient-Based Learning Applied to Document Recognition"
        }
      },
      {
        "name": {
          "value": "ResNet-18",
          "justification": "ResNet-18 is used in the experiments for datasets like CIFAR-10, CIFAR-100, and TinyImageNet as mentioned in the text and tables of the paper.",
          "quote": "An SGD optimizer with a momentum (ρ = 0.9) and cross-entropy loss was used to train each client before aggregation at the server in all our experiments. The momentum parameters of adaptive federated optimizers... The code is available at https://github.com/arvi797/FL. Details of the datasets explored and the respective skews induced is summarised in Table 1...ResNet-18."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "ResNet-18 is a well-known model and not a contribution of this paper.",
          "quote": "ResNet-18."
        },
        "is_executed": {
          "value": true,
          "justification": "Since extensive experiments are carried out on a variety of datasets using ResNet-18, it is clear that the model was executed in the study.",
          "quote": "ResNet-18."
        },
        "is_compared": {
          "value": true,
          "justification": "ResNet-18's performance is compared within experimental results using various federated learning methods, including GMA and other standard techniques.",
          "quote": "Average best test performance of the algorithms and their GMA versions are reported below."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "The title of the reference paper for ResNet-18 is provided here as it is the original paper where ResNet is introduced.",
          "quote": "Deep Residual Learning for Image Recognition"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is one of the datasets used extensively in the experiments as mentioned in the text and tables of the paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1. MNIST."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-Based Learning Applied to Document Recognition",
          "justification": "The MNIST dataset was introduced in this paper and is widely referenced for image classification tasks.",
          "quote": "Gradient-Based Learning Applied to Document Recognition"
        }
      },
      {
        "name": {
          "value": "FashionMNIST (FMNIST)",
          "justification": "FashionMNIST is one of the datasets used extensively in the experiments as mentioned in the text and tables of the paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1. FMNIST."
        },
        "aliases": [
          "FMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
          "justification": "The FMNIST dataset was introduced in this paper and is widely referenced for image classification tasks.",
          "quote": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms"
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is one of the datasets used extensively in the experiments as mentioned in the text and tables of the paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1. CIFAR-10."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The CIFAR-10 dataset was introduced in this technical report and is widely referenced for image classification tasks.",
          "quote": "Learning Multiple Layers of Features from Tiny Images"
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is one of the datasets used extensively in the experiments as mentioned in the text and tables of the paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1. CIFAR-100."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The CIFAR-100 dataset was introduced in this technical report and is widely referenced for image classification tasks.",
          "quote": "Learning Multiple Layers of Features from Tiny Images"
        }
      },
      {
        "name": {
          "value": "TinyImageNet",
          "justification": "TinyImageNet is one of the datasets used extensively in the experiments as mentioned in the text and tables of the paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1. TinyImageNet."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Tiny ImageNet Visual Recognition Challenge",
          "justification": "The TinyImageNet dataset was introduced in this paper and is widely referenced for image classification tasks.",
          "quote": "Tiny ImageNet Visual Recognition Challenge"
        }
      },
      {
        "name": {
          "value": "Federated EMNIST (FEMNIST)",
          "justification": "Federated EMNIST is one of the datasets used extensively in the experiments as mentioned in the text and tables of the paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1. FEMNIST."
        },
        "aliases": [
          "FEMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "LEAF: A Benchmark for Federated Settings",
          "justification": "The FEMNIST dataset was introduced in this paper and is widely referenced for federated learning benchmarks.",
          "quote": "LEAF: A Benchmark for Federated Settings"
        }
      },
      {
        "name": {
          "value": "Federated CMNIST (FedCMNIST)",
          "justification": "Federated CMNIST is one of the datasets used extensively in the experiments as mentioned in the text and tables of the paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1. FedCMNIST."
        },
        "aliases": [
          "FedCMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Invariant Risk Minimization Games",
          "justification": "The FedCMNIST dataset was used in the context of federated learning and OOD settings in the referenced paper.",
          "quote": "Invariant Risk Minimization Games"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly used for implementing the experiments as mentioned in the supplementary materials.",
          "quote": "Code for our experiments is included in the supplementary materials and will be made available at the time of publication."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "The referenced paper for PyTorch highlights its application and benefits in automatic differentiation, which is relevant for the experiments conducted in this study.",
          "quote": "Automatic Differentiation in PyTorch"
        }
      },
      {
        "name": {
          "value": "TensorFlow",
          "justification": "TensorFlow is explicitly used for implementing the experiments as mentioned in the supplementary materials.",
          "quote": "Code for our experiments is included in the supplementary materials and will be made available at the time of publication."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
          "justification": "The referenced paper for TensorFlow highlights its application and benefits in large-scale machine learning, which is relevant for the experiments conducted in this study.",
          "quote": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2286,
    "prompt_tokens": 21226,
    "total_tokens": 23512
  }
}