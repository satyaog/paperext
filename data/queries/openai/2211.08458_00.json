{
  "paper": "2211.08458.txt",
  "words": 8633,
  "extractions": {
    "title": {
      "value": "Latent Bottlenecked Attentive Neural Processes",
      "justification": "The title succinctly describes the core contribution of the paper, which is the proposal of the Latent Bottlenecked Attentive Neural Processes (LBANPs).",
      "quote": "L ATENT B OTTLENECKED ATTENTIVE N EURAL P ROCESSES"
    },
    "description": "The paper introduces Latent Bottlenecked Attentive Neural Processes (LBANPs), a novel meta-learning model that addresses the high computational complexity of previous state-of-the-art Neural Processes (NPs) by encoding context datasets into a constant number of latent vectors and utilizing self-attention. The proposed method achieves competitive results while significantly improving scalability and efficiency.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental data and empirical evaluations to demonstrate the effectiveness and efficiency of the proposed LBANPs model.",
      "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion, and contextual multi-armed bandits."
    },
    "primary_research_field": {
      "name": {
        "value": "Meta-Learning",
        "justification": "The paper focuses on Neural Processes (NPs) which are a popular method in meta-learning to estimate predictive uncertainty by conditioning on a context dataset.",
        "quote": "Neural Processes (NPs) are popular methods in meta-learning that can estimate predictive uncertainty on target datapoints by conditioning on a context dataset."
      },
      "aliases": [
        "Meta-Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Processes",
          "justification": "The primary contribution of the paper is the introduction of Latent Bottlenecked Attentive Neural Processes (LBANPs), enhancing existing NP models.",
          "quote": "We propose Latent Bottlenecked Attentive Neural Processes (LBANP), a new computationally efficient sub-quadratic NP variant."
        },
        "aliases": [
          "NPs"
        ]
      },
      {
        "name": {
          "value": "Attention Mechanisms",
          "justification": "The paper introduces enhancements in attention mechanisms such as cross-attention and self-attention in the context of Neural Processes.",
          "quote": "The model encodes the context dataset into a fixed number of latent vectors on which self-attention is performed. When making predictions, the model retrieves higher-order information from the context dataset via multiple cross-attention mechanisms on the latent vectors."
        },
        "aliases": [
          "Attention Mechanisms"
        ]
      },
      {
        "name": {
          "value": "Image Completion",
          "justification": "One of the application domains of LBANPs demonstrated in the paper is image completion.",
          "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion, and contextual multi-armed bandits."
        },
        "aliases": [
          "Image Completion"
        ]
      },
      {
        "name": {
          "value": "Contextual Bandits",
          "justification": "The model's performance is evaluated in the domain of contextual multi-armed bandits.",
          "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion, and contextual multi-armed bandits."
        },
        "aliases": [
          "Contextual Bandits"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Latent Bottlenecked Attentive Neural Processes (LBANPs)",
          "justification": "The paper proposes LBANPs as a new, computationally efficient NP variant.",
          "quote": "We propose Latent Bottlenecked Attentive Neural Processes (LBANP), a new computationally efficient sub-quadratic NP variant."
        },
        "aliases": [
          "LBANPs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is the primary contribution of the research paper.",
          "quote": "We propose Latent Bottlenecked Attentive Neural Processes (LBANP), a new computationally efficient sub-quadratic NP variant."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was run in multiple experimental settings, including meta-regression and image completion.",
          "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion, and contextual multi-armed bandits."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was compared with other state-of-the-art NP variants in various tasks.",
          "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion, and contextual multi-armed bandits."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "LBANPs is an original contribution and not based on a previous model.",
          "quote": "We propose Latent Bottlenecked Attentive Neural Processes (LBANP), a new computationally efficient sub-quadratic NP variant."
        }
      },
      {
        "name": {
          "value": "Transformer Neural Processes (TNPs)",
          "justification": "TNPs are mentioned multiple times as a baseline and a state-of-the-art NP variant for comparison.",
          "quote": "Transformer Neural Processes (Nguyen & Grover, 2022)"
        },
        "aliases": [
          "TNPs"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses TNPs for comparison but does not contribute it.",
          "quote": "In contrast, recent state-of-the-art methods have proposed to use self-attention mechanisms such as transformers. However, these state-of-the-art methods are computationally expensive in that they require quadratic computation in the number of context datapoints."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper mentioned TNPs for comparison purposes but did not detail executing the model.",
          "quote": "TNP-D uses significantly more memory than LBANPs when making predictions."
        },
        "is_compared": {
          "value": 1,
          "justification": "TNPs were used as a baseline to compare the performance and efficiency of LBANPs.",
          "quote": "In contrast, recent state-of-the-art methods have proposed to use self-attention mechanisms such as transformers. However, these state-of-the-art methods are computationally expensive in that they require quadratic computation in the number of context datapoints."
        },
        "referenced_paper_title": {
          "value": "Transformer Neural Processes: Uncertainty-Aware Meta Learning via Sequence Modeling",
          "justification": "The paper provides the full title of the referenced TNPs paper for comparison.",
          "quote": "Transformer Neural Processes (Nguyen & Grover, 2022)."
        }
      },
      {
        "name": {
          "value": "Efficient Query Transformer Neural Processes (EQTNPs)",
          "justification": "The EQTNPs model was introduced as an intermediary step towards developing LBANPs and was empirically evaluated.",
          "quote": "Figure 1 describes how the architecture of TNP is modified to give a more efficient variant that we name Efficient Queries TNPs (EQTNPs)."
        },
        "aliases": [
          "EQTNPs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper contributes EQTNPs as an efficient modification of TNPs.",
          "quote": "Figure 1 describes how the architecture of TNP is modified to give a more efficient variant that we name Efficient Queries TNPs (EQTNPs)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was run in multiple experimental settings, including meta-regression and image completion.",
          "quote": "We compare LBANPs with the following members of the NP family:...and Transformer Neural Processes (TNPs) (Nguyen & Grover, 2022). In addition, we compare with their attentive variants (Kim et al., 2019) (CANPs, ANPs, and BANPs)."
        },
        "is_compared": {
          "value": 1,
          "justification": "LBANPs are directly compared with EQTNPs.",
          "quote": "EQTNPs are more efficient than TNPs, they unfortunately still require quadratic computation to compute the embeddings of the context dataset, limiting its applications."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "EQTNPs is an original contribution within the same paper.",
          "quote": "Figure 1 describes how the architecture of TNP is modified to give a more efficient variant that we name Efficient Queries TNPs (EQTNPs)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "EMNIST",
          "justification": "The paper uses the EMNIST dataset for evaluating image completion tasks.",
          "quote": "For these experiments, we consider two datasets: EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)."
        },
        "aliases": [
          "EMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "EMNIST: Extending MNIST to Handwritten Letters",
          "justification": "The dataset is cited with its reference for clear attribution.",
          "quote": "For these experiments, we consider two datasets: EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)."
        }
      },
      {
        "name": {
          "value": "CelebA",
          "justification": "The paper uses the CelebA dataset for evaluating image completion tasks.",
          "quote": "For these experiments, we consider two datasets: EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)."
        },
        "aliases": [
          "CelebA"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep Learning Face Attributes in the Wild",
          "justification": "The dataset is cited with its reference for clear attribution.",
          "quote": "For these experiments, we consider two datasets: EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments, including the implementation of LBANPs and baselines, were conducted using PyTorch.",
          "quote": "The code is available at https://github.com/BorealisAI/latent-bottlenecked-anp."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The library is commonly known and used, hence does not require a specific reference paper.",
          "quote": "The code is available at https://github.com/BorealisAI/latent-bottlenecked-anp."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2063,
    "prompt_tokens": 16039,
    "total_tokens": 18102
  }
}