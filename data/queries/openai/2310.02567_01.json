{
  "paper": "2310.02567.txt",
  "words": 9458,
  "extractions": {
    "title": {
      "value": "Improving Automatic VQA Evaluation Using Large Language Models",
      "justification": "The title is clearly indicated at the beginning of the paper and encapsulates the main topic discussed in the research.",
      "quote": "Improving Automatic VQA Evaluation Using Large Language Models Oscar Mañas1,2 , Benno Krojer1,3 , Aishwarya Agrawal1,2 1"
    },
    "description": "This paper introduces a novel automatic metric for evaluating Visual Question Answering (VQA) systems called LAVE, which leverages the in-context learning capabilities of instruction-tuned large language models (LLMs). The metric aims to align more closely with human judgment compared to existing metrics like VQA Accuracy. LAVE is formulated as an answer-rating task where the LLM scores the correctness of a candidate answer given a question and a set of reference answers. The authors demonstrate that LAVE correlates better with human judgment across several VQA models and benchmarks and address VQA Accuracy's failure modes by capturing correct answers missed by traditional metrics.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experiments and evaluations conducted on various VQA models and datasets to demonstrate the effectiveness of the proposed LAVE metric.",
      "quote": "Our results demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper addresses the task of Visual Question Answering (VQA), which is a significant problem in the field of Computer Vision.",
        "quote": "Visual question answering (VQA) (Antol et al. 2015) has become an essential benchmark for assessing the progress of multimodal vision-language systems."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Visual Question Answering",
          "justification": "The entire focus of the paper is on the evaluation of Visual Question Answering systems and improving their metrics.",
          "quote": "8 years after the visual question answering (VQA) task was proposed, accuracy remains the primary metric for automatic evaluation."
        },
        "aliases": [
          "VQA"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BLIP-2",
          "justification": "BLIP-2 is used as a state-of-the-art VQA model in evaluations.",
          "quote": "We focus on BLIP-2 and PromptCap since their generation is most open-ended."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PromptCap",
          "justification": "PromptCap is used as a state-of-the-art VQA model in evaluations.",
          "quote": "We focus on BLIP-2 and PromptCap since their generation is most open-ended."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BLIP",
          "justification": "BLIP is used as another representative VQA model for finetuning and evaluation.",
          "quote": "We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA) and VG-QA (BLIPVG), which represents the finetuning-OOD paradigm."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "VQAv2",
          "justification": "VQAv2 is one of the VQA datasets used for evaluation.",
          "quote": "Our results demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "VG-QA",
          "justification": "VG-QA is one of the VQA datasets used for evaluation.",
          "quote": "Our results demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "OK-VQA",
          "justification": "OK-VQA is one of the VQA datasets used for evaluation.",
          "quote": "Our results demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several VQA models and benchmarks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "HuggingFace Transformers",
          "justification": "HuggingFace Transformers library is used for model implementation and experimentation.",
          "quote": "We leverage the HuggingFace Transformers’ (Wolf et al. 2020) implementation of Flan-T5 and LLaMA (for Vicuna), and use GPT-3.5-Turbo through OpenAI’s API1."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1061,
    "prompt_tokens": 16175,
    "total_tokens": 17236
  }
}