{
  "paper": "qjh2DRM0JXh.txt",
  "words": 2203,
  "extractions": {
    "title": {
      "value": "From IID to the Independent Mechanisms assumption in continual learning",
      "justification": "The title is clearly stated in the paper.",
      "quote": "From IID to the Independent Mechanisms assumption in continual learning"
    },
    "description": "This paper explores how the Independent Mechanisms (IM) assumption can be beneficial in continual learning (CL) by addressing different types of distribution shifts. The authors present a theoretical framework and demonstrate the application through preliminary results using a Mixture of Experts (MoE) model.",
    "type": {
      "value": "Empirical",
      "justification": "The paper presents preliminary results with a Mixture of Experts (MoE) model and discusses various theoretical aspects of the IM assumption in CL.",
      "quote": "Preliminary result with Mixture of Experts (MoE). Here, we design a simple attention based MoE model and train it continually on two streams of 5 and 7 tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Continual Learning",
        "justification": "The paper focuses on advancing the field of continual learning by discussing how the IM assumption can be applied to it.",
        "quote": "Continual learning (CL) requires learning without iid-ness and developing algorithms capable of knowledge retention and transfer, the later one can be boosted through systematic generalization."
      },
      "aliases": [
        "CL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Causal Learning",
          "justification": "The paper discusses the Independent Mechanisms (IM) assumption, which is rooted in causal learning.",
          "quote": "The Independent mechanisms (IM) assumption states that in causal factorization of the joint, the mechanism p(Y |X, T, Z) contains no information about the causes pt (X, T, Z) and VV (Schölkopf et al. 2012)."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Modular Learning",
          "justification": "The paper mentions the use of modular architecture for representing causal mechanisms.",
          "quote": "We envision a model fθ (·) parameterized with a set of M modules that compete with each other for explaining the current observation. The benefit of such system for CL is discussed next."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Mixture of Experts (MoE)",
          "justification": "The paper presents preliminary results using a Mixture of Experts (MoE) model to demonstrate the discussed theories.",
          "quote": "Preliminary result with Mixture of Experts (MoE). Here, we design a simple attention-based MoE model and train it continually on two streams of 5 and 7 tasks."
        },
        "aliases": [
          "MoE"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model itself is not a new contribution but is used to demonstrate the theoretical discussion.",
          "quote": "Preliminary result with Mixture of Experts (MoE)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The MoE model is executed to obtain preliminary results as discussed in the paper.",
          "quote": "Here, we design a simple attention based MoE model and train it continually on two streams of 5 and 7 tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "The MoE model is compared with other methods like EWC in the paper.",
          "quote": "MCL has a much larger mean MSE at the beginning, it reaches MSE comparable to EWC (Kirkpatrick et al. 2017) at the end of the sequence."
        },
        "referenced_paper_title": {
          "value": "Learning independent causal mechanisms",
          "justification": "The referenced paper mainly discusses the theoretical foundation of the Independent Mechanisms (IM) assumption, which is crucial for the MoE model's application in this paper.",
          "quote": "The IM assumption can be extended to the mechanism p(Y |X, T ), which can be thought of as a composition of autonomous modules that operate independently from each other (Parascandolo et al. 2018; Goyal et al. 2019)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Unknown Dataset for Math Equations Domain",
          "justification": "The dataset for math equations domain is referred to but not specifically named.",
          "quote": "Such setting can be instantiated in the math equations domain similar to Mittal, Bengio, and Lajoie (2022): X1 , X2 ∼ R[−1,1] , and T describe the math operations to be performed (+/-/* etc.) (one or many operations per equation)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Is a Modular Architecture Enough?",
          "justification": "The setting is similar to the one used in Mittal, Bengio, and Lajoie (2022).",
          "quote": "Such setting can be instantiated in the math equations domain similar to Mittal, Bengio, and Lajoie (2022)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Not Explicitly Mentioned",
          "justification": "The paper does not mention any specific Deep Learning libraries used.",
          "quote": "N/A"
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "No specific libraries are mentioned in relation to previous works.",
          "quote": "N/A"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1041,
    "prompt_tokens": 4912,
    "total_tokens": 5953
  }
}