{
  "paper": "pfuqQQCB34.txt",
  "words": 23439,
  "extractions": {
    "title": {
      "value": "VARIANCE REDUCTION IS AN ANTIDOTE TO BYZANTINE WORKERS: BETTER RATES, WEAKER ASSUMPTIONS AND COMMUNICATION COMPRESSION AS A CHERRY ON THE TOP",
      "justification": "This is the title of the paper.",
      "quote": "VARIANCE REDUCTION IS AN ANTIDOTE TO BYZANTINE WORKERS: BETTER RATES, WEAKER ASSUMPTIONS AND COMMUNICATION COMPRESSION AS A CHERRY ON THE TOP"
    },
    "description": "The paper proposes a new Byzantine-tolerant method called Byz-VR-MARINA that incorporates variance reduction and compression techniques. It demonstrates the effectiveness of variance reduction in combating Byzantine workers and communication compression to enhance communication efficiency. Theoretical convergence guarantees are provided, showcasing better performance compared to previous methods under certain conditions.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes both theoretical analysis and numerical experiments to validate the effectiveness and efficiency of the proposed method Byz-VR-MARINA.",
      "quote": "We derive theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-Łojasiewicz loss functions. Numerical experiments corroborate our theoretical findings."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The primary focus of the paper is on Byzantine-robustness in the context of Federated Learning.",
        "quote": "Byzantine-robustness has been gaining a lot of attention due to the growth of the interest in collaborative and federated learning."
      },
      "aliases": [
        "FL",
        "Federated Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Distributed Learning",
          "justification": "The paper discusses distributed optimization algorithms and their application in training large machine learning models.",
          "quote": "Distributed optimization algorithms play a vital role in the training of the modern machine learning models."
        },
        "aliases": [
          "Distributed Optimization"
        ]
      },
      {
        "name": {
          "value": "Communication Efficiency",
          "justification": "The paper addresses the issue of communication bottlenecks in distributed learning.",
          "quote": "One of the most popular approaches to reducing the communication bottleneck is to use communication compression."
        },
        "aliases": [
          "Communication Compression"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Byz-VR-MARINA",
          "justification": "Byz-VR-MARINA is the primary model proposed in the paper, combining Byzantine-robustness, variance reduction, and compression.",
          "quote": "This work addresses this gap and proposes Byz-VR-MARINA – a new Byzantine-tolerant method with variance reduction and compression."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "Byz-VR-MARINA is introduced and theoretically analyzed as a contribution of the paper.",
          "quote": "This work addresses this gap and proposes Byz-VR-MARINA – a new Byzantine-tolerant method with variance reduction and compression."
        },
        "is_executed": {
          "value": 1,
          "justification": "The numerical experiments for Byz-VR-MARINA indicate that it was executed.",
          "quote": "Numerical experiments corroborate our theoretical findings."
        },
        "is_compared": {
          "value": 1,
          "justification": "Byz-VR-MARINA is compared to other state-of-the-art Byzantine-tolerant methods and baselines in the paper.",
          "quote": "We provide the first theoretical convergence guarantees for Byzantine-tolerant methods with compression in the non-convex case for arbitrary adversaries."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "Byz-VR-MARINA is the main contribution and original to this paper.",
          "quote": "This work addresses this gap and proposes Byz-VR-MARINA – a new Byzantine-tolerant method with variance reduction and compression."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "a9a dataset",
          "justification": "The a9a dataset is used in the experiments to test the proposed method.",
          "quote": "We consider a9a LIBSVM dataset (Chang & Lin, 2011) and set λ = 0.01."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Libsvm: a library for support vector machines",
          "justification": "The paper implementing the a9a dataset is found in the references section as Chang & Lin (2011).",
          "quote": "We consider a9a LIBSVM dataset (Chang & Lin, 2011) and set λ = 0.01."
        }
      },
      {
        "name": {
          "value": "w8a dataset",
          "justification": "The w8a dataset is used in the experiments to validate the proposed method.",
          "quote": "In Figures 4–6, we perform the same experiments, but for the different LIBSVM dataset: w8a."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Libsvm: a library for support vector machines",
          "justification": "The paper implementing the w8a dataset is found in the references section as Chang & Lin (2011).",
          "quote": "In Figures 4–6, we perform the same experiments, but for the different LIBSVM dataset: w8a."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The implementation of the algorithm and experiments is based on PyTorch.",
          "quote": "Our implementation is based on PyTorch (Paszke et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pytorch: an imperative style, high-performance deep learning library",
          "justification": "The referenced paper for PyTorch is by Paszke et al., 2019.",
          "quote": "Our implementation is based on PyTorch (Paszke et al., 2019)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1446,
    "prompt_tokens": 44972,
    "total_tokens": 46418
  }
}