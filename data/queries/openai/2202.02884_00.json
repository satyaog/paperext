{
  "paper": "2202.02884.txt",
  "words": 9812,
  "extractions": {
    "title": {
      "value": "Exploring Self-Attention Mechanisms for Speech Separation",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "Exploring Self-Attention Mechanisms for Speech Separation"
    },
    "description": "This paper studies Transformers for speech separation, extending the previous work on SepFormer. It explores the use of efficient self-attention mechanisms such as Longformer, Linformer, and Reformer for speech separation and also evaluates on additional datasets such as LibriMix, WHAM!, and WHAMR!.",
    "type": {
      "value": "Empirical",
      "justification": "The paper provides experimental results on various datasets, compares the performance of different models and attention mechanisms, and discusses computational efficiency.",
      "quote": "This paper studies in-depth Transformers for speech separation. In particular, we extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets..."
    },
    "primary_research_field": {
      "name": {
        "value": "Speech Separation",
        "justification": "The main focus of the paper is on speech separation using different self-attention mechanisms within Transformer models.",
        "quote": "This paper studies in-depth Transformers for speech separation."
      },
      "aliases": [
        "Source Separation"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Transformers",
          "justification": "The paper evaluates the performance of Transformer models for the task of speech separation.",
          "quote": "We extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Speech Enhancement",
          "justification": "The paper adapts SepFormer to perform speech enhancement and provides experimental evidence on denoising tasks.",
          "quote": "Moreover, we adapted the SepFormer to perform speech enhancement and provide experimental evidence on VoiceBank-DEMAND, WHAM!, and WHAMR! datasets."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Self-Attention Mechanisms",
          "justification": "The paper investigates efficient self-attention mechanisms such as Longformer, Linformer, and Reformer for speech separation.",
          "quote": "Finally, we investigate, for the first time in speech separation, the use of efficient self-attention mechanisms such as Linformers, Lonformers, and ReFormers."
        },
        "aliases": [
          "Efficient Attention Mechanisms"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SepFormer",
          "justification": "SepFormer is the main model discussed in the paper and its performance and various adaptations are evaluated.",
          "quote": "We extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets"
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "SepFormer is discussed as a contribution of the authors in their previous work which is being extended in this paper.",
          "quote": "Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experimental results demonstrating the execution of SepFormer.",
          "quote": "We extend our previous findings on the SepFormer by providing results on more challenging noisy and noisy-reverberant datasets, such as LibriMix, WHAM!, and WHAMR!."
        },
        "is_compared": {
          "value": 1,
          "justification": "SepFormer is numerically compared to other models like Conv-TasNet and DPRNN in the paper.",
          "quote": "We found that the Reformer-based attention outperforms the popular Conv-TasNet model on the WSJ0-2Mix dataset while being faster at inference and comparable in terms of memory consumption."
        },
        "referenced_paper_title": {
          "value": "Attention is All You Need in Speech Separation",
          "justification": "The SepFormer model was first introduced in this previous paper by the authors.",
          "quote": "Recently, we proposed the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "WSJ0-2Mix",
          "justification": "The dataset is used extensively in the experiments for evaluating the performance of various models.",
          "quote": "... the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets"
        },
        "aliases": [
          "WSJ0-2Mix"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep clustering: Discriminative embeddings for segmentation and separation",
          "justification": "The WSJ0-2Mix dataset is commonly referenced in the context of this previous work.",
          "quote": "Notable early works include Deep Clustering [19], where a recurrent neural network is trained on an affinity matrix in order to estimate embeddings for each source from the magnitude spectra of the mixture."
        }
      },
      {
        "name": {
          "value": "WSJ0-3Mix",
          "justification": "The dataset is used for model evaluation in the paper.",
          "quote": "... the SepFormer, which obtains state-of-the-art performance in speech separation with the WSJ0-2/3 Mix datasets."
        },
        "aliases": [
          "WSJ0-3Mix"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep clustering: Discriminative embeddings for segmentation and separation",
          "justification": "The WSJ0-3Mix dataset is referenced in the context of the same prior work as WSJ0-2Mix.",
          "quote": "Notable early works include Deep Clustering [19], where a recurrent neural network is trained on an affinity matrix in order to estimate embeddings for each source from the magnitude spectra of the mixture."
        }
      },
      {
        "name": {
          "value": "LibriMix",
          "justification": "The paper provides experimental results for the SepFormer on this dataset.",
          "quote": "Here, we propose additional experiments and insights on more realistic and challenging datasets such as Libri2/3-Mix [15]."
        },
        "aliases": [
          "Libri2/3-Mix"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "LibriMix: An Open-Source Dataset for Generalizable Speech Separation",
          "justification": "LibriMix dataset is introduced in this referenced paper.",
          "quote": "Moreover, we adapted the SepFormer to perform speech enhancement and provide experimental evidence on VoiceBank-DEMAND, WHAM!, and WHAMR! datasets."
        }
      },
      {
        "name": {
          "value": "WHAM!",
          "justification": "The paper provides experimental results for SepFormer on this dataset.",
          "quote": "We propose additional experiments and insights on more realistic and challenging datasets such as Libri2/3-Mix [15], which includes long mixtures, WHAM! [16] ..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "WHAM!: Extending Speech Separation to Noisy Environments",
          "justification": "WHAM! dataset is introduced in this referenced paper.",
          "quote": "WHAM! [16], and WHAMR! datasets [17] which are essentially derived from the WSJ0-2Mix dataset by adding environmental noise and environmental noise plus reverberation respectively."
        }
      },
      {
        "name": {
          "value": "WHAMR!",
          "justification": "The paper provides experimental results for SepFormer on this dataset.",
          "quote": "We propose additional experiments and insights on more realistic and challenging datasets such as Libri2/3-Mix [15] ... and WHAMR!."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "WHAMR!: Noisy and Reverberant Single-Channel Speech Separation",
          "justification": "WHAMR! dataset is introduced in this referenced paper.",
          "quote": "WHAMR! datasets [17] which are essentially derived from the WSJ0-2Mix dataset by adding environmental noise and environmental noise plus reverberation respectively."
        }
      },
      {
        "name": {
          "value": "VoiceBank-DEMAND",
          "justification": "The paper provides experimental results for SepFormer on this dataset.",
          "quote": "Moreover, we adapted the SepFormer to perform speech enhancement and provide experimental evidence on VoiceBank-DEMAND, WHAM!, and WHAMR! datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Investigating RNN-based speech enhancement methods for noise-robust Text-to-Speech",
          "justification": "VoiceBank-DEMAND dataset is introduced in this referenced paper.",
          "quote": "The popular speech enhancement VoiceBank-DEMAND dataset [37] is also used to compare the SepFormer architecture with other state-of-the-art denoising models."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "The training recipes for the main experiments are available in the SpeechBrain toolkit.",
          "quote": "The training recipes for the main experiments are available in the Speechbrain [18] toolkit."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A General-Purpose Speech Toolkit",
          "justification": "This referenced paper introduces the SpeechBrain toolkit.",
          "quote": "The training recipes for the main experiments are available in the Speechbrain [18] toolkit."
        }
      },
      {
        "name": {
          "value": "PyTorch",
          "justification": "The models were likely implemented and profiled using PyTorch, as suggested by the references to using PyTorch profiler and training models using it.",
          "quote": "All the models run in the same NVIDIA RTX8000-48GB GPU using the PyTorch profiler [45]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Profiler",
          "justification": "The referenced paper link is for the PyTorch Profiler.",
          "quote": "All the models run in the same NVIDIA RTX8000-48GB GPU using the PyTorch profiler [45]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1919,
    "prompt_tokens": 18679,
    "total_tokens": 20598
  }
}