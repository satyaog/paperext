{
  "paper": "UJkgGbLfWA.txt",
  "words": 9295,
  "extractions": {
    "title": {
      "value": "Guiding Language Model Math Reasoning with Planning Tokens",
      "justification": "The title is directly found at the beginning of the paper.",
      "quote": "Guiding Language Model Math Reasoning with Planning Tokens"
    },
    "description": "This paper introduces 'planning tokens' to improve the reasoning capabilities of Large Language Models (LLMs) in solving math word problems. The approach embeds these tokens into the model, guiding it at each reasoning step. The method is tested on various LLMs and math problem datasets, demonstrating notable accuracy improvements while requiring negligible additional trainable parameters.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents an empirical evaluation of a method to improve LLMs by introducing planning tokens and shows its effectiveness through experiments on multiple datasets and models.",
      "quote": "We demonstrate our method’s effectiveness by applying it to three different LLMs, showing notable accuracy improvements across three math word problem datasets w.r.t. vanilla fine-tuning baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on improving the reasoning capabilities of Large Language Models, a central topic in NLP.",
        "quote": "To solve this, we introduce ‘planning tokens’ at the start of each reasoning step, serving as a guide for the model, and add their embeddings to the model parameters."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Mathematical Reasoning",
          "justification": "The specific application domain is the reasoning capabilities of LLMs to solve math word problems.",
          "quote": "One of the current challenges for large language models (LLMs) is to solve complex tasks, like math problems."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Llama2-7B",
          "justification": "Llama2-7B is one of the models used in the experiments.",
          "quote": "For example, in Figure 1, we show a common error made by Llama2-7B (Touvron et al., 2023) when fine-tuned on the GSM8K dataset (Cobbe et al., 2021)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Llama2-7B is used and analyzed but is not introduced as a new model in this paper.",
          "quote": "For example, in Figure 1, we show a common error made by Llama2-7B (Touvron et al., 2023) when fine-tuned on the GSM8K dataset (Cobbe et al., 2021)."
        },
        "is_executed": {
          "value": true,
          "justification": "Llama2-7B was fine-tuned and tested on the GSM8K dataset as part of the experimental setup.",
          "quote": "For example, in Figure 1, we show a common error made by Llama2-7B (Touvron et al., 2023) when fine-tuned on the GSM8K dataset (Cobbe et al., 2021)."
        },
        "is_compared": {
          "value": true,
          "justification": "Its performance is compared against other models and fine-tuning approaches.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
          "justification": "The reference is provided by the citation of Llama2 in the paper.",
          "quote": "(Touvron et al., 2023)"
        }
      },
      {
        "name": {
          "value": "Llama2-13B",
          "justification": "Llama2-13B is another version of the Llama2 model used in the experiments.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Llama2-13B is used and analyzed but is not introduced as a new model in this paper.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "is_executed": {
          "value": true,
          "justification": "Llama2-13B was fine-tuned and tested on the same datasets as part of the experiments.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "is_compared": {
          "value": true,
          "justification": "Its performance is compared against other models and approaches.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
          "justification": "The reference is provided by the citation of Llama2 in the paper.",
          "quote": "(Touvron et al., 2023)"
        }
      },
      {
        "name": {
          "value": "Phi 1.5",
          "justification": "Phi 1.5 is another model used in the empirical analysis.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Phi 1.5 is used and analyzed but is not introduced as a new model in this paper.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "is_executed": {
          "value": true,
          "justification": "Phi 1.5 was fine-tuned and tested on the same datasets as part of the experiments.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "is_compared": {
          "value": true,
          "justification": "Its performance is compared against other models and approaches.",
          "quote": "Overall, our method improves upon the baseline without planning tokens by 3.3% accuracy points on average over three pre-trained language models (Phi 1.5, 7B Llama 2, and 13B Llama 2) and three math word datasets."
        },
        "referenced_paper_title": {
          "value": "Textbooks Are All You Need",
          "justification": "The reference is provided by the citation of Phi 1.5 in the paper.",
          "quote": "(Gunasekar et al., 2023)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GSM8K",
          "justification": "The GSM8K dataset is one of the datasets explicitly mentioned and used in the experiments.",
          "quote": "For example, in Figure 1, we show a common error made by Llama2-7B (Touvron et al., 2023) when fine-tuned on the GSM8K dataset (Cobbe et al., 2021)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Training Verifiers to Solve Math Word Problems",
          "justification": "The reference for GSM8K is provided by the citation in the paper.",
          "quote": "(Cobbe et al., 2021)"
        }
      },
      {
        "name": {
          "value": "MATH",
          "justification": "The MATH dataset is mentioned and used in the experimental section.",
          "quote": "Next, we use the MATH dataset (Hendrycks et al., 2021), a collection of 12.5K challenging competition mathematics problems."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Measuring Mathematical Problem Solving with the MATH Dataset",
          "justification": "The reference for the MATH dataset is provided by the citation in the paper.",
          "quote": "(Hendrycks et al., 2021)"
        }
      },
      {
        "name": {
          "value": "AQUA-RAT",
          "justification": "The AQUA-RAT dataset is mentioned and used in the experiments as well.",
          "quote": "Lastly, we also use the AQUA-RAT dataset (Ling et al., 2017) containing 100K samples of mathematical problems, along with sequences of human-readable mathematical expressions in natural language."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
          "justification": "The reference for AQUA-RAT is provided by the citation in the paper.",
          "quote": "(Ling et al., 2017)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "LoRA",
          "justification": "LoRA is a fine-tuning technique explicitly used in the experimental setups for parameter-efficient tuning.",
          "quote": "Our method can also be combined with parameter-efficient fine-tuning techniques such as LoRA (Hu et al., 2021)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "LoRA: Low-Rank Adaptation of Large Language Models",
          "justification": "The reference is provided by the citation of LoRA in the paper.",
          "quote": "(Hu et al., 2021)"
        }
      },
      {
        "name": {
          "value": "AdaFactor",
          "justification": "AdaFactor is used as an optimizer in the training setup.",
          "quote": "We train all models for 10 epochs, with a learning rate of 2e-5 using the AdaFactor optimizer (Shazeer & Stern, 2018) for full fine-tuning..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost",
          "justification": "The reference is provided by the citation of AdaFactor in the paper.",
          "quote": "(Shazeer & Stern, 2018)"
        }
      },
      {
        "name": {
          "value": "AdamW",
          "justification": "AdamW is used as an optimizer in the experiments.",
          "quote": "We train all models for 10 epochs, with a learning rate of 2e-5 using the AdaFactor optimizer (Shazeer & Stern, 2018) for full fine-tuning, and a learning rate of 2e-4 using the AdamW optimizer (Loshchilov & Hutter, 2017) for parameter efficient tuning."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Decoupled Weight Decay Regularization",
          "justification": "The reference is provided by the citation of AdamW in the paper.",
          "quote": "(Loshchilov & Hutter, 2017)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2873,
    "prompt_tokens": 17073,
    "total_tokens": 19946
  }
}