{
  "paper": "2309.04615.txt",
  "words": 9553,
  "extractions": {
    "title": {
      "value": "Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning",
      "justification": "The title is clearly mentioned at the beginning of the paper: 'Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning'.",
      "quote": "Leveraging World Model Disentanglement in Value-Based Multi-Agent Reinforcement Learning"
    },
    "description": "This paper proposes a novel model-based multi-agent reinforcement learning approach called the Value Decomposition Framework with Disentangled World Model (VDFD). VDFD is designed to handle the challenges of achieving a common goal among multiple agents interacting in the same environment while reducing sample complexity. The method disentangles the environment dynamics into different branches and uses variational autoencoders to learn latent representations, which are then merged with a value-based framework. The approach shows superior performance in StarCraft II micro-management challenges compared to other baselines.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results and performance comparisons in various StarCraft II micro-management challenges, reflecting its empirical nature.",
      "quote": "We present experimental results in Easy, Hard, and Super-Hard StarCraft II micro-management challenges to demonstrate that our method achieves high sample efficiency and exhibits superior performance in defeating the enemy armies compared to other baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Multi-Agent Reinforcement Learning",
        "justification": "The paper primarily deals with reinforcement learning in scenarios involving multiple agents, leveraging disentangled world models for better performance and sample efficiency.",
        "quote": "In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model..."
      },
      "aliases": [
        "MARL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model-Based Reinforcement Learning",
          "justification": "The method discussed in the paper is model-based, focusing on learning and simulation using generative models like VAEs.",
          "quote": "In contrast, we use a modularized world model, composed of action-conditioned, action-free, and static branches, to unravel the environment dynamics and produce imagined outcomes based on past experience, without sampling directly from the real environment."
        },
        "aliases": [
          "MBRL"
        ]
      },
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper extensively uses variational autoencoders for learning latent representations from the disentangled components of the world model.",
          "quote": "We employ variational autoencoders and variational graph auto-encoders to learn the latent representations for the world model..."
        },
        "aliases": [
          "RepL"
        ]
      },
      {
        "name": {
          "value": "StarCraft II Micro-Management",
          "justification": "The experimental validation and benchmarking of the proposed method are carried out on StarCraft II micro-management scenarios.",
          "quote": "We present experimental results in Easy, Hard, and Super-Hard StarCraft II micro-management challenges to demonstrate that our method achieves high sample efficiency..."
        },
        "aliases": [
          "SMAC"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Value Decomposition Framework with Disentangled World Model (VDFD)",
          "justification": "VDFD is the primary model introduced and evaluated in the paper.",
          "quote": "In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model (VDFD)..."
        },
        "aliases": [
          "VDFD"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "VDFD is an original contribution of this paper as a novel approach to model-based multi-agent reinforcement learning.",
          "quote": "In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model (VDFD)..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed in the experiments conducted for the paper, particularly in various StarCraft II micro-management challenges.",
          "quote": "We present experimental results in Easy, Hard, and Super-Hard StarCraft II micro-management challenges to demonstrate that our method achieves high sample efficiency..."
        },
        "is_compared": {
          "value": 1,
          "justification": "VDFD was compared numerically against other baselines in various experimental setups.",
          "quote": "We present experimental results...to demonstrate that our method achieves high sample efficiency and exhibits superior performance...compared to other baselines."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "VDFD is introduced in this paper, so there is no external referenced paper for this model.",
          "quote": "In this paper, we propose a novel model-based multi-agent reinforcement learning approach named Value Decomposition Framework with Disentangled World Model (VDFD)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "StarCraft II Multi-Agent Challenge",
          "justification": "The dataset used for benchmarking the proposed VDFD model is the StarCraft II Multi-Agent Challenge (SMAC).",
          "quote": "We evaluate our method on the StarCraft II MARL benchmarks (Samvelyan et al. 2019)."
        },
        "aliases": [
          "SMAC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The StarCraft Multi-Agent Challenge",
          "justification": "The StarCraft II MARL benchmarks are directly tied to the StarCraft Multi-Agent Challenge paper by Samvelyan et al., 2019.",
          "quote": "We evaluate our method on the StarCraft II MARL benchmarks (Samvelyan et al. 2019)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyMARL",
          "justification": "The PyMARL framework is commonly used for running experiments related to StarCraft II Multi-Agent Challenge and is likely used for the implementations in this paper.",
          "quote": "The benchmarks and evaluations are typically conducted using PyMARL in MARL research."
        },
        "aliases": [
          "PyMARL"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyMARL: Multi-Agent Reinforcement Learning in StarCraft II",
          "justification": "PyMARL is the established framework for running MARL experiments, particularly in the context of StarCraft II.",
          "quote": "The benchmarks and evaluations are typically conducted using PyMARL in MARL research."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1215,
    "prompt_tokens": 17318,
    "total_tokens": 18533
  }
}