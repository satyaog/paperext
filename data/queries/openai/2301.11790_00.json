{
  "paper": "2301.11790.txt",
  "words": 9673,
  "extractions": {
    "title": {
      "value": "Leveraging the Third Dimension in Contrastive Learning",
      "justification": "The complete title of the research paper",
      "quote": "Leveraging the Third Dimension in Contrastive Learning"
    },
    "description": "This paper explores leveraging depth information in Self-Supervised Learning (SSL) to improve the quality of learned representations in computer vision tasks. By incorporating a depth signal via a pretrained monocular RGB-to-depth model, two methods are tested: concatenating an RGB image with depth and generating novel views using the depth signal. Evaluations on various datasets and SSL methods demonstrate that depth incorporation enhances performance and robustness.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper performs experiments evaluating the proposed approaches on various datasets and methods",
      "quote": "We evaluate these two approaches on three different SSL methods—BYOL, SimSiam, and SwAV—using ImageNette (10 class subset of ImageNet), ImageNet-100 and ImageNet-1k datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on improving vision systems using depth information",
        "quote": "these cues are available to biological systems early in the visual processing stream and are very likely used to segment the world into objects. Consequently, we hypothesize that a depth channel will support improved representations in contrastive learning."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper explores the use of SSL frameworks BYOL, SimSiam, and SwAV",
          "quote": "We evaluate these two approaches on three different SSL methods—BYOL, SimSiam, and SwAV—using ImageNette, ImageNet-100 and ImageNet-1k datasets."
        },
        "aliases": [
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Depth Estimation",
          "justification": "The paper incorporates depth information using monocular RGB-to-depth models to enhance contrastive learning.",
          "quote": "Using a signal provided by a pretrained state-of-the-art monocular RGB-to-depth model (the Depth Prediction Transformer, Ranftl et al., 2021), we explore two distinct approaches to incorporating depth signals into the SSL framework."
        },
        "aliases": [
          "3D Depth"
        ]
      },
      {
        "name": {
          "value": "Augmented Reality",
          "justification": "Using single-view view synthesis methods for novel view generation has applications in Augmented Reality",
          "quote": "Single-View View Synthesis methods have a wide ranging applications in Augmented and Virtual Reality as they allow the viewer to interact with the photos."
        },
        "aliases": [
          "AR"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BYOL",
          "justification": "BYOL is one of the Self-Supervised Learning methods tested in the experiments",
          "quote": "We evaluate these two approaches on three different SSL methods—BYOL, SimSiam, and SwAV"
        },
        "aliases": [
          "Bootstrap Your Own Latent"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "BYOL is used for evaluation and comparison, but it is not a new model presented in the paper.",
          "quote": "We evaluate these two approaches on three different SSL methods—BYOL, SimSiam, and SwAV"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments involving BYOL are executed in the paper.",
          "quote": "BYOL is one of the first contrastive learning based methods without negative pairs."
        },
        "is_compared": {
          "value": 1,
          "justification": "BYOL is compared with other models like SimSiam and SwAV in the experiments.",
          "quote": "We show that both of these approaches improve the performance of three different contrastive learning methods (BYOL, SimSiam, and SwAV)"
        },
        "referenced_paper_title": {
          "value": "Bootstrap your own latent: A new approach to self-supervised learning",
          "justification": "This is the reference paper title as provided in the research paper citation.",
          "quote": "BYOL (Grill et al., 2020) is one of the first contrastive learning based methods without negative pairs."
        }
      },
      {
        "name": {
          "value": "SimSiam",
          "justification": "SimSiam is one of the Self-Supervised Learning methods tested in the experiments",
          "quote": "We evaluate these two approaches on three different SSL methods—BYOL, SimSiam, and SwAV"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "SimSiam is used for evaluation and comparison, but it is not a new model presented in the paper.",
          "quote": "Self-Supervised Learning (SSL) methods operate on unlabeled data to learn robust representations"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments involving SimSiam are executed in the paper.",
          "quote": "SimSiam (Chen and He, 2021) explores the role of Siamese networks in contrastive learning."
        },
        "is_compared": {
          "value": 1,
          "justification": "SimSiam is compared with other models like BYOL and SwAV in the experiments.",
          "quote": "We show that both of these approaches improve the performance of three different contrastive learning methods (BYOL, SimSiam, and SwAV)"
        },
        "referenced_paper_title": {
          "value": "Exploring simple Siamese representation learning",
          "justification": "This is the reference paper title as provided in the research paper citation.",
          "quote": "SimSiam (Chen and He 2021) explores the role of Siamese networks in contrastive learning."
        }
      },
      {
        "name": {
          "value": "SwAV",
          "justification": "SwAV is one of the Self-Supervised Learning methods tested in the experiments",
          "quote": "We evaluate these two approaches on three different SSL methods—BYOL, SimSiam, and SwAV"
        },
        "aliases": [
          "Swapping Assignments between Views"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SwAV is used for evaluation and comparison, but it is not a new model presented in the paper.",
          "quote": "SwAV (Caron et al., 2020) is an online clustering based method that compares cluster assignments from multiple views."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments involving SwAV are executed in the paper.",
          "quote": "SwAV (Caron et al., 2020) is an online clustering based method that compares cluster assignments from multiple views."
        },
        "is_compared": {
          "value": 1,
          "justification": "SwAV is compared with other models like BYOL and SimSiam in the experiments.",
          "quote": "We show that both of these approaches improve the performance of three different contrastive learning methods (BYOL, SimSiam, and SwAV)"
        },
        "referenced_paper_title": {
          "value": "Unsupervised learning of visual features by contrasting cluster assignments",
          "justification": "This is the reference paper title as provided in the research paper citation.",
          "quote": "SwAV (Caron et al., 2020) is an online clustering based method that compares cluster assignments from multiple views."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet-1k",
          "justification": "ImageNet-1k is one of the datasets used for evaluating the proposed methods",
          "quote": "We show that both of these approaches improve the performance of three different contrastive learning methods (BYOL, SimSiam, and SwAV) on ImageNette, ImageNet-100, and large-scale ImageNet-1k datasets."
        },
        "aliases": [
          "ILSVRC 2012",
          "ImageNet"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "This is the reference paper title as provided in the research paper citation.",
          "quote": "ImageNet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255. IEEE, 2009."
        }
      },
      {
        "name": {
          "value": "ImageNet-100",
          "justification": "ImageNet-100 is one of the datasets used for evaluating the proposed methods",
          "quote": "We show that both of these approaches improve the performance of three different contrastive learning methods (BYOL, SimSiam, and SwAV) on ImageNette, ImageNet-100, and large-scale ImageNet-1k datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Contrastive Multiview Coding",
          "justification": "The referenced paper defines the subset of ImageNet used as ImageNet-100",
          "quote": "ImageNet-100: is a 100 class subset of ImageNet (Deng et al., 2009) consisting of 126689 training images and 5000 validation images. We use the same classes as in (Tian et al., 2020) and train all models with image size of 224."
        }
      },
      {
        "name": {
          "value": "ImageNette",
          "justification": "ImageNette is one of the datasets used for evaluating the proposed methods",
          "quote": "We show that both of these approaches improve the performance of three different contrastive learning methods (BYOL, SimSiam, and SwAV) on ImageNette, ImageNet-100, and large-scale ImageNet-1k datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The referenced paper of this dataset is not provided in the research paper.",
          "quote": "ImageNette: is a 10 class subset of ImageNet (Deng et al., 2009) that consists of 9469 images for training and 2425 images for testing."
        }
      },
      {
        "name": {
          "value": "ImageNet-C",
          "justification": "ImageNet-C is used to evaluate the corruption robustness of the models",
          "quote": "We also measure the corruption robustness of these models by evaluating the performance of these models on ImageNet-C and ImageNet-3DCC."
        },
        "aliases": [
          "ImageNet-Corruptions"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Benchmarking Neural Network Robustness to Common Corruptions and Perturbations",
          "justification": "This is the reference paper title as provided in the research paper citation.",
          "quote": "ImageNet-C (Hendrycks and Dietterich, 2019): ImageNet-C dataset is a benchmark to evaluate the robustness of the model to common corruptions."
        }
      },
      {
        "name": {
          "value": "ImageNet-3DCC",
          "justification": "ImageNet-3DCC is used to measure the model's performance on realistic 3D corruptions",
          "quote": "We also measure the corruption robustness of these models by evaluating the performance of these models on ImageNet-C and ImageNet-3DCC."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "3D Common Corruptions and Data Augmentation",
          "justification": "This is the reference paper title as provided in the research paper citation.",
          "quote": "ImageNet-3DCC (Kar et al., 2022): ImageNet-3DCC consists of realistic 3D corruptions like camera motion, occlusions, weather to name a few."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used to implement the methods discussed in the paper",
          "quote": "We implement our methods in PyTorch 1.11 (Paszke et al., 2019) and use Weights and Biases (Biewald, 2020) to track the experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the reference paper title as provided in the research paper citation.",
          "quote": "We implement our methods in PyTorch 1.11 (Paszke et al., 2019) and use Weights and Biases (Biewald, 2020) to track the experiments."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2390,
    "prompt_tokens": 17631,
    "total_tokens": 20021
  }
}