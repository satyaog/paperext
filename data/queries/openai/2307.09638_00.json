{
  "paper": "2307.09638.txt",
  "words": 11338,
  "extractions": {
    "title": {
      "value": "Promoting Exploration in Memory-Augmented Adam using Critical Momenta",
      "justification": "This is the exact title of the paper.",
      "quote": "Promoting Exploration in Memory-Augmented Adam using Critical Momenta"
    },
    "description": "The paper proposes a new memory-augmented version of the Adam optimizer, called Adam+CM (Adam with Critical Momenta), designed to explore flatter minima in the loss landscape by incorporating a buffer of critical momentum terms during training. This approach aims to improve model performance on various tasks, including image classification and language modeling, by encouraging exploration and avoiding sharp minima. The paper provides both theoretical analysis and empirical results to validate the effectiveness of the proposed optimizer.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper primarily focuses on empirical results to demonstrate the effectiveness of the proposed optimizer through various experiments and benchmarks.",
      "quote": "We empirically demonstrate that it can improve model performance for image classification on ImageNet and CIFAR10/100, language modelling on Penn Treebank, and online learning tasks on TinyImageNet and 5-dataset."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization Algorithms",
        "justification": "The primary contribution of the paper is a new optimization algorithm (Adam+CM) designed to improve exploration in adaptive optimizers.",
        "quote": "To address this, we propose a new memory-augmented version of Adam that encourages exploration towards flatter minima by incorporating a buffer of critical momentum terms during training."
      },
      "aliases": [
        "Optimization"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper discusses and evaluates the proposed optimizer on image classification tasks using datasets like CIFAR-10, CIFAR-100, and ImageNet.",
          "quote": "We empirically demonstrate that it can improve model performance for image classification on ImageNet and CIFAR10/100"
        },
        "aliases": [
          "CV"
        ]
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper evaluates the optimizer on a language modeling task using the Penn Treebank dataset.",
          "quote": "We empirically demonstrate that it can improve model performance for... language modelling on Penn Treebank"
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Adam+CM",
          "justification": "The main contribution of the paper is the Adam+CM optimizer.",
          "quote": "We propose to instead store critical momenta (CM) during training, leading to a new memory-augmented version of Adam (Adam+CM) that can effectively escape sharp basins and converge to flat loss regions."
        },
        "aliases": [
          "Adam with Critical Momenta"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The Adam+CM optimizer is newly introduced in this paper.",
          "quote": "We propose to instead store critical momenta (CM) during training, leading to a new memory-augmented version of Adam (Algorithm 1) that can effectively escape sharp basins and converge to flat loss regions."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper provides empirical results showing that Adam+CM was executed in various experiments.",
          "quote": "We empirically demonstrate that it can improve model performance for image classification on ImageNet and CIFAR10/100, language modelling on Penn Treebank, and online learning tasks on TinyImageNet and 5-dataset."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of Adam+CM with other optimization algorithms like Adam, Adam+SAM, and Adam+CG.",
          "quote": "Results from the previous section show that our proposed approach of exploration in adaptive optimizers (Adam+CM) is less sensitive to the sharpness of the loss surface, less prone to gradient cancellation, and finds flatter solutions on a variety of loss surfaces."
        },
        "referenced_paper_title": {
          "value": "Adam: A method for stochastic optimization",
          "justification": "The Adam optimizer, which is the basis for Adam+CM, was originally introduced in this referenced paper by Kingma and Ba (2015).",
          "quote": "In particular, Adam (Kingma & Ba, 2015) combines momentum with an adaptive learning rate and has become the preeminent choice of optimizer across a variety of models and tasks, particularly in large-scale deep learning models."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "The ImageNet dataset was used to empirically demonstrate the effectiveness of the Adam+CM optimizer in image classification tasks.",
          "quote": "We empirically demonstrate that it can improve model performance for image classification on ImageNet and CIFAR10/100"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "The ImageNet dataset is widely known and does not require a specific reference in the paper.",
          "quote": "ImageNet: A large-scale hierarchical image database"
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The CIFAR-10 dataset was used to empirically demonstrate the effectiveness of the Adam+CM optimizer in image classification tasks.",
          "quote": "We empirically demonstrate that it can improve model performance for image classification on ImageNet and CIFAR10/100"
        },
        "aliases": [
          "CIFAR10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The CIFAR-10 dataset is widely known and does not require a specific reference in the paper.",
          "quote": "CIFAR-10"
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "The CIFAR-100 dataset was used to empirically demonstrate the effectiveness of the Adam+CM optimizer in image classification tasks.",
          "quote": "We empirically demonstrate that it can improve model performance for image classification on ImageNet and CIFAR10/100"
        },
        "aliases": [
          "CIFAR100"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The CIFAR-100 dataset is widely known and does not require a specific reference in the paper.",
          "quote": "CIFAR-100"
        }
      },
      {
        "name": {
          "value": "Penn Treebank",
          "justification": "The Penn Treebank dataset was used to evaluate the Adam+CM optimizer in a language modeling task.",
          "quote": "We empirically demonstrate that it can improve model performance for... language modelling on Penn Treebank"
        },
        "aliases": [
          "PTB"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Building a large annotated corpus of English: The Penn Treebank",
          "justification": "The Penn Treebank dataset is widely known and does not require a specific reference in the paper.",
          "quote": "The Penn Treebank (PTB)"
        }
      },
      {
        "name": {
          "value": "TinyImageNet",
          "justification": "The TinyImageNet dataset was used to empirically demonstrate the effectiveness of the Adam+CM optimizer in online learning tasks.",
          "quote": "We empirically demonstrate that it can improve model performance for... online learning tasks on TinyImageNet and 5-dataset"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TinyImageNet",
          "justification": "The TinyImageNet dataset is widely known and does not require a specific reference in the paper.",
          "quote": "TinyImageNet"
        }
      },
      {
        "name": {
          "value": "5-dataset",
          "justification": "The 5-dataset was used to empirically demonstrate the effectiveness of the Adam+CM optimizer in online learning tasks.",
          "quote": "We empirically demonstrate that it can improve model performance for... online learning tasks on TinyImageNet and 5-dataset"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "An empirical investigation of the role of pre-training in lifelong learning",
          "justification": "The 5-dataset is referenced in the context of lifelong learning, which aligns with the paper's focus on online learning tasks.",
          "quote": "5-dataset"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments were conducted using the PyTorch deep learning library, as mentioned in the context of training models.",
          "quote": "We used a publicly available EfficientNet implementation in PyTorch (Paszke et al., 2019)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "PyTorch is a well-known deep learning library and its reference is provided in the context of model implementation.",
          "quote": "PyTorch: An imperative style, high-performance deep learning library"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1702,
    "prompt_tokens": 21698,
    "total_tokens": 23400
  }
}