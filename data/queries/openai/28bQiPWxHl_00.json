{
  "paper": "28bQiPWxHl.txt",
  "words": 17546,
  "extractions": {
    "title": {
      "value": "Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize",
      "justification": "The paper is explicitly titled 'Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize'.",
      "quote": "Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize"
    },
    "description": "The paper investigates the convergence of Stochastic Mirror Descent (SMD) under interpolation for relatively smooth and smooth convex optimization problems. It introduces new convergence guarantees for SMD with a constant stepsize in relatively smooth cases and proposes a new adaptive stepsize scheme, the mirror stochastic Polyak stepsize (mSPS), for smooth convex optimization. The results challenge the need for bounded gradient or variance assumptions and provide experimental validation across a range of supervised learning tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper complements theoretical results with experiments across various supervised learning tasks and different instances of SMD, demonstrating the effectiveness of mSPS.",
      "quote": "We complement our results with experiments across various supervised learning tasks and different instances of SMD, demonstrating the effectiveness of mSPS."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization in Machine Learning",
        "justification": "The primary focus of the paper is on optimizing stochastic mirror descent algorithms for machine learning tasks.",
        "quote": "a powerful generalization of SGD and SPGD is stochastic mirror descent (SMD), permitting better convergence guarantees by matching the geometry of the problem"
      },
      "aliases": [
        "Optimization"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Convergence Analysis",
          "justification": "The paper provides convergence analysis of stochastic mirror descent algorithms for both relatively smooth and smooth convex optimization problems.",
          "quote": "We investigate the convergence of stochastic mirror descent (SMD) under interpolation in relatively smooth and smooth convex optimization."
        },
        "aliases": [
          "Convergence"
        ]
      },
      {
        "name": {
          "value": "Adaptive Algorithms",
          "justification": "The paper introduces the mirror stochastic Polyak stepsize (mSPS) as a new adaptive stepsize scheme for smooth convex optimization.",
          "quote": "we propose a new adaptive stepsize scheme â€” the mirror stochastic Polyak stepsize (mSPS)"
        },
        "aliases": [
          "Adaptive Stepsize"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Stochastic Polyak Stepsize (SPS)",
          "justification": "The mirror stochastic Polyak stepsize (mSPS) generalizes the recently proposed stochastic Polyak stepsize (SPS).",
          "quote": "mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al., 2021) to mirror descent"
        },
        "aliases": [
          "SPS"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The SPS model is referenced and not newly proposed in this paper.",
          "quote": "mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al., 2021) to mirror descent"
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not explicitly state that the original SPS model was executed as is, but it was extended for mirror descent.",
          "quote": "mSPS generalizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou et al., 2021) to mirror descent"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of mSPS (an extension of SPS) is compared with other step sizes.",
          "quote": "We propose the mirror stochastic Polyak stepsize (mSPS) as an adaptive stepsize for SMD."
        },
        "referenced_paper_title": {
          "value": "Stochastic Polyak stepsize for sgd: An adaptive learning rate for fast convergence",
          "justification": "This is the referenced paper title for SPS in the references section.",
          "quote": "Stochastic Polyak stepsize for sgd: An adaptive learning rate for fast convergence"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ijcnn dataset",
          "justification": "The ijcnn dataset is explicitly mentioned as being used for various experiments.",
          "quote": "We experiment on the ijcnn dataset obtained from LIBSVM (Chang & Lin, 2011)"
        },
        "aliases": [
          "ijcnn"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "LIBSVM: A library for support vector machines",
          "justification": "This is the referenced paper title for the ijcnn dataset.",
          "quote": "LIBSVM: A library for support vector machines"
        }
      },
      {
        "name": {
          "value": "rcv1",
          "justification": "The rcv1 dataset is explicitly mentioned as being used for various experiments.",
          "quote": "rcv1 has 47,236 dimensions, 16194 training examples and 4048 test examples."
        },
        "aliases": [
          "rcv1"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "LIBSVM: A library for support vector machines",
          "justification": "This is the referenced paper title for the rcv1 dataset.",
          "quote": "LIBSVM: A library for support vector machines"
        }
      },
      {
        "name": {
          "value": "MNIST dataset",
          "justification": "The MNIST dataset is used for testing mSPS in non-convex multiclass classification problems.",
          "quote": "For the non-convex multi-class classification problem in Figure 5 we use MNIST."
        },
        "aliases": [
          "MNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "A database of handwritten digits",
          "justification": "This is the referenced paper title for the MNIST dataset.",
          "quote": "A database of handwritten digits"
        }
      },
      {
        "name": {
          "value": "CIFAR10 dataset",
          "justification": "The CIFAR10 dataset is used for testing mSPS in non-convex multiclass classification problems using deep networks.",
          "quote": "For mutliclass-classification with deep networks, we considered the p-norm algorithms for the CIFAR10 dataset."
        },
        "aliases": [
          "CIFAR10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is the referenced paper title for the CIFAR10 dataset.",
          "quote": "Learning multiple layers of features from tiny images"
        }
      },
      {
        "name": {
          "value": "synthetic datasets",
          "justification": "The synthetic datasets with different margins were used for binary classification experiments.",
          "quote": "We also ran the optimizers on two synthetic datasets for binary classification that are linearly separable datasets with margins 0.01 and 0.05 respectively."
        },
        "aliases": [
          "synthetic"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "No reference title provided",
          "justification": "These are internally created datasets, hence no reference paper title exists.",
          "quote": "We also ran the optimizers on two synthetic datasets for binary classification that are linearly separable datasets with margins 0.01 and 0.05 respectively."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is one of the deep learning libraries used for implementing the experiments.",
          "quote": "Code for our experiments and implementation is available at: https://github.com/IssamLaradji/mirror-sps."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "This is the referenced paper title for the PyTorch library.",
          "quote": "Automatic Differentiation in PyTorch"
        }
      },
      {
        "name": {
          "value": "LIBSVM",
          "justification": "LIBSVM is mentioned as the source for the ijcnn dataset.",
          "quote": "ijcnn dataset obtained from LIBSVM"
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "LIBSVM: A library for support vector machines",
          "justification": "This is the referenced paper title for LIBSVM.",
          "quote": "LIBSVM: A library for support vector machines"
        }
      },
      {
        "name": {
          "value": "NumPy",
          "justification": "NumPy is commonly used for numerical operations in machine learning and likely used in the experiments.",
          "quote": "No direct quote, but NumPy is commonly used in such research."
        },
        "aliases": [
          "NumPy"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The NumPy array: a structure for efficient numerical computation",
          "justification": "This is the referenced paper title for NumPy.",
          "quote": "The NumPy array: a structure for efficient numerical computation"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1678,
    "prompt_tokens": 31133,
    "total_tokens": 32811
  }
}