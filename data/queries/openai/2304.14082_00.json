{
  "paper": "2304.14082.txt",
  "words": 7746,
  "extractions": {
    "title": {
      "value": "JaxPruner: A Concise Library for Sparsity Research",
      "justification": "The title accurately reflects the main subject and purpose of the paper which is introducing the JaxPruner library for sparsity research.",
      "quote": "This work introduces JaxPruner, a JAX-based sparsity library for machine learning research."
    },
    "description": "This work introduces JaxPruner, a JAX-based sparsity library to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms. The library simplifies integration with other JAX-based libraries and demonstrates its utility through examples in various codebases like Scenic, t5x, Dopamine, and FedJAX. The paper also benchmarks algorithms implemented in JaxPruner on multiple datasets and discusses future plans.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes empirical benchmark results of JaxPruner on multiple standard datasets and demonstrates the utility of the library through various experiments.",
      "quote": "We benchmark pruning and sparse training algorithms in 4 different domains and discuss them in subsequent sections"
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on developing and evaluating a library for deep learning models, specifically for sparsity research in neural networks.",
        "quote": "JaxPruner aims to accelerate research on sparse neural networks..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Optimization",
          "justification": "The paper deals specifically with optimizing neural network models through sparsity techniques such as pruning and sparse training.",
          "quote": "JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ViT-B/16",
          "justification": "This model is used in the benchmarks for image classification experiments within the paper.",
          "quote": "We apply JaxPruner algorithms to train 80% sparse ViT-B/16..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ResNet-50",
          "justification": "This model is used in the empirical evaluation for image classification tasks.",
          "quote": "We train 80% sparse ResNet-50 models on ImageNet..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "T5-Base",
          "justification": "This model is used for the language modeling benchmarks in the experiments.",
          "quote": "We also build a JaxPruner integration with the t5x library... we prune 80% of the weights... of our LM architecture"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DQN",
          "justification": "This model is used for deep reinforcement learning experiments within the Dopamine library.",
          "quote": "Though it is possible to run any of the Atari games and agents, we choose MsPacman and DQN for our experiments."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet-2012",
          "justification": "The ImageNet-2012 dataset is used for benchmarking ViT-B/16, PlainViT-S/16, and ResNet-50 models.",
          "quote": "We benchmark pruning and sparse training algorithms in 4 different domains... ImageNet-2012 [36] image classification using the ViT-B/16, PlainViT-S/16 (PViT) and ResNet-50 architectures."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Federated EMNIST",
          "justification": "The dataset is used for character recognition tasks in the federated learning benchmarks.",
          "quote": "We test the effect of various pruning algorithms on the federated EMNIST character recognition benchmark [40]"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "C4",
          "justification": "The dataset is used for language modeling benchmarks with the T5-Base model.",
          "quote": "We train from scratch a T5-base (220M parameter) model to predict missing words within a corrupted span of text on the C4 dataset"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MsPacman Atari 2600",
          "justification": "The dataset is used with the DQN model for deep reinforcement learning benchmarks.",
          "quote": "We choose MsPacman and DQN for our experiments... report the average returns calculated over 125000 environment steps at the end of the training."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JaxPruner",
          "justification": "JaxPruner is the main library introduced and discussed in the paper for sparsity research.",
          "quote": "This work introduces JaxPruner, a JAX-based sparsity library for machine learning research."
        },
        "aliases": [],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "JAX",
          "justification": "JAX is the foundational library on which JaxPruner is built, and it is utilized for its features such as function transformations and gradient calculations.",
          "quote": "JAX [3] has seen increasing adoption by the research community...functions like taking gradients... reduces the time required for implementing complex ideas"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Optax",
          "justification": "Optax is an optimization library used in conjunction with JaxPruner for various pruning and sparse training algorithms.",
          "quote": "JaxPruner implements key baselines for each family of algorithms and makes it easy to extend them... with Optax, a widely-used optimization library in JAX"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Scenic",
          "justification": "Scenic is one of the libraries integrated with JaxPruner for facilitating research and it is used in the examples provided.",
          "quote": "We demonstrate the ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine, and FedJAX"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "t5x",
          "justification": "t5x is one of the libraries integrated with JaxPruner for facilitating research specifically in the domain of language modeling.",
          "quote": "We build a JaxPruner integration with the t5x library [30], which opens access to a suite of Transformer-based Language Models"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Dopamine",
          "justification": "Dopamine is one of the libraries integrated with JaxPruner to conduct experiments in deep reinforcement learning.",
          "quote": "We integrate JaxPruner with Dopamine as it has been used in the past for sparsity research"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "FedJAX",
          "justification": "FedJAX is one of the libraries integrated with JaxPruner for facilitating research specifically in the domain of federated learning.",
          "quote": "FedJAX [6] supports federated learning research through JAX-based federated algorithm design and simulation, and can easily be integrated with JaxPruner"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1571,
    "prompt_tokens": 14490,
    "total_tokens": 16061
  }
}