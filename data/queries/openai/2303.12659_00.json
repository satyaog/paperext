{
  "paper": "2303.12659.txt",
  "words": 9949,
  "extractions": {
    "title": {
      "value": "Posthoc Interpretation via Quantization",
      "justification": "The title of the paper.",
      "quote": "Posthoc Interpretation via Quantization"
    },
    "description": "The paper introduces a novel method called Posthoc Interpretation via Quantization (PIQ) for interpreting decisions made by trained classifiers. By utilizing vector quantization, it transforms classifier representations into a discrete, class-specific latent space, offering clear, human-understandable explanations for neural network decisions. The method is evaluated through studies involving images and audio.",
    "type": {
      "value": "empirical study",
      "justification": "The paper involves experiments and studies evaluating the proposed PIQ method through quantitative and qualitative analysis, including user studies.",
      "quote": "We evaluated our method through quantitative and qualitative studies involving black-and-white images, color images, and audio."
    },
    "primary_research_field": {
      "name": {
        "value": "Explainable AI",
        "justification": "The focus of the paper is on interpretability and providing explanations for neural network decisions, which falls under the domain of Explainable AI.",
        "quote": "Interpretability is the ability to understand and explain a modelâ€™s predictions."
      },
      "aliases": [
        "XAI",
        "Interpretable Machine Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper presents methods evaluated on various image datasets, including MNIST, FashionMNIST, Quickdraw, and ImageNet.",
          "quote": "We present experimental results on images and audio. On images, we provide evidence on handwritten digits from the MNIST dataset LeCun and Cortes [2010], clothing items from the FashionMNIST dataset Xiao et al. [2017], hand drawings from the Quickdraw dataset Ha and Eck [2017], and real-world images from the ImageNet dataset Russakovsky et al. [2014]."
        },
        "aliases": [
          "Vision"
        ]
      },
      {
        "name": {
          "value": "Audio Processing",
          "justification": "The paper also evaluates the proposed method on audio data from the ESC50 dataset.",
          "quote": "For audio, we show results on audio clips for sound events from the ESC50 dataset Piczak [2015a]."
        },
        "aliases": [
          "Sound Processing",
          "Speech Processing"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Posthoc Interpretation via Quantization (PIQ)",
          "justification": "The PIQ method is the central model proposed by the paper.",
          "quote": "In this paper, we introduce a new approach, called Posthoc Interpretation via Quantization (PIQ), for interpreting decisions made by trained classifiers."
        },
        "aliases": [
          "PIQ"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The method PIQ is introduced as a novel contribution of this paper.",
          "quote": "In this paper, we introduce a new approach, called Posthoc Interpretation via Quantization (PIQ), for interpreting decisions made by trained classifiers."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed for evaluation purposes as demonstrated in their experiments.",
          "quote": "We present experimental results on images and audio."
        },
        "is_compared": {
          "value": 1,
          "justification": "PIQ is compared quantitatively and qualitatively with several other interpretation methods, such as LIME, VIBI, FLINT, L2I, and GradCAM.",
          "quote": "We also perform a user study of human preferences by comparing PIQ to previous methods such as LIME Ribeiro et al. [2016], VIBI Bang et al. [2021], FLINT Parekh et al. [2020], L2I Parekh et al. [2022], and GradCAM Selvaraju et al. [2016]."
        },
        "referenced_paper_title": {
          "value": "Neural discrete representation learning",
          "justification": "PIQ involves vector quantization, a technique first introduced for Vector-Quantized VAE by van den Oord et al.",
          "quote": "To train our interpretation module, we use the vector quantization objective, which was first introduced for Vector-Quantized VAE van den Oord et al. [2017], to discretize this latent space."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST dataset is used for evaluation of PIQ on handwritten digits.",
          "quote": "We provide evidence on handwritten digits from the MNIST dataset LeCun and Cortes [2010]."
        },
        "aliases": [
          "MNIST dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "MNIST handwritten digit database",
          "justification": "The dataset reference points to the original work by Yann LeCun and Corinna Cortes.",
          "quote": "MNIST dataset LeCun and Cortes [2010]."
        }
      },
      {
        "name": {
          "value": "FashionMNIST",
          "justification": "FashionMNIST dataset is used for evaluation of PIQ on clothing items.",
          "quote": "We present experimental results on images and audio. On images, we provide evidence on handwritten digits from the MNIST dataset LeCun and Cortes [2010], clothing items from the FashionMNIST dataset Xiao et al. [2017]."
        },
        "aliases": [
          "Fashion MNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
          "justification": "The dataset reference points to the original work by Han Xiao, Kashif Rasul, and Roland Vollgraf.",
          "quote": "FashionMNIST dataset Xiao et al. [2017]."
        }
      },
      {
        "name": {
          "value": "Quickdraw",
          "justification": "Quickdraw dataset is used for evaluation of PIQ on hand drawings.",
          "quote": "We present experimental results on images and audio. On images, we provide evidence on... hand drawings from the Quickdraw dataset Ha and Eck [2017]."
        },
        "aliases": [
          "Quick, Draw!"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "A neural representation of sketch drawings",
          "justification": "The dataset reference points to the original work by David Ha and Douglas Eck.",
          "quote": "Quickdraw dataset Ha and Eck [2017]."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet dataset is used for evaluation of PIQ on real-world images.",
          "quote": "We present experimental results on images and audio... and real-world images from the ImageNet dataset Russakovsky et al. [2014]."
        },
        "aliases": [
          "ILSVRC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Imagenet large scale visual recognition challenge",
          "justification": "The dataset reference points to the original work by Olga Russakovsky et al.",
          "quote": "ImageNet dataset Russakovsky et al. [2014]."
        }
      },
      {
        "name": {
          "value": "ESC50",
          "justification": "ESC50 dataset is used for evaluation of PIQ on sound events.",
          "quote": "For audio, we show results on audio clips for sound events from the ESC50 dataset Piczak [2015a]."
        },
        "aliases": [
          "ESC-50"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ESC: Dataset for Environmental Sound Classification",
          "justification": "The dataset reference points to the original work by Karol J. Piczak.",
          "quote": "ESC50 dataset Piczak [2015a]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Segment Anything Model (SAM)",
          "justification": "SAM is used for obtaining target masks for complex images like those from the ImageNet dataset.",
          "quote": "We used a foundational image segmentation model, SAM Kirillov et al. [2023], to obtain the training target masks."
        },
        "aliases": [
          "SAM",
          "Segment Anything"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Segment anything",
          "justification": "The library reference points to the original work by Kirillov et al.",
          "quote": "SAM Kirillov et al. [2023]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1545,
    "prompt_tokens": 16967,
    "total_tokens": 18512
  }
}