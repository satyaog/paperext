{
  "paper": "2310.03882.txt",
  "words": 11073,
  "extractions": {
    "title": {
      "value": "Small batch deep reinforcement learning",
      "justification": "The exact title as given in the provided text.",
      "quote": "Small batch deep reinforcement learning"
    },
    "description": "The paper investigates the impact of batch size on value-based deep reinforcement learning with replay memories. It presents an empirical study showing that smaller batch sizes can lead to significant performance gains and computational savings, thereby challenging the commonly held belief that larger batch sizes are always more beneficial.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents an empirical study based on experiments and observations.",
      "quote": "In this work we conduct a broad empirical study of batch size in online value-based deep reinforcement learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Reinforcement Learning",
        "justification": "The primary focus of the paper is on improving deep reinforcement learning by studying the effects of batch size.",
        "quote": "One of the central concerns for deep reinforcement learning (RL) is how to efficiently make the most use of the collected data for policy improvement."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Batch Size Optimization",
          "justification": "The paper specifically investigates the impact of different batch sizes in deep reinforcement learning.",
          "quote": "The number of sampled transitions at each learning step is known as the batch size, and is meant to produce an unbiased estimator of the underlying data distribution."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "DQN",
          "justification": "The model is one of the standard value-based agents used in the experiments.",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents, which were initially benchmarked on the ALE suite: DQN [Mnih et al., 2015]"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Rainbow",
          "justification": "The model is one of the standard value-based agents used in the experiments.",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents,... Rainbow [Hessel et al., 2018]"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "QR-DQN",
          "justification": "The model is prominently used and analyzed throughout the paper.",
          "quote": "For reasons which will be clarified below, most of our evaluations and analyses were conducted with the QR-DQN agent."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "IQN",
          "justification": "The model is one of the value-based agents evaluated in the experiments.",
          "quote": "We begin by investigating the impact reducing the batch size can have on four popular value-based agents,... and IQN [Dabney et al., 2018b]"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Arcade Learning Environment",
          "justification": "The dataset is used for evaluating deep reinforcement learning agents across various Atari 2600 games.",
          "quote": "We evaluate our agents on 20 games chosen by Fedus et al. [2020] for their analysis of replay ratios, picked to offer a diversity of difficulty and dynamics [...] All experiments were run on NVIDIA Tesla P100 GPUs."
        },
        "aliases": [
          "ALE"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "The experiments are conducted using JAX implementations of RL agents.",
          "quote": "Experimental setup: We use the Jax implementations of RL agents, with their default hyperparameter values, provided by the Dopamine library [Castro et al., 2018] and applied to the Arcade Learning Environment (ALE) [Bellemare et al., 2013]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Dopamine",
          "justification": "The Dopamine library is used for implementing and running experiments.",
          "quote": "Experimental setup: We use the Jax implementations of RL agents, with their default hyperparameter values, provided by the Dopamine library [Castro et al., 2018]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1313,
    "prompt_tokens": 20494,
    "total_tokens": 21807
  }
}