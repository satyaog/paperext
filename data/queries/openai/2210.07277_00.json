{
  "paper": "2210.07277.txt",
  "words": 13195,
  "extractions": {
    "title": {
      "value": "The Hidden Uniform Cluster Prior in Self-Supervised Learning",
      "justification": "The title of the paper from the given text",
      "quote": "The Hidden Uniform Cluster Prior in Self-Supervised Learning"
    },
    "description": "The paper discusses the issue of uniform feature priors embedded in many self-supervised learning (SSL) methods. It shows that these priors, which promote uniform clustering of data, can hamper performance on class-imbalanced datasets. The paper presents a modified approach, termed Prior Matching for Siamese Networks (PMSN), which adapts feature priors to better align with the distribution of real-world data, specifically targeting class-imbalanced datasets to improve representation quality.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves theoretical analysis and empirical experiments to validate the impact of uniform feature priors on SSL methods and proposes a new empirical approach PMSN for better performance.",
      "quote": "First, we theoretically show that current methods with volume maximization regularizers such as VICReg (Bardes et al., 2021), SwAV (Caron et al., 2020), MSN (Assran et al., 2022) and SimCLR (Chen et al., 2020b) (with limited assumptions), have a uniform feature prior; ... we empirically validate that joint-embedding methods employing volume maximization regularizers are sensitive to the mini-batch class distributions."
    },
    "primary_research_field": {
      "name": {
        "value": "Self-Supervised Learning",
        "justification": "The paper primarily focuses on self-supervised learning methods and their inherent uniform feature priors.",
        "quote": "A successful paradigm in representation learning is to perform self-supervised pretraining using tasks based on mini-batch statistics (e.g., SimCLR, VICReg, SwAV, MSN)."
      },
      "aliases": [
        "SSL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Visual Representation Learning",
          "justification": "The paper evaluates self-supervised learning methods primarily within the context of visual data and representation learning for tasks like image classification.",
          "quote": "In the context of visual data, these approaches typically learn representations by training a neural network encoder to produce similar embeddings for two or more views of the same image."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "SimCLR is used as a representative SSL method with uniform feature prior.",
          "quote": "we theoretically show that current methods with volume maximization regularizers such as VICReg (Bardes et al., 2021), SwAV (Caron et al., 2020), MSN (Assran et al., 2022) and SimCLR (Chen et al., 2020b) (with limited assumptions), have a uniform feature prior"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model SimCLR is not a contribution of this paper but is used as a benchmark.",
          "quote": "we theoretically show that current methods with volume maximization regularizers such as VICReg (Bardes et al., 2021), SwAV (Caron et al., 2020), MSN (Assran et al., 2022) and SimCLR (Chen et al., 2020b) (with limited assumptions), have a uniform feature prior"
        },
        "is_executed": {
          "value": 1,
          "justification": "SimCLR was executed and evaluated as part of the study.",
          "quote": "We use the VISSL (Goyal et al., 2021) code base to pretrain a ResNet-50 with SimCLR (Chen et al., 2020b), with a batch size of 4096 for 300 epochs."
        },
        "is_compared": {
          "value": 1,
          "justification": "SimCLR was compared to other models in the scope of the paper.",
          "quote": "We explore three joint-embedding methods employing diverse collapse prevention strategies: SimCLR (Chen et al., 2020b), VICReg (Bardes et al., 2021), and MSN (Assran et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "A Simple Framework for Contrastive Learning of Visual Representations",
          "justification": "This is the reference paper title for the SimCLR model as cited within the given text.",
          "quote": "SimCLR (Chen et al., 2020b)"
        }
      },
      {
        "name": {
          "value": "VICReg",
          "justification": "VICReg is used as a representative SSL method with volume maximization regularizers.",
          "quote": "we theoretically show that current methods with volume maximization regularizers such as VICReg (Bardes et al., 2021), SwAV (Caron et al., 2020), MSN (Assran et al., 2022) and SimCLR (Chen et al., 2020b) (with limited assumptions), have a uniform feature prior"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model VICReg is not a contribution of this paper but is used as a benchmark.",
          "quote": "we theoretically show that current methods with volume maximization regularizers such as VICReg (Bardes et al., 2021), SwAV (Caron et al., 2020), MSN (Assran et al., 2022) and SimCLR (Chen et al., 2020b) (with limited assumptions), have a uniform feature prior"
        },
        "is_executed": {
          "value": 1,
          "justification": "VICReg was executed and evaluated as part of the study.",
          "quote": "We pretrain a ResNet-50 with VICReg (Bardes et al., 2021) using the LARS optimizer with a batch size of 1024 for 300 epochs."
        },
        "is_compared": {
          "value": 1,
          "justification": "VICReg was compared to other models in the scope of the paper.",
          "quote": "We explore three joint-embedding methods employing diverse collapse prevention strategies: SimCLR (Chen et al., 2020b), VICReg (Bardes et al., 2021), and MSN (Assran et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
          "justification": "This is the reference paper title for the VICReg model as cited within the given text.",
          "quote": "VICReg (Bardes et al., 2021)"
        }
      },
      {
        "name": {
          "value": "SwAV",
          "justification": "SwAV is used as a representative SSL method with volume maximization regularizers.",
          "quote": "we theoretically show that current methods with volume maximization regularizers such as VICReg (Bardes et al., 2021), SwAV (Caron et al., 2020), MSN (Assran et al., 2022) and SimCLR (Chen et al., 2020b) (with limited assumptions), have a uniform feature prior"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model SwAV is not a contribution of this paper but is used as a benchmark.",
          "quote": "we theoretically show that current methods with volume maximization regularizers such as VICReg (Bardes et al., 2021), SwAV (Caron et al., 2020), MSN (Assran et al., 2022) and SimCLR (Chen et al., 2020b) (with limited assumptions), have a uniform feature prior"
        },
        "is_executed": {
          "value": 1,
          "justification": "It is mentioned that joint-embedding methods employing various techniques were evaluated, which includes SwAV.",
          "quote": "Given the recent success of joint-embedding methods, there is a growing literature that aims to build a better understanding of their behaviour...."
        },
        "is_compared": {
          "value": 1,
          "justification": "SwAV was theoretically compared to other SSL models in terms of its volume maximization regularizer.",
          "quote": "Second, we empirically validate that joint-embedding methods employing volume maximization regularizers are sensitive to the mini-batch class distributions."
        },
        "referenced_paper_title": {
          "value": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
          "justification": "This is the reference paper title for the SwAV model as cited within the given text.",
          "quote": "SwAV (Caron et al., 2020)"
        }
      },
      {
        "name": {
          "value": "MSN",
          "justification": "MSN is used as a method for self-supervised learning and it is extended in this research.",
          "quote": "we theoretically show that current methods with volume maximization regularizers such as VICReg (Bardes et al., 2021), SwAV (Caron et al., 2020), MSN (Assran et al., 2022) and SimCLR (Chen et al., 2020b) (with limited assumptions), have a uniform feature prior"
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The extension of MSN to PMSN is the main contribution of this paper.",
          "quote": "In particular, we extend Masked Siamese Networks (MSN) of Assran et al. (2022) to support the use of arbitrary features priors, and refer to this extension as Prior Matching for Siamese Networks (PMSN)."
        },
        "is_executed": {
          "value": 1,
          "justification": "MSN was executed and evaluated as part of the study.",
          "quote": "We pretrain a ViT-S/16 with MSN (Assran et al., 2022) using the AdamW optimizer with a batch size of 1024 for 300 epochs."
        },
        "is_compared": {
          "value": 1,
          "justification": "MSN was compared to other models in the scope of the paper.",
          "quote": "We explore three joint-embedding methods employing diverse collapse prevention strategies: SimCLR (Chen et al., 2020b), VICReg (Bardes et al., 2021), and MSN (Assran et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "Masked Siamese Networks for Label-Efficient Learning",
          "justification": "This is the reference paper title for the MSN model as cited within the given text.",
          "quote": "MSN (Assran et al., 2022)"
        }
      },
      {
        "name": {
          "value": "PMSN",
          "justification": "PMSN is the main proposed model in the paper as an extension of MSN.",
          "quote": "we extend Masked Siamese Networks (MSN) of Assran et al. (2022) to support the use of arbitrary features priors, and refer to this extension as Prior Matching for Siamese Networks (PMSN)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "PMSN is the main contribution of this paper as an extended method of MSN to handle class-imbalanced datasets better.",
          "quote": "we extend Masked Siamese Networks (MSN) of Assran et al. (2022) to support the use of arbitrary features priors, and refer to this extension as Prior Matching for Siamese Networks (PMSN)."
        },
        "is_executed": {
          "value": 1,
          "justification": "PMSN was executed and evaluated as part of the study.",
          "quote": "we extend Masked Siamese Networks (MSN) of Assran et al. (2022) to support the use of arbitrary features priors, and refer to this extension as Prior Matching for Siamese Networks (PMSN)."
        },
        "is_compared": {
          "value": 1,
          "justification": "PMSN was compared to its base model MSN and others to show its improvements.",
          "quote": "When pretraining on the iNaturalist 2018 dataset (Van Horn et al., 2018), which is naturally long-tailed, we demonstrate that moving away from uniform priors leads to more semantic representations and improved transfer on downstream tasks."
        },
        "referenced_paper_title": {
          "value": "Masked Siamese Networks for Label-Efficient Learning",
          "justification": "PMSN is an extension of MSN which was introduced in the paper titled 'Masked Siamese Networks for Label-Efficient Learning'.",
          "quote": "we extend Masked Siamese Networks (MSN) of Assran et al. (2022) to support the use of arbitrary features priors, and refer to this extension as Prior Matching for Siamese Networks (PMSN)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet dataset is referenced multiple times as a dataset for pretraining and evaluating the SSL methods.",
          "quote": "While this prior has led to remarkably semantic representations when pretraining on class-balanced data, such as ImageNet, we demonstrate that it can hamper performance when pretraining on class-imbalanced data."
        },
        "aliases": [
          "ImageNet-1K"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "The reference paper for ImageNet is 'ImageNet Large Scale Visual Recognition Challenge'.",
          "quote": "ImageNet dataset (Russakovsky et al., 2015)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "VISSL",
          "justification": "The VISSL library was used for code and pretraining in experiments with SimCLR.",
          "quote": "We use the VISSL (Goyal et al., 2021) code base to pretrain a ResNet-50 with SimCLR (Chen et al., 2020b)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "VISSL: Extensible, High performance, and versatile Self-Supervised Learning library",
          "justification": "The paper mentioned VISSL by reference to 'Goyal et al., 2021'.",
          "quote": "VISSL (Goyal et al., 2021)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2802,
    "prompt_tokens": 24222,
    "total_tokens": 27024
  }
}