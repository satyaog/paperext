{
  "paper": "2303.03915.txt",
  "words": 16866,
  "extractions": {
    "title": {
      "value": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset",
      "justification": "This is the exact title of the research paper provided by the user.",
      "quote": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"
    },
    "description": "This paper details the creation and analysis of the BigScience ROOTS corpus, a comprehensive multilingual dataset consisting of 1.6TB of text spanning 59 languages. It documents the data curation efforts, including the roles of various working groups, the preprocessing and filtering steps, as well as the ethical considerations behind the project. The dataset was used to train the 176-billion-parameter BLOOM language model.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper focuses on the empirical creation, curation, and analysis of a large multilingual dataset. It includes specific methods for data cleaning, filtering, and deduplication, and provides statistical analyses of the dataset.",
      "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on creating a multilingual corpus for training large language models, which is a core area of Natural Language Processing.",
        "quote": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multilingual Datasets and Large Language Models",
          "justification": "The sub-field focuses on multilingual datasets and their use in training large-scale language models, aligning with the paper's goal of creating and analyzing a multilingual dataset for the BLOOM language model.",
          "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Opencollaboration Open-access Multilingual language model."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BLOOM",
          "justification": "The BLOOM model is trained using the ROOTS corpus, making it the main model discussed in the paper.",
          "quote": "that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ROOTS",
          "justification": "The ROOTS dataset is the main contribution of the paper, meticulously created and processed as a large-scale multilingual text corpus.",
          "quote": "assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages."
        },
        "aliases": [],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stanza",
          "justification": "Stanza is used for sentence tokenization in multiple languages as part of the data processing pipeline.",
          "quote": "For Arabic, Catalan, Basque, Indonesian, and Chinese (both simplified and traditional), we use the Stanza tokenizer (Qi et al., 2020)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "NLTK",
          "justification": "NLTK is used for sentence tokenization in multiple languages as part of the data processing pipeline.",
          "quote": "For English, French, Portuguese, and Spanish, we use the NLTK tokenizer (Bird et al., 2009)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Indic NLP Library",
          "justification": "Indic NLP Library is used for sentence tokenization for multiple Indian languages as part of the data processing pipeline.",
          "quote": "For Bengalic, Gujarati, Hindi, Kannada, Malayalam, Marathi, Punjabi, Tamil, and Telugu, we use the Indic NLP library tokenizer (Kunchukuttan, 2020)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Underthesea",
          "justification": "Underthesea is used for sentence tokenization for the Vietnamese language as part of the data processing pipeline.",
          "quote": "For Vietnamese, we use the Underthesea tokenizer."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SentencePiece",
          "justification": "SentencePiece is used to train unigram tokenizers for multiple languages for OSCAR data filtering.",
          "quote": "Following Wenzek et al. (2020), we trained SentencePiece unigram tokenizers (Kudo, 2018)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "KenLM",
          "justification": "KenLM is used to train 5-gram models after tokenization for OSCAR data filtering.",
          "quote": "KenLM 5-gram models after tokenization (Heafield, 2011) on Wikipedia article openings for every language that was extracted from OSCAR."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1054,
    "prompt_tokens": 28622,
    "total_tokens": 29676
  }
}