{
  "paper": "JoISqbH8vl.txt",
  "words": 10255,
  "extractions": {
    "title": {
      "value": "Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations",
      "justification": "The title was explicitly mentioned at the beginning of the research paper.",
      "quote": "Self-Supervised Disentanglement by Leveraging Structure in Data Augmentations"
    },
    "description": "The paper introduces a principled approach to self-supervised representation learning that uses data augmentations to disentangle style features of data rather than discard them, aiming to learn more universal representations. The proposed framework involves multiple style embedding spaces and maximizes joint entropy for improved downstream performance.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experiments on synthetic datasets and ImageNet to demonstrate the benefits of the proposed methods.",
      "quote": "We empirically demonstrate the benefits our approach on synthetic datasets and then present promising but limited results on ImageNet."
    },
    "primary_research_field": {
      "name": {
        "value": "Self-Supervised Learning",
        "justification": "The paper's focus is on self-supervised representation learning using data augmentations.",
        "quote": "Self-supervised representation learning often uses data augmentations to induce some invariance to 'style' attributes of the data."
      },
      "aliases": [
        "SSL"
      ]
    },
    "sub_research_fields": [],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "The dataset was explicitly mentioned in the context of the experiments conducted in the paper.",
          "quote": "We empirically demonstrate the benefits our approach on synthetic datasets and then present promising but limited results on ImageNet."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet large scale visual recognition challenge",
          "justification": "The dataset reference aligns with the well-known large-scale dataset used for visual recognition tasks.",
          "quote": "ImageNet large scale visual recognition challenge"
        }
      },
      {
        "name": {
          "value": "ColorDSprites",
          "justification": "The dataset is used in experiments discussed in the paper.",
          "quote": "We now make use of a colored version of the DSprites dataset (Locatello et al., 2019) which contains images of 2D shapes generated from 6 independent ground-truth factors."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Challenging common assumptions in the unsupervised learning of disentangled representations",
          "justification": "The reference provided aligns with the description in the text about DSprites.",
          "quote": "Challenging common assumptions in the unsupervised learning of disentangled representations"
        }
      },
      {
        "name": {
          "value": "DSprites",
          "justification": "The base dataset DSprites is mentioned in the context of ColorDSprites.",
          "quote": "We now make use of a colored version of the DSprites dataset (Locatello et al., 2019)"
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "beta-VAE: Learning basic visual concepts with a constrained variational framework",
          "justification": "The original DSprites dataset reference is provided in alignment with the paper's context.",
          "quote": "beta-VAE: Learning basic visual concepts with a constrained variational framework"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "The SimCLR method was explicitly mentioned as a part of the methods used in the experiments.",
          "quote": "Similar to von Kügelgen et al. (2021, Fig. 10), we find that with standard SimCLR (fixed λ), excess capacity is used to encode some style information"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A simple framework for contrastive learning of visual representations",
          "justification": "The reference for SimCLR is correctly identified as a commonly known paper.",
          "quote": "A simple framework for contrastive learning of visual representations"
        }
      },
      {
        "name": {
          "value": "VICReg",
          "justification": "VICReg is listed as one of the SSL methods used in the paper.",
          "quote": "We then train SimCLR and VICReg models on the (unlabelled) dataset using different augmentation strengths."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Variance-Invariance-Covariance Regularization for Self-Supervised Learning",
          "justification": "The reference for VICReg aligns with the context provided in the paper.",
          "quote": "Variance-Invariance-Covariance Regularization for Self-Supervised Learning"
        }
      },
      {
        "name": {
          "value": "BarlowTwins",
          "justification": "BarlowTwins was mentioned in contrast to other SSL methods used in the study.",
          "quote": "In particular, contrastive methods (Chen et al., 2020a,b, 2021; He et al., 2020) do so by pushing apart the embeddings of different images, while non-contrastive methods do so by architectural design (Grill et al., 2020; Chen and He, 2021) or by regularizing the covariance of embeddings (Zbontar et al., 2021; Bardes et al., 2022; Ermolov et al., 2021)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "justification": "The reference for BarlowTwins is provided in context.",
          "quote": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1332,
    "prompt_tokens": 19506,
    "total_tokens": 20838
  }
}