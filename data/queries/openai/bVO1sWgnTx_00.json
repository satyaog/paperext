{
  "paper": "bVO1sWgnTx.txt",
  "words": 3768,
  "extractions": {
    "title": {
      "value": "Efficient Classification of Long Documents via State-Space Models",
      "justification": "This is the exact title of the research paper provided.",
      "quote": "Efficient Classification of Long Documents via State-Space Models"
    },
    "description": "The paper explores the use of State-Space Models (SSMs) for the task of long document classification, comparing their performance and efficiency against traditional transformer-based models. The study introduces the SSM-pooler model, which achieves comparable performance while being more efficient. Extensive experiments are conducted on six diverse long document classification datasets, covering binary, multi-class, and multi-label classification tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experiments and empirical results on the performance of SSMs and transformer-based models on long document classification. It includes quantitative comparisons and discussions on the efficiency of the models.",
      "quote": "In this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The study focuses on document classification, a key task within the field of Natural Language Processing (NLP).",
        "quote": "Transformer-based models have achieved state of the art performance on numerous NLP applications."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Document Classification",
          "justification": "The main focus of this study is on the classification of long documents.",
          "quote": "In this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Transformer (6-layer)",
          "justification": "The Transformer (6-layer) model is used as a baseline in the experiments.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "aliases": [
          "Transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The Transformer model is a well-established model and not a contribution of this paper.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments involve executing the Transformer model for performance comparison.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of the 6-layer Transformer model to other models.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "referenced_paper_title": {
          "value": "Attention Is All You Need",
          "justification": "The referenced paper for the Transformer model is the original Transformer paper by Vaswani et al., titled 'Attention Is All You Need'.",
          "quote": "Transformer (Vaswani et al., 2017)"
        }
      },
      {
        "name": {
          "value": "Longformer",
          "justification": "Longformer is used as a baseline model for comparison in the experiments.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Longformer is not a contribution of this paper; it is used as a baseline model for comparison.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves executing the Longformer model for performance comparisons.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of Longformer against other models.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "referenced_paper_title": {
          "value": "Longformer: The Long-Document Transformer",
          "justification": "The referenced paper for Longformer is the original paper by Beltagy et al., titled 'Longformer: The Long-Document Transformer'.",
          "quote": "Longformer (Beltagy et al., 2020)."
        }
      },
      {
        "name": {
          "value": "S4 (Structured State Space Sequence model)",
          "justification": "The S4 model is one of the main models investigated in the study for long document classification.",
          "quote": "The Structured State Space sequence model (S4) (Gu et al., 2022) utilizes this structure and leverages the Cauchy kernel method to simplify the kernel computation of SSM."
        },
        "aliases": [
          "S4"
        ],
        "is_contributed": {
          "value": false,
          "justification": "S4 is an existing model referenced for comparison in the paper.",
          "quote": "The Structured State Space sequence model (S4) (Gu et al., 2022) utilizes this structure and leverages the Cauchy kernel method to simplify the kernel computation of SSM."
        },
        "is_executed": {
          "value": true,
          "justification": "S4 is executed as part of the experiments in the paper.",
          "quote": "The Structured State Space sequence model (S4) (Gu et al., 2022) utilizes this structure and leverages the Cauchy kernel method to simplify the kernel computation of SSM."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of S4 is compared against other models in the study.",
          "quote": "For models without pre-training, we compare 6-layer S4 and S4-pooler models (11m parameters) with 6-layer Transformer (70m parameters) and Longformer models (99m parameters)."
        },
        "referenced_paper_title": {
          "value": "Efficiently Modeling Long Sequences with Structured State Spaces",
          "justification": "The referenced paper for the S4 model is by Gu et al., titled 'Efficiently Modeling Long Sequences with Structured State Spaces'.",
          "quote": "The Structured State Space sequence model (S4) (Gu et al., 2022) utilizes this structure and leverages the Cauchy kernel method to simplify the kernel computation of SSM."
        }
      },
      {
        "name": {
          "value": "S4-pooler",
          "justification": "S4-pooler is introduced in this study as a modified version of the S4 model for improved efficiency.",
          "quote": "Building on this idea, Hungry Hungry Hippo (H3) (Dao et al., 2022) extends the mechanism by including an additional gate and a short convolution obtained via a shift \nSSM to improve the language modeling ability of SSM-based systems."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "S4-pooler is a new model proposed in this study.",
          "quote": "We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient."
        },
        "is_executed": {
          "value": true,
          "justification": "The S4-pooler is tested in various experiments to validate its performance and efficiency.",
          "quote": "We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient."
        },
        "is_compared": {
          "value": true,
          "justification": "S4-poolerâ€™s performance is compared to other models in the study.",
          "quote": "We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient."
        },
        "referenced_paper_title": {
          "value": "Efficiently Modeling Long Sequences with Structured State Spaces",
          "justification": "Although directly introduced in this paper, S4-pooler builds on the concepts introduced by Gu et al. in 'Efficiently Modeling Long Sequences with Structured State Spaces'.",
          "quote": "The Structured State Space sequence model (S4) (Gu et al., 2022) utilizes this structure and leverages the Cauchy kernel method to simplify the kernel computation of SSM."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ECtHR",
          "justification": "ECtHR is one of the datasets used for evaluating the classification models.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Paragraph-level Rationale Extraction through Regularization: A Case Study on European Court of Human Rights Cases",
          "justification": "The specific referenced paper for this dataset is 'Paragraph-level Rationale Extraction through Regularization: A Case Study on European Court of Human Rights Cases' by Chalkidis et al.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        }
      },
      {
        "name": {
          "value": "Hyperpartisan",
          "justification": "Hyperpartisan is one of the datasets used for evaluating the classification models.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Semeval-2019 task 4: Hyperpartisan news detection",
          "justification": "The specific referenced paper for this dataset is 'Semeval-2019 task 4: Hyperpartisan news detection' by Kiesel et al.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        }
      },
      {
        "name": {
          "value": "20News",
          "justification": "20News is one of the datasets used for evaluating the classification models.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        },
        "aliases": [
          "20 Newsgroups"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Newsweeder: Learning to Filter Netnews",
          "justification": "The specific referenced paper for this dataset is 'Newsweeder: Learning to Filter Netnews' by Lang.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        }
      },
      {
        "name": {
          "value": "EURLEX",
          "justification": "EURLEX is one of the datasets used for evaluating the classification models.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Large-Scale Multi-Label Text Classification on EU Legislation",
          "justification": "The specific paper referenced for this dataset is 'Large-Scale Multi-Label Text Classification on EU Legislation' by Chalkidis et al.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        }
      },
      {
        "name": {
          "value": "AMZ (Amazon product reviews)",
          "justification": "AMZ is one of the datasets used for evaluating the classification models.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        },
        "aliases": [
          "Amazon product reviews",
          "AMZ"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering",
          "justification": "The specific paper referenced for this dataset is 'Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering' by He and McAuley.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        }
      },
      {
        "name": {
          "value": "BOOK",
          "justification": "BOOK is one of the datasets used for evaluating the classification models.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "New Alignment Methods for Discriminative Book Summarization",
          "justification": "The specific paper referenced for this dataset is 'New Alignment Methods for Discriminative Book Summarization' by Bamman and Smith.",
          "quote": "We first evaluated the effectiveness and efficiency on six commonly-used long text classification datasets including Book (Bamman and Smith, 2013), ECtHR (Chalkidis et al., 2021), Hyperpartisan (Kiesel et al., 2019), 20News (Lang, 1995), EURLEX (Chalkidis et al., 2019) and Amazon product reviews (AMZ) (He and McAuley, 2016)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3574,
    "prompt_tokens": 8125,
    "total_tokens": 11699
  }
}