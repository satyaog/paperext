{
  "paper": "nFWRuJXPkU.txt",
  "words": 12851,
  "extractions": {
    "title": {
      "value": "Using Confounded Data in Latent Model-Based Reinforcement Learning",
      "justification": "This is the title provided in the paper's header.",
      "quote": "Using Confounded Data in Latent Model-Based Reinforcement Learning"
    },
    "description": "This paper addresses the issue of confounding in offline reinforcement learning (RL). It proposes a method to exploit confounded offline data in model-based RL to enhance the sample-efficiency of an interactive agent that collects and learns from online, unconfounded data. The method leverages do-calculus to treat model-based RL as a causal inference problem and introduces a generic method for learning a causal transition model that accounts for confounding through a hidden latent variable. The paper demonstrates the method's effectiveness through synthetic experiments.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents practical experiments to validate the proposed method.",
      "quote": "We showcase our method on a series of synthetic experiments."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The research focuses on issues inherent in reinforcement learning, specifically offline and model-based reinforcement learning.",
        "quote": "bridging the gap between the fields of RL and causality."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model-Based Reinforcement Learning",
          "justification": "The method proposed focuses on model-based reinforcement learning.",
          "quote": "we propose a safe method to exploit confounded offline data in model-based RL, which improves the sample-efficiency of an interactive agent."
        },
        "aliases": [
          "MBRL"
        ]
      },
      {
        "name": {
          "value": "Causal Inference",
          "justification": "The approach utilizes causal inference principles, specifically do-calculus, to address the problem.",
          "quote": "we import ideas from the well-established framework of do-calculus to express model-based RL as a causal inference problem."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Latent Model-Based RL",
          "justification": "The method presented in the paper is referred to as a latent model-based RL method.",
          "quote": "Then, we propose a generic method for learning a causal transition model from offline and online data, which captures and corrects the confounding effect using a hidden latent variable."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The paper contributes this method as its central proposition.",
          "quote": "Then, we propose a generic method for learning a causal transition model from offline and online data, which captures and corrects the confounding effect using a hidden latent variable."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model's performance was demonstrated in synthetic experiments.",
          "quote": "We illustrate the effectiveness of our method with a practical implementation for the tabular setting, and three experiments on synthetic toy problems."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model was compared against other baselines in the experiments.",
          "quote": "We compare our augmented method against two baselines: no obs which discards the observational dataset, and naive which naively combines observational and interventional data as if there was no confounding."
        },
        "referenced_paper_title": {
          "value": "Using Confounded Data in Latent Model-Based Reinforcement Learning",
          "justification": "This is the title of the current paper, indicating that it is the main model introduced.",
          "quote": "Then, we propose a generic method for learning a causal transition model from offline and online data, which captures and corrects the confounding effect using a hidden latent variable."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Tiger Problem",
          "justification": "This is a synthetic POMDP used for evaluating the proposed method.",
          "quote": "Tiger is a classic small-scale POMDP from Cassandra et al. [4] with |S| = 6 hidden states and time horizon T = 10."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Acting optimally in partially observable stochastic domains",
          "justification": "This referenced paper introduces the Tiger problem used in this study.",
          "quote": "Tiger is a classic small-scale POMDP from Cassandra et al. [4] with |S| = 6 hidden states and time horizon T = 10."
        }
      },
      {
        "name": {
          "value": "Hidden Treasures",
          "justification": "This is another synthetic POMDP used for evaluating the proposed method.",
          "quote": "Hidden treasures is a 3x3 grid-world problem inspired from Sutton et al. [37], with |S| = 36 hidden states and a time horizon T = 10."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning",
          "justification": "This paper provides the basis for the Hidden Treasures problem used in the study.",
          "quote": "Hidden treasures is a 3x3 grid-world problem inspired from Sutton et al. [37], with |S| = 36 hidden states and a time horizon T = 10."
        }
      },
      {
        "name": {
          "value": "Sloppy Dark Room",
          "justification": "This is another synthetic POMDP used for evaluating the proposed method.",
          "quote": "Sloppy dark room is a 5x5 grid-world inspired from Alt et al. [1], with |S| = 21 hidden states and a time horizon T = 30."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "POMDPs in continuous time and discrete spaces",
          "justification": "This paper provides the basis for Sloppy Dark Room used in the study.",
          "quote": "Sloppy dark room is a 5x5 grid-world inspired from Alt et al. [1], with |S| = 21 hidden states and a time horizon T = 30."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is mentioned as the primary library used for model training.",
          "quote": "The training was done using the Adam optimizer implemented in PyTorch."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A Method for Stochastic Optimization",
          "justification": "The paper referencing Adam is the original paper introducing the optimizer, which is part of PyTorch's library.",
          "quote": "The training was done using the Adam optimizer implemented in PyTorch."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1259,
    "prompt_tokens": 22010,
    "total_tokens": 23269
  }
}