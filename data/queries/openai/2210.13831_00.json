{
  "paper": "2210.13831.txt",
  "words": 18806,
  "extractions": {
    "title": {
      "value": "Convergence of Proximal Point and Extragradient-Based Methods Beyond Monotonicity: the Case of Negative Comonotonicity",
      "justification": "Extracted from the first page of the paper.",
      "quote": "Convergence of Proximal Point and Extragradient-Based Methods Beyond Monotonicity: the Case of Negative Comonotonicity"
    },
    "description": "This paper focuses on the convergence of proximal point and extragradient-based methods for variational inequalities beyond monotonicity assumptions by considering negative comonotonicity. It provides complexity analyses, constructs worst-case examples, and derives convergence rates for these methods.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper provides theoretical analysis and proofs for the convergence rates and worst-case complexity of different first-order methods under the negative comonotonicity assumption.",
      "quote": "In this work, we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization",
        "justification": "The paper focuses on optimization methods and their convergence rates under negative comonotonicity.",
        "quote": "The study of efficient first-order methods for solving variational inequality problems (VIP) have known a surge of interest due to the development of recent machine learning (ML) formulations involving multiple objectives."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Variational Inequality Problems",
          "justification": "The paper discusses solving variational inequality problems using different first-order methods.",
          "quote": "The study of efficient first-order methods for solving variational inequality problems (VIP) have known a surge of interest due to the development of recent machine learning (ML) formulations involving multiple objectives."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Proximal Point (PP) Method",
          "justification": "The paper provides detailed analysis and convergence rates for the Proximal Point Method.",
          "quote": "We derive the first non-asymptotic convergence rates for PP under negative comonotonicity and star-negative comonotonicity and show their tightness via constructing worst-case examples."
        },
        "aliases": [
          "PP Method"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Proximal Point Method is an existing method analyzed in the paper.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper is theoretical and does not specifically mention execution on GPU or CPU.",
          "quote": "In particular, we derive the first non-asymptotic convergence rates for PP under negative comonotonicity and star-negative comonotonicity and show their tightness via constructing worst-case examples;"
        },
        "is_compared": {
          "value": 1,
          "justification": "The Proximal Point Method is compared to Extragradient and Optimistic Gradient methods in terms of convergence rates.",
          "quote": "In this work, we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
        },
        "referenced_paper_title": {
          "value": "Monotone Operator Theory in Hilbert Spaces",
          "justification": "This is a classic reference in the field of optimization frequently mentioned in conjunction with proximal point methods.",
          "quote": "The study of simple first-order methods, such as the extragradient and optimistic gradient methods, in the scope of variational inequality problems (VIP), has a foundational basis in monotone operator theory."
        }
      },
      {
        "name": {
          "value": "Extragradient Method",
          "justification": "The paper discusses in detail the Extragradient Method, its convergence rates, and provides worst-case analyses.",
          "quote": "We derive upper and lower convergence bounds as well as tight conditions on its stepsize for PP under negative comonotonicity. We eventually consider the last-iterate convergence of EG and OG and provide an almost complete picture in that case."
        },
        "aliases": [
          "EG Method"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The Extragradient Method is an existing method analyzed in the paper.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper is theoretical and does not focus on implementation or execution on hardware.",
          "quote": "In this work, we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Extragradient Method is compared to Proximal Point and Optimistic Gradient methods in terms of complexity and convergence rates.",
          "quote": "we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
        },
        "referenced_paper_title": {
          "value": "The Extragradient Method for Finding Saddle Points and Other Problems",
          "justification": "This is the foundational paper by Korpelevich which introduced the Extragradient Method.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place."
        }
      },
      {
        "name": {
          "value": "Optimistic Gradient Method",
          "justification": "The paper addresses the Optimistic Gradient Method, its convergence rates, and worst-case scenario analyses.",
          "quote": "We eventually consider the last-iterate convergence of EG and OG and provide an almost complete picture in that case, listing the remaining open questions."
        },
        "aliases": [
          "OG Method"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The Optimistic Gradient Method is an existing method analyzed in the paper.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper is theoretical and does not focus on practical implementation on hardware.",
          "quote": "we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Optimistic Gradient Method is compared to Proximal Point and Extragradient methods in terms of theoretical performance and convergence guarantees.",
          "quote": "we eventually consider the last-iterate convergence of EG and OG and provide an almost complete picture in that case, listing the remaining open questions."
        },
        "referenced_paper_title": {
          "value": "A Modification of the Arrow-Hurwicz Method for Search of Saddle Points",
          "justification": "This is the original work by Popov which introduced the Optimistic Gradient Method.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "Mentioned as an example where extragradient-based methods have been applied in training WGAN.",
          "quote": "Examples of the usage of extragradient-based methods in practice, we refer to (Daskalakis et al., 2018) who use a variant of OG with Adam (Kingma & Ba, 2014) estimators to train WGAN (Gulrajani et al., 2017) on CIFAR10"
        },
        "aliases": [
          "CIFAR10"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the foundational paper that introduced CIFAR-10 dataset.",
          "quote": "CIFAR10 (Krizhevsky et al., 2009)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PEPit",
          "justification": "The PEPit package was used for performance estimation and validation of the results numerically.",
          "quote": "We used the framework through the packages PESTO (Taylor et al., 2017b) and PEPit (Goujaud et al., 2022), thereby providing a simple way to validate our results numerically."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PEPit: Computer-Assisted Worst-Case Analyses of First-Order Optimization Methods in Python",
          "justification": "This is the reference for the PEPit package used in the paper.",
          "quote": "PEPit (Goujaud et al., 2022)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1929,
    "prompt_tokens": 37703,
    "total_tokens": 39632
  }
}