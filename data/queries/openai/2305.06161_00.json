{
  "paper": "2305.06161.txt",
  "words": 25940,
  "extractions": {
    "title": {
      "value": "StarCoder: may the source be with you!",
      "justification": "Title is clearly mentioned in the paper.",
      "quote": "StarCoder: may the source be with you!"
    },
    "description": "The paper presents StarCoder and StarCoderBase, two large language models (LLMs) specifically designed for code generation and manipulation. It provides thorough details on their development, including training data curation, model architecture, and evaluation metrics. The models are open-access and aim to address issues related to copyright, privacy, and transparency in AI.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper focuses on the empirical evaluation of the performance of StarCoder and StarCoderBase across various benchmarks.",
      "quote": "We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper discusses LLMs focused on code, which falls under the domain of Natural Language Processing (NLP).",
        "quote": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Code Generation",
          "justification": "The paper specifically focuses on large language models designed to generate and manipulate code.",
          "quote": "Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention."
        },
        "aliases": [
          "Code LLMs"
        ]
      },
      {
        "name": {
          "value": "Model Evaluation",
          "justification": "The paper places significant emphasis on the benchmarking and comparison of their models with existing ones.",
          "quote": "We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
        },
        "aliases": [
          "Benchmarking"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "StarCoder",
          "justification": "The paper introduces StarCoder as a fine-tuned version of StarCoderBase specifically for Python.",
          "quote": "We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "StarCoder is an original contribution of the authors.",
          "quote": "We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder."
        },
        "is_executed": {
          "value": 1,
          "justification": "StarCoder was executed and evaluated on various benchmarks, which implies computational execution.",
          "quote": "StarCoder substantially outperforms existing LLMs that are also fine-tuned on Python."
        },
        "is_compared": {
          "value": 1,
          "justification": "StarCoder was benchmarked against other models in multiple programming languages.",
          "quote": "We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
        },
        "referenced_paper_title": {
          "value": "n/a",
          "justification": "StarCoder is an original model introduced in this paper.",
          "quote": "n/a"
        }
      },
      {
        "name": {
          "value": "StarCoderBase",
          "justification": "The paper introduces StarCoderBase as a foundational model trained on code from The Stack.",
          "quote": "StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "StarCoderBase is an original contribution of the authors.",
          "quote": "StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
        },
        "is_executed": {
          "value": 1,
          "justification": "StarCoderBase was executed and evaluated on various programming languages and benchmarks.",
          "quote": "We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
        },
        "is_compared": {
          "value": 1,
          "justification": "StarCoderBase was benchmarked against other models in multiple programming languages.",
          "quote": "We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
        },
        "referenced_paper_title": {
          "value": "n/a",
          "justification": "StarCoderBase is an original model introduced in this paper.",
          "quote": "n/a"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "The Stack",
          "justification": "The Stack is clearly identified as the dataset used for training StarCoderBase.",
          "quote": "StarCoderBase is trained on 1 trillion tokens sourced from The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Stack: 3 TB of permissively licensed source code",
          "justification": "The reference title is provided for The Stack dataset used in the paper.",
          "quote": "The Stack (Kocetkov et al., 2022)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face Transformers",
          "justification": "The Hugging Face Transformers library is mentioned for the tokenizer used in the models.",
          "quote": "The model’s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens—including the sentinel tokens from table 10."
        },
        "aliases": [
          "transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-art natural language processing",
          "justification": "The reference paper and authors are correctly mentioned for Hugging Face Transformers.",
          "quote": "The model’s tokenizer follows our insights presented in Ben Allal et al. (2023) and uses those same design choices: we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokens—including the sentinel tokens from table 10."
        }
      },
      {
        "name": {
          "value": "Elasticsearch",
          "justification": "The Elasticsearch library is used for the search index in the attribution tools.",
          "quote": "We index the training dataset using Elasticsearch 7.17 and provide two search tools to query it: one focused on the Python subset and one covering the entire dataset."
        },
        "aliases": [
          "Elastic"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "n/a",
          "justification": "Elasticsearch is commonly known and does not have a specific reference paper.",
          "quote": "n/a"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1526,
    "prompt_tokens": 50144,
    "total_tokens": 51670
  }
}