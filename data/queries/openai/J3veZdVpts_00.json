{
  "paper": "J3veZdVpts.txt",
  "words": 8573,
  "extractions": {
    "title": {
      "value": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
      "justification": "The title is directly provided in the research paper.",
      "quote": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies"
    },
    "description": "This paper proposes a sequential approach to evaluate offline Reinforcement Learning (RL) algorithms. The approach emphasizes evaluating algorithms based on their data efficiency and robustness to distribution changes in the dataset, aiming to harmonize offline and online evaluation methodologies.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents a new methodology for evaluating offline RL algorithms and provides experimental results comparing several existing algorithms using this methodology.",
      "quote": "In this paper, we propose a sequential approach to evaluate offline RL algorithms... We compare several existing offline RL algorithms using this approach and present insights from a variety of tasks and offline datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on the evaluation methodologies for offline and online Reinforcement Learning (RL) algorithms.",
        "quote": "Reinforcement learning (RL) has shown great promise with algorithms learning in environments with large state and action spaces purely from scalar reward signals."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Offline Reinforcement Learning",
          "justification": "The paper deals extensively with offline RL methodologies and datasets.",
          "quote": "Offline RL algorithms try to address this issue by bootstrapping the learning process from existing logged data without needing to interact with the environment from the very beginning."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Online Reinforcement Learning",
          "justification": "The paper aims to harmonize the evaluation methodologies of offline and online RL.",
          "quote": "While online RL algorithms are typically evaluated as a function of the number of environment interactions, there isnâ€™t a single established protocol for evaluating offline RL methods."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Implicit Q Learning (IQL)",
          "justification": "IQL is one of the offline RL algorithms evaluated in the paper.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Implicit Q Learning (IQL)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper evaluates IQL but does not mention it as an original contribution.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Implicit Q Learning (IQL)..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper involves practical implementation and evaluation of IQL.",
          "quote": "For each dataset, we train algorithms following Alg. 1, initializing the replay buffer with 5000 data points at the start of training."
        },
        "is_compared": {
          "value": 1,
          "justification": "IQL is compared to other algorithms in the scope of the paper.",
          "quote": "We compare several existing offline RL algorithms using the sequential approach and present insights from a variety of tasks and offline datasets."
        },
        "referenced_paper_title": {
          "value": "Offline Reinforcement Learning with Implicit Q-Learning",
          "justification": "The main paper that introduces IQL has been referenced to report results.",
          "quote": "Implicit Q Learning (IQL) (Kostrikov et al., 2022)..."
        }
      },
      {
        "name": {
          "value": "Conservative Q-Learning (CQL)",
          "justification": "CQL is one of the algorithms evaluated in the study.",
          "quote": "These algorithms were evaluated on the D4RL benchmark (Fu et al., 2020)... including Conservative Q-Learning (CQL) (Kumar et al., 2020)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper evaluates CQL but does not mention it as an original contribution.",
          "quote": "These algorithms were evaluated on the D4RL benchmark (Fu et al., 2020)... including Conservative Q-Learning (CQL) (Kumar et al., 2020)..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper involves practical implementation and evaluation of CQL.",
          "quote": "For each dataset, we train algorithms following Alg. 1, initializing the replay buffer with 5000 data points at the start of training."
        },
        "is_compared": {
          "value": 1,
          "justification": "CQL is compared to other algorithms in the scope of the paper.",
          "quote": "We compare several existing offline RL algorithms using the sequential approach and present insights from a variety of tasks and offline datasets."
        },
        "referenced_paper_title": {
          "value": "Conservative Q-Learning for Offline Reinforcement Learning",
          "justification": "The main paper that introduces CQL has been referenced to report results.",
          "quote": "Conservative Q-Learning (CQL) (Kumar et al., 2020)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "D4RL",
          "justification": "D4RL is the main benchmark used for evaluating the offline RL algorithms in the paper.",
          "quote": "These algorithms were evaluated on the D4RL benchmark (Fu et al., 2020), which consists of three environments: Halfcheetah-v2, Walker2d-v2 and Hopper-v2."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
          "justification": "The main paper that introduces D4RL has been referenced to report results.",
          "quote": "These algorithms were evaluated on the D4RL benchmark (Fu et al., 2020)..."
        }
      },
      {
        "name": {
          "value": "DeepMind Control Suite (DMC)",
          "justification": "DMC is another dataset used for evaluation in the study.",
          "quote": "We also created a dataset from the DeepMind Control Suite (DMC) (Tassa et al., 2018) environments following the same procedure as outlined by the authors of D4RL."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepMind Control Suite",
          "justification": "The main paper that introduces the DMC suite has been referenced to report results.",
          "quote": "We also created a dataset from the DeepMind Control Suite (DMC) (Tassa et al., 2018) environments following the same procedure as outlined by the authors of D4RL."
        }
      },
      {
        "name": {
          "value": "v-d4rl",
          "justification": "v-d4rl is used in the study for evaluating algorithms in visual offline RL domains.",
          "quote": "Finally, to study algorithms in visual offline RL domains, we used the v-d4rl benchmark (Lu et al., 2023) which follows the philosophy of D4RL and creates datasets of images from the DMC Suite with varying difficulties."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations",
          "justification": "The main paper that introduces v-d4rl has been referenced to report results.",
          "quote": "Finally, to study algorithms in visual offline RL domains, we used the v-d4rl benchmark (Lu et al., 2023)..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "rliable",
          "justification": "rliable library is used for aggregated performance evaluation of the algorithms across environments.",
          "quote": "This is done using the rliable (Agarwal et al., 2021) library to plot interval estimates of normalized performance measures such as median, mean, interquartile mean (IQM) and optimality gap."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep Reinforcement Learning at the Edge of the Statistical Precipice",
          "justification": "The main paper that introduces rliable has been referenced to report results.",
          "quote": "This is done using the rliable (Agarwal et al., 2021) library..."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2814,
    "prompt_tokens": 35886,
    "total_tokens": 38700
  }
}