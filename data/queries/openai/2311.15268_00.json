{
  "paper": "2311.15268.txt",
  "words": 11002,
  "extractions": {
    "title": {
      "value": "UNLEARNING VIA SPARSE REPRESENTATIONS",
      "justification": "This title is clear and directly taken from the research paper's header.",
      "quote": "UNLEARNING VIA SPARSE REPRESENTATIONS"
    },
    "description": "This paper proposes a novel machine unlearning technique that is nearly compute-free and zero-shot, leveraging discrete representational bottlenecks. The proposed method aims to efficiently unlearn a specific subset of a trained model's dataset (forget set) while preserving the model's performance on the remaining data (retain set). The method is shown to be effective and computationally efficient, and is compared against a state-of-the-art unlearning method, SCRUB, across three datasets: CIFAR-10, CIFAR-100, and LACUNA-100.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experiments on three datasets (CIFAR-10, CIFAR-100, and LACUNA-100) to evaluate the performance of the proposed unlearning technique, which is a hallmark of empirical research.",
      "quote": "We evaluate the proposed technique on the problem of class unlearning using three datasets: CIFAR-10, CIFAR-100, and LACUNA100."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Unlearning",
        "justification": "The paper focuses on techniques and methodologies for making machine learning models forget specific data, which falls squarely under machine unlearning.",
        "quote": "Machine unlearning, which involves erasing knowledge about a forget set from a trained model, can prove to be costly and infeasible by existing techniques."
      },
      "aliases": [
        "Machine Unlearning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Sparse Representations",
          "justification": "The paper specifically leverages sparse representations via a discrete representational bottleneck for the unlearning task.",
          "quote": "We propose a nearly compute-free zero-shot unlearning technique based on a discrete representational bottleneck."
        },
        "aliases": [
          "Sparse Representations"
        ]
      },
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The concept of unlearning and retaining existing knowledge to make room for new data is related to continual learning.",
          "quote": "Moreover, since the representations are discrete, this may be achieved zero-shot, i.e., without requiring any additional compute in the form of retraining or fine tuning, by directly intervening on individual representations."
        },
        "aliases": [
          "Continual Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SCRUB",
          "justification": "The paper uses SCRUB as a comparative model in its empirical evaluations.",
          "quote": "We compare the proposed technique to SCRUB, a state-of-the-art approach which uses knowledge distillation for unlearning."
        },
        "aliases": [
          "SCRUB"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SCRUB is not introduced in this paper, but is used as a baseline for comparison.",
          "quote": "We compare the proposed technique to SCRUB, a state-of-the-art approach which uses knowledge distillation for unlearning."
        },
        "is_executed": {
          "value": 1,
          "justification": "SCRUB is executed to compare its performance with the proposed unlearning technique.",
          "quote": "We compare the proposed technique to SCRUB, a state-of-the-art approach which uses knowledge distillation for unlearning. Across all three datasets, the proposed technique performs as well as, if not better than SCRUB while incurring almost no computational cost."
        },
        "is_compared": {
          "value": 1,
          "justification": "SCRUB is numerically compared to the proposed unlearning technique on three datasets.",
          "quote": "We compare the proposed technique to SCRUB, a state-of-the-art approach which uses knowledge distillation for unlearning."
        },
        "referenced_paper_title": {
          "value": "Towards Unbounded Machine Unlearning",
          "justification": "The referenced paper is explicitly stated as the source of the SCRUB model.",
          "quote": "We compare the proposed methods to SCRUB (Kurmanji et al., 2023), a recent state-of-the-art approach that requires additional compute to unlearn, on three datasets: CIFAR-10, CIFAR-100, and LACUNA-100, and also further investigate the effects of retraining the zero-shot unlearned models on the retain set."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The paper uses the CIFAR-10 dataset to evaluate the proposed unlearning method.",
          "quote": "We evaluate the proposed technique on the problem of class unlearning using three datasets: CIFAR-10, CIFAR-100, and LACUNA100."
        },
        "aliases": [
          "CIFAR-10"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This paper by Alex Krizhevsky is the original paper introducing the CIFAR-10 dataset.",
          "quote": "CIFAR-10 (Krizhevsky et al., 2009)"
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "The paper uses the CIFAR-100 dataset to evaluate the proposed unlearning method.",
          "quote": "We evaluate the proposed technique on the problem of class unlearning using three datasets: CIFAR-10, CIFAR-100, and LACUNA100."
        },
        "aliases": [
          "CIFAR-100"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This paper by Alex Krizhevsky is the original paper introducing the CIFAR-100 dataset.",
          "quote": "CIFAR-100 (Krizhevsky et al., 2009)"
        }
      },
      {
        "name": {
          "value": "LACUNA-100",
          "justification": "The paper uses the LACUNA-100 dataset to evaluate the proposed unlearning method.",
          "quote": "We evaluate the proposed technique on the problem of class unlearning using three datasets: CIFAR-10, CIFAR-100, and LACUNA100."
        },
        "aliases": [
          "LACUNA-100"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks",
          "justification": "This paper by Golatkar et al. is the one that introduced the LACUNA-100 dataset.",
          "quote": "LACUNA-100 (Golatkar et al., 2020a)"
        }
      },
      {
        "name": {
          "value": "VGG-Faces",
          "justification": "Although LACUNA-100 is derived from VGG-Faces, the use of VGG-Faces as a base dataset is implicit.",
          "quote": "LACUNA-100 is derived from VGG-Faces (Cao et al., 2018) by sampling 100 different celebrities and sampling 500 images per celebrity."
        },
        "aliases": [
          "VGG-Faces"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "VGGFace2: A Dataset for Recognising Faces across Pose and Age",
          "justification": "This reference clearly mentions the use of VGG-Faces to derive LACUNA-100.",
          "quote": "LACUNA-100 is derived from VGG-Faces (Cao et al., 2018) by sampling 100 different celebrities and sampling 500 images per celebrity."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "As an empirical study involving modern deep learning, it is highly likely that PyTorch was used, although it's not explicitly mentioned.",
          "quote": "Not explicitly mentioned, but inferred from standard practices in deep learning research."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "This reference is standard and likely used for implementing models and experiments.",
          "quote": "Not explicitly mentioned, but inferred from standard practices in deep learning research."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1563,
    "prompt_tokens": 18857,
    "total_tokens": 20420
  }
}