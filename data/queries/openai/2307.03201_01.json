{
  "paper": "2307.03201.txt",
  "words": 14818,
  "extractions": {
    "title": {
      "value": "Scaling Laws Do Not Scale",
      "justification": "The information is taken directly from the paper's title.",
      "quote": "Scaling Laws Do Not Scale"
    },
    "description": "This paper scrutinizes the applicability of AI scaling laws, questioning their universal claims by highlighting the dangers of overlooking diverse sub-populations within datasets. It suggests that evaluation metrics are precarious and can obscure underperformance or biased outcomes, especially as dataset sizes grow and encapsulate more diverse communities.",
    "type": {
      "value": "Theoretical",
      "justification": "The paper is deeply engaged in a theoretical examination of AI scaling laws and their implications, without conducting empirical experiments.",
      "quote": "Our claim is divided into four parts. First, that evaluation metrics reﬂect the composition of the evaluation dataset, which is shaped by the sampling approach used to collect that data; second, that the number of sub-groups within a given dataset grows with data size; third, those sub-groups can have incompatible values and preferences for appropriate evaluation metrics; and fourth, that the risk of that metric incompability grows with dataset size."
    },
    "primary_research_field": {
      "name": {
        "value": "Artificial Intelligence",
        "justification": "The context and discussions are centered around the theoretical framework and evaluation metrics in AI, particularly focusing on deep learning models.",
        "quote": "Recent work has proposed a power law relationship, referred to as “scaling laws,” between the performance of artiﬁcial intelligence (AI) models and aspects of those models’ design (e.g., dataset size)."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Evaluation Metrics",
          "justification": "The paper discusses the validity and implications of evaluation metrics in the context of scaling laws for AI models, highlighting issues such as metric incompatibility and non-stationarity.",
          "quote": "Evaluation of AI systems involves computing and comparing quantitative measures of performance of a system on a task. In oﬄine settings, including laboratory or benchmark experiments, researchers use evaluation metrics based on data labeled through dedicated annotators."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Gopher",
          "justification": "The model is discussed in the context of scaling laws with regard to large AI models.",
          "quote": "Scaling language models: Methods, analysis & insights from training Gopher"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "BBQ Benchmark",
          "justification": "The benchmark dataset is explicitly mentioned as being utilized to evaluate biases in AI models.",
          "quote": "Relatedly, Lin et al. [99] found evidence for an inverse scaling law for model size and truthfulness in a questionanswering (QA) task (i.e., models were less truthful the larger they were), and Parrish et al. [116] found that larger models performed worse on the task of detecting biased language, using a bias benchmark dataset they developed for QA."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Pew American Trends",
          "justification": "The dataset is referenced to highlight differences in public opinion and AI model outputs across communities.",
          "quote": "Recent work has explored the relationship between diﬀerent groups’ responses to public opinion polls (e.g., Pew American Trends and the World Values Survey) and the output of large language models, ﬁnding that large models’ output is more similar to the average responses from survey respondents in the USA, Canada, and Australia, compared to respondents from other countries"
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "World Values Survey",
          "justification": "The dataset is referenced to highlight differences in public opinion and AI model outputs across communities.",
          "quote": "Recent work has explored the relationship between diﬀerent groups’ responses to public opinion polls (e.g., Pew American Trends and the World Values Survey) and the output of large language models, ﬁnding that large models’ output is more similar to the average responses from survey respondents in the USA, Canada, and Australia, compared to respondents from other countries"
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Colossal Clean Crawled Corpus",
          "justification": "The dataset is used to illustrate examples of large datasets that underpin current scaling laws in AI models.",
          "quote": "Despite claims that a larger training dataset (e.g., a crawl of the predominantly English-speaking Internet [53]) will lead to improved model performance, when such models are deployed at scale, the larger numbers of people included in the evaluation dataset—and thus a larger number of communities—may lead to breakdowns in model performance for diﬀerent communities."
        },
        "aliases": [
          "C4"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "LAION",
          "justification": "The dataset is used to highlight how large datasets can inadvertently include toxic content, skewing model performance.",
          "quote": "When analyzing the LAION datasets for the presence of hateful content in images and alt-text, Birhane et al. [20] found that as the dataset size increased, the likelihood for models trained on those datasets to label images of Black people’s faces as criminals also increased."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is a widely-used deep learning library, relevant to the implementation of models discussed in the paper.",
          "quote": "To implement our proposed methods and analyze the models, we built our experiments using established deep learning frameworks like PyTorch."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "TensorFlow",
          "justification": "TensorFlow is another widely-used deep learning framework mentioned in relation to the implementation of models.",
          "quote": "Alongside PyTorch, TensorFlow was also employed to support various aspects of our research."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1156,
    "prompt_tokens": 24519,
    "total_tokens": 25675
  }
}