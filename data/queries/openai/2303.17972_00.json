{
  "paper": "2303.17972.txt",
  "words": 4392,
  "extractions": {
    "title": {
      "value": "ε kú <mask>: Integrating Yorùbá cultural greetings into machine translation",
      "justification": "Title of the paper at the beginning of the provided text.",
      "quote": "ε kú <mask>: Integrating Yorùbá cultural greetings into machine translation"
    },
    "description": "This paper investigates the performance of massively multilingual neural machine translation (NMT) systems in translating Yorùbá greetings into English. It introduces IkiniYorùbá, a Yorùbá-English translation dataset focused on Yorùbá greetings, and evaluates the performance of different multilingual NMT systems, including Google Translate, NLLB, and a custom fine-tuned model.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper evaluates the performance of different models on a dataset and provides experimental results.",
      "quote": "This paper investigates the performance of massively multilingual neural machine translation (NMT) systems in translating Yorùbá greetings..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on machine translation, which is a sub-field of Natural Language Processing (NLP).",
        "quote": "...multilingual neural machine translation (NMT)..."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Translation",
          "justification": "The primary focus of the paper is the translation of Yorùbá greetings into English using different NMT systems.",
          "quote": "...multilingual neural machine translation (NMT)..."
        },
        "aliases": [
          "NMT"
        ]
      },
      {
        "name": {
          "value": "Cultural Linguistics",
          "justification": "The paper specifically addresses the translation of cultural aspects, such as Yorùbá greetings, which are important in Yorùbá culture.",
          "quote": "...Yorùbá greetings (ε kú <mask>), which are a big part of Yorùbá language and culture..."
        },
        "aliases": [
          ""
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Google Translate",
          "justification": "One of the NMT systems evaluated in the paper is Google Translate.",
          "quote": "We analysed the performance of different multilingual NMT systems including Google Translate and NLLB..."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Google Translate is not introduced by the authors but is used as a benchmark.",
          "quote": "...Google Translate..."
        },
        "is_executed": {
          "value": 1,
          "justification": "Google Translate is a cloud-based service and runs on server-side infrastructure, which typically involves GPUs.",
          "quote": "We generated translations for the test sets using the Google Translate web application."
        },
        "is_compared": {
          "value": 1,
          "justification": "Google Translate is compared with other models like NLLB and the custom fine-tuned model.",
          "quote": "We analysed the performance of different multilingual NMT systems including Google Translate and NLLB..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Google Translate is a widely known and used tool and does not have a specific referenced paper in this context.",
          "quote": "We analysed the performance of different multilingual NMT systems including Google Translate and NLLB..."
        }
      },
      {
        "name": {
          "value": "NLLB (No Language Left Behind)",
          "justification": "NLLB is another NMT system evaluated in this study.",
          "quote": "...Meta’s NLLB (NLLB Team et al., 2022)..."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "NLLB is not introduced by the authors but is used as a benchmark.",
          "quote": "...Meta’s NLLB (NLLB Team et al., 2022)..."
        },
        "is_executed": {
          "value": 1,
          "justification": "NLLB is a neural network-based model often executed on GPUs for training and inference.",
          "quote": "...Meta’s NLLB (NLLB Team et al., 2022)..."
        },
        "is_compared": {
          "value": 1,
          "justification": "NLLB is compared with other models like Google Translate and the custom fine-tuned model.",
          "quote": "We analysed the performance of different multilingual NMT systems including Google Translate and NLLB..."
        },
        "referenced_paper_title": {
          "value": "No Language Left Behind: Scaling Human-Centered Machine Translation",
          "justification": "This is the referenced paper for the NLLB model.",
          "quote": "(NLLB Team et al., 2022)"
        }
      },
      {
        "name": {
          "value": "M2M-100",
          "justification": "Another model evaluated is M2M-100, which was fine-tuned in this study.",
          "quote": "...M2M-100 (Fan et al., 2020)..."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The authors fine-tuned the M2M-100 model on the training split of IkiniYorùbá.",
          "quote": "...we trained a Yorùbá-English model by finetuning an existing NMT model on the training split of IkiniYorùbá..."
        },
        "is_executed": {
          "value": 1,
          "justification": "M2M-100 is a neural network-based model typically executed on GPUs for training and inference.",
          "quote": "...M2M-100..."
        },
        "is_compared": {
          "value": 1,
          "justification": "M2M-100 is compared with other models like Google Translate and NLLB.",
          "quote": "We analysed the performance of different multilingual NMT systems including Google Translate and NLLB..."
        },
        "referenced_paper_title": {
          "value": "Beyond English-Centric Multilingual Machine Translation",
          "justification": "This is the referenced paper for the M2M-100 model.",
          "quote": "(Fan et al., 2020)"
        }
      },
      {
        "name": {
          "value": "Custom fine-tuned Yorùbá-English model",
          "justification": "The authors fine-tuned an existing NMT model specifically on the IkiniYorùbá dataset.",
          "quote": "...we trained a Yorùbá-English model by finetuning an existing NMT model on the training split of IkiniYorùbá..."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The custom fine-tuned model is a direct contribution of this study.",
          "quote": "...we trained a Yorùbá-English model by finetuning an existing NMT model on the training split of IkiniYorùbá..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The fine-tuning process typically requires GPU execution.",
          "quote": "...we trained a Yorùbá-English model by finetuning an existing NMT model on the training split of IkiniYorùbá..."
        },
        "is_compared": {
          "value": 1,
          "justification": "The custom fine-tuned model is compared against Google Translate, NLLB, and the base M2M-100 model.",
          "quote": "...this achieved better performance when compared to the pre-trained multilingual NMT models..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This custom model does not reference a specific paper as it is a contribution of this study.",
          "quote": "...we trained a Yorùbá-English model by finetuning an existing NMT model on the training split of IkiniYorùbá..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "IkiniYorùbá",
          "justification": "The dataset introduced in this paper for translating Yorùbá greetings into English.",
          "quote": "...introduces a new dataset dubbed IkiniYorùbá, a Yorùbá-English translation dataset of popular Yorùbá greetings..."
        },
        "aliases": [
          ""
        ],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "IkiniYorùbá is a new dataset introduced by the authors in this paper.",
          "quote": "...introduces a new dataset dubbed IkiniYorùbá, a Yorùbá-English translation dataset of popular Yorùbá greetings..."
        }
      },
      {
        "name": {
          "value": "Movie Transcripts subset of MENYO-20k",
          "justification": "Used for conversational data in evaluating translation models.",
          "quote": "For our experiments, we used the movie transcripts subset of the MENYO-20k (Adelani et al., 2020) dataset..."
        },
        "aliases": [
          ""
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation",
          "justification": "The MENYO-20k dataset is properly referenced in the paper.",
          "quote": "For our experiments, we used the movie transcripts subset of the MENYO-20k (Adelani et al., 2020) dataset..."
        }
      },
      {
        "name": {
          "value": "JW300",
          "justification": "Listed as an automatically collected dataset that includes Yorùbá.",
          "quote": "Examples of automatically collected datasets that include Yorùbá are JW300 (Agić and Vulić, 2019)..."
        },
        "aliases": [
          ""
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "JW300: A Wide-coverage Parallel Corpus for Low-Resource Languages",
          "justification": "The paper references JW300 as a dataset that includes Yorùbá.",
          "quote": "Examples of automatically collected datasets that include Yorùbá are JW300 (Agić and Vulić, 2019)..."
        }
      },
      {
        "name": {
          "value": "CCMatrix",
          "justification": "Listed as an automatically collected dataset that includes Yorùbá.",
          "quote": "Examples of automatically collected datasets that include Yorùbá are ... CCMatrix (Schwenk et al., 2021)..."
        },
        "aliases": [
          ""
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web",
          "justification": "The paper references CCMatrix as a dataset that includes Yorùbá.",
          "quote": "Examples of automatically collected datasets that include Yorùbá are ... CCMatrix (Schwenk et al., 2021)..."
        }
      },
      {
        "name": {
          "value": "CCAligned",
          "justification": "Listed as an automatically collected dataset that includes Yorùbá.",
          "quote": "Examples of automatically collected datasets that include Yorùbá are ... CCAligned (El-Kishky et al., 2020)."
        },
        "aliases": [
          ""
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "CCAligned: A Massive Collection of Cross-lingual Web-document Pairs",
          "justification": "The paper references CCAligned as a dataset that includes Yorùbá.",
          "quote": "Examples of automatically collected datasets that include Yorùbá are ... CCAligned (El-Kishky et al., 2020)."
        }
      },
      {
        "name": {
          "value": "MENYO-20k",
          "justification": "The Movie Transcripts subset of MENYO-20k is used in the experiments.",
          "quote": "For our experiments, we used the movie transcripts subset of the MENYO-20k (Adelani et al., 2020) dataset..."
        },
        "aliases": [
          ""
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "MENYO-20k: A Multi-domain English-Yorùbá Corpus for Machine Translation and Domain Adaptation",
          "justification": "The MENYO-20k dataset is properly referenced in the paper.",
          "quote": "For our experiments, we used the movie transcripts subset of the MENYO-20k (Adelani et al., 2020) dataset..."
        }
      },
      {
        "name": {
          "value": "MAFAND-MT",
          "justification": "Listed as a manually translated dataset for Yorùbá.",
          "quote": "...examples of manually translated datasets for Yoruba include MENYO-20k (Adelani et al., 2021), MAFAND-MT (Adelani et al., 2022)..."
        },
        "aliases": [
          ""
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "A Few Thousand Translations Go a Long Way! Leveraging Pre-trained Models for African News Translation",
          "justification": "The paper references MAFAND-MT as a manually translated dataset for Yorùbá.",
          "quote": "...examples of manually translated datasets for Yoruba include MENYO-20k (Adelani et al., 2021), MAFAND-MT (Adelani et al., 2022)..."
        }
      },
      {
        "name": {
          "value": "FLORES-101",
          "justification": "Listed as a manually translated dataset for Yorùbá.",
          "quote": "...manually translated datasets for Yoruba include ... FLORES-101 (Goyal et al., 2022)..."
        },
        "aliases": [
          ""
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation",
          "justification": "The paper references FLORES-101 as a manually translated dataset for Yorùbá.",
          "quote": "...datasets for Yoruba include ... FLORES-101 (Goyal et al., 2022)..."
        }
      },
      {
        "name": {
          "value": "NTREX",
          "justification": "Listed as a manually translated dataset for Yorùbá.",
          "quote": "...manually translated datasets for Yoruba include ... and NTREX (Federmann et al., 2022)..."
        },
        "aliases": [
          ""
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "NTREX-128 – News Test References for MT Evaluation of 128 Languages",
          "justification": "The paper references NTREX as a manually translated dataset for Yorùbá.",
          "quote": "...datasets for Yoruba include ... and NTREX (Federmann et al., 2022)..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "HuggingFace Transformers",
          "justification": "HuggingFace Transformers library was used for generating translations with Meta’s M2M-100 and NLLB models.",
          "quote": "...for Meta’s M2M-100 and NLLB models, we used the HuggingFace transformers library."
        },
        "aliases": [
          ""
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "It is a well-known library and does not have a specific referenced paper in this context.",
          "quote": "...for Meta’s M2M-100 and NLLB models, we used the HuggingFace transformers library."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2903,
    "prompt_tokens": 9029,
    "total_tokens": 11932
  }
}