{
  "paper": "c347c96c51ee413beecd5f0cab07c31b.txt",
  "words": 8414,
  "extractions": {
    "title": {
      "value": "Deep Multi-Representation Learning for Data Clustering",
      "justification": "The title is directly mentioned at the beginning and throughout the paper.",
      "quote": "Deep Multi-Representation Learning for Data Clustering"
    },
    "description": "This paper proposes a deep multi-representation learning (DML) framework for data clustering, where difficult-to-cluster data groups are associated with their own optimized latent spaces. Autoencoders are used to generate both cluster-specific and general latent spaces, and a novel loss function combining weighted reconstruction and clustering losses is introduced. The method outperforms state-of-the-art clustering approaches, especially on imbalanced datasets.",
    "type": {
      "value": "empirical",
      "justification": "The paper tests the proposed DML framework on several benchmark datasets and compares its performance with other clustering methods.",
      "quote": "Experimental results on benchmark datasets demonstrate that the proposed DML framework and loss function outperform state-of-the-art clustering approaches."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning for Data Clustering",
        "justification": "The paper focuses on the application of deep learning techniques for improving data clustering performance.",
        "quote": "Deep clustering incorporates embedding into clustering in order to find a lower-dimensional space suitable for clustering task."
      },
      "aliases": [
        "DML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Autoencoders",
          "justification": "The paper uses autoencoders to generate cluster-specific and general latent spaces for data clustering.",
          "quote": "Autoencoders are employed for generating the cluster-specific and general latent spaces."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Loss Functions in Deep Learning",
          "justification": "The paper proposes a novel loss function combining weighted reconstruction and clustering losses.",
          "quote": "To specialize each autoencoder in its associated data cluster(s), we propose a novel and effective loss function which consists of weighted reconstruction and clustering losses of the data points."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "General Autoencoder",
          "justification": "The general autoencoder is used to generate latent spaces for easy clusters in the DML framework.",
          "quote": "DML trains a common autoencoder, called General AE, for the easy clusters and cluster-specific autoencoders for the difficult ones."
        },
        "aliases": [
          "General AE"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The general autoencoder is an integral part of the proposed DML framework.",
          "quote": "To summarize, the main contributions of this work are: We propose the novel concept of deep multi-representation learning which performs the data clustering task by employing multiple autoencoders: a general autoencoder for the easy clusters and cluster-specific autoencoders for the difficult ones."
        },
        "is_executed": {
          "value": true,
          "justification": "The general autoencoder is trained as part of the proposed method.",
          "quote": "For each dataset’s best result is shown in bold. The second-top results are denoted by an asterisk (*)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of the proposed method, including the general autoencoder, with various other clustering methods.",
          "quote": "Effectiveness of DML in boosting performance of the state-of-the-art AE-based clustering methods is shown in Table I."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The general autoencoder is a part of the proposed DML framework and does not have a separate reference paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Cluster-Specific Autoencoder",
          "justification": "Cluster-specific autoencoders are used to generate distinct, optimized latent spaces for difficult-to-cluster groups in the DML framework.",
          "quote": "DML trains a common autoencoder, called General AE, for the easy clusters and cluster-specific autoencoders for the difficult ones."
        },
        "aliases": [
          "Cluster-Specific AE"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The cluster-specific autoencoder is an integral part of the proposed DML framework.",
          "quote": "To summarize, the main contributions of this work are: We propose the novel concept of deep multi-representation learning which performs the data clustering task by employing multiple autoencoders: a general autoencoder for the easy clusters and cluster-specific autoencoders for the difficult ones."
        },
        "is_executed": {
          "value": true,
          "justification": "Cluster-specific autoencoders are trained as part of the proposed method.",
          "quote": "For each dataset’s best result is shown in bold. The second-top results are denoted by an asterisk (*)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of the proposed method, including the cluster-specific autoencoders, with various other clustering methods.",
          "quote": "Effectiveness of DML in boosting performance of the state-of-the-art AE-based clustering methods is shown in Table I."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The cluster-specific autoencoder is a part of the proposed DML framework and does not have a separate reference paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is used to evaluate the clustering performance of the proposed DML framework.",
          "quote": "Effectiveness of our proposed DML framework is evaluated on six commonly used datasets. The datasets are: (1) MNIST comprises of 60,000 training and 10,000 test gray-scale handwritten digits with size 28 × 28."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Yann LeCun et al. “Gradient-based learning applied to document recognition.” Proceedings of the IEEE, 1998.",
          "justification": "The referenced paper provides documentation for the MNIST dataset.",
          "quote": "Yann LeCun et al. “Gradient-based learning applied to document recognition.” Proceedings of the IEEE, 1998."
        }
      },
      {
        "name": {
          "value": "Fashion MNIST",
          "justification": "Fashion MNIST is used to evaluate the clustering performance of the proposed DML framework.",
          "quote": "(2) Fashion MNIST contains various types of fashion items. The number of samples and image size are as same as those of the MNIST dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Han Xiao, Kashif Rasul, and Roland Vollgraf. “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.” arXiv preprint arXiv:1708.07747, 2017.",
          "justification": "The referenced paper provides documentation for the Fashion MNIST dataset.",
          "quote": "Han Xiao, Kashif Rasul, and Roland Vollgraf. “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.” arXiv preprint arXiv:1708.07747, 2017."
        }
      },
      {
        "name": {
          "value": "2MNIST",
          "justification": "2MNIST, created by concatenating MNIST and Fashion MNIST, is used to evaluate the clustering performance of the proposed DML framework.",
          "quote": "(3) 2MNIST is a more challenging dataset which is created by concatenation of the two MNIST and Fashion MNIST datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "2MNIST is a synthesized combination of MNIST and Fashion MNIST and does not have a separate reference paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used to evaluate the clustering performance of the proposed DML framework.",
          "quote": "(4) CIFAR-10 contains of 60,000 RGB images from 10 different categories, where the size of each image is 32 × 32."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Alex Krizhevsky, Geoffrey Hinton, et al. \"Learning multiple layers of features from tiny images.\" University of Toronto, 2009.",
          "justification": "The referenced paper provides documentation for the CIFAR-10 dataset.",
          "quote": "Alex Krizhevsky, Geoffrey Hinton, et al. \"Learning multiple layers of features from tiny images.\" University of Toronto, 2009."
        }
      },
      {
        "name": {
          "value": "STL-10",
          "justification": "STL-10 is used to evaluate the clustering performance of the proposed DML framework.",
          "quote": "(5) STL-10 comprises of 13,000 96 × 96 RGB images from different objects."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam Coates, Andrew Ng, and Honglak Lee. \"An analysis of single-layer networks in unsupervised feature learning.\" Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, 2011.",
          "justification": "The referenced paper provides documentation for the STL-10 dataset.",
          "quote": "Adam Coates, Andrew Ng, and Honglak Lee. \"An analysis of single-layer networks in unsupervised feature learning.\" Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, 2011."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is used to evaluate the clustering performance of the proposed DML framework.",
          "quote": "(6) CIFAR-100 is similar to the CIFAR-10 in terms of number of samples and image size. However, it has 20 super groups based on the similarity between images."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Alex Krizhevsky, Geoffrey Hinton, et al. \"Learning multiple layers of features from tiny images.\" University of Toronto, 2009.",
          "justification": "The referenced paper provides documentation for the CIFAR-100 dataset.",
          "quote": "Alex Krizhevsky, Geoffrey Hinton, et al. \"Learning multiple layers of features from tiny images.\" University of Toronto, 2009."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1992,
    "prompt_tokens": 15719,
    "total_tokens": 17711
  }
}