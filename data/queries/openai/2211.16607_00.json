{
  "paper": "2211.16607.txt",
  "words": 16798,
  "extractions": {
    "title": {
      "value": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
      "justification": "This is the title presented at the beginning of the paper.",
      "quote": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer"
    },
    "description": "The paper proposes a new information bottleneck approach, named Transfer Entropy Bottleneck (TEB), designed for learning a model that manages the directed information transferred between two statistically dependent streams. TEB aims to capture the conditional information thereby enhancing predictions of one stream using both its history and the history of the other stream. Experiments demonstrate the efficacy of this method on synthetic tasks involving images and time-series data.",
    "type": {
      "value": "empirical study",
      "justification": "The paper introduces a novel method, implements it, and tests it empirically on three synthetic tasks, involving dual stream modeling problems.",
      "quote": "Here, we develop an information bottleneck approach for conditional learning on two dependent streams of data."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper is focused on developing a new machine learning method for learning a model that captures and utilizes transfer entropy for predicting data streams.",
        "quote": "Estimating transfer entropy with a deep learning approach has been extensively explored."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The study involves deep learning approaches to model and capture the transfer entropy for enhancing predictions.",
          "quote": "Estimating transfer entropy with a deep learning approach has been extensively explored."
        },
        "aliases": [
          "DL"
        ]
      },
      {
        "name": {
          "value": "Sequence Modeling",
          "justification": "The paper focuses on learning models for predicting future values in sequences based on historical values of dual streams.",
          "quote": "Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable."
        },
        "aliases": [
          "Temporal Modeling",
          "Temporal Predictions"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Transfer Entropy Bottleneck (TEB)",
          "justification": "The primary model proposed and defined in the paper is Transfer Entropy Bottleneck (TEB).",
          "quote": "Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable."
        },
        "aliases": [
          "TEB"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is introduced and developed within the paper.",
          "quote": "We term Transfer Entropy Bottleneck (TEB), that learns a compressed conditional information representation."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed in experiments involving synthetic tasks such as rotating MNIST and time-series data.",
          "quote": "Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable."
        },
        "is_compared": {
          "value": 1,
          "justification": "The TEB model is compared to other models in the experiments, including deterministic and stochastic baselines.",
          "quote": "To evaluate performance of the directed information transfer from X to Y , we want to determine whether the predicted image at time t has the correct color for the balls."
        },
        "referenced_paper_title": {
          "value": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
          "justification": "The referenced paper is the same as the current paper since TEB is introduced here.",
          "quote": "Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Rotating MNIST",
          "justification": "The paper employs the Rotating MNIST dataset for one of its experiments to validate the TEB model.",
          "quote": "We create a dataset of videos of rotating MNIST digits."
        },
        "aliases": [
          "Rot-MNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-based learning applied to document recognition",
          "justification": "The MNIST dataset originates from the paper by LeCun et al.",
          "quote": "Lecun et al., 1998"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The implementations and experiments are carried out using the PyTorch framework.",
          "quote": "All of the implementations were done using PyTorch"
        },
        "aliases": [
          "torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The library is well-known and its seminal paper is commonly referenced.",
          "quote": "Paszke et al., 2019."
        }
      },
      {
        "name": {
          "value": "torchdiffeq",
          "justification": "The torchdiffeq package is used for implementing the neuralODE component in the TEB model.",
          "quote": "the decoder d is neuralODE, implemented using the torchdiffeq package"
        },
        "aliases": [
          "torchdiffeq"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Neural Ordinary Differential Equations",
          "justification": "The referenced paper introduces the concept of Neural ODEs, which likely is the foundation of the torchdiffeq package.",
          "quote": "Chen et al., 2018"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1344,
    "prompt_tokens": 27294,
    "total_tokens": 28638
  }
}