{
  "paper": "2301.12040.txt",
  "words": 12562,
  "extractions": {
    "title": {
      "value": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts",
      "justification": "The title is stated clearly at the beginning of the paper.",
      "quote": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts"
    },
    "description": "The paper introduces the ProtST framework, which aims to enhance protein sequence pre-training and understanding by integrating biomedical texts. It proposes a new dataset called ProtDescribe, which pairs protein sequences with textual descriptions of their properties. The framework uses multimodal learning tasks, including unimodal mask prediction, multimodal representation alignment, and multimodal mask prediction, to improve the representation power of protein language models (PLMs). The enhanced PLMs are shown to perform well on various downstream tasks, including supervised learning and zero-shot prediction.",
    "type": {
      "value": "empirical study",
      "justification": "The paper presents experimental results and evaluates the performance of the proposed models against benchmarks, which is characteristic of empirical research.",
      "quote": "We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Bioinformatics",
        "justification": "The research focuses on enhancing protein sequence representation using machine learning, which falls under the field of Bioinformatics.",
        "quote": "Recent studies have proven the great promise of machine learning methods in predicting protein structures and functionality."
      },
      "aliases": [
        "Bioinformatics"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-modal Learning",
          "justification": "The paper proposes a framework that integrates multimodal learning tasks to enhance the representation power of protein language models.",
          "quote": "Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts."
        },
        "aliases": [
          "Multi-modal Learning"
        ]
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper utilizes textual descriptions and involves language models like PubMedBERT, which is relevant to the field of Natural Language Processing.",
          "quote": "Compared to the texts from general domains like newswire and Web, biomedical texts differ a lot in terms of vocabulary and expressions. To tackle such differences, language models specific to the biomedical domain (Beltagy et al., 2019; Lee et al., 2020; Gu et al., 2021) are actively studied."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Computational Biology",
          "justification": "The study leverages computational methods to analyze biological sequences, specifically proteins, making it a part of Computational Biology.",
          "quote": "Protein language models (PLMs) (Elnaggar et al., 2020; Rives et al., 2021; Lin et al., 2022) pre-trained on large-scale protein sequence corpus succeed in acquiring powerful protein representations."
        },
        "aliases": [
          "CompBio"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ProtST",
          "justification": "ProtST is the primary model introduced in the paper, designed to integrate protein sequences with biomedical texts.",
          "quote": "Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts."
        },
        "aliases": [
          "ProtST"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The ProtST framework is a novel contribution of the paper and is the main subject of the study.",
          "quote": "Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts."
        },
        "is_executed": {
          "value": 1,
          "justification": "The ProtST model was evaluated through various computational experiments, indicating it was executed using computational resources.",
          "quote": "The paper discusses various experimental setups and configurations showing that the ProtST model was executed."
        },
        "is_compared": {
          "value": 1,
          "justification": "ProtST-induced PLMs were compared to previous models on diverse representation learning benchmarks.",
          "quote": "We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "ProtST is a new framework introduced by this paper and does not reference an external paper for its origin.",
          "quote": "Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts."
        }
      },
      {
        "name": {
          "value": "ProtBert-BFD",
          "justification": "ProtBert-BFD is one of the state-of-the-art Protein Language Models enhanced by the ProtST framework.",
          "quote": "For ProtBert, we employ the ProtBert-BFD version which is trained on the BFD database (Steinegger & Söding, 2018)."
        },
        "aliases": [
          "ProtBert-BFD"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ProtBert-BFD is an existing model used as a baseline for comparison in this study.",
          "quote": "For ProtBert, we employ the ProtBert-BFD version which is trained on the BFD database (Steinegger & Söding, 2018)."
        },
        "is_executed": {
          "value": 1,
          "justification": "ProtBert-BFD was used in computational experiments to evaluate its performance against ProtST-enhanced models.",
          "quote": "For ProtBert, we employ the ProtBert-BFD version which is trained on the BFD database (Steinegger & Söding, 2018)."
        },
        "is_compared": {
          "value": 1,
          "justification": "ProtBert-BFD was compared with the ProtST-enhanced models in the study.",
          "quote": "For ProtBert, we employ the ProtBert-BFD version which is trained on the BFD database (Steinegger & Söding, 2018)."
        },
        "referenced_paper_title": {
          "value": "ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing",
          "justification": "The title of the referenced paper is provided in the context of ProtBert-BFD.",
          "quote": "ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing"
        }
      },
      {
        "name": {
          "value": "ESM-1b",
          "justification": "ESM-1b is another state-of-the-art Protein Language Model enhanced by the ProtST framework.",
          "quote": "For ESM-1b, we employ the ESM-1b model trained on a large-scale protein sequence corpus."
        },
        "aliases": [
          "ESM-1b"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ESM-1b is an existing model used as a baseline for comparison in this study.",
          "quote": "For ESM-1b, we employ the ESM-1b model trained on a large-scale protein sequence corpus."
        },
        "is_executed": {
          "value": 1,
          "justification": "ESM-1b was used in computational experiments to evaluate its performance against ProtST-enhanced models.",
          "quote": "For ESM-1b, we employ the ESM-1b model trained on a large-scale protein sequence corpus."
        },
        "is_compared": {
          "value": 1,
          "justification": "ESM-1b was compared with the ProtST-enhanced models in the study.",
          "quote": "For ESM-1b, we employ the ESM-1b model trained on a large-scale protein sequence corpus."
        },
        "referenced_paper_title": {
          "value": "Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences",
          "justification": "The title of the referenced paper is provided in the context of the ESM-1b model.",
          "quote": "Elnaggar et al., 2020; Rives et al., 2021"
        }
      },
      {
        "name": {
          "value": "ESM-2",
          "justification": "ESM-2 is one of the state-of-the-art Protein Language Models enhanced by the ProtST framework.",
          "quote": "For ESM-2, we employ the ESM-2-650M model to fairly compare with ESM-1b under the same model size."
        },
        "aliases": [
          "ESM-2"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ESM-2 is an existing model used as a baseline for comparison in this study.",
          "quote": "For ESM-2, we employ the ESM-2-650M model to fully compare with ESM-1b under the same model size."
        },
        "is_executed": {
          "value": 1,
          "justification": "ESM-2 was used in computational experiments to evaluate its performance against ProtST-enhanced models.",
          "quote": "For ESM-2, we employ the ESM-2-650M model to fully compare with ESM-1b under the same model size."
        },
        "is_compared": {
          "value": 1,
          "justification": "ESM-2 was compared with the ProtST-enhanced models in the study.",
          "quote": "For ESM-2, we employ the ESM-2-650M model to fully compare with ESM-1b under the same model size."
        },
        "referenced_paper_title": {
          "value": "Language Models of Protein Sequences at the Scale of Evolution Enable Accurate Structure Prediction",
          "justification": "The title of the referenced paper is provided in the context of the ESM-2 model.",
          "quote": "Lin et al., 2022"
        }
      },
      {
        "name": {
          "value": "PubMedBERT",
          "justification": "PubMedBERT is used to represent biomedical text descriptions in the ProtST framework.",
          "quote": "In this work, we employ a performant biomedical language model, PubMedBERT (Gu et al., 2021), to represent the biomedical text descriptions of proteins."
        },
        "aliases": [
          "PubMedBERT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "PubMedBERT is an existing model used for text representation in this study.",
          "quote": "In this work, we employ a performant biomedical language model, PubMedBERT (Gu et al., 2021), to represent the biomedical text descriptions of proteins."
        },
        "is_executed": {
          "value": 1,
          "justification": "PubMedBERT was used in computational experiments for text representation in the ProtST framework.",
          "quote": "In this work, we employ a performant biomedical language model, PubMedBERT (Gu et al., 2021), to represent the biomedical text descriptions of proteins."
        },
        "is_compared": {
          "value": 0,
          "justification": "PubMedBERT was not the focus of direct comparison but was used to augment protein language models.",
          "quote": "In this work, we employ a performant biomedical language model, PubMedBERT (Gu et al., 2021), to represent the biomedical text descriptions of proteins."
        },
        "referenced_paper_title": {
          "value": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "justification": "The title of the referenced paper is provided in the context of PubMedBERT.",
          "quote": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ProtDescribe",
          "justification": "ProtDescribe is a new dataset introduced in the paper, pairing protein sequences with textual descriptions of their properties.",
          "quote": "To attain biomedical-text-enhanced protein sequence representation learning, we first build the ProtDescribe dataset, a paired dataset of protein sequences and textual property descriptions."
        },
        "aliases": [
          "ProtDescribe"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "ProtDescribe is a new dataset introduced by this paper and does not reference an external paper for its origin.",
          "quote": "To attain biomedical-text-enhanced protein sequence representation learning, we first build the ProtDescribe dataset, a paired dataset of protein sequences and textual property descriptions."
        }
      },
      {
        "name": {
          "value": "Swiss-Prot",
          "justification": "The Swiss-Prot database is used to provide high-quality protein annotations for constructing the ProtDescribe dataset.",
          "quote": "We resort to the Swiss-Prot database (Bairoch & Apweiler, 2000) for high-quality protein annotations and construct each protein’s property description with the selected annotations of it."
        },
        "aliases": [
          "Swiss-Prot"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The SWISS-PROT Protein Sequence Data Bank and its New Supplement TREMBL",
          "justification": "The full reference title of the Swiss-Prot paper is mentioned in the context of the ProtDescribe dataset.",
          "quote": "Bairoch & Apweiler, 2000"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Adam optimizer",
          "justification": "The Adam optimizer is used for training the models in the study.",
          "quote": "An Adam optimizer (Kingma & Ba, 2014) (learning rate: 1.0 × 10−5 , weight decay: 0) is used to train the whole model for 20 epochs on 4 Tesla V100 GPUs."
        },
        "aliases": [
          "Adam optimizer"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A Method for Stochastic Optimization",
          "justification": "The full reference of the Adam optimizer paper is mentioned in the context of the training configuration.",
          "quote": "An Adam optimizer (Kingma & Ba, 2014) (learning rate: 1.0 × 10−5 , weight decay: 0) is used to train the whole model for 20 epochs on 4 Tesla V100 GPUs."
        }
      },
      {
        "name": {
          "value": "AutoDock Vina",
          "justification": "AutoDock Vina is used for estimating docking pose and binding affinity in the study.",
          "quote": "AutoDock Vina (Trott & Olson, 2010) is used for docking."
        },
        "aliases": [
          "AutoDock Vina"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "AutoDock Vina: Improving the Speed and Accuracy of Docking with a New Scoring Function, Efficient Optimization, and Multithreading",
          "justification": "The full reference of the AutoDock Vina paper is mentioned in the context of docking and binding affinity estimation.",
          "quote": "AutoDock Vina (Trott & Olson, 2010) is used for docking."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2826,
    "prompt_tokens": 25266,
    "total_tokens": 28092
  }
}