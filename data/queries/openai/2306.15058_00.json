{
  "paper": "2306.15058.txt",
  "words": 4798,
  "extractions": {
    "title": {
      "value": "BatchGFN: Generative Flow Networks for Batch Active Learning",
      "justification": "This is the title of the paper.",
      "quote": "BatchGFN: Generative Flow Networks for Batch Active Learning"
    },
    "description": "This paper introduces BatchGFN, a novel approach for active learning using generative flow networks to sample sets of data points proportionally to a batch reward, aiming to create highly informative batches for efficient model training.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper provides empirical evidence through experiments and benchmarks to support the proposed method.",
      "quote": "We show our approach enables sampling near-optimal utility batches at inference time with a single forward pass per point in the batch in toy regression problems. ... We also present early results for amortizing training across acquisition steps, which will enable scaling to real-world tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Active Learning",
        "justification": "The paper primarily focuses on active learning methodologies using generative flow networks.",
        "quote": "We propose BatchGFN; a parameterized sampler for batch AL which uses a GFN trained to sample informative query batches of data to label."
      },
      "aliases": [
        "AL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Learning",
          "justification": "Active Learning is a sub-field of Machine Learning.",
          "quote": "Active learning (AL) seeks to overcome this labelling bottleneck by iteratively selecting the most useful points to label to improve model performance."
        },
        "aliases": [
          "ML"
        ]
      },
      {
        "name": {
          "value": "Bayesian Inference",
          "justification": "The paper references Bayesian methods such as BALD for constructing batch rewards.",
          "quote": "For example, the BALD (Gal et al., 2017; Houlsby et al., 2011) algorithm takes a Bayesian perspective for a model with parameters θ and selects points that maximize the mutual information (MI) between the model predictions and its parameters."
        },
        "aliases": [
          "Bayesian"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BatchGFN",
          "justification": "BatchGFN is the primary model proposed and evaluated in the paper.",
          "quote": "We introduce BatchGFN—a novel approach for pool-based active learning that uses generative flow networks to sample sets of data points proportional to a batch reward."
        },
        "aliases": [
          "Batch Generative Flow Networks"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "BatchGFN is the novel model contribution of this paper.",
          "quote": "We propose BatchGFN; a parameterized sampler for batch AL which uses a GFN trained to sample informative query batches of data to label."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed and tested in the scope of the paper.",
          "quote": "BatchGFN has a time complexity of O(B) for sampling a batch, requiring only B forward passes of PF."
        },
        "is_compared": {
          "value": 1,
          "justification": "BatchGFN was empirically compared against other models like BatchBALD.",
          "quote": "Finally, we present early results into amortizing training across acquisition steps. ... Active learning with BatchGFN is on par with BatchBALD."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "This is the main model of the paper, so there are no external reference papers for this model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BatchBALD",
          "justification": "BatchBALD is a baseline model referenced and compared in the experiments.",
          "quote": "We find that BatchGFN is on par with BatchBALD and significantly outperforms BALD and random acquisition."
        },
        "aliases": [
          "Batch Bayesian Active Learning by Disagreement"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "BatchBALD is not a contribution of this paper; it is used as a baseline for comparison.",
          "quote": "For example, the BALD (Gal et al., 2017; Houlsby et al., 2011) algorithm takes a Bayesian perspective for a model..."
        },
        "is_executed": {
          "value": 1,
          "justification": "BatchBALD was executed and its results were compared to BatchGFN.",
          "quote": "BatchBALD is orders of magnitude more expensive when using MC sampling, whereas BatchGFN inference will be unaffected."
        },
        "is_compared": {
          "value": 1,
          "justification": "BatchBALD was numerically compared to BatchGFN.",
          "quote": "Active learning with BatchGFN is on par with BatchBALD."
        },
        "referenced_paper_title": {
          "value": "BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning",
          "justification": "This is the title of the referenced paper for BatchBALD.",
          "quote": "In practice, we would like to sample high JMI batches for applications such as AL. Figure 3 shows the JMI of sampled batches from the BatchGFN at different reward temperatures for the 1D regression task. We find that by decreasing the temperature we are able to sample higher JMI batches with greater sample efficiency compared to other stochastic approaches. The batches sampled are on par with BatchBALD while being less computationally expensive."
        }
      },
      {
        "name": {
          "value": "BALD",
          "justification": "BALD is referenced as a method in the context of constructing batch rewards.",
          "quote": "For example, the BALD (Gal et al., 2017; Houlsby et al., 2011) algorithm takes a Bayesian perspective for a model with parameters θ and selects points that maximize the mutual information (MI) between the model predictions and its parameters."
        },
        "aliases": [
          "Bayesian Active Learning by Disagreement"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "BALD is not a contribution of this paper; it is used as a referenced method.",
          "quote": "For example, the BALD (Gal et al., 2017; Houlsby et al., 2011) algorithm takes a Bayesian perspective for a model..."
        },
        "is_executed": {
          "value": 1,
          "justification": "BALD was executed and its results were compared to BatchGFN.",
          "quote": "Active learning with BatchGFN is on par with BatchBALD and significantly outperforms BALD and random acquisition."
        },
        "is_compared": {
          "value": 1,
          "justification": "BALD was numerically compared to BatchGFN.",
          "quote": "Active learning with BatchGFN is on par with BatchBALD and significantly outperforms BALD and random acquisition."
        },
        "referenced_paper_title": {
          "value": "Deep Bayesian Active Learning with Image Data",
          "justification": "This is the referenced paper for the BALD method.",
          "quote": "For example, the BALD (Gal et al., 2017; Houlsby et al., 2011) algorithm takes a Bayesian perspective for a model..."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "torch",
          "justification": "torch (PyTorch) is used for implementing and running the models.",
          "quote": "Our code is available at https://github.com/s-a-malik/batchgfn. It relies on torchgfn (Lahlou et al., 2023) for implementing the GFlowNets."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific reference paper provided for PyTorch in this context.",
          "quote": "Our code is available at https://github.com/s-a-malik/batchgfn. It relies on torchgfn for implementing the GFlowNets."
        }
      },
      {
        "name": {
          "value": "torchgfn",
          "justification": "torchgfn is explicitly mentioned as the library used for implementing GFlowNets.",
          "quote": "Our code is available at https://github.com/s-a-malik/batchgfn. It relies on torchgfn (Lahlou et al., 2023) for implementing the GFlowNets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "torchgfn: A PyTorch GFlowNet Library",
          "justification": "This is the referenced paper for the torchgfn library.",
          "quote": "Our code is available at https://github.com/s-a-malik/batchgfn. It relies on torchgfn (Lahlou et al., 2023) for implementing the GFlowNets."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2041,
    "prompt_tokens": 11119,
    "total_tokens": 13160
  }
}