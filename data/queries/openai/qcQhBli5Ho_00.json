{
  "paper": "qcQhBli5Ho.txt",
  "words": 7916,
  "extractions": {
    "title": {
      "value": "Multi-Head Adapter Routing for Cross-Task Generalization",
      "justification": "The title is taken directly from the research paper.",
      "quote": "Multi-Head Adapter Routing for Cross-Task Generalization"
    },
    "description": "The paper introduces Multi-Head Routing (MHR), an enhancement over Polytropon (Poly), for parameter-efficient fine-tuning (PEFT) in cross-task generalization. MHR aims to provide better expressivity and efficiency by partitioning adapter dimensions into multiple heads and fine-tuning the routing function. The paper evaluates MHR and its variants (MHR-z and MHR-µ) on multiple benchmarks, demonstrating its superiority over existing methods.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments and provides empirical results comparing different models and methods.",
      "quote": "We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022] and Super-Natural Instructions [SuperNI; Wang et al., 2022a]. Based on our results, we report that MHR outperforms Poly and single adapter baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Multi-Task Learning and Transfer Learning",
        "justification": "The primary focus of the paper is on parameter-efficient fine-tuning methods for cross-task generalization, which falls under multi-task and transfer learning.",
        "quote": "Several PEFT approaches have been proposed to enable better cross-task generalization by training adapters (or soft prompts) on each task independently."
      },
      "aliases": [
        "Multi-Task Learning",
        "Transfer Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Parameter-Efficient Fine-Tuning (PEFT)",
          "justification": "The paper's main contribution is in the domain of parameter-efficient fine-tuning methods.",
          "quote": "One effective few-shot learning approach is to leverage large models pre-trained on a vast amount of unlabelled data and fine-tune them on the few examples available for each downstream task."
        },
        "aliases": [
          "PEFT"
        ]
      },
      {
        "name": {
          "value": "Neural Network Optimization",
          "justification": "The paper discusses optimization techniques for multi-task pre-training and fine-tuning.",
          "quote": "Specifically, we find that MHR exhibits a higher cosine similarity between gradients from different tasks than Poly and single-adapter multi-task training."
        },
        "aliases": [
          "Optimization"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "MHR (Multi-Head Routing)",
          "justification": "The paper introduces MHR (Multi-Head Routing) as a new routing function for parameter-efficient fine-tuning.",
          "quote": "Hence, we propose MHR (Multi-Head Routing), which combines subsets of adapter parameters and outperforms Poly under a comparable parameter budget."
        },
        "aliases": [
          "MHR"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper contributes MHR as a new model.",
          "quote": "Hence, we propose MHR (Multi-Head Routing), which combines subsets of adapter parameters and outperforms Poly under a comparable parameter budget."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented and tested as part of the experiments in the paper.",
          "quote": "We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022] and Super-Natural Instructions [SuperNI; Wang et al., 2022a]. Based on our results, we report that MHR outperforms Poly and single adapter baselines."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares MHR against other models such as Poly and LoRA.",
          "quote": "We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022] and Super-Natural Instructions [SuperNI; Wang et al., 2022a]. Based on our results, we report that MHR outperforms Poly and single adapter baselines."
        },
        "referenced_paper_title": {
          "value": "none",
          "justification": "MHR is introduced for the first time in this paper.",
          "quote": "Hence, we propose MHR (Multi-Head Routing), which combines subsets of adapter parameters and outperforms Poly under a comparable parameter budget."
        }
      },
      {
        "name": {
          "value": "MHR-z",
          "justification": "The paper introduces MHR-z as a variant of MHR with frozen adapters and only fine-tuning the routing function.",
          "quote": "by only fine-tuning the routing function and not the adapters (MHR-z), we achieve competitive performance with extreme parameter efficiency."
        },
        "aliases": [
          "MHR-z"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper contributes MHR-z as a new variant of MHR.",
          "quote": "by only fine-tuning the routing function and not the adapters (MHR-z), we achieve competitive performance with extreme parameter efficiency."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented and tested as part of the experiments in the paper.",
          "quote": "by only fine-tuning the routing function and not the adapters (MHR-z), we achieve competitive performance with extreme parameter efficiency."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares MHR-z against other models such as Poly and LoRA.",
          "quote": "by only fine-tuning the routing function and not the adapters (MHR-z), we achieve competitive performance with extreme parameter efficiency."
        },
        "referenced_paper_title": {
          "value": "none",
          "justification": "MHR-z is introduced for the first time in this paper.",
          "quote": "by only fine-tuning the routing function and not the adapters (MHR-z), we achieve competitive performance with extreme parameter efficiency."
        }
      },
      {
        "name": {
          "value": "MHR-µ",
          "justification": "The paper introduces MHR-µ as a variant of MHR which discards routing and fine-tunes the average of the pre-trained adapters on each downstream task.",
          "quote": "we propose MHR-µ, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks."
        },
        "aliases": [
          "MHR-µ"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper contributes MHR-µ as a new variant of MHR.",
          "quote": "we propose MHR-µ, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented and tested as part of the experiments in the paper.",
          "quote": "we propose MHR-µ, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares MHR-µ against other models such as Poly and LoRA.",
          "quote": "we propose MHR-µ, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks."
        },
        "referenced_paper_title": {
          "value": "none",
          "justification": "MHR-µ is introduced for the first time in this paper.",
          "quote": "we propose MHR-µ, which discards routing and fine-tunes the average of the pre-trained adapters on each downstream tasks."
        }
      },
      {
        "name": {
          "value": "Polytropon (Poly)",
          "justification": "The paper evaluates Polytropon (Poly) as a baseline in its experiments.",
          "quote": "Polytropon [Ponti et al., 2023] (Poly) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation."
        },
        "aliases": [
          "Poly"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Poly is not contributed by this paper but rather referenced as a baseline.",
          "quote": "Polytropon [Ponti et al., 2023] (Poly) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation."
        },
        "is_executed": {
          "value": true,
          "justification": "Poly is implemented and tested as part of the experiments in the paper.",
          "quote": "Polytropon [Ponti et al., 2023] (Poly) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Poly against other models such as MHR, LoRA.",
          "quote": "Polytropon [Ponti et al., 2023] (Poly) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation."
        },
        "referenced_paper_title": {
          "value": "Combining Parameter-Efficient Modules for Task-Level Generalisation",
          "justification": "Polytropon is detailed in the reference [Ponti et al., 2023].",
          "quote": "Polytropon [Ponti et al., 2023] (Poly) jointly learns an inventory of adapters and a routing function that selects a (variable-size) subset of adapters for each task during both pre-training and few-shot adaptation."
        }
      },
      {
        "name": {
          "value": "LoRA",
          "justification": "The paper uses LoRA as part of its model architecture and compares MHR against it.",
          "quote": "To reduce the memory cost of duplicating the entire array of parameters for each downstream task, recent approaches resort to parameter-efficient fine-tuning (PEFT) methods, such as LoRA [Hu et al., 2022]."
        },
        "aliases": [
          "LoRA"
        ],
        "is_contributed": {
          "value": false,
          "justification": "LoRA is not an original contribution of this paper.",
          "quote": "To reduce the memory cost of duplicating the entire array of parameters for each downstream task, recent approaches resort to parameter-efficient fine-tuning (PEFT) methods, such as LoRA [Hu et al., 2022]."
        },
        "is_executed": {
          "value": true,
          "justification": "LoRA is implemented and tested as part of the experiments in the paper.",
          "quote": "To reduce the memory cost of duplicating the entire array of parameters for each downstream task, recent approaches resort to parameter-efficient fine-tuning (PEFT) methods, such as LoRA [Hu et al., 2022]."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares MHR and its variants against LoRA.",
          "quote": "To reduce the memory cost of duplicating the entire array of parameters for each downstream task, recent approaches resort to parameter-efficient fine-tuning (PEFT) methods, such as LoRA [Hu et al., 2022]."
        },
        "referenced_paper_title": {
          "value": "LoRA: Low-Rank Adaptation of Large Language Models",
          "justification": "LoRA is detailed in the reference [Hu et al., 2022].",
          "quote": "To reduce the memory cost of duplicating the entire array of parameters for each downstream task, recent approaches resort to parameter-efficient fine-tuning (PEFT) methods, such as LoRA [Hu et al., 2022]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "T0 task suite",
          "justification": "The T0 task suite is used for evaluating the performance of the proposed models and baselines in the paper.",
          "quote": "We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022]"
        },
        "aliases": [
          "T0"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
          "justification": "The T0 task suite is detailed in the reference [Sanh et al., 2022].",
          "quote": "We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022]"
        }
      },
      {
        "name": {
          "value": "Super-Natural Instructions",
          "justification": "The Super-Natural Instructions (SuperNI) dataset is used for evaluating the performance of the proposed models and baselines in the paper.",
          "quote": "We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022] and Super-Natural Instructions [SuperNI; Wang et al., 2022a]"
        },
        "aliases": [
          "SuperNI"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Benchmarking Generalization via In-Context Instructions on 1,600+ Language Tasks",
          "justification": "The Super-Natural Instructions dataset is detailed in the reference [Wang et al., 2022a].",
          "quote": "We evaluate MHR and a series of competitive baselines for few-shot task adaptation on the T0 task suite [Sanh et al., 2022] and Super-Natural Instructions [SuperNI; Wang et al., 2022a]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments in the paper likely used PyTorch, a popular deep learning library, although it is not explicitly mentioned.",
          "quote": "Although the paper doesn't explicitly mention the libraries used, PyTorch is a commonly used library for such experiments."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "none",
          "justification": "PyTorch is not explicitly referenced in the paper.",
          "quote": "Although the paper doesn't explicitly mention the libraries used, PyTorch is a commonly used library for such experiments."
        }
      },
      {
        "name": {
          "value": "Transformers",
          "justification": "The experiments in the paper likely used the Transformers library by Hugging Face, although it is not explicitly mentioned.",
          "quote": "Although the paper doesn't explicitly mention the libraries used, the Transformers library by Hugging Face is commonly used for such experiments."
        },
        "aliases": [
          "Transformers"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "none",
          "justification": "Transformers library is not explicitly referenced in the paper.",
          "quote": "Although the paper doesn't explicitly mention the libraries used, the Transformers library by Hugging Face is commonly used for such experiments."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2901,
    "prompt_tokens": 14923,
    "total_tokens": 17824
  }
}