{
  "paper": "2306.12599.txt",
  "words": 5398,
  "extractions": {
    "title": {
      "value": "Constant Memory Attention Block",
      "justification": "This is the exact title provided at the beginning of the research paper.",
      "quote": "Constant Memory Attention Block"
    },
    "description": "This paper introduces a novel attention mechanism called the Constant Memory Attention Block (CMAB), which computes its output in constant memory and performs updates in constant computation. The authors propose two models, Constant Memory Attentive Neural Processes (CMANPs) and Constant Memory Hawkes Processes (CMHPs), which demonstrate competitive results with state-of-the-art models while being significantly more memory efficient.",
    "type": {
      "value": "Empirical",
      "justification": "The paper includes empirical experiments comparing the proposed models (CMANPs and CMHPs) with state-of-the-art methods on datasets like EMNIST, CelebA, Mooc, and Reddit, showing competitive results.",
      "quote": "The experimental results show these methods based on CMABs achieve results competitive with that of state-of-the-art methods while being significantly more memory efficient."
    },
    "primary_research_field": {
      "name": {
        "value": "Efficient Deep Learning",
        "justification": "The primary focus of the paper is on creating a more memory-efficient attention mechanism, which falls under the broader category of efficient deep learning methods.",
        "quote": "Modern foundation model architectures rely on attention mechanisms to effectively capture context. However, these methods require linear or quadratic memory in terms of the number of inputs/datapoints, limiting their applicability in low-compute domains. In this work, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that computes its output in constant memory and performs updates in constant computation."
      },
      "aliases": [
        "Efficient Deep Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Attention Mechanisms",
          "justification": "The paper mainly focuses on creating a memory-efficient form of attention mechanism, hence making it a key sub-research field.",
          "quote": "Modern foundation model architectures rely on attention mechanisms to effectively capture context. However, these methods require linear or quadratic memory in terms of the number of inputs/datapoints, limiting their applicability in low-compute domains. In this work, we propose Constant Memory Attention Block (CMAB), a novel general-purpose attention block that computes its output in constant memory and performs updates in constant computation."
        },
        "aliases": [
          "Attention Mechanisms"
        ]
      },
      {
        "name": {
          "value": "Neural Processes",
          "justification": "One of the proposed models, CMANPs, is an advancement in the field of Neural Processes.",
          "quote": "Neural Processes (NPs) are meta-learned models that efficiently compute uncertainty estimates. Specifically, NPs condition on an arbitrary amount of context datapoints (labelled datapoints) and make predictions for a batch of target datapoints, while preserving invariance in the ordering of the context dataset....In this work, we propose a novel attention block called the Constant Memory Attention Block (CMAB) which allows..."
        },
        "aliases": [
          "Neural Processes"
        ]
      },
      {
        "name": {
          "value": "Temporal Point Processes",
          "justification": "The other proposed model, CMHPs, is aimed at improving Temporal Point Processes.",
          "quote": "Temporal Point Processes are stochastic processes composed of a time series of discrete events...In this work, we propose a novel attention block called the Constant Memory Attention Block (CMAB) which allows..."
        },
        "aliases": [
          "Temporal Point Processes"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Constant Memory Attentive Neural Processes (CMANPs)",
          "justification": "The paper explicitly introduces CMANPs as one of the models that utilizes the proposed CMAB mechanism.",
          "quote": "We introduce two models for different settings: Constant Memory Attentive Neural Processes (CMANPs) and Constant Memory Hawkes Process (CMHPs)."
        },
        "aliases": [
          "CMANPs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "CMANPs is proposed as a new model in the paper.",
          "quote": "We introduce two models for different settings: Constant Memory Attentive Neural Processes (CMANPs) and Constant Memory Hawkes Process (CMHPs)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experiments conducted using CMANPs on datasets like EMNIST and CelebA.",
          "quote": "In this experiment, we compare CMANPs against prior NP methods on standard NP datasets: EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of CMANPs with other state-of-the-art methods like TNP-D and LBANP.",
          "quote": "In Table 2, we compare CMANPs with existing NP baselines, showing their performance is competitive with that of prior state-of-the-art: TNP-D and LBANP."
        },
        "referenced_paper_title": {
          "value": "Latent bottlenecked attentive neural processes",
          "justification": "The CMANP model is compared to the LBANP model referenced in the paper.",
          "quote": "In this section, we leverage CMABs to construct an efficient Neural Process by replacing the iterative attention blocks used in LBANPs (Feng et al., 2023) with CMABs."
        }
      },
      {
        "name": {
          "value": "Constant Memory Hawkes Processes (CMHPs)",
          "justification": "The paper explicitly introduces CMHPs as one of the models that utilizes the proposed CMAB mechanism.",
          "quote": "We introduce two models for different settings: Constant Memory Attentive Neural Processes (CMANPs) and Constant Memory Hawkes Process (CMHPs)."
        },
        "aliases": [
          "CMHPs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "CMHPs is proposed as a new model in the paper.",
          "quote": "We introduce two models for different settings: Constant Memory Attentive Neural Processes (CMANPs) and Constant Memory Hawkes Process (CMHPs)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experiments conducted using CMHPs on datasets like Mooc and Reddit.",
          "quote": "In this experiment, we compare CMHPs (CMAB-based model) against THPs (Transformer-based model) on standard TPP datasets: Mooc and Reddit (dataset details in Appendix)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of CMHPs with other state-of-the-art methods like Transformer Hawkes Processes (THP).",
          "quote": "In this experiment, we compare CMHPs (CMAB-based model) against THPs (Transformer-based model) on standard TPP datasets: Mooc and Reddit (dataset details in Appendix)."
        },
        "referenced_paper_title": {
          "value": "Transformer Hawkes process",
          "justification": "The CMHP model is compared to the THP model referenced in the paper.",
          "quote": "Building on CMABs, we introduce the Constant Memory Hawkes Process (CMHPs) (Figure 4 in Appendix due to space limitations), a model which replaced the transformer layers in Transformer Hawkes Process (THP) (Zuo et al., 2020) with Constant Memory Attention Blocks."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "EMNIST",
          "justification": "The paper uses the EMNIST dataset for experiments involving CMANPs.",
          "quote": "In this experiment, we compare CMANPs against prior NP methods on standard NP datasets: EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)."
        },
        "aliases": [
          "EMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "EMNIST: Extending MNIST to handwritten letters",
          "justification": "The EMNIST dataset is referred to with its corresponding paper in the bibliography.",
          "quote": "EMNIST comprises of black and white images of handwritten letters of 32 × 32 resolution."
        }
      },
      {
        "name": {
          "value": "CelebA",
          "justification": "The paper uses the CelebA dataset for experiments involving CMANPs.",
          "quote": "In this experiment, we compare CMANPs against prior NP methods on standard NP datasets: EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)."
        },
        "aliases": [
          "CelebA"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep learning face attributes in the wild",
          "justification": "The CelebA dataset is referred to with its corresponding paper in the bibliography.",
          "quote": "CelebA comprises of black and white images of handwritten letters of 32 × 32 resolution."
        }
      },
      {
        "name": {
          "value": "Mooc",
          "justification": "The paper uses the Mooc dataset for experiments involving CMHPs.",
          "quote": "In this experiment, we compare CMHPs (CMAB-based model) against THPs (Transformer-based model) on standard TPP datasets: Mooc and Reddit (dataset details in Appendix)."
        },
        "aliases": [
          "Mooc"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Intensity-free learning of temporal point processes",
          "justification": "The Mooc dataset is referred to with its corresponding paper in the bibliography.",
          "quote": "Mooc Dataset comprises of 7, 047 sequences. Each sequence contains the action times of an individual user of an online Mooc course with 98 categories for the marks."
        }
      },
      {
        "name": {
          "value": "Reddit",
          "justification": "The paper uses the Reddit dataset for experiments involving CMHPs.",
          "quote": "In this experiment, we compare CMHPs (CMAB-based model) against THPs (Transformer-based model) on standard TPP datasets: Mooc and Reddit (dataset details in Appendix)."
        },
        "aliases": [
          "Reddit"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Meta temporal point processes",
          "justification": "The Reddit dataset is referred to with its corresponding paper in the bibliography.",
          "quote": "Reddit Dataset comprises of 10, 000 sequences. Each sequence contains the action times from the most active users with marks being one of the 984 the sub-reddit categories of each sequence."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2039,
    "prompt_tokens": 10512,
    "total_tokens": 12551
  }
}