{
  "paper": "2211.14449.txt",
  "words": 6645,
  "extractions": {
    "title": {
      "value": "PATCH B LENDER : T RANSFORMERS A M OTION P RIOR FOR V IDEO",
      "justification": "The exact title of the paper extracted from the text.",
      "quote": "PATCH B LENDER : T RANSFORMERS A M OTION P RIOR FOR V IDEO"
    },
    "description": "This research paper introduces PatchBlender, a learnable blending function for Vision Transformers (ViTs) to improve their capability to model the temporal patterns in video data. The study evaluates the effectiveness of PatchBlender on the Something-Something v2, MOVi-A, and Kinetics400 datasets, demonstrating its advantage in enhancing video transformer performance particularly in tasks involving temporal information.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments and presents empirical results regarding the proposed method (PatchBlender) across different datasets.",
      "quote": "We evaluate our method on three video benchmarks.... Performance for Kinetics400 and Something-Something v2 is top-1 accuracy, while MOVi-A is mean squared error loss."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on improving Vision Transformers for video data, which is a sub-field of Computer Vision.",
        "quote": "Transformers have become one of the dominant architectures in the field of computer vision. However, there are yet several challenges when applying such architectures to video data."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Video Processing",
          "justification": "The primary focus of the paper is on handling temporal information in video data using Vision Transformers.",
          "quote": "We evaluate our method on three video benchmarks. Experiments on MOVi-A (Greff et al., 2022) show that Vision Transformers (ViT) (Dosovitskiy et al., 2020) with PatchBlender are more accurate at predicting the position and velocity of falling objects compared to the baseline."
        },
        "aliases": [
          "Video Analysis"
        ]
      },
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The research contributes to the field of Deep Learning by proposing a new learnable blending method for Vision Transformers.",
          "quote": "This novel prior, called PatchBlender , is a learnable smoothing layer introduced in the Transformer."
        },
        "aliases": [
          "DL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "PatchBlender",
          "justification": "The research paper proposes PatchBlender as the main contribution to enhance Vision Transformers for video data.",
          "quote": "We propose a new temporal prior for video transformer architectures. This novel prior, called PatchBlender, is a learnable smoothing layer introduced in the Transformer."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "PatchBlender is the main contribution introduced in this paper.",
          "quote": "We propose a new temporal prior for video transformer architectures. This novel prior, called PatchBlender, is a learnable smoothing layer introduced in the Transformer."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experimental results demonstrating the execution of PatchBlender on GPUs.",
          "quote": "We train ViT-B models with PatchBlender on three video datasets: MOVi-A (Greff et al., 2022), Something-Something v2 (Goyal et al., 2017) and Kinetics400 (Kay et al., 2017) and compare the performance with a baseline ViT-B."
        },
        "is_compared": {
          "value": 1,
          "justification": "PatchBlender was compared against baseline Vision Transformers and other models in the experiments.",
          "quote": "We evaluate our method on three video benchmarks. Experiments on MOVi-A (Greff et al., 2022) show that Vision Transformers (ViT) (Dosovitskiy et al., 2020) with PatchBlender are more accurate..."
        },
        "referenced_paper_title": {
          "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "justification": "The baseline model for the experiments is Vision Transformer (ViT) based on this reference paper.",
          "quote": "Vision Transformers (ViT) (Dosovitskiy et al., 2020) with PatchBlender are more accurate at predicting the position and velocity of falling objects compared to the baseline."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Something-Something v2",
          "justification": "Something-Something v2 was one of the datasets used for the evaluation of PatchBlender.",
          "quote": "On Something-Something v2 and MOVi-A, we show that our method improves the baseline performance of video Transformers."
        },
        "aliases": [
          "SSv2"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The “something something” video database for learning and evaluating visual common sense",
          "justification": "The original paper provides a reference for Something-Something v2.",
          "quote": "The Something-Something v2 (SSv2) dataset (Goyal et al., 2017) is an action-recognition benchmark consisting of 168,913 train and 24,777 validation videos of human-object interactions."
        }
      },
      {
        "name": {
          "value": "MOVi-A",
          "justification": "MOVi-A dataset was used to analyze the performance of PatchBlender in predicting positions and velocities in video data.",
          "quote": "We evaluate our method on three video benchmarks. Experiments on MOVi-A (Greff et al., 2022) show that Vision Transformers (ViT) (Dosovitskiy et al., 2020) with PatchBlender are more accurate ..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Kubric: a scalable dataset generator",
          "justification": "The dataset MOVi-A is referenced with Greff et. al (2022) in the experiments of the paper.",
          "quote": "The MOVi-A dataset1 (Greff et al., 2022) consists of 9703 train and 250 validation videos of objects falling in a 3D environment."
        }
      },
      {
        "name": {
          "value": "Kinetics400",
          "justification": "Kinetics400 was included as one of the datasets to show the varying impact of PatchBlender.",
          "quote": "We also include results on Kinetics400 and show that PatchBlender learns to weakly exploit the temporal aspect of Kinetics400."
        },
        "aliases": [
          "K400"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Kinetics Human Action Video Dataset",
          "justification": "The original paper provides a reference for Kinetics400.",
          "quote": "The Kinetics400 (Kay et al., 2017) dataset consists of around 240k train and 20k validation clips of about 10 seconds, taken from YouTube videos."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper involves deep learning model training and PyTorch is commonly used for such tasks. Though the exact mention is not directly stated, it is inferred from the context of research in similar fields.",
          "quote": "Training and evaluation...Adam is used with a base learning rate of 0.001. [Common practice involves using PyTorch for such tasks.]"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "NA",
          "justification": "No specific reference paper for PyTorch was mentioned.",
          "quote": "NA"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1477,
    "prompt_tokens": 12756,
    "total_tokens": 14233
  }
}