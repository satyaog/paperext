{
  "paper": "2311.03096.txt",
  "words": 10890,
  "extractions": {
    "title": {
      "value": "Weight-Sharing Regularization",
      "justification": "This is the title given in the header of the paper.",
      "quote": "Weight-Sharing Regularization"
    },
    "description": "The paper introduces a novel weight-sharing regularization technique for deep learning models. It focuses on the proximal mapping of this regularization and proposes efficient parallel algorithms for its computation. The paper provides theoretical insights, algorithmic contributions, and empirical validation.",
    "type": {
      "value": "Empirical",
      "justification": "The paper includes experiments on MNIST and CIFAR10 datasets and proposes new algorithms.",
      "quote": "Our experiments reveal that weight-sharing regularization enables fully connected networks to learn convolution-like filters even when pixels have been shuffled while convolutional neural networks fail in this setting... In experiments, for the first time we are able to effectively apply weight-sharing regularization at scale in the context of deep neural networks and recover convolution-like filters in Multi-Layer Perceptrons (MLPs), even when pixels have been shuffled."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The research focuses on proposing and evaluating a novel regularization technique for machine learning models.",
        "quote": "A common technique used in machine learning to avoid overfitting and improve generalization is regularization... Motivated by these facts, we propose weight-sharing regularization."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper presents a new formulation for proximal mapping and introduces an optimization algorithm.",
          "quote": "The proximal gradient descent update... the efficient computation of the proximal update will be a central contribution of this paper... We provide a new formulation of the proximal mapping of R in terms of the solution to an Ordinary Differential Equation (ODE)..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The empirical validation includes experiments on image datasets such as MNIST and CIFAR10.",
          "quote": "Our experiments reveal that weight-sharing regularization enables fully connected networks to learn convolution-like filters even when pixels have been shuffled while convolutional neural networks fail in this setting... In experiments, for the first time we are able to effectively apply weight-sharing regularization at scale in the context of deep neural networks and recover convolution-like filters in Multi-Layer Perceptrons (MLPs), even when pixels have been shuffled."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Convolutional Neural Networks (CNNs)",
          "justification": "The research paper mentions CNNs as one of the modern deep learning architectures using weight-sharing.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to transformers (Vaswani et al., 2017), use some form of weight-sharing, e.g., CNNs apply the same weights at different locations of the input image."
        },
        "aliases": [
          "CNNs"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper discusses CNNs as an existing baseline.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to transformers (Vaswani et al., 2017), use some form of weight-sharing, e.g., CNNs apply the same weights at different locations of the input image."
        },
        "is_executed": {
          "value": 1,
          "justification": "CNNs are used in the experiments.",
          "quote": "While the answer to this question is positive, an invariant CNN model achieves higher accuracy compared to an MLP model regularized with R(w)... In this experiment, the advantage of weight-sharing regularization becomes evident, as it achieves the best accuracy among all models."
        },
        "is_compared": {
          "value": 1,
          "justification": "CNNs are used as a baseline model for comparison against the proposed method.",
          "quote": "While the answer to this question is positive, an invariant CNN model achieves higher accuracy compared to an MLP model regularized with R(w)... In this experiment, the advantage of weight-sharing regularization becomes evident, as it achieves the best accuracy among all models."
        },
        "referenced_paper_title": {
          "value": "Backpropagation applied to handwritten zip code recognition",
          "justification": "The paper refers to LeCun et al., 1989 as the original paper on CNNs.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989)..."
        }
      },
      {
        "name": {
          "value": "Transformers",
          "justification": "The research paper mentions transformers as one of the modern deep learning architectures using weight-sharing.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to transformers (Vaswani et al., 2017), use some form of weight-sharing."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper discusses transformers as an existing baseline.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to transformers (Vaswani et al., 2017), use some form of weight-sharing."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no mention of transformers being executed within the paper's experiments.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to transformers (Vaswani et al., 2017), use some form of weight-sharing."
        },
        "is_compared": {
          "value": 0,
          "justification": "The comparison in the paper is mainly against CNNs and MLPs.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to transformers (Vaswani et al., 2017), use some form of weight-sharing."
        },
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "The paper refers to Vaswani et al., 2017 as the original paper on transformers.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to transformers (Vaswani et al., 2017)..."
        }
      },
      {
        "name": {
          "value": "Multi-layer Perceptrons (MLPs)",
          "justification": "The paper discusses using weight-sharing regularization in MLPs.",
          "quote": "Our experiments reveal that weight-sharing regularization enables fully connected networks to learn convolution-like filters even when pixels have been shuffled while convolutional neural networks fail in this setting."
        },
        "aliases": [
          "MLPs"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "While the paper introduces weight-sharing regularization for MLPs, MLP itself is not a new model.",
          "quote": "fully connected networks with no structure or weight-sharing are prone to overfitting the dataset."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper conducts experiments with MLPs using weight-sharing regularization.",
          "quote": "Our experiments reveal that weight-sharing regularization enables fully connected networks to learn convolution-like filters even when pixels have been shuffled."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares MLPs with weight-sharing against CNNs and other baselines.",
          "quote": "In this setting, our results indicate that an MLP with weight-sharing regularization achieves the best accuracy, outperforming CNN by a significant margin."
        },
        "referenced_paper_title": {
          "value": "No reference paper provided",
          "justification": "The paper does not provide a specific reference for MLPs.",
          "quote": "No reference quote available"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The paper conducts experiments using the MNIST dataset.",
          "quote": "We consider a translation-invariant version of the MNIST dataset which we refer to as MNIST on a torus... While it achieves 99.89% training accuracy, test accuracy peaks at 92.40%."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Backpropagation applied to handwritten zip code recognition",
          "justification": "The original MNIST paper is referenced as LeCun et al., 1989.",
          "quote": "All modern deep learning architectures, from Convolutional Neural Networks (CNNs) (LeCun et al., 1989) to transformers (Vaswani et al., 2017), use some form of weight-sharing, e.g., CNNs apply the same weights at different locations of the input image."
        }
      },
      {
        "name": {
          "value": "CIFAR10",
          "justification": "The paper also uses the CIFAR10 dataset for experiments.",
          "quote": "We consider the task of training a shallow CNN and its corresponding fully connected network on CIFAR10 (Krizhevsky et al., 2009) as suggested by (Neyshabur, 2020)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The referenced paper for CIFAR10 is Krizhevsky et al., 2009.",
          "quote": "We consider the task of training a shallow CNN and its corresponding fully connected network on CIFAR10 (Krizhevsky et al., 2009) as suggested by (Neyshabur, 2020)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2293,
    "prompt_tokens": 18665,
    "total_tokens": 20958
  }
}