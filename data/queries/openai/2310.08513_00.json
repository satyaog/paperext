{
  "paper": "2310.08513.txt",
  "words": 14576,
  "extractions": {
    "title": {
      "value": "How Connectivity Structure Shapes Rich and Lazy Learning in Neural Circuits",
      "justification": "The title directly reflects the paper's investigation into how different initial weight structures affect the learning regimes in neural networks, with 'rich' and 'lazy' learning as the core focus.",
      "quote": "H OW CONNECTIVITY STRUCTURE SHAPES RICH AND LAZY LEARNING IN NEURAL CIRCUITS"
    },
    "description": "This paper investigates the impact of initial weight structures, particularly their effective rank, on the learning dynamics of neural networks. It demonstrates that high-rank initializations lead to lazier learning, whereas low-rank initializations result in richer learning. These findings are grounded in theoretical derivations and empirical validations using recurrent neural networks (RNNs) on various tasks.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper includes theoretical derivations in a two-layer feedforward linear network and extends these results empirically, focusing on the theory behind network initialization and its effects.",
      "quote": "Through theoretical derivation in two-layer feedforward linear network, we demonstrate that higher-rank initialization results in effectively lazier learning on average across tasks"
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The study revolves around neural networks, a fundamental aspect of Deep Learning, and explores their learning dynamics based on initial weight structures.",
        "quote": "In deep learning, structure, encompassing architecture and initial connectivity, crucially dictates learning speed and effectiveness"
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Network Learning Dynamics",
          "justification": "The paper specifically addresses how the learning dynamics of neural networks are influenced by their initial weight structures, which falls under the study of neural network learning dynamics.",
          "quote": "here we investigate how the structure of the initial weights \\\\u2014 in particular their effective rank \\\\u2014 influences the network learning regime"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Recurrent Neural Networks",
          "justification": "RNNs are used to validate the theoretical findings through numerical experiments on neuroscience tasks.",
          "quote": "We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known neuroscience tasks"
        },
        "aliases": [
          "RNNs"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Neurogym",
          "justification": "The paper uses Neurogym tasks like two-alternative forced choice, delayed-match-to-sample, and context-dependent decision-making to validate their theoretical findings in RNNs.",
          "quote": "For our investigations, we applied this initialization scheme across a variety of cognitive tasks \\\\u2014 including two-alternative forced choice (2AF), delayed-match-to-sample (DMS), context-dependent decision-making (CXT) tasks \\\\u2014 implemented with Neurogym"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Sequential MNIST",
          "justification": "sMNIST is employed to demonstrate the findings on a well-known machine learning benchmark task.",
          "quote": "and the well-known machine learning benchmark sequential MNIST (sMNIST). Figure 1 indicates that low-rank initial weights result in effectively richer learning and greater network changes."
        },
        "aliases": [
          "sMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper utilizes PyTorch for implementing and training the RNN models used in their experiments.",
          "quote": "We used PyTorch Version 1.10.2"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 757,
    "prompt_tokens": 24510,
    "total_tokens": 25267
  }
}