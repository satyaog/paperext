{
  "paper": "2306.11922.txt",
  "words": 10021,
  "extractions": {
    "title": {
      "value": "No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths",
      "justification": "The research paper mentions this title explicitly at the beginning and in the header on each page.",
      "quote": "No Wrong Turns: The Simple Geometry Of Neural Networks Optimization Paths"
    },
    "description": "The paper investigates the optimization dynamics of neural networks, focusing on geometric properties along optimization paths. It provides empirical analysis across various tasks and network architectures, suggesting that optimization paths exhibit stable dynamics, which theoretically guarantees linear convergence and effective learning rate schedules.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper provides empirical analysis and experimental results to understand the optimization dynamics of neural networks.",
      "quote": "We conduct our experiments on image classification, semantic segmentation and language modeling across different batch sizes, network architectures, datasets, optimizers, and initialization seeds. We discuss the impact of each factor."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization in Deep Learning",
        "justification": "The primary focus of the paper is on understanding the optimization dynamics and pathways of neural networks.",
        "quote": "Understanding the optimization dynamics of neural networks is necessary for closing the gap between theory and practice."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Network Training",
          "justification": "The paper delves into the training dynamics and pathways of neural networks during optimization.",
          "quote": "Stochastic first-order optimization algorithms are known to efficiently locate favorable minima in deep neural networks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Loss Landscape Analysis",
          "justification": "The paper analyzes and discusses the loss landscapes of neural networks and their impact on optimization.",
          "quote": "Our analysis reveals that these quantities exhibit predictable, consistent behavior throughout training, despite the stochasticity induced by sampling minibatches."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet18",
          "justification": "ResNet18 is explicitly mentioned as one of the models used for experiments in the study.",
          "quote": "ResNet18 (12M)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet18 is a well-established model and is not introduced as a new model in this paper",
          "quote": "ResNet18 (12M)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experiments conducted using ResNet18.",
          "quote": "CIFAR-10 (ResNet-18): Across the entire training run, not a single iteration exhibits a negative γ (or equivalently RSI)."
        },
        "is_compared": {
          "value": 1,
          "justification": "ResNet18's performance is compared to other variations and models within the experiments.",
          "quote": "ResNet18 (12M)\nResNet18 2w (46M)"
        },
        "referenced_paper_title": {
          "value": "Deep residual learning for image recognition",
          "justification": "The referenced foundational paper for the ResNet model.",
          "quote": "ResNet18"
        }
      },
      {
        "name": {
          "value": "Transformer",
          "justification": "Transformer is explicitly mentioned as one of the models used for experiments in the study.",
          "quote": "1e 2 WikiText-2 language modeling (transformer)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Transformer is a well-established model and is not introduced as a new model in this paper.",
          "quote": "1e 2 WikiText-2 language modeling (transformer)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experiments conducted using the Transformer model.",
          "quote": "WikiText-2 (Transformer): Throughout the training, γ remains strictly positive and exceeds 0.05 following the second epoch."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Transformer model's performance and optimization characteristics are compared to other models and settings in the experiments.",
          "quote": "1e 2 WikiText-2 language modeling (transformer)\nBatchsizes"
        },
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "The foundational paper for the Transformer model.",
          "quote": "Transformer"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is explicitly mentioned as one of the datasets used for experiments in the study.",
          "quote": "CIFAR-10 classification (ResNet18)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The foundational paper for the CIFAR-10 dataset.",
          "quote": "CIFAR-10 classification (ResNet18)"
        }
      },
      {
        "name": {
          "value": "ImageNet-1K",
          "justification": "ImageNet-1K is explicitly mentioned as one of the datasets used for experiments in the study.",
          "quote": "ImageNet classification"
        },
        "aliases": [
          "ImageNet"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "The foundational paper for the ImageNet dataset.",
          "quote": "ImageNet classification"
        }
      },
      {
        "name": {
          "value": "WikiText-2",
          "justification": "WikiText-2 is explicitly mentioned as one of the datasets used for experiments in the study.",
          "quote": "WikiText-2 language modeling (transformer)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Pointer sentinel mixture models",
          "justification": "The foundational paper for the WikiText-2 dataset.",
          "quote": "WikiText-2 language modeling (transformer)"
        }
      },
      {
        "name": {
          "value": "Vaihingen",
          "justification": "Vaihingen is explicitly mentioned as one of the datasets used for experiments in the study.",
          "quote": "Vaihingen semantic segmentation"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The ISPRS benchmark on urban object classification and 3D building reconstruction",
          "justification": "The foundational paper for the Vaihingen dataset.",
          "quote": "ImageNet classification"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly mentioned as the framework used for conducting the experiments.",
          "quote": "All experiments were coded in PyTorch"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The foundational paper for the PyTorch library.",
          "quote": "All experiments were coded in PyTorch [36]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1682,
    "prompt_tokens": 18996,
    "total_tokens": 20678
  }
}