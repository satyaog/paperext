{
  "paper": "xSsW2Am-ukZ.txt",
  "words": 15080,
  "extractions": {
    "title": {
      "value": "UNMASKING THE LOTTERY TICKET HYPOTHESIS: WHAT’S ENCODED IN A WINNING TICKET’S MASK?",
      "justification": "This is the exact title of the paper provided in the given text.",
      "quote": "UNMASKING THE LOTTERY TICKET HYPOTHESIS: WHAT’S ENCODED IN A WINNING TICKET’S MASK?"
    },
    "description": "This paper investigates the principles underlying iterative magnitude pruning (IMP), an algorithm used to find highly sparse and trainable subnetworks, through the lens of error landscape geometry. The authors aim to understand the mechanisms that allow IMP to identify 'winning tickets' and how these subnetworks can be trained to achieve the same accuracy as the full network.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts extensive empirical investigations on benchmark datasets and network architectures to understand how IMP works and why it succeeds or fails.",
      "quote": "We do so through extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet) and modern network architectures (ResNet-20, ResNet-18, and ResNet-50)."
    },
    "primary_research_field": {
      "name": {
        "value": "Model Pruning",
        "justification": "The primary focus of the paper is on understanding iterative magnitude pruning (IMP) and its effectiveness in identifying sparse subnetworks that can be trained to achieve the same accuracy as the full network.",
        "quote": "A state of the art algorithm for doing so is iterative magnitude pruning (IMP) (Frankle et al., 2020a)."
      },
      "aliases": [
        "Model Compression",
        "Neural Network Pruning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Network Training",
          "justification": "The research extensively analyzes how training procedures, such as gradient descent, interact with pruned networks and the robustness of these methods.",
          "quote": "Second, how does SGD starting from the masked rewind point extract and use this information?"
        },
        "aliases": [
          "SGD Training",
          "Network Training"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet-20",
          "justification": "ResNet-20 is one of the network architectures used for empirical investigations in this paper.",
          "quote": "We do so through extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet) and modern network architectures (ResNet-20, ResNet-18, and ResNet-50)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is utilized for the experiments but is not a novel contribution of this paper.",
          "quote": "ResNet-20 is one of the network architectures used for empirical investigations in this paper."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments described in the paper were executed using the ResNet-20 architecture.",
          "quote": "ResNet-20"
        },
        "is_compared": {
          "value": 1,
          "justification": "Numerical comparisons of model performance at various sparsity levels are a key part of the empirical investigation.",
          "quote": "We do so through extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet) and modern network architectures (ResNet-20, ResNet-18, and ResNet-50)."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "ResNet-20 is an architecture from the well-known 'Deep Residual Learning for Image Recognition' paper, commonly cited in studies involving ResNet variants.",
          "quote": "ResNet-20"
        }
      },
      {
        "name": {
          "value": "ResNet-18",
          "justification": "ResNet-18 is one of the network architectures used for empirical investigations in this paper.",
          "quote": "We do so through extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet) and modern network architectures (ResNet-20, ResNet-18, and ResNet-50)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is utilized for the experiments but is not a novel contribution of this paper.",
          "quote": "ResNet-18 is one of the network architectures used for empirical investigations in this paper."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments described in the paper were executed using the ResNet-18 architecture.",
          "quote": "ResNet-18"
        },
        "is_compared": {
          "value": 1,
          "justification": "Numerical comparisons of model performance at various sparsity levels are a key part of the empirical investigation.",
          "quote": "We do so through extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet) and modern network architectures (ResNet-20, ResNet-18, and ResNet-50)."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "ResNet-18 is an architecture from the well-known 'Deep Residual Learning for Image Recognition' paper, commonly cited in studies involving ResNet variants.",
          "quote": "ResNet-18"
        }
      },
      {
        "name": {
          "value": "ResNet-50",
          "justification": "ResNet-50 is one of the network architectures used for empirical investigations in this paper.",
          "quote": "We do so through extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet) and modern network architectures (ResNet-20, ResNet-18, and ResNet-50)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is utilized for the experiments but is not a novel contribution of this paper.",
          "quote": "ResNet-50 is one of the network architectures used for empirical investigations in this paper."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments described in the paper were executed using the ResNet-50 architecture.",
          "quote": "ResNet-50"
        },
        "is_compared": {
          "value": 1,
          "justification": "Numerical comparisons of model performance at various sparsity levels are a key part of the empirical investigation.",
          "quote": "We do so through extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet) and modern network architectures (ResNet-20, ResNet-18, and ResNet-50)."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "ResNet-50 is an architecture from the well-known 'Deep Residual Learning for Image Recognition' paper, commonly cited in studies involving ResNet variants.",
          "quote": "ResNet-50"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is one of the benchmark datasets used in the empirical investigations to understand the performance of IMP.",
          "quote": "extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-10 is a widely recognized dataset introduced in the paper 'Learning Multiple Layers of Features from Tiny Images' by Alex Krizhevsky.",
          "quote": "CIFAR-10"
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is one of the benchmark datasets used in the empirical investigations to understand the performance of IMP.",
          "quote": "extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-100 is a widely recognized dataset introduced in the paper 'Learning Multiple Layers of Features from Tiny Images' by Alex Krizhevsky.",
          "quote": "CIFAR-100"
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is one of the benchmark datasets used in the empirical investigations to understand the performance of IMP.",
          "quote": "extensive empirical investigations on a range of benchmark datasets (CIFAR-10, CIFAR-100, and ImageNet)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "ImageNet is a widely recognized dataset introduced in the paper 'ImageNet: A Large-Scale Hierarchical Image Database' by Deng et al.",
          "quote": "ImageNet"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2151,
    "prompt_tokens": 23549,
    "total_tokens": 25700
  }
}