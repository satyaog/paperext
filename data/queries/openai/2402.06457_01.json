{
  "paper": "2402.06457.txt",
  "words": 8032,
  "extractions": {
    "title": {
      "value": "V-STaR: Training Verifiers for Self-Taught Reasoners",
      "justification": "This is the official title of the research paper.",
      "quote": "V-STaR: Training Verifiers for Self-Taught Reasoners"
    },
    "description": "This paper introduces V-STaR, an approach designed to train verifiers by utilizing both correct and incorrect solutions generated during the self-improvement process of Large Language Models (LLMs). V-STaR contrasts itself with existing methods by leveraging Direct Preference Optimization to improve verifier and generator performance iteratively. The empirical results exhibit significant enhancements in test accuracy over prior methods in code generation and math reasoning benchmarks.",
    "type": {
      "value": "empirical study",
      "justification": "The paper involves empirical evaluations of V-STaR against baseline methods in various tasks, primarily focused on experimental results.",
      "quote": "We empirically evaluate V-STaR on math reasoning... and on code-generation..."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on methods to improve reasoning abilities and verification processes in Large Language Models, which are a subfield of Deep Learning.",
        "quote": "To improve the reasoning performance of LLMs..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Training and Optimization",
          "justification": "The core contribution of the paper is improving the training process of verifiers and generators using techniques like Direct Preference Optimization.",
          "quote": "To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "LLaMA2",
          "justification": "The paper uses LLaMA2 as one of the baseline models for comparison and experimentation.",
          "quote": "train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running VSTaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CodeLLaMA",
          "justification": "CodeLLaMA is another model used in the experiments for code generation benchmarks.",
          "quote": "Fine-tuning LLaMA2 (Touvron et al., 2023) and CodeLLaMA (Rozière et al., 2023), we compare V-STaR to other self-improvement and verification-based methods, as well a non-iterative V-STaR baseline that uses the same number of generation samples to bootstrap a generator and verifier."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GSM8K",
          "justification": "The GSM8K dataset is used for evaluating the models on math reasoning tasks.",
          "quote": "We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021)..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MATH",
          "justification": "The MATH dataset is used as another benchmark to evaluate the transfer generalization performance of the models for math reasoning.",
          "quote": "We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021) and a subset of MATH (Hendrycks et al., 2021)..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MBPP",
          "justification": "The MBPP dataset is used for training and evaluating models on code generation tasks.",
          "quote": "...and on code-generation using MBPP (Austin et al., 2021)..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "HumanEval",
          "justification": "HumanEval is another dataset used for evaluating code generation tasks.",
          "quote": "We also evaluate the transfer generalization performance of V-STaR using Hendrycks’ MATH (Hendrycks et al., 2021) HumanEval (Chen et al., 2021)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "LoRA",
          "justification": "The paper mentions the use of LoRA adapters in their training setup.",
          "quote": "We run our experiments by training LLaMA2 (Touvron et al., 2023) and CodeLLaMA (Rozière et al., 2023) 7B and 13B models using LoRA adapters (Hu et al., 2022)."
        },
        "aliases": [
          "Low-Rank Adaptation"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1132,
    "prompt_tokens": 13990,
    "total_tokens": 15122
  }
}