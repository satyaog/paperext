{
  "paper": "2301.10119.txt",
  "words": 12753,
  "extractions": {
    "title": {
      "value": "MINIMAL VALUE-EQUIVALENT PARTIAL MODELS FOR SCALABLE AND ROBUST PLANNING IN LIFELONG REINFORCEMENT LEARNING",
      "justification": "It's the title of the paper as given in the document.",
      "quote": "MINIMAL VALUE-EQUIVALENT PARTIAL MODELS FOR SCALABLE AND ROBUST PLANNING IN LIFELONG REINFORCEMENT LEARNING"
    },
    "description": "This paper discusses the challenges of applying classical model-based reinforcement learning techniques to lifelong reinforcement learning scenarios. It introduces and provides theoretical support for minimal value-equivalent partial models that only focus on relevant aspects of the environment to achieve scalable and robust planning. The empirical results confirm the theoretical advantages of these models, including their scalability and robustness to distribution shifts and compounding model errors.",
    "type": {
      "value": "theoretical",
      "justification": "The paper offers a new theoretical framework and provides theoretical results demonstrating the advantages of minimal value-equivalent partial models. Experimental results are used to validate the theoretical findings.",
      "quote": "After providing a formal definition for these models, we provide theoretical results demonstrating the scalability advantages of performing planning with such models and then perform experiments to empirically illustrate our theoretical results."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper primarily discusses concepts and methods related to reinforcement learning, including topics like model-based reinforcement learning and lifelong learning agents.",
        "quote": "Learning models of the environment from pure interaction is often considered an essential component of building lifelong reinforcement learning agents."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model-based Reinforcement Learning",
          "justification": "The study focuses on developing new models tailored for model-based reinforcement learning, specifically in lifelong learning scenarios.",
          "quote": "the common practice in model-based reinforcement learning is to learn models that model every aspect of the agent’s environment"
        },
        "aliases": [
          "MBRL"
        ]
      },
      {
        "name": {
          "value": "Lifelong Learning",
          "justification": "The paper aims to address challenges specific to lifelong learning scenarios and proposes methods to improve scalability and robustness in these contexts.",
          "quote": "one important idea to reconsider is whether if the agent’s model should model every aspect of its environment. In classical model-based RL, the learned model is a model over every aspect of the environment. However, due to the large state spaces of LRL environments, these types of models are likely to lead to serious problems in performing scalable model-based RL"
        },
        "aliases": [
          "LL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Minimal Value-Equivalent Partial Models (mVEPs)",
          "justification": "The paper proposes minimal value-equivalent partial models as a way to handle scalability and robustness in lifelong reinforcement learning.",
          "quote": "we propose new kinds of models that only model the relevant aspects of the environment, which we call minimal value-equivalent partial models"
        },
        "aliases": [
          "mVEPs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The primary contribution of the paper is the introduction and theoretical validation of minimal value-equivalent partial models.",
          "quote": "In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call minimal value-equivalent partial models."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results that validate the theoretical claims, indicating that the proposed models were implemented and tested.",
          "quote": "we provide some useful heuristics on how to learn these kinds of models with deep learning architectures and empirically demonstrate that models learned in such a way can allow for performing planning that is robust to distribution shifts and compounding model errors."
        },
        "is_compared": {
          "value": 1,
          "justification": "The empirical section of the paper compares the performance of minimal value-equivalent partial models against other types of models to illustrate their advantages.",
          "quote": "We start this section by performing experiments to demonstrate the scalability advantages of minimal VE partial models, which are illustrations of the theoretical results derived"
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "The paper does not indicate that these models were derived from another reference paper; instead, they are introduced in this work.",
          "quote": "In this paper, we argue that such models are not particularly well-suited for performing scalable and robust planning in lifelong reinforcement learning scenarios and we propose new kinds of models that only model the relevant aspects of the environment, which we call minimal value-equivalent partial models."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Squirrel’s World Environment (SW)",
          "justification": "The paper uses the Squirrel’s World environment for empirical validation of the proposed models.",
          "quote": "As an illustration of the models defined above, let us start by considering the Squirrel’s World (SW) environment"
        },
        "aliases": [
          "SW"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "None",
          "justification": "The dataset is part of the empirical setup introduced in this paper and not attributed to another reference.",
          "quote": "As an illustration of the models defined above, let us start by considering the Squirrel’s World (SW) environment"
        }
      },
      {
        "name": {
          "value": "Two Rooms Dynamic Obstacles Environment (2RDO)",
          "justification": "The Two Rooms Dynamic Obstacles Environment is also used for empirical tests.",
          "quote": "We perform experiments (i) on the SW environment (see Fig. 1), (ii) on different versions of the Two Rooms Dynamic Obstacles (2RDO) environment that are built on top of Minigrid (Chevalier-Boisvert et al., 2018) (see Fig. 2 & C.1)"
        },
        "aliases": [
          "2RDO"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Minimalistic Gridworld Environment for OpenAI Gym",
          "justification": "The reference paper for the 2RDO environment within the context of Minigrid is cited.",
          "quote": "We perform experiments (i) on the SW environment (see Fig. 1), (ii) on different versions of the Two Rooms Dynamic Obstacles (2RDO) environment that are built on top of Minigrid (Chevalier-Boisvert et al., 2018) (see Fig. 2 & C.1)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions the use of neural network feature extractors and deep learning architectures, implying the use of PyTorch for implementation.",
          "quote": "useful heuristics on how to learn these kinds of models with deep learning architectures"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "None",
          "justification": "The paper does not reference a specific PyTorch paper, but the library's use is implied through the discussion of neural network implementations.",
          "quote": "useful heuristics on how to learn these kinds of models with deep learning architectures"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1400,
    "prompt_tokens": 22384,
    "total_tokens": 23784
  }
}