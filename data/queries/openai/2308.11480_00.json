{
  "paper": "2308.11480.txt",
  "words": 10103,
  "extractions": {
    "title": {
      "value": "Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection",
      "justification": "The title clearly specifies that the focus of the paper is on broad out-of-distribution detection.",
      "quote": "Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection"
    },
    "description": "This paper addresses the challenge of detecting out-of-distribution (OOD) inputs in machine learning systems. It proposes a comprehensive benchmark called BROAD (Benchmarking Resilience Over Anomaly Diversity) to evaluate the performance of OOD detection methods across five distinct types of distribution shifts. The study reveals that existing methods often fail to perform reliably on a variety of OOD inputs and presents an ensemble approach using Gaussian mixtures for more consistent and comprehensive OOD detection.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper predominantly focuses on evaluating various existing OOD detection methods empirically across different benchmarks and introduces an ensemble approach for enhanced performance.",
      "quote": "In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them."
    },
    "primary_research_field": {
      "name": {
        "value": "Out-Of-Distribution Detection",
        "justification": "The primary focus of the paper is out-of-distribution detection, as it discusses various types of OOD inputs and evaluates existing methods for detecting them.",
        "quote": "Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts."
      },
      "aliases": [
        "OOD Detection"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Adversarial Detection",
          "justification": "The paper evaluates detection methods on adversarial perturbations, which places it within the adversarial detection subfield.",
          "quote": "Adversarial perturbations are examined using two well-established attack methods: Projected Gradient Descent (PGD)[55] and AutoAttack[12]."
        },
        "aliases": [
          "Adversarial Perturbation Detection"
        ]
      },
      {
        "name": {
          "value": "Synthetic Image Detection",
          "justification": "The paper includes experiments on detecting synthetic images generated by algorithms like BigGAN and Stable Diffusion.",
          "quote": "This category of distribution shift encompasses images generated by computer algorithms. To emulate this shift, we curated two datasets: one derived from a conditional BigGAN model [7], and another inspired by stable diffusion techniques [68]."
        },
        "aliases": [
          "Synthetic Image Generation Detection"
        ]
      },
      {
        "name": {
          "value": "Corruption Robustness",
          "justification": "The paper evaluates detection methods on corrupted images, which places it within the corruption robustness subfield.",
          "quote": "The term corruptions refers to images that have undergone a range of perceptual perturbations. To simulate this type of distribution shift, we employed four distinct corruptions from ImageNet-C [26]: defocus blur, Gaussian noise, snow, and brightness."
        },
        "aliases": [
          "Corruption Detection"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ViM",
          "justification": "ViM is one of the models evaluated in the paper.",
          "quote": "While this approach still relies on heuristic scores, it presents an ensemble method that is able to amalgamate their respective information, while maintaining the dimension of its underlying variables at a significantly low level."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The ViM model was used for evaluation but was not contributed by the paper.",
          "quote": "In the case of CAD ET, we solely utilize the intra-similarity score min with five transformations to minimize computational demands. For DOCTOR, we employ Dα in the Totally Black Box (TBB) setting, disregarding Dβ as it is functionally equivalent to MSP in the TBB setting when rescaling the detection threshold is accounted for (resulting in identical AUC scores)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The models were executed on GPUs as per the experimental setup mentioned in the paper.",
          "quote": "All evaluations are conducted on a single A100 GPU, with the inference time normalized by the cost of a forward pass (cf. App. B)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The ViM model is compared numerically with other models in the paper through various benchmarks.",
          "quote": "We observe that while detection performances are generally superior when utilizing a ViT backbone, a finding consistent with previous studies [80], the difference is method-dependent. For instance, MDSl ranks as the best baseline on ViT (when averaged over distribution shift types), but it is the third-worst with a ResNet-50."
        },
        "referenced_paper_title": {
          "value": "Vim: Out-of-distribution with virtual-logit matching",
          "justification": "The referenced paper provides the foundational basis for the ViM model used in this study.",
          "quote": "ViM: Out-of-distribution with virtual-logit matching. In Conference on Computer Vision and Pattern Recognition, 2022."
        }
      },
      {
        "name": {
          "value": "MDS",
          "justification": "MDS is another model evaluated in the paper.",
          "quote": "MDS: Mahalanobis Distance is one of the methods evaluated, and the paper discusses its performance extensively."
        },
        "aliases": [
          "Mahalanobis Distance"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The MDS model was used for evaluation but was not contributed by the paper.",
          "quote": "In its standard form, the Mahalanobis detection method computes the layer-wise Mahalanobis distance, followed by training a logistic regressor on OOD samples to facilitate detection based on a weighted average of these distances."
        },
        "is_executed": {
          "value": 1,
          "justification": "The models were executed on GPUs as per the experimental setup mentioned in the paper.",
          "quote": "All evaluations are conducted on a single A100 GPU, with the inference time normalized by the cost of a forward pass (cf. App. B)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The MDS model is compared numerically with other models in the paper through various benchmarks.",
          "quote": "For the Vision Transformer (ViT), we focus on MDS on the class token, disregarding patch tokens."
        },
        "referenced_paper_title": {
          "value": "A simple unified framework for detecting out-of-distribution samples and adversarial attacks",
          "justification": "The referenced paper provides the foundational basis for the MDS model used in this study.",
          "quote": "K. Lee, K. Lee, H. Lee, and J. Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, 2018."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "BROAD",
          "justification": "BROAD is the benchmark dataset introduced and contributed by the paper.",
          "quote": "We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity)."
        },
        "aliases": [
          "Benchmarking Resilience Over Anomaly Diversity"
        ],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection",
          "justification": "The BROAD dataset is introduced and used within this paper.",
          "quote": "We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity)."
        }
      },
      {
        "name": {
          "value": "ImageNet-1K",
          "justification": "ImageNet-1K serves as the in-distribution reference for the BROAD benchmark.",
          "quote": "In this study, we employ ImageNet-1K [15] as our in-distribution."
        },
        "aliases": [
          "ImageNet",
          "ImageNet1K"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "The ImageNet-1K dataset is well-known and widely used, referenced for its role in this paper.",
          "quote": "In this study, we employ ImageNet-1K [15] as our in-distribution."
        }
      },
      {
        "name": {
          "value": "iNaturalist",
          "justification": "iNaturalist is one of the datasets used for evaluating novel class distribution shifts.",
          "quote": "For this particular setting, we employ three widely used benchmarks: iNaturalist [33, 76], ImageNet-O [30], and OpenImage-O [80, 41]."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "The inaturalist species classification and detection dataset",
          "justification": "The paper references iNaturalist as one of its benchmark datasets.",
          "quote": "For this particular setting, we employ three widely used benchmarks: iNaturalist [33, 76], ImageNet-O [30], and OpenImage-O [80, 41]."
        }
      },
      {
        "name": {
          "value": "OpenImage-O",
          "justification": "OpenImage-O is another dataset used for evaluating novel class distribution shifts.",
          "quote": "For this particular setting, we employ three widely used benchmarks: iNaturalist [33, 76], ImageNet-O [30], and OpenImage-O [80, 41]."
        },
        "aliases": [
          "OpenImages-O"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Openimages: A public dataset for large-scale multi-label and multi-class image classification",
          "justification": "The paper references OpenImage-O as one of its benchmark datasets.",
          "quote": "For this particular setting, we employ three widely used benchmarks: iNaturalist [33, 76], ImageNet-O [30], and OpenImage-O [80, 41]."
        }
      },
      {
        "name": {
          "value": "ImageNet-O",
          "justification": "ImageNet-O is another dataset used for evaluating novel class distribution shifts.",
          "quote": "For this particular setting, we employ three widely used benchmarks: iNaturalist [33, 76], ImageNet-O [30], and OpenImage-O [80, 41]."
        },
        "aliases": [
          "ImageNet Outliers"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Natural adversarial examples",
          "justification": "The paper references ImageNet-O as one of its benchmark datasets.",
          "quote": "For this particular setting, we employ three widely used benchmarks: iNaturalist [33, 76], ImageNet-O [30], and OpenImage-O [80, 41]."
        }
      },
      {
        "name": {
          "value": "PGD",
          "justification": "Projected Gradient Descent (PGD) is used for evaluating adversarial perturbations.",
          "quote": "Adversarial perturbations are examined using two well-established attack methods: Projected Gradient Descent (PGD)[55] and AutoAttack[12]."
        },
        "aliases": [
          "Projected Gradient Descent"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Towards deep learning models resistant to adversarial attacks",
          "justification": "The paper references PGD as one of its benchmark methods for adversarial attacks.",
          "quote": "Adversarial perturbations are examined using two well-established attack methods: Projected Gradient Descent (PGD)[55] and AutoAttack[12]."
        }
      },
      {
        "name": {
          "value": "AutoAttack",
          "justification": "AutoAttack is another method used for evaluating adversarial perturbations.",
          "quote": "Adversarial perturbations are examined using two well-established attack methods: Projected Gradient Descent (PGD)[55] and AutoAttack[12]."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
          "justification": "The paper references AutoAttack as one of its benchmark methods for adversarial attacks.",
          "quote": "Adversarial perturbations are examined using two well-established attack methods: Projected Gradient Descent (PGD)[55] and AutoAttack[12]."
        }
      },
      {
        "name": {
          "value": "COCO",
          "justification": "The CoCo dataset is referenced in the context of constructing the CoComageNet subset used in the BROAD benchmark.",
          "quote": "The CoComageNet benchmark is constructed as a subset of the CoCo dataset [48], specifically, the 2017 training images."
        },
        "aliases": [
          "Common Objects in Context",
          "MS COCO"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Microsoft COCO: Common objects in context",
          "justification": "The paper constructs CoComageNet as a subset of the COCO dataset.",
          "quote": "The CoComageNet benchmark is constructed as a subset of the CoCo dataset [48], specifically, the 2017 training images."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "OpenOOD",
          "justification": "The paper uses OpenOOD for reproducing its experiments and providing its code repository.",
          "quote": "The code to get BROAD is available at https://github.com/ServiceNow/broad. Code to reproduce our experiments is available as an OpenOOD [83] fork at https://github.com/ServiceNow/broad-openood."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "OpenOOD: Benchmarking Generalized Out-Of-Distribution Detection",
          "justification": "The paper mentions the usage of OpenOOD for reproducing experiments and providing the code.",
          "quote": "Code to reproduce our experiments is available as an OpenOOD [83] fork at https://github.com/ServiceNow/broad-openood."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2629,
    "prompt_tokens": 22605,
    "total_tokens": 25234
  }
}