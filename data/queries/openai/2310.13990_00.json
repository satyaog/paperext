{
  "paper": "2310.13990.txt",
  "words": 12931,
  "extractions": {
    "title": {
      "value": "A Novel Information-Theoretic Objective to Disentangle Representations for Fair Classification",
      "justification": "This information is clearly presented at the beginning of the paper and in the header.",
      "quote": "A Novel Information-Theoretic Objective to Disentangle Representations for Fair Classification"
    },
    "description": "This paper presents a novel method, called CLINIC, for learning disentangled representations in the context of fair classification using an information-theoretic approach. The authors propose new losses to minimize mutual information between latent representations and sensitive attributes conditional to the target, without requiring additional parameters. The efficacy of CLINIC is validated through extensive experiments on several datasets, showing superior performance to existing methods.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes extensive numerical experiments, training over 2,000 neural networks to validate the proposed method CLINIC.",
      "quote": "The resulting set of losses, called CLINIC, is parameter free and thus, it is easier and faster to train. CLINIC losses are studied through extensive numerical experiments by training over 2k neural networks."
    },
    "primary_research_field": {
      "name": {
        "value": "Fair Classification",
        "justification": "The paper focuses on learning disentangled representations with respect to a sensitive attribute for the purpose of fair classification.",
        "quote": "Moreover, it is particularly well-suited for fairness applications, such as fair classification, which are nowadays increasingly sought after."
      },
      "aliases": [
        "Fairness"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper focuses on disentangled representations within the context of natural language processing tasks.",
          "quote": "Learning disentangled representations with respect to a sensitive attribute is challenging and previous works in the Natural Language Processing (NLP) community were based on two types of approach."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper addresses the challenge of learning abstract and disentangled representations from high-dimensional data.",
          "quote": "Learning disentangled representations from high dimensional data ultimately aims at separating a few explanatory factors (Bengio et al., 2013) that contain meaningful information on the objects of interest, regardless of specific variations or contexts."
        },
        "aliases": [
          "Disentangled Representations",
          "Representation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLINIC",
          "justification": "The main contribution of the paper is the CLINIC method for learning disentangled representations.",
          "quote": "Our analysis motivates the introduction of new losses tailored for classification, called CLINIC (Conditional mutuaL InformatioN mInimization for fair ClassifIcAtioN)."
        },
        "aliases": [
          "Conditional mutuaL InformatioN mInimization for fair ClassifIcAtioN"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "CLINIC is introduced and thoroughly evaluated in the paper.",
          "quote": "Our contributions: We introduce a new method to learn disentangled representations, with a particular focus on fair representations in the context of NLP."
        },
        "is_executed": {
          "value": 1,
          "justification": "The CLINIC losses were validated through extensive numerical experiments which involved training over 2,000 neural models.",
          "quote": "CLINIC losses are studied through extensive numerical experiments by training over 2k neural networks."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of CLINIC with various baseline methods including adversarial training and mutual information upper bounds.",
          "quote": "Our results show that the CLINIC’s objective is better suited than existing methods, it is faster to train and requires less tuning as it does not have learnable parameters."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "CLINIC is a new method introduced in this paper, so there is no prior referenced paper on it.",
          "quote": "Our contributions: We introduce a new method to learn disentangled representations, with a particular focus on fair representations in the context of NLP."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "DIAL",
          "justification": "The DIAL dataset is used in the experiments to validate the proposed method.",
          "quote": "We use the DIAL dataset (Blodgett et al., 2016) to ensure backward comparison with previous works (Colombo et al., 2021c; Xie et al., 2017; Barrett et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Demographic Dialectal Variation in Social Media: A Case Study of African-American English",
          "justification": "The dataset's reference is provided in the context of its description.",
          "quote": "We use the DIAL dataset (Blodgett et al., 2016) to ensure backward comparison with previous works (Colombo et al., 2021c; Xie et al., 2017; Barrett et al., 2019)."
        }
      },
      {
        "name": {
          "value": "TrustPilot (TRUST)",
          "justification": "The TrustPilot (TRUST) dataset is used in the experiments to validate the proposed method.",
          "quote": "We additionally report results on TrustPilot (TRUST) (Hovy et al., 2015) that has also been used in Coavoux et al. (2018)."
        },
        "aliases": [
          "TRUST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "User Review Sites as a Resource for Large-Scale Sociolinguistic Studies",
          "justification": "The dataset's reference is provided in the context of its description.",
          "quote": "We additionally report results on TrustPilot (TRUST) (Hovy et al., 2015) that has also been used in Coavoux et al. (2018)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The implementation details mention the use of the PyTorch library.",
          "quote": "All the models have been implemented in Pytorch (Paszke et al., 2017)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "This is the standard reference for the PyTorch library.",
          "quote": "All the models have been implemented in Pytorch (Paszke et al., 2017)."
        }
      },
      {
        "name": {
          "value": "Transformers",
          "justification": "The implementation of tokenizers and pre-trained models relies on the Transformers library.",
          "quote": "The tokenizers, the PT have been taken from transformers (Wolf et al.)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Huggingface’s Transformers: State-of-the-Art Natural Language Processing",
          "justification": "This is the standard reference for the Transformers library.",
          "quote": "The tokenizers, the PT have been taken from transformers (Wolf et al.)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1369,
    "prompt_tokens": 25300,
    "total_tokens": 26669
  }
}