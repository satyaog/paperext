{
  "paper": "2206.08005.txt",
  "words": 16180,
  "extractions": {
    "title": {
      "value": "Evaluating Self-Supervised Learning for Molecular Graph Embeddings",
      "justification": "Extracted from the paper title.",
      "quote": "Evaluating Self-Supervised Learning for Molecular Graph Embeddings"
    },
    "description": "This paper evaluates various self-supervised learning (SSL) methods for generating molecular graph embeddings, specifically using a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. These embeddings are assessed for their transferability and generalizability across multiple downstream molecular property prediction (MPP) tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts an empirical evaluation of different SSL methods on molecular graph embeddings using a suite of probing tasks and multiple downstream datasets. The nature of the study involves experimental analysis and performance benchmarking.",
      "quote": "... we present 'Molecular Graph Representation Evaluation' (M OL G RAPH E VAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes... Our goal is to unbiasedly evaluate molecular graph embeddings obtained by GSSL methods on existing downstream tasks and a new suite of probe tasks ... we summarised some key findings based on a total of 90,918 probe models and 1,875 pre-trained GNNs."
    },
    "primary_research_field": {
      "name": {
        "value": "Graph Representation Learning",
        "justification": "The primary focus of the paper is on evaluating self-supervised learning methods for obtaining embeddings of molecular graphs, which falls under Graph Representation Learning.",
        "quote": "Graph Self-Supervised Learning (GSSL) paves the way for learning molecular graph embeddings without human annotations that are transferable to various downstream datasets..."
      },
      "aliases": [
        "GSSL",
        "Molecular Graph Embeddings"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper evaluates various graph self-supervised learning methods for generating molecular embeddings.",
          "quote": "... we present 'Molecular Graph Representation Evaluation' (M OL G RAPH E VAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes..."
        },
        "aliases": [
          "GSSL",
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Molecular Property Prediction",
          "justification": "The paper focuses on predicting molecular properties using the learned graph embeddings, which is a key application of these embeddings.",
          "quote": "For instance, a molecular property prediction (MPP) model can expedite and economise the design process by reducing the need for synthesising and measuring molecules."
        },
        "aliases": [
          "MPP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Graph Isomorphism Network (GIN)",
          "justification": "The GIN model is used as the backbone for learning molecular graph embeddings.",
          "quote": "Following previous GSSL methods on molecular graphs, we adopt the Graph Isomorphism Network (GIN) [42] as the backbone model and incorporate edge features during message passing following [11]."
        },
        "aliases": [
          "GIN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The GIN model is not newly contributed by this paper but is employed as a backbone model.",
          "quote": "Following previous GSSL methods on molecular graphs, we adopt the Graph Isomorphism Network (GIN) [42] as the backbone model and incorporate edge features during message passing following [11]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The GIN model is executed in the experiments conducted in this paper.",
          "quote": "Following previous GSSL methods on molecular graphs, we adopt the Graph Isomorphism Network (GIN) [42] as the backbone model and incorporate edge features during message passing following [11]."
        },
        "is_compared": {
          "value": 1,
          "justification": "The GIN model is numerically compared across different self-supervised learning methods and downstream tasks.",
          "quote": "We use all qualified molecules (around 0.33 million, i.e., leave out the molecules that appeared in downstream datasets) from the GEOM dataset [46] to pre-train the GIN backbone."
        },
        "referenced_paper_title": {
          "value": "How Powerful Are Graph Neural Networks?",
          "justification": "The referenced paper provides the original introduction and details of the GIN model.",
          "quote": "Following previous GSSL methods on molecular graphs, we adopt the Graph Isomorphism Network (GIN) [42] as the backbone model and incorporate edge features during message passing following [11]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GEOM",
          "justification": "The GEOM dataset is used to pre-train the GIN backbone model.",
          "quote": "We use all qualified molecules (around 0.33 million, i.e., leave out the molecules that appeared in downstream datasets) from the GEOM dataset [46] to pre-train the GIN backbone."
        },
        "aliases": [
          "GEOM"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Geometric Scattering for Graphs",
          "justification": "The referenced paper provides details of the GEOM dataset.",
          "quote": "We use all qualified molecules (around 0.33 million, i.e., leave out the molecules that appeared in downstream datasets) from the GEOM dataset [46] to pre-train the GIN backbone."
        }
      },
      {
        "name": {
          "value": "BBBP",
          "justification": "The BBBP dataset is one of the downstream datasets used for evaluating the pre-trained embeddings.",
          "quote": "Table 1: Evaluating GSSL methods on molecular property prediction tasks. For each downstream dataset, we report the mean and standard deviation of the ROC-AUC scores over three random scaffold splits... BBBP 2,039"
        },
        "aliases": [
          "BBBP"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MoleculeNet: A Benchmark for Molecular Machine Learning",
          "justification": "The referenced paper provides details of the BBBP dataset.",
          "quote": "Table 1: Evaluating GSSL methods on molecular property prediction tasks. For each downstream dataset, we report the mean and standard deviation of the ROC-AUC scores over three random scaffold splits... BBBP 2,039"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch Geometric (PyG)",
          "justification": "The paper uses PyTorch Geometric as part of its implementation for GNN models.",
          "quote": "Since all the weights (Edge embedding layers, GNN layers, and Linear layers) in the GINs are extracted from some uniform distribution of some positive ranges. As the GIN layer essentially consists of multiplications and additions, the expected statistics of the node embeddings from randomised GINs are proportional to the number of connected neighbours (i.e., node degrees)."
        },
        "aliases": [
          "PyG"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch Geometric: Learning and Interpreting Graph Neural Networks for NLP",
          "justification": "This paper uses PyTorch Geometric for implementation of GNN architectures.",
          "quote": "Since all the weights (Edge embedding layers, GNN layers, and Linear layers) in the GINs are extracted from some uniform distribution of some positive ranges. As the GIN layer essentially consists of multiplications and additions, the expected statistics of the node embeddings from randomised GINs are proportional to the number of connected neighbours (i.e., node degrees)."
        }
      },
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper uses PyTorch for implementing neural network models and training.",
          "quote": "The GNN Layers in fact only have MLP weights (see PyG Doc), same initialisation as Linear layers. Linear Layers samples from uniform distribution for both weight and bias (PyTorch Doc)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the reference paper for the PyTorch library used in the research.",
          "quote": "The GNN Layers in fact only have MLP weights (see PyG Doc), same initialisation as Linear layers. Linear Layers samples from uniform distribution for both weight and bias (PyTorch Doc)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1607,
    "prompt_tokens": 33732,
    "total_tokens": 35339
  }
}