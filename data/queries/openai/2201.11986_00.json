{
  "paper": "2201.11986.txt",
  "words": 11053,
  "extractions": {
    "title": {
      "value": "Gradient Masked Averaging for Federated Learning",
      "justification": "The title is the one that appears at the beginning of the paper.",
      "quote": "Gradient Masked Averaging for Federated Learning"
    },
    "description": "This paper proposes a gradient masked averaging approach to improve generalization in federated learning by focusing on learning invariant mechanisms across clients. The approach aims to address the information loss and poor generalization caused by standard averaging methods in heterogeneous data settings. The proposed method can be integrated into existing federated learning algorithms as a drop-in replacement. The empirical results show consistent performance improvements across various datasets and federated learning algorithms.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper primarily focuses on empirical evaluations and experiments to demonstrate the effectiveness of the proposed gradient masked averaging technique in federated learning settings.",
      "quote": "We perform extensive experiments on multiple FL algorithms with in-distribution, real-world, feature-skewed out-of-distribution, and quantity imbalanced datasets and show that it provides consistent improvements."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The paper explicitly discusses federated learning, its challenges, and proposes a solution to improve it.",
        "quote": "Federated learning (FL) is an emerging paradigm that permits a large number of clients with heterogeneous data to coordinate learning of a unified global model without the need to share data amongst each other."
      },
      "aliases": [
        "FL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Out-of-Distribution Generalization",
          "justification": "The paper explores concepts from out-of-distribution generalization to tackle data heterogeneity in federated learning.",
          "quote": "Inspired from recent works in Out-of-Distribution generalization, we propose a gradient masked averaging approach for FL as an alternative to the standard averaging of client updates."
        },
        "aliases": [
          "OOD Generalization"
        ]
      },
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper discusses optimization techniques and their impact on federated learning.",
          "quote": "Many existing FL works attempt to tackle this problem through the lens of optimization, proposing constrained gradient optimization based approaches."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Gradient Masked Averaging (GMA)",
          "justification": "The model proposed in this paper is called Gradient Masked Averaging and is discussed extensively as the main contribution.",
          "quote": "Inspired from recent works in Out-of-Distribution generalization, we propose a gradient masked averaging approach for FL as an alternative to the standard averaging of client updates."
        },
        "aliases": [
          "GMA"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "This GMA model is the primary contribution of the paper.",
          "quote": "we propose a new aggregation method called gradient masked averaging (GMA) with the goal of improving generalization across clients and of the global model."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was thoroughly evaluated in various experiments, indicating its execution.",
          "quote": "We perform extensive experiments on multiple FL algorithms with in-distribution, real-world, feature-skewed out-of-distribution, and quantity imbalanced datasets and show that it provides consistent improvements."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of the proposed GMA model with standard federated learning models.",
          "quote": "Our performance gains are particularly large in cases where the clients are non-iid."
        },
        "referenced_paper_title": {
          "value": "Invariant Risk Minimization Games",
          "justification": "The paper references several works to build upon the idea of gradient masking, especially those from the domain of Out-of-Distribution generalization.",
          "quote": "Parascandolo et al. (2020) proposed a simple approach to improve generalization in the Out-of-Distribution (OOD) setting."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is used as one of the datasets for the experiments in this paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Y. LeCun, C. Cortes, and C.J. Burges, \"MNIST handwritten digit database\"",
          "justification": "MNIST is a well-known dataset frequently used in machine learning and cited in various papers.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        }
      },
      {
        "name": {
          "value": "FMNIST",
          "justification": "Fashion MNIST is used as one of the datasets for the experiments in this paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        },
        "aliases": [
          "Fashion MNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,” arXiv:1708.07747",
          "justification": "Fashion MNIST, referenced as FMNIST, is a known dataset for benchmarking machine learning models and is cited in several publications.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used as one of the datasets for the experiments in this paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A. Krizhevsky and G. Hinton, “Learning Multiple Layers of Features from Tiny Images,” 2009",
          "justification": "CIFAR-10 is a widely used dataset for evaluating machine learning models and is cited in numerous papers.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is used as one of the datasets for the experiments in this paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A. Krizhevsky and G. Hinton, “Learning Multiple Layers of Features from Tiny Images,” 2009",
          "justification": "CIFAR-100 is a widely used dataset for evaluating machine learning models and is cited in numerous papers.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        }
      },
      {
        "name": {
          "value": "TinyImageNet",
          "justification": "TinyImageNet is used as one of the datasets for the experiments in this paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A. Chrabaszcz, I. Loshchilov, and F. Hutter, “A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets,” arXiv:1707.08819",
          "justification": "TinyImageNet is a popular dataset that serves as a downscaled version of ImageNet, making it referenced in multiple studies.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        }
      },
      {
        "name": {
          "value": "FEMNIST",
          "justification": "FEMNIST is used as one of the datasets for the experiments in this paper.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "S. Caldas et al. 'LEAF: A Benchmark for Federated Settings', 2019",
          "justification": "FEMNIST is a well-known dataset used in federated learning benchmarks, referenced in the context of federated settings.",
          "quote": "Details of the datasets explored and the respective skews induced is summarised in Table 1."
        }
      },
      {
        "name": {
          "value": "Federated Extended MNIST (FedEMNIST)",
          "justification": "Federated Extended MNIST (FedEMNIST) is used as one of the datasets for the experiments in this paper, particularly for real-world evaluations.",
          "quote": "Federated EMNIST Caldas et al. (2019) where the data points have a user identifier. The test performance of the algorithms and their gradient masked alternatives are given in Table 3."
        },
        "aliases": [
          "FedEMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "S. Caldas et al. 'LEAF: A Benchmark for Federated Settings', 2019",
          "justification": "Federated Extended MNIST (FedEMNIST) is cited as a source dataset in the experimental evaluations, emphasizing its relevance in federated learning research.",
          "quote": "Federated EMNIST Caldas et al. (2019) where the data points have a user identifier. The test performance of the algorithms and their gradient masked alternatives are given in Table 3."
        }
      },
      {
        "name": {
          "value": "CMNIST",
          "justification": "CMNIST is used as one of the datasets for the experiments in this paper specifically for out-of-distribution generalization evaluation.",
          "quote": "FedCMNIST Francis et al. (2021), a federated multiclass version of CMNIST Arjovsky et al. (2020) with multiple color-label correlations was used for this."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Invariant Risk Minimization",
          "justification": "CMNIST is based on the colored MNIST dataset used in studies related to invariant risk minimization.",
          "quote": "FedCMNIST Francis et al. (2021), a federated multiclass version of CMNIST Arjovsky et al. (2020) with multiple color-label correlations was used for this."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is commonly used for implementing machine learning experiments and is likely the library used for the implementations in this paper.",
          "quote": "Code for our experiments is include in the supplementary materials and will be made available at the time of publication."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A. Paszke et al., “PyTorch: An imperative style, high-performance deep learning library,” NIPS 2019",
          "justification": "PyTorch is a popular library for deep learning implementations, often cited in research papers.",
          "quote": "Code for our experiments is include in the supplementary materials and will be made available at the time of publication."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2173,
    "prompt_tokens": 22022,
    "total_tokens": 24195
  }
}