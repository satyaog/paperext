{
  "paper": "QExVqoGknN.txt",
  "words": 21528,
  "extractions": {
    "title": {
      "value": "Target-based Surrogates for Stochastic Optimization",
      "justification": "This is clearly mentioned at the beginning of the document.",
      "quote": "Target-based Surrogates for Stochastic Optimization"
    },
    "description": "The research paper presents a framework named Stochastic Surrogate Optimization (SSO), which constructs surrogate functions in a target space that can be minimized efficiently. The framework was validated on both supervised learning and imitation learning problems, demonstrating its efficiency and effectiveness. The framework entails both theoretical guarantees and empirical evaluations to support its claims.",
    "type": {
      "value": "Theoretical and Empirical",
      "justification": "The paper provides theoretical guarantees and convergence analysis for the proposed SSO algorithm, as well as empirical evaluations through experiments.",
      "quote": "In the full-batch setting, we prove that our surrogate is a global upper-bound on the loss, and can be (locally) minimized using a black-box optimization algorithm. We prove that the resulting majorization-minimization algorithm ensures convergence to a stationary point of the loss. Next, we instantiate our framework in the stochastic setting and propose the SSO algorithm... To evaluate our framework, we consider a suite of supervised learning and imitation learning problems. Our experiments indicate the benefits of target optimization and the effectiveness of SSO."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimisation (Optimization)",
        "justification": "The primary focus of the paper is on optimization techniques for machine learning.",
        "quote": "We consider minimizing functions for which it is expensive to compute the (possibly stochastic) gradient. Such functions are prevalent in reinforcement learning, imitation learning and adversarial training. Our target optimization framework uses the (expensive) gradient computation to construct surrogate functions in a target space (e.g. the logits output by a linear model for classification) that can be minimized efficiently."
      },
      "aliases": [
        "Optimization"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Supervised Learning",
          "justification": "The framework was evaluated on supervised learning problems.",
          "quote": "To evaluate our framework, we consider a suite of supervised learning and imitation learning problems."
        },
        "aliases": [
          "Supervised Learning"
        ]
      },
      {
        "name": {
          "value": "Imitation Learning",
          "justification": "The framework was evaluated on imitation learning problems.",
          "quote": "To evaluate our framework, we consider a suite of supervised learning and imitation learning problems."
        },
        "aliases": [
          "Imitation Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Stochastic Surrogate Optimization (SSO)",
          "justification": "The SSO model is the main contribution of the paper and is extensively described and evaluated.",
          "quote": "Next, we instantiate our framework in the stochastic setting and propose the SSO algorithm, which can be viewed as projected stochastic gradient descent in the target space."
        },
        "aliases": [
          "SSO"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is proposed by the authors in this paper.",
          "quote": "Next, we instantiate our framework in the stochastic setting and propose the SSO algorithm."
        },
        "is_executed": {
          "value": 1,
          "justification": "The models including SSO were executed during experimental evaluations in the paper.",
          "quote": "To evaluate our framework, we consider a suite of supervised learning and imitation learning problems. Our experiments indicate the benefits of target optimization and the effectiveness of SSO."
        },
        "is_compared": {
          "value": 1,
          "justification": "The SSO algorithm is compared with other optimization methods in the experiments.",
          "quote": "Our experiments indicate the benefits of target optimization and the effectiveness of SSO."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The SSO algorithm is the primary contribution of this paper, hence it is not referenced from another paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Mujoco",
          "justification": "The Mujoco benchmark suite was used for evaluating their framework in the experiments.",
          "quote": "Using the Mujoco benchmark suite (Todorov et al., 2012) we demonstrate that SSO results in superior empirical performance"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Mujoco: A physics engine for model-based control",
          "justification": "The paper references Mujoco benchmark suite for empirical evaluation.",
          "quote": "Using the Mujoco benchmark suite (Todorov et al., 2012) we demonstrate that SSO results in superior empirical performance (Section 5)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used as an implementation framework for the experiments.",
          "quote": "For SSO, since optimization of the surrogate is a deterministic problem, we use the standard back-tracking Armijo line-search (Armijo, 1966) with the same hyper-parameters across all experiments. For each experiment, we plot the average loss against the number of calls to the (stochastic) gradient oracle. The mean and the relevant quantiles are reported using three random seeds... We use either the theoretically chosen step-size when available, or the default step-size provided by Paszke et al. (2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The paper references the corresponding PyTorch paper.",
          "quote": "For each experiment, we plot the average loss against the number of calls to the (stochastic) gradient oracle... or the default step-size provided by Paszke et al. (2019)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1091,
    "prompt_tokens": 40549,
    "total_tokens": 41640
  }
}