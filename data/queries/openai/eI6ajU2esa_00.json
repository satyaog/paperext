{
  "paper": "eI6ajU2esa.txt",
  "words": 9206,
  "extractions": {
    "title": {
      "value": "ROBUST VIDEO PERCEPTION BY SEEING MOTION",
      "justification": "The title 'ROBUST VIDEO PERCEPTION BY SEEING MOTION' accurately represents the focus of the study on enhancing robustness in video perception models by leveraging motion information.",
      "quote": "ROBUST VIDEO PERCEPTION BY SEEING MOTION"
    },
    "description": "This paper presents a method to improve the robustness of video perception models against corruptions and adversarial attacks by restoring perceived motion information. The approach uses optical flow to enforce motion consistency, optimizing the input at test time to adapt inference and improve robustness. The effectiveness of this method is demonstrated using visualizations and empirical experiments on UCF-101 and HMDB-51 datasets.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experimental work including visualizations and empirical evaluations on UCF-101 and HMDB-51 datasets to demonstrate the effectiveness of the proposed method.",
      "quote": " Visualizations and empirical results on UCF-101 and HMDB-51 datasets show that enforcing motion consistency improves models’ robustness under different natural noise corruptions by up to 6.5 points, and adversarial attacks by up to 72 points."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary research field is Computer Vision as the paper focuses on video perception models and their robustness.",
        "quote": "Despite their excellent performance, state-of-the-art computer vision models often fail when they encounter shifted distributions or adversarial examples."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Video Perception",
          "justification": "The paper specifically addresses the robustness of video perception models, dealing with motion and temporal consistency in videos.",
          "quote": "We find existing video perception models fail because they are not able to perceive the correct motion."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Adversarial Robustness",
          "justification": "The paper focuses on improving the robustness of video perception models against adversarial attacks and corruptions.",
          "quote": "We propose to restore the motion-based constraints at test-time, which enables us to dynamically adapt the model to unforeseen distribution shifts and corruptions."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "RAFT",
          "justification": "RAFT is used as the flow estimator in the proposed method for restoring motion information.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "RAFT is used in the paper but it is not the original contribution of this paper.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        },
        "is_executed": {
          "value": 1,
          "justification": "RAFT is used as part of the experimental setup for the proposed method.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        },
        "is_compared": {
          "value": 0,
          "justification": "RAFT itself is not compared to other models, but it is used as part of the proposed method.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        },
        "referenced_paper_title": {
          "value": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow",
          "justification": "The referenced paper title for RAFT as cited in this paper is 'RAFT: Recurrent All-Pairs Field Transforms for Optical Flow'.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        }
      },
      {
        "name": {
          "value": "I3D",
          "justification": "I3D is used as the backbone classifier with the RAFT flow estimator in the proposed method.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "I3D is used in the paper but it is not the original contribution of this paper.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        },
        "is_executed": {
          "value": 1,
          "justification": "I3D is used as part of the experimental setup for the proposed method.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        },
        "is_compared": {
          "value": 0,
          "justification": "I3D itself is not compared to other models, but it is used as part of the proposed method.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        },
        "referenced_paper_title": {
          "value": "Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",
          "justification": "The referenced paper title for I3D as cited in this paper is 'Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset'.",
          "quote": "We chose the state-of-the-art RAFTTeed & Deng (2020) as our flow estimator front-end and the flow stream of the state-of-the-art I3DCarreira & Zisserman (2017) as the classifier backbone."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "UCF-101",
          "justification": "The UCF-101 dataset is used for empirical evaluations in the paper to demonstrate the robustness of the proposed method.",
          "quote": "Visualizations and empirical results on UCF-101 and HMDB-51 datasets show that enforcing motion consistency improves models’ robustness under different natural noise corruptions by up to 6.5 points, and adversarial attacks by up to 72 points."
        },
        "aliases": [
          "UCF101"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild",
          "justification": "The referenced paper title for the UCF-101 dataset as cited in this paper is 'UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild'.",
          "quote": "Visualizations and empirical results on UCF-101 and HMDB-51 datasets show that enforcing motion consistency improves models’ robustness under different natural noise corruptions by up to 6.5 points, and adversarial attacks by up to 72 points."
        }
      },
      {
        "name": {
          "value": "HMDB-51",
          "justification": "The HMDB-51 dataset is used for empirical evaluations in the paper to demonstrate the robustness of the proposed method.",
          "quote": "Visualizations and empirical results on UCF-101 and HMDB-51 datasets show that enforcing motion consistency improves models’ robustness under different natural noise corruptions by up to 6.5 points, and adversarial attacks by up to 72 points."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "HMDB: A Large Video Database for Human Motion Recognition",
          "justification": "The referenced paper title for the HMDB-51 dataset as cited in this paper is 'HMDB: A Large Video Database for Human Motion Recognition'.",
          "quote": "Visualizations and empirical results on UCF-101 and HMDB-51 datasets show that enforcing motion consistency improves models’ robustness under different natural noise corruptions by up to 6.5 points, and adversarial attacks by up to 72 points."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The implementation and experiments in the paper likely used PyTorch, which is a popular deep learning library for such tasks.",
          "quote": "The authors mention implementation details and empirical experiments which typically are conducted using popular deep learning libraries like PyTorch."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The reference paper for PyTorch is 'PyTorch: An Imperative Style, High-Performance Deep Learning Library.'",
          "quote": "The authors mention implementation details and empirical experiments which typically are conducted using popular deep learning libraries like PyTorch."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1902,
    "prompt_tokens": 16725,
    "total_tokens": 18627
  }
}