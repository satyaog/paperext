{
  "paper": "2308.06198.txt",
  "words": 11304,
  "extractions": {
    "title": {
      "value": "DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity",
      "justification": "The title is extracted from the paper.",
      "quote": "DIG In: Evaluating Disparities in Image Generations with Indicators for Geographic Diversity"
    },
    "description": "The paper introduces three indicators to evaluate the realism, diversity, and prompt-generation consistency of text-to-image generative systems when prompted to generate objects from across the world. These indicators enable automatic benchmarking of geographic disparities in state-of-the-art visual content creation systems.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts a comprehensive evaluation of geographic biases in text-to-image generative systems using proposed indicators.",
      "quote": "We use our proposed indicators to analyze potential geographic biases in state-of-the-art visual content creation systems."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on evaluating images generated by text-to-image systems, which is a sub-field of Computer Vision.",
        "quote": "In this work, we introduce three indicators to evaluate the realism, diversity and prompt-generation consistency of text-to-image generative systems."
      },
      "aliases": [
        "CV",
        "Vision"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generative Models",
          "justification": "The paper evaluates the performance and biases of text-to-image generative systems.",
          "quote": "Recent text-to-image generative systems such as Stable Diffusion (Rombach et al., 2021), DALL-E 2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022), and Make-a-Scene (Gafni et al., 2022) have shown unprecedented quality."
        },
        "aliases": [
          "GANs",
          "VAEs",
          "Diffusion Models"
        ]
      },
      {
        "name": {
          "value": "Fairness and Bias",
          "justification": "The paper's primary goal is to evaluate and highlight geographic biases in text-to-image generative models.",
          "quote": "Our comprehensive evaluation constitutes a crucial step towards ensuring a positive experience of visual content creation for everyone."
        },
        "aliases": [
          "Ethics in AI",
          "Bias in AI"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Stable Diffusion",
          "justification": "Stable Diffusion is one of the text-to-image generative systems evaluated in the paper.",
          "quote": "Recent text-to-image generative systems such as Stable Diffusion (Rombach et al., 2021)..."
        },
        "aliases": [
          "LDM"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is not introduced by this paper; it is used for evaluation.",
          "quote": "Recent text-to-image generative systems such as Stable Diffusion (Rombach et al., 2021)..."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed to generate images for evaluation in the study.",
          "quote": "We then use the indicators to evaluate widely used state-of-the-art text-to-image generative systems. These include two versions of latent diffusion models (Rombach et al., 2021)..."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared to other state-of-the-art models in the paper.",
          "quote": "We use our proposed indicators to analyze potential geographic biases in state-of-the-art visual content creation systems."
        },
        "referenced_paper_title": {
          "value": "High-Resolution Image Synthesis with Latent Diffusion Models",
          "justification": "This is the referenced paper for Stable Diffusion.",
          "quote": "Stable Diffusion (Rombach et al., 2021)"
        }
      },
      {
        "name": {
          "value": "DALL-E 2",
          "justification": "DALL-E 2 is evaluated in the paper.",
          "quote": "Recent text-to-image generative systems such as ... DALL-E 2 (Ramesh et al., 2022)..."
        },
        "aliases": [
          "DALL-E"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is not introduced by this paper; it is used for evaluation.",
          "quote": "Recent text-to-image generative systems such as ... DALL-E 2 (Ramesh et al., 2022)..."
        },
        "is_executed": {
          "value": false,
          "justification": "Although mentioned, the model is not explicitly stated to have been executed in this paper.",
          "quote": "Recent text-to-image generative systems such as ... DALL-E 2 (Ramesh et al., 2022)..."
        },
        "is_compared": {
          "value": false,
          "justification": "While mentioned, there is no explicit comparative analysis involving DALL-E 2 in the study.",
          "quote": "Recent text-to-image generative systems such as ... DALL-E 2 (Ramesh et al., 2022)..."
        },
        "referenced_paper_title": {
          "value": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
          "justification": "This is the referenced paper for DALL-E 2.",
          "quote": "DALL-E 2 (Ramesh et al., 2022)"
        }
      },
      {
        "name": {
          "value": "Imagen",
          "justification": "Imagen is evaluated in the paper.",
          "quote": "Recent text-to-image generative systems ... Imagen (Saharia et al., 2022)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is not introduced by this paper; it is used for evaluation.",
          "quote": "Recent text-to-image generative systems ... Imagen (Saharia et al., 2022)"
        },
        "is_executed": {
          "value": false,
          "justification": "Although mentioned, the model is not explicitly stated to have been executed in this paper.",
          "quote": "Recent text-to-image generative systems ... Imagen (Saharia et al., 2022)"
        },
        "is_compared": {
          "value": false,
          "justification": "While mentioned, there is no explicit comparative analysis involving Imagen in the study.",
          "quote": "Recent text-to-image generative systems ... Imagen (Saharia et al., 2022)"
        },
        "referenced_paper_title": {
          "value": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
          "justification": "This is the referenced paper for Imagen.",
          "quote": "Imagen (Saharia et al., 2022)"
        }
      },
      {
        "name": {
          "value": "Make-a-Scene",
          "justification": "Make-a-Scene is evaluated in the paper.",
          "quote": "Recent text-to-image generative systems ... Make-a-Scene (Gafni et al., 2022)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is not introduced by this paper; it is used for evaluation.",
          "quote": "Recent text-to-image generative systems ... Make-a-Scene (Gafni et al., 2022)"
        },
        "is_executed": {
          "value": false,
          "justification": "Although mentioned, the model is not explicitly stated to have been executed in this paper.",
          "quote": "Recent text-to-image generative systems ... Make-a-Scene (Gafni et al., 2022)"
        },
        "is_compared": {
          "value": false,
          "justification": "While mentioned, there is no explicit comparative analysis involving Make-a-Scene in the study.",
          "quote": "Recent text-to-image generative systems ... Make-a-Scene (Gafni et al., 2022)"
        },
        "referenced_paper_title": {
          "value": "Make-a-Scene: Scene-based Text-to-Image Generation with Human Priors",
          "justification": "This is the referenced paper for Make-a-Scene.",
          "quote": "Make-a-Scene (Gafni et al., 2022)"
        }
      },
      {
        "name": {
          "value": "GLIDE",
          "justification": "GLIDE is one of the models evaluated in this study.",
          "quote": "We use the indicators to evaluate ... as well as GLIDE (Nichol et al., 2021)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is not introduced by this paper; it is used for evaluation.",
          "quote": "We use the indicators to evaluate ... as well as GLIDE (Nichol et al., 2021)."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed to generate images for evaluation in the study.",
          "quote": "We use the indicators to evaluate widely used state-of-the-art text-to-image generative systems. ... We also evaluate a diffusion model utilizing CLIP image embeddings (Ramesh et al., 2022), denoted as “DM w/ CLIP Latents,” as well as GLIDE (Nichol et al., 2021)."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared to other state-of-the-art models in the paper.",
          "quote": "We use our proposed indicators to analyze potential geographic biases in state-of-the-art visual content creation systems."
        },
        "referenced_paper_title": {
          "value": "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models",
          "justification": "This is the referenced paper for GLIDE.",
          "quote": "GLIDE (Nichol et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GeoDE",
          "justification": "GeoDE is used as a reference dataset to compute realism and diversity of the generated images.",
          "quote": "We use GeoDE (Ramaswamy et al., 2023) ... as reference datasets to compute realism and diversity of the generated images."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "GeoDE: A Geographically Diverse Evaluation Dataset for Object Recognition",
          "justification": "This is the referenced paper for the GeoDE dataset.",
          "quote": "GeoDE (Ramaswamy et al., 2023)"
        }
      },
      {
        "name": {
          "value": "DollarStreet",
          "justification": "DollarStreet is used as a reference dataset to compute realism and diversity of the generated images.",
          "quote": "We use... DollarStreet (Rojas et al., 2022) as reference datasets to compute realism and diversity of the generated images."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World",
          "justification": "This is the referenced paper for the DollarStreet dataset.",
          "quote": "DollarStreet (Rojas et al., 2022)"
        }
      },
      {
        "name": {
          "value": "LAION-2B-en",
          "justification": "LAION-2B-en is analyzed for biases in training data.",
          "quote": "In a small experiment, we prompt using Arabic, Hausa, and Zulu, which correspond to the most spoken non-English languages in the three African countries best represented in the GeoDE dataset (Egypt, Nigeria, and South Africa, respectively)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "No specific referenced paper title was found for LAION-2B-en.",
          "quote": ""
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2750,
    "prompt_tokens": 19288,
    "total_tokens": 22038
  }
}