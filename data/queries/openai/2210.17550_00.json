{
  "paper": "2210.17550.txt",
  "words": 22078,
  "extractions": {
    "title": {
      "value": "Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization",
      "justification": "This is the title mentioned at the beginning of the paper.",
      "quote": "Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization"
    },
    "description": "The paper presents a new first-order optimization algorithm called AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent, designed for separable convex-concave minimax optimization problems. The algorithm leverages Nesterov acceleration and optimistic gradient descent to achieve optimal convergence rates in both deterministic and stochastic settings. The authors showcase the theoretical analysis of the algorithm and compare it with existing methods.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper primarily focuses on the formulation of a new algorithm and its theoretical convergence analysis.",
      "quote": "We provide theoretical analysis of our algorithm for general separable SC-SC problem (2) and compare the results with existing literature under special cases in the form of (3) (bi-SC-SC, bi-C-SC and Bilinear)."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization",
        "justification": "The primary focus of the paper is on optimization techniques within the context of machine learning.",
        "quote": "We propose a new first-order optimization algorithm — AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent—for separable convex-concave minimax optimization."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "First-Order Methods",
          "justification": "The paper deals with a first-order optimization algorithm.",
          "quote": "We propose a new first-order optimization algorithm — AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent—for separable convex-concave minimax optimization."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Separable Minimax Optimization",
          "justification": "The paper focuses on separable convex-concave minimax optimization problems.",
          "quote": "AG-OG Descent Ascent—for separable convex-concave minimax optimization."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent",
          "justification": "This is the new optimization algorithm introduced by the authors.",
          "quote": "We propose a new first-order optimization algorithm — AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent—for separable convex-concave minimax optimization."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The algorithm is a novel contribution of this paper.",
          "quote": "We propose a new first-order optimization algorithm — AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent—for separable convex-concave minimax optimization."
        },
        "is_executed": {
          "value": 0,
          "justification": "The focus of the paper is on theoretical analysis rather than implementation details.",
          "quote": "We provide theoretical analysis of our algorithm for general separable SC-SC problem (2) and compare the results with existing literature."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the AG-OG algorithm is compared with existing methods.",
          "quote": "We provide theoretical analysis of our algorithm for general separable SC-SC problem (2) and compare the results with existing literature under special cases in the form of (3) (bi-SC-SC, bi-C-SC and Bilinear)."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No specific reference paper title is given for the model.",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 890,
    "prompt_tokens": 42208,
    "total_tokens": 43098
  }
}