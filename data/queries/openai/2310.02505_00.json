{
  "paper": "2310.02505.txt",
  "words": 15603,
  "extractions": {
    "title": {
      "value": "Learning to Reach Goals via Diffusion",
      "justification": "The title is directly mentioned at the beginning of the provided paper abstract.",
      "quote": "Learning to Reach Goals via Diffusion"
    },
    "description": "The paper presents a novel approach to goal-conditioned reinforcement learning (GCRL) using denoising diffusion models. The proposed method, named Merlin, learns a goal-conditioned policy to reverse away from potential goal states, avoiding the need to learn a separate value function. The approach is validated on various offline goal-reaching tasks, demonstrating significant performance and computational efficiency improvement over state-of-the-art methods.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper experimentally validates their approach with various offline goal-reaching tasks, demonstrating performance enhancements.",
      "quote": "We experimentally validate our approach in various offline goal-reaching tasks, demonstrating substantial performance enhancements compared to state-of-the-art methods while improving computational efficiency over other diffusion-based RL methods by an order of magnitude."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on goal-conditioned reinforcement learning, a sub-domain of reinforcement learning (RL).",
        "quote": "We present a novel perspective on goal-conditioned reinforcement learning by framing it within the context of denoising diffusion models."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Goal-conditioned Reinforcement Learning",
          "justification": "The paper specifically emphasizes goal-conditioned reinforcement learning (GCRL) and presents findings that enhance this paradigm.",
          "quote": "Goal-conditioned RL (GCRL) is a paradigm to learn general policies that can reach arbitrary target states within an environment without requiring extensive retraining."
        },
        "aliases": [
          "GCRL"
        ]
      },
      {
        "name": {
          "value": "Diffusion Models",
          "justification": "The approach leverages diffusion models, adapted for use within the RL context.",
          "quote": "To tackle the GCRL problem of reaching specified goal states from arbitrary initial states, this paper draws inspiration from diffusion models - a powerful class of generative models that can map random noise in high-dimensional spaces to target data samples through iterative denoising."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Merlin",
          "justification": "The model name is explicitly provided and described in the context of the paper.",
          "quote": "This approach, which we call Merlin, can reach specified goals from an arbitrary initial state without learning a separate value function."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The Merlin model is the main contribution of the paper which is proposed for goal-conditioned reinforcement learning.",
          "quote": "This approach, which we call Merlin, can reach specified goals from an arbitrary initial state without learning a separate value function."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experimental validation of the Merlin model, implying execution.",
          "quote": "We experimentally validate our approach in various offline goal-reaching tasks, demonstrating substantial performance enhancements compared to state-of-the-art methods while improving computational efficiency over other diffusion-based RL methods by an order of magnitude."
        },
        "is_compared": {
          "value": 1,
          "justification": "Merlin is compared numerically to other state-of-the-art methods in the experimental results section.",
          "quote": "We experimentally validate our approach in various offline goal-reaching tasks, demonstrating substantial performance enhancements compared to state-of-the-art methods..."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper does not cite another paper as a reference for the Merlin model as it is a novel contribution.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "PointReach",
          "justification": "PointReach is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]. The tasks include the relatively easier PointReach..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The PointReach task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "PointRooms",
          "justification": "PointRooms is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]. The tasks include the relatively easier... PointRooms..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The PointRooms task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "Reacher",
          "justification": "Reacher is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]. The tasks include the relatively easier... Reacher..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The Reacher task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "SawyerReach",
          "justification": "SawyerReach is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]. The tasks include the relatively easier... SawyerReach..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The SawyerReach task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "SawyerDoor",
          "justification": "SawyerDoor is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]. The tasks include the relatively easier... SawyerDoor..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The SawyerDoor task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "FetchReach",
          "justification": "FetchReach is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]. The tasks include the relatively easier... FetchReach..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The FetchReach task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "FetchPush",
          "justification": "FetchPush is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]... and the harder tasks include... FetchPush..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The FetchPush task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "FetchPick",
          "justification": "FetchPick is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]... and the harder tasks include... FetchPick..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The FetchPick task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "FetchSlide",
          "justification": "FetchSlide is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]... and the harder tasks include... FetchSlide..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The FetchSlide task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      },
      {
        "name": {
          "value": "HandReach",
          "justification": "HandReach is mentioned as one of the tasks used to validate the Merlin approach.",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]... and the harder tasks include... HandReach..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rethinking goal-conditioned supervised learning and its connection to offline RL",
          "justification": "The HandReach task is part of the benchmark introduced in Yang et al. (2021).",
          "quote": "Tasks. We evaluate Merlin on several goal-conditioned control tasks using the benchmark introduced in Yang et al. [2021]."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2659,
    "prompt_tokens": 34206,
    "total_tokens": 36865
  }
}