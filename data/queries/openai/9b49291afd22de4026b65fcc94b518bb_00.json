{
  "paper": "9b49291afd22de4026b65fcc94b518bb.txt",
  "words": 16419,
  "extractions": {
    "title": {
      "value": "Goal-driven optimization of single-neuron properties in artificial networks reveals regularization role of neural diversity and adaptation",
      "justification": "The title explicitly states the main focus and findings of the paper.",
      "quote": "Goal-driven optimization of single-neuron properties in artificial networks reveals regularization role of neural diversity and adaptation"
    },
    "description": "The paper investigates how single-neuron input-output adaptive mechanisms, optimized in an end-to-end fashion in artificial recurrent neural networks, impact network computations and network robustness to noise and changes in input statistics. By introducing Adaptive Recurrent Units (ARUs) that mimic the diversity of neuronal activation functions observed in the brain, the study demonstrates improved network performance and stability.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves systematic investigation and experimentation with artificial recurrent neural networks to evaluate the impact of adaptive single-neuron properties on network performance.",
      "quote": "The paper investigates how goal-driven requirements at the network level influence single-neuron coding properties by using tools from dynamical systems theory and performing numerical experiments."
    },
    "primary_research_field": {
      "name": {
        "value": "Artificial Neural Networks",
        "justification": "The research focuses on optimizing and investigating properties of artificial recurrent neural networks.",
        "quote": "This is achieved by interconnected Adaptive Recurrent Units (ARU), which perform online control of a novel two-parameter family of activation functions mimicking the diversity of f-I curves found in common neural types in the brain."
      },
      "aliases": [
        "ANN"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neuromorphic Engineering",
          "justification": "The study aims to mimic the diversity of f-I curves found in biological neurons and implements biologically realistic adaptive mechanisms in artificial networks.",
          "quote": "Our network of ARUs shows much-improved robustness to noise and changes in input statistics. Importantly, we find that ARUs recover precise biological coding strategies such as gain scaling and fractional order differentiation."
        },
        "aliases": [
          "Neuromorphic Computing"
        ]
      },
      {
        "name": {
          "value": "Dynamical Systems",
          "justification": "The paper utilizes tools from dynamical systems theory to elucidate the role of emergent single-neuron properties and adaptive mechanisms in neural networks.",
          "quote": "Using tools from dynamical systems theory, we elucidate the role of these emergent single neuron properties."
        },
        "aliases": [
          "Dynamical Systems Theory"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Adaptive Recurrent Unit (ARU)",
          "justification": "The ARU is a novel model introduced and investigated in the paper, which performs online modulation of activation functions.",
          "quote": "We call our novel adaptive artificial neuron Adaptive Recurrent Unit (ARU)."
        },
        "aliases": [
          "ARU"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The ARU model is introduced as a novel contribution in this paper.",
          "quote": "We call our novel adaptive artificial neuron Adaptive Recurrent Unit (ARU)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper involves experiments where ARUs are implemented in simulated recurrent neural networks.",
          "quote": "In this work, we use a goal-driven approach to investigate adaptive input-output properties of neurons that emerge from end-to-end optimization of recurrent neural networks."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of ARUs is compared to traditional RNNs and other gating architectures.",
          "quote": "In terms of performance for the psMNIST task, we found that learning heterogeneous activation did not provide a significant advantages over the already well performing optimization settings, outperforming the fixed setting but not necessarily the homogeneous setting."
        },
        "referenced_paper_title": {
          "value": "Not Applicable",
          "justification": "The ARU is a novel contribution of the paper itself.",
          "quote": "We call our novel adaptive artificial neuron Adaptive Recurrent Unit (ARU)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is used in sequential form to assess the performance of the models.",
          "quote": "In our numerical analysis of information propagation metrics during learning and associated emergent phenomena, we focus primarily on the task of classifying MNIST (Le et al., 2015) digits from a permuted sequential sequence of pixels (psMNIST)."
        },
        "aliases": [
          "psMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "LeCun, Y., Cortes, C., and Burges, C. J. (2010). MNIST handwritten digit database. ATT Labs [Online].",
          "justification": "The paper references MNIST dataset as a well-established benchmark for evaluating the performance of neural networks.",
          "quote": "MNIST handwritten digit database. ATT Labs [Online]."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The CIFAR-10 dataset is used in a grayscaled sequential form to assess the performance of the models.",
          "quote": "The second task, a grayscaled and sequential version of the CIFAR10 classification task (gsCIFAR10), is further used as a more computationally demanding task to explore more complex regimes."
        },
        "aliases": [
          "gsCIFAR-10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. Technical report, MIT & NYU.",
          "justification": "The paper references CIFAR-10 dataset as a well-established benchmark for evaluating the performance of neural networks.",
          "quote": "Learning multiple layers of features from tiny images. Technical report, MIT & NYU."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1350,
    "prompt_tokens": 27273,
    "total_tokens": 28623
  }
}