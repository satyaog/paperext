{
  "paper": "2310.08513.txt",
  "words": 14576,
  "extractions": {
    "title": {
      "value": "How Connectivity Structure Shapes Rich and Lazy Learning in Neural Circuits",
      "justification": "This is the exact title of the paper as it appears at the beginning of the document.",
      "quote": "HOW CONNECTIVITY STRUCTURE SHAPES RICH AND LAZY LEARNING IN NEURAL CIRCUITS"
    },
    "description": "This paper investigates the impact of initial weight structures, particularly the effective rank, on the learning dynamics of neural networks. Through both theoretical and empirical analysis, the study finds that high-rank initializations yield lazier learning, whereas low-rank initializations foster richer learning. The findings are validated using recurrent neural networks on neuroscience-relevant tasks and experimentally-driven initial connectivity patterns.",
    "type": {
      "value": "theoretical",
      "justification": "The paper involves theoretical derivations and empirical validations regarding the impact of initial weight structures on learning dynamics.",
      "quote": "Through theoretical derivation in two-layer feedforward linear network, we demonstrate that higher-rank initialization results in effectively lazier learning on average across tasks (Theorem 1)."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The study applies to deep learning, focusing on how connectivity structures in neural networks influence learning dynamics.",
        "quote": "In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically influence its learning dynamics."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neuroscience",
          "justification": "The paper explores the implications of neural connectivity structures on learning dynamics, which is a key topic within neuroscience.",
          "quote": "Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications for metabolic costs of plasticity and risks of catastrophic forgetting."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Recurrent Neural Networks",
          "justification": "The paper uses RNNs to validate their theoretical findings on learning dynamics.",
          "quote": "We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known neuroscience tasks."
        },
        "aliases": [
          "RNNs"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "train"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Neurogym",
          "justification": "The dataset is used to validate the findings of the paper through various neuroscience tasks.",
          "quote": "We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known neuroscience tasks (Figure 1) ... including ... tasks implemented with Neurogym."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Sequential MNIST",
          "justification": "The dataset is used to validate the findings of the paper on a standard machine learning benchmark task.",
          "quote": "We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known neuroscience tasks (Figure 1) and demonstrate the applicability to different initial connectivity structures extracted from neuroscience data ... and the well-known machine learning benchmark sequential MNIST (sMNIST)."
        },
        "aliases": [
          "sMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The library is used to implement and run the neural network models and experiments described in the paper.",
          "quote": "Our code is accessible at https://github.com/Helena-Yuhan-Liu/BioRNN_RichLazy. We used PyTorch Version 1.10.2"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 699,
    "prompt_tokens": 24559,
    "total_tokens": 25258
  }
}