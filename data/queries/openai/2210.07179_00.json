{
  "paper": "2210.07179.txt",
  "words": 13480,
  "extractions": {
    "title": {
      "value": "MAPL : Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting",
      "justification": "This is the exact title of the paper as given by the user.",
      "quote": "MAPL : Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting"
    },
    "description": "The paper proposes MAPL, a parameter-efficient method for adapting unimodal vision-language models for few-shot learning. It discusses the technique to connect a frozen vision encoder with a language model using lightweight mapping, achieving effective results in tasks like image captioning and visual question answering without extensive computation or data.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts extensive experiments on several benchmarks, including image captioning and visual question answering, to demonstrate the effectiveness of the proposed method, MAPL.",
      "quote": "Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary focus is on adapting models for tasks that include image captioning and visual question answering, which are core areas in the field of Computer Vision.",
        "quote": "Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The research involves adapting vision-language models, utilizing language models like GPT, which falls under Natural Language Processing.",
          "quote": "...to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The research specifically focuses on connecting vision and language models, dealing with multiple modalities of data (i.e., text and images).",
          "quote": "We propose to connect a vision encoder, such as CLIP, to an autoregressive language model, such as GPT, with minimal additional training on multimodal data."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "MAPL",
          "justification": "MAPL is the proposed method and main focus of the paper.",
          "quote": "We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "MAPL is the proposed model and contribution of the paper.",
          "quote": "We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments in the paper, including training and evaluations, involve executing the MAPL model.",
          "quote": "MAPL can be trained in just a few hours using modest computational resources and public datasets."
        },
        "is_compared": {
          "value": 1,
          "justification": "MAPL is compared numerically to other methods and baselines in the experiments section.",
          "quote": "Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "MAPL is the contribution of the current paper and does not reference another paper for its proposal.",
          "quote": "We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings."
        }
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is utilized as the vision encoder in the MAPL model.",
          "quote": "We propose to connect a vision encoder, such as CLIP (Radford et al., 2021), to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CLIP is not contributed by this paper, but used as a component.",
          "quote": "We propose to connect a vision encoder, such as CLIP (Radford et al., 2021), to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "is_executed": {
          "value": 1,
          "justification": "CLIP is executed as part of the MAPL pipeline for vision-language tasks.",
          "quote": "We propose to connect a vision encoder, such as CLIP (Radford et al., 2021), to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "is_compared": {
          "value": 0,
          "justification": "CLIP is used as part of the MAPL model and not compared directly to other models.",
          "quote": "We propose to connect a vision encoder, such as CLIP (Radford et al., 2021), to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models From Natural Language Supervision",
          "justification": "The paper uses CLIP as a component, referencing the original CLIP paper to support this usage.",
          "quote": "Learning transferable visual models from natural language supervision."
        }
      },
      {
        "name": {
          "value": "GPT",
          "justification": "GPT is utilized as the language model in the MAPL model.",
          "quote": "We propose to connect a vision encoder, such as CLIP (Radford et al., 2021), to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT is not contributed by this paper, but used as a component.",
          "quote": "We propose to connect a vision encoder, such as CLIP (Radford et al., 2021), to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "is_executed": {
          "value": 1,
          "justification": "GPT is executed as part of the MAPL pipeline for vision-language tasks.",
          "quote": "We propose to connect a vision encoder, such as CLIP (Radford et al., 2021), to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "is_compared": {
          "value": 0,
          "justification": "GPT is used as part of the MAPL model and not compared directly to other models.",
          "quote": "We propose to connect a vision encoder, such as CLIP (Radford et al., 2021), to an autoregressive language model (LM), such as GPT (Radford et al., 2018, 2019; Brown et al., 2020), with minimal additional training on multimodal data."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The paper uses GPT as a component, referencing the original GPT papers to support this usage.",
          "quote": "Language models are few-shot learners."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Conceptual Captions",
          "justification": "Conceptual Captions is used to train the MAPL model in domain-agnostic settings.",
          "quote": "For domain-agnostic training, we use the CC dataset, which is gathered by automatically scraping images and their corresponding alt-text fields from web pages"
        },
        "aliases": [
          "CC"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset For Automatic Image Captioning",
          "justification": "The Conceptual Captions dataset is used in the paper, referencing the original repository and paper.",
          "quote": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset For Automatic Image Captioning."
        }
      },
      {
        "name": {
          "value": "COCO Captions",
          "justification": "COCO Captions is used to evaluate the MAPL model for image captioning tasks.",
          "quote": "For image captioning, we evaluate on the Karpathy-test split (Karpathy and Fei-Fei, 2015) of COCO Captions"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Microsoft COCO Captions: Data Collection and Evaluation Server",
          "justification": "COCO Captions is used in this paper, referencing the original paper and dataset repository.",
          "quote": "Microsoft COCO Captions: Data Collection and Evaluation Server"
        }
      },
      {
        "name": {
          "value": "VQAv2",
          "justification": "The VQAv2 dataset is used to evaluate the MAPL model for visual question answering.",
          "quote": "For VQA, we evaluate on the validation splits of VQAv2 (Goyal et al., 2017)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
          "justification": "VQAv2 is used in the paper, and the original paper is referenced.",
          "quote": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"
        }
      },
      {
        "name": {
          "value": "OK-VQA",
          "justification": "OK-VQA is used to evaluate the MAPL model for visual question answering.",
          "quote": "For VQA, we evaluate on the validation splits of...OK-VQA (Marino et al., 2019)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge",
          "justification": "OK-VQA is used in the paper, and the original paper is referenced.",
          "quote": "OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"
        }
      },
      {
        "name": {
          "value": "TextVQA",
          "justification": "TextVQA is used to evaluate the MAPL model for visual question answering.",
          "quote": "For VQA, we evaluate on the validation splits of...TextVQA (Singh et al., 2019)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Towards VQA Models That Can Read",
          "justification": "TextVQA is used in the paper, and the original paper is referenced.",
          "quote": "Towards VQA Models That Can Read"
        }
      },
      {
        "name": {
          "value": "VizWiz-VQA",
          "justification": "VizWiz-VQA is used to evaluate the MAPL model for visual question answering.",
          "quote": "For VQA, we evaluate on the validation splits of...VizWiz-VQA (Gurari et al., 2018)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "VizWiz Grand Challenge: Answering Visual Questions from Blind People",
          "justification": "VizWiz-VQA is used in the paper, and the original paper is referenced.",
          "quote": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"
        }
      },
      {
        "name": {
          "value": "TextCaps",
          "justification": "TextCaps is used to evaluate the MAPL model for image captioning tasks.",
          "quote": "For image captioning, we evaluate on...the validation splits of...TextCaps (Sidorov et al., 2020)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "TextCaps: A Dataset for Image Captioning with Reading Comprehension",
          "justification": "TextCaps is used in this paper, referencing the original paper and dataset repository.",
          "quote": "TextCaps: A Dataset for Image Captioning with Reading Comprehension"
        }
      },
      {
        "name": {
          "value": "VizWiz-Captions",
          "justification": "VizWiz-Captions is used to evaluate the MAPL model for image captioning tasks.",
          "quote": "For image captioning, we evaluate on...the validation splits of...VizWiz-Captions (Gurari et al., 2018)"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "VizWiz Grand Challenge: Answering Visual Questions from Blind People",
          "justification": "VizWiz-Captions is used in the paper, referencing the original paper and dataset repository.",
          "quote": "VizWiz Grand Challenge: Answering Visual Questions from Blind People"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments and training of the MAPL model are likely implemented in PyTorch, based on the common usage of PyTorch in similar research.",
          "quote": "This information wasn't explicitly stated in the provided content but considering the nature of the implementation and standard practices in the field of computer vision and language models, PyTorch is a reasonable inference."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "Given the context, it is reasonable to assume that the original paper of PyTorch was referenced for its use in this implementation.",
          "quote": "Automatic Differentiation in PyTorch"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2853,
    "prompt_tokens": 26149,
    "total_tokens": 29002
  }
}