{
  "paper": "QT4tXTqTTr.txt",
  "words": 886,
  "extractions": {
    "title": {
      "value": "Goal Misgeneralization as Implicit Goal Conditioning",
      "justification": "The provided document is a research paper with this title.",
      "quote": "Goal Misgeneralization as Implicit Goal Conditioning"
    },
    "description": "The paper investigates goal misgeneralization in reinforcement learning by examining how agents conditioned on specific goals can misgeneralize and pursue unintended goals due to subtle environmental features. Experiments involve training agents in a 4x4 gridworld environment under different conditions to study goal ambiguity and its impacts.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper is based on experiments and evaluations conducted in a simulated environment with agents to study goal misgeneralization.",
      "quote": "To elicit this behavior, we create a simple 4x4 gridworld environment with three goal types... We train deep reinforcement learning agents with PPO [4], providing goal conditioning as a part of the agent’s observations."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper primarily discusses issues related to reinforcement learning, particularly goal misgeneralization in RL agents.",
        "quote": "While many examples of goal misspecification [2] have been dissected in the reinforcement learning literature, few works have focused on the relatively new goal misgeneralization."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Goal Misgeneralization",
          "justification": "The paper focuses on the concept of goal misgeneralization within reinforcement learning.",
          "quote": "Langosco et al. [3] identify a few simple examples of goal misgeneralization."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "PPO",
          "justification": "Proximal Policy Optimization (PPO) is the reinforcement learning algorithm used for training agents in the experiments.",
          "quote": "We train deep reinforcement learning agents with PPO [4], providing goal conditioning as a part of the agent’s observations."
        },
        "aliases": [
          "Proximal Policy Optimization"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "PPO is not a novel contribution of this paper; it is an existing algorithm referenced for use in experiments.",
          "quote": "We train deep reinforcement learning agents with PPO [4]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The PPO model was executed as part of the experiments described in the paper.",
          "quote": "We train deep reinforcement learning agents with PPO [4]"
        },
        "is_compared": {
          "value": 0,
          "justification": "PPO was used as the primary model for experimentation and was not compared numerically with other models.",
          "quote": "Our initial results are visible in Figure 2. When trained in the typical colorblind setting..."
        },
        "referenced_paper_title": {
          "value": "Proximal policy optimization algorithms, 2017",
          "justification": "This is the title of the reference paper in which the PPO algorithm was introduced.",
          "quote": "Proximal policy optimization algorithms, 2017."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ProcGen CoinRun",
          "justification": "The CoinRun environment from the ProcGen benchmark is used to illustrate a point related to goal misgeneralization.",
          "quote": "For instance, an agent trained in ProcGen’s CoinRun environment [1]..."
        },
        "aliases": [
          "CoinRun"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Leveraging procedural generation to benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019",
          "justification": "This is the title of the reference paper for the ProcGen CoinRun environment mentioned in the text.",
          "quote": "Leveraging procedural generation to benchmark reinforcement learning. arXiv preprint arXiv:1912.01588, 2019."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PPO",
          "justification": "PPO is both a model and a library used in the experiments conducted in the paper.",
          "quote": "We train deep reinforcement learning agents with PPO [4], providing goal conditioning as a part of the agent’s observations."
        },
        "aliases": [
          "Proximal Policy Optimization"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Proximal policy optimization algorithms, 2017",
          "justification": "This is the title of the reference paper in which the PPO algorithm and library were introduced.",
          "quote": "Proximal policy optimization algorithms, 2017."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 870,
    "prompt_tokens": 2600,
    "total_tokens": 3470
  }
}