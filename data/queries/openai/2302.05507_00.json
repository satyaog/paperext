{
  "paper": "2302.05507.txt",
  "words": 9270,
  "extractions": {
    "title": {
      "value": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments",
      "justification": "This is the exact title mentioned at the beginning of the paper and in the title area.",
      "quote": "Language Decision Transformers with Exponential Tilt for Interactive Text Environments"
    },
    "description": "This paper proposes Language Decision Transformers (LDTs), a framework that integrates transformer language models with decision transformers to address challenges in text-based game environments. The LDTs framework introduces exponential tilt for guiding agents towards high goals, novel goal conditioning methods, and a model for predicting future observations.",
    "type": {
      "value": "Empirical Study",
      "justification": "The research involves conducting experiments to measure the performance of the proposed Language Decision Transformers on various text-based games and comparing them to existing methods.",
      "quote": "Our experiments show that LDTs achieve the highest scores among many different types of agents on some of the most challenging Jericho games, such as Enchanter."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on leveraging transformer language models and decision transformers, which are core elements in the field of Natural Language Processing.",
        "quote": "Recently, the excitement around the use of Large Language Models (LLMs) for dialogue has brought the setting of interactive dialogue into the spotlight."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper addresses challenges in text-based game environments using reinforcement learning techniques.",
          "quote": "Text-based game environments are challenging because agents must deal with long sequences of text, execute compositional actions using text and learn from sparse rewards."
        },
        "aliases": [
          "RL"
        ]
      },
      {
        "name": {
          "value": "Game Playing",
          "justification": "The research uses interactive text-based games to validate the proposed models.",
          "quote": "We test our proposed solutions on 33 different, realistic and complex text environments and show that LDTs performs 10% better than previous baselines on the hardest environments, and up to 30% better on average across all environments."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Language Decision Transformers",
          "justification": "The primary model introduced and analyzed in the paper is the Language Decision Transformer.",
          "quote": "We address these challenges by proposing Language Decision Transformers (LDTs), a framework that is based on transformer language models and decision transformers (DTs)."
        },
        "aliases": [
          "LDTs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "LDTs is introduced as a novel framework in the scope of the paper.",
          "quote": "We address these challenges by proposing Language Decision Transformers (LDTs), a framework that is based on transformer language models and decision transformers (DTs)."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments are run using the LDTs framework as the main focus of the study.",
          "quote": "To evaluate our approach, we conducted extensive experiments on 33 different complex text environments."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of LDTs is compared against various other models in the experiments.",
          "quote": "We test our proposed solutions on 33 different realistic and complex text environments and show that LDTs perform 10% better than previous baselines on the hardest environments."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "LDTs is a newly proposed model in the paper itself, so there's no referenced paper that introduces LDTs.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Jericho",
          "justification": "The Jericho text-based games are used extensively for training and evaluating the proposed models.",
          "quote": "In this work, we use the Jericho text games (Hausknecht et al., 2020), which provide a single golden path trajectory per game."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Interactive fiction games: A colossal adventure",
          "justification": "The dataset Jericho refers to the paper 'Interactive fiction games: A colossal adventure' by Hausknecht et. al., 2020.",
          "quote": "We use the Jericho text games (Hausknecht et al., 2020), which provide a single golden path trajectory per game."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Transformers",
          "justification": "The study builds upon pre-trained transformer models for its experimental setup.",
          "quote": "We use the pre-trained LongT5-base model as hosted by HuggingFace (Wolf et al., 2020) in all experiments as the base for our encoder-decoder architecture."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-art natural language processing",
          "justification": "The library Transformers refer to the paper 'Transformers: State-of-the-art natural language processing' by Wolf et al., 2020.",
          "quote": "We use the pre-trained LongT5-base model as hosted by HuggingFace (Wolf et al., 2020) in all experiments as the base for our encoder-decoder architecture."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2280,
    "prompt_tokens": 35431,
    "total_tokens": 37711
  }
}