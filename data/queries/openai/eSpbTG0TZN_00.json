{
  "paper": "eSpbTG0TZN.txt",
  "words": 10909,
  "extractions": {
    "title": {
      "value": "Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels",
      "justification": "Title is clearly mentioned in the document.",
      "quote": "Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels"
    },
    "description": "The paper proposes a new method for mastering the Unsupervised Reinforcement Learning Benchmark (URLB) from visual sensory data using unsupervised model-based RL for pre-training the agent, and a task-aware fine-tuning strategy combined with a novel hybrid planner, Dyna-MPC. The method shows superior performance on URLB and demonstrates robustness on Real-World RL benchmarks.",
    "type": {
      "value": "Empirical",
      "justification": "The paper involves a large-scale empirical study to validate the method.",
      "quote": "The approach is empirically evaluated through a large-scale empirical study, which we use to validate our design choices and analyze our models."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper primarily focuses on advancements in unsupervised reinforcement learning.",
        "quote": "Modern successes of deep reinforcement learning (RL) have shown promising results for control problems... To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model-Based Reinforcement Learning",
          "justification": "The paper adopts a world-model-based approach for pre-training the agent.",
          "quote": "we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Visual Reinforcement Learning",
          "justification": "The paper addresses controlling artificial agents from pixel-based visual sensory data.",
          "quote": "Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "DreamerV2",
          "justification": "The DreamerV2 model is grounded upon to learn a world model predicting the outcomes of actions in the environment.",
          "quote": "In this work, we ground upon the DreamerV2 agent (Hafner et al., 2021), which learns a world model (Ha & Schmidhuber, 2018; Hafner et al., 2019b) predicting the outcomes of actions in the environment."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not a novel contribution but rather an existing model used within the research.",
          "quote": "we ground upon the DreamerV2 agent (Hafner et al., 2021)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The DreamerV2 model was executed to validate the proposed method.",
          "quote": "we ground upon the DreamerV2 agent...The dynamics is captured into a latent space Z, providing a compact representation of the high-dimensional inputs."
        },
        "is_compared": {
          "value": 1,
          "justification": "DreamerV2 is compared numerically to other models within the study.",
          "quote": "Plan2Explore performance can also improve significantly when combined with our introduced adaptation strategies."
        },
        "referenced_paper_title": {
          "value": "Mastering Atari with Discrete World Models",
          "justification": "The referenced paper is mentioned with the DreamerV2 model.",
          "quote": "DreamerV2 (Hafner et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Unsupervised RL Benchmark (URLB)",
          "justification": "The primary dataset used in the paper is the Unsupervised RL Benchmark (URLB).",
          "quote": "Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. (2021)), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings."
        },
        "aliases": [
          "URLB"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "URLB: Unsupervised Reinforcement Learning Benchmark",
          "justification": "The referenced paper is clearly mentioned in relation to the dataset.",
          "quote": "Unsupervised RL Benchmark (URLB; Laskin et al. (2021))"
        }
      },
      {
        "name": {
          "value": "Real-World RL benchmark",
          "justification": "The Real-World RL benchmark was used to validate the robustness of the proposed method.",
          "quote": "We also show robust performance on the Real-Word RL benchmark, hinting at resiliency to environment perturbations during adaptation."
        },
        "aliases": [
          "RWRL"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "An empirical investigation of the challenges of real world reinforcement learning",
          "justification": "The referenced paper is mentioned in relation to the dataset.",
          "quote": "Real-World RL benchmark (Dulac-Arnold et al., 2020)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TensorFlow",
          "justification": "The paper mentions the usage of TensorFlow for implementing neural network models.",
          "quote": "The hyperparameters for the agent, which we keep fixed across all domains and tasks, can be found in Appendix I."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
          "justification": "The referenced paper for TensorFlow is appropriately cited.",
          "quote": "TensorFlow"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2455,
    "prompt_tokens": 38230,
    "total_tokens": 40685
  }
}