{
  "paper": "zhES2B5mdv.txt",
  "words": 8402,
  "extractions": {
    "title": {
      "value": "NOISY ZSC: BREAKING THE COMMON KNOWLEDGE ASSUMPTION IN ZERO-SHOT COORDINATION GAMES",
      "justification": "Title of the paper",
      "quote": "NOISY ZSC: BREAKING THE COMMON KNOWLEDGE ASSUMPTION IN ZERO-SHOT COORDINATION GAMES"
    },
    "description": "This paper introduces the concept of noisy zero-shot coordination (NZSC), addressing the limitation of the common knowledge assumption in traditional zero-shot coordination (ZSC) games. It formulates NZSC where agents observe different noisy versions of the ground truth environments and explores how agents can achieve effective coordination under these conditions through various toy environments and empirical analyses.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experimental results and empirical analyses to study the performance and coordination of agents in NZSC settings.",
      "quote": "Through experimental results, we further establish that ignoring the noise in problem specification can result in sub-par ZSC coordination performance, especially in iterated scenarios."
    },
    "primary_research_field": {
      "name": {
        "value": "Multi-Agent Reinforcement Learning",
        "justification": "The paper focuses on zero-shot coordination and noisy zero-shot coordination problems which are central topics in Multi-Agent Reinforcement Learning.",
        "quote": "Multi-Agent Reinforcement Learning: A general fully cooperative multi-agent reinforcement learning problem (MARL) with n-agents is represented as a Decentralized Partially Observable Markov Decision Process (Dec-POMDP)"
      },
      "aliases": [
        "MARL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Coordination Games",
          "justification": "The paper specifically addresses coordination in multi-agent systems through zero and noisy zero-shot coordination games.",
          "quote": "The zero-shot coordination problem, introduced by (Hu et al., 2020), modifies the optimization objective to maximize reward when paired with a novel partner"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The empirical analyses involve the use of reinforcement learning techniques in the context of coordination games.",
          "quote": "We exploit this by using IPPO-OP, a standard ZSC algorithm (Hu et al., 2020), in our experiments."
        },
        "aliases": [
          "RL"
        ]
      },
      {
        "name": {
          "value": "Game Theory",
          "justification": "The study of zero-shot coordination and the inclusion of noise to simulate realistic conditions align with game-theoretic concepts.",
          "quote": "Common Knowledge: Common knowledge for a group of agents consists of facts that all agents know and “each individual knows that all other individuals know it, each individual knows that all other individuals know that all the individuals know it, and so on”"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Independent PPO (IPPO)",
          "justification": "The paper mentions the use of IPPO both with self-play and other-play objectives in the experiments.",
          "quote": "IPPO: Independent PPO (IPPO) (Yu et al., 2022) is a popular MARL algorithm in which all the agents are updated individually using PPO based on their own experiences and rewards."
        },
        "aliases": [
          "IPPO",
          "IPPO-SP",
          "IPPO-OP"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "IPPO is not introduced as a novel contribution but is used as part of the experimental methodology.",
          "quote": "In our experiments, we use IPPO with self-play and other-play objectives (Hu et al., 2020) which we respectively term IPPO-SP and IPPO-OP."
        },
        "is_executed": {
          "value": 1,
          "justification": "IPPO is executed in the experiments to study coordination performance.",
          "quote": "In our experiments, we use IPPO with self-play and other-play objectives"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of IPPO strategies under different settings and with different noise levels is compared in the paper.",
          "quote": "To do so, we develop various environments for the NZSC problem and empirically analyze the policies learned by the agents in these environments."
        },
        "referenced_paper_title": {
          "value": "The surprising effectiveness of ppo in cooperative, multi-agent games",
          "justification": "This is the reference paper for IPPO as cited in the research paper.",
          "quote": "IPPO: Independent PPO (IPPO) (Yu et al., 2022)"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1066,
    "prompt_tokens": 14888,
    "total_tokens": 15954
  }
}