{
  "paper": "2307.07457.txt",
  "words": 7133,
  "extractions": {
    "title": {
      "value": "Structured Pruning of Neural Networks for Constraints Learning",
      "justification": "This is the title of the paper provided by the user.",
      "quote": "Structured Pruning of Neural Networks for Constraints Learning"
    },
    "description": "The paper demonstrates the effectiveness of pruning artificial neural networks in accelerating the solution time of mixed-integer programming problems that incorporate ANNs. It emphasizes the advantages of structured pruning over unstructured pruning, particularly for improving the scalability of embedding ANNs into MIPs. The paper includes experimental results using feed-forward neural networks to validate the approach.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results to validate the effectiveness of structured pruning on feed-forward neural networks.",
      "quote": "To highlight the potential of this approach, we conduct experiments using feed-forward neural networks with multiple layers to construct adversarial examples. Our results demonstrate that pruning offers remarkable reductions in solution times without hindering the quality of the final decision."
    },
    "primary_research_field": {
      "name": {
        "value": "Model Compression",
        "justification": "The main focus of the paper is on pruning techniques, which are a significant part of model compression.",
        "quote": "We discuss why pruning is more suitable in this context compared to other ML compression techniques, and we identify the most appropriate pruning strategies."
      },
      "aliases": [
        "Model Compression"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Artificial Neural Networks",
          "justification": "The pruning techniques discussed are applied to artificial neural networks.",
          "quote": "In particular, we employ the Structured Perspective Regularization, i.e., we train the ANN by adding the SPR term to the loss, which will lead to a weight tensor with a structured sparsity."
        },
        "aliases": [
          "ANNs",
          "Neural Networks"
        ]
      },
      {
        "name": {
          "value": "Mixed Integer Programming",
          "justification": "The paper deals with the embedding of pruned ANNs into Mixed Integer Programming formulations.",
          "quote": "In this paper, we showcase the effectiveness of pruning, one of these techniques, when applied to ANNs prior to their integration into MIPs."
        },
        "aliases": [
          "MIPs",
          "Mixed-Integer Linear Programs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Feed-forward Neural Networks",
          "justification": "The experiments conducted in the paper use feed-forward neural networks.",
          "quote": "To highlight the potential of this approach, we conduct experiments using feed-forward neural networks with multiple layers to construct adversarial examples."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Feed-forward neural networks are mentioned as part of the experimental setup but are not introduced as a novel contribution.",
          "quote": "we conduct experiments using feed-forward neural networks with multiple layers"
        },
        "is_executed": {
          "value": 1,
          "justification": "The feed-forward neural networks were executed and tested as part of the experiments.",
          "quote": "we conduct experiments using feed-forward neural networks"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of pruned and unpruned feed-forward neural networks is compared in the experiments.",
          "quote": "Table 1 shows the results using âˆ† = 5 on 4 different architectures with an increasing number of neurons and layers. When pruning small architectures, like the 2x50 and 2x100 networks, pruning the ANN results in at least halving the time used by Gurobi."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no directly referenced prior paper for Feed-forward Neural Networks.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset was used to test the effectiveness of the pruning techniques discussed.",
          "quote": "To test the effectiveness of our pruning techniques, we ran some experiments on network robustness using the MNIST dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no directly referenced prior paper for MNIST.",
          "quote": "N/A"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is mentioned as one of the ML packages compatible with the tools discussed.",
          "quote": "The maturity of the field is demonstrated by the fact that one of the leading commercial MIP solvers, Gurobi, recently released a package that allows feed-forward ReLU networks to be part of MIP formulations, with compatibility for popular ML packages such as PyTorch, Keras, and scikit-learn."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no directly referenced prior paper for PyTorch.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Gurobi",
          "justification": "Gurobi is mentioned multiple times as the solver used for the experiments.",
          "quote": "Indeed, the Optimized Big-M Bounds Tightening (OBBT) method has been developed in [11] to find effective values for this constant. As previously mentioned, the state-of-the-art solver Gurobi now includes an open-source Python package that automatically embeds ANNs with ReLU activation into a Gurobi model."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep neural networks and mixed integer linear optimization",
          "justification": "This reference is mentioned in relation to Gurobi's capabilities and methods.",
          "quote": "Indeed, the Optimized Big-M Bounds Tightening (OBBT) method has been developed in [11] to find effective values for this constant."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1375,
    "prompt_tokens": 15391,
    "total_tokens": 16766
  }
}