{
  "paper": "2212.04025.txt",
  "words": 9631,
  "extractions": {
    "title": {
      "value": "A Novel Stochastic Gradient Descent Algorithm for Learning Principal Subspaces",
      "justification": "Title of the paper provided by the user specifies this.",
      "quote": "A Novel Stochastic Gradient Descent Algorithm for Learning Principal Subspaces"
    },
    "description": "This paper focuses on developing a novel stochastic gradient descent algorithm for learning the principal subspaces of very large or infinite dimensional matrices, such as those encountered in neuroscience, image compression, or deep reinforcement learning. The algorithm is designed to derive a principal subspace from sample entries and can be integrated with neural networks.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes both theoretical analysis and experimental results on synthetic matrices, MNIST dataset, and the PuddleWorld environment to demonstrate the practical utility of the proposed method.",
      "quote": "We complement our theoretical analysis with a series of experiments on synthetic matrices, the MNIST dataset (LeCun, 1998) and the reinforcement learning domain PuddleWorld (Sutton, 1995) demonstrating the usefulness of our approach."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning Algorithms",
        "justification": "The paper focuses on developing a new stochastic gradient descent algorithm, a key aspect of machine learning.",
        "quote": "Our method consists in defining a loss function whose minimizer is the desired principal subspace, and constructing a gradient estimate of this loss whose bias can be controlled."
      },
      "aliases": [
        "ML Algorithms",
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Networks",
          "justification": "The proposed method specifically addresses how to learn principal subspaces when the subspace is represented by a neural network.",
          "quote": "Although a number of sample-based methods exist for this problem (e.g. Ojaâ€™s rule (Oja, 1982)), these assume access to full columns of the matrix or particular matrix structure such as symmetry and cannot be combined as-is with neural networks (Baldi and Hornik, 1989)."
        },
        "aliases": [
          "Deep Learning"
        ]
      },
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper applies its newly proposed algorithm in the domain of reinforcement learning, specifically in the PuddleWorld environment.",
          "quote": "We further demonstrate the effectiveness of our method for representation learning in reinforcement learning, specifically by learning a neural network-based approximation to the principal subspace of the successor measure (Blier et al., 2021) in the Puddle World domain (Sutton, 1995)."
        },
        "aliases": [
          "RL"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The MNIST dataset, a widely used dataset in computer vision, is used for experimental validation of the proposed algorithm.",
          "quote": "We complement our theoretical analysis with a series of experiments on synthetic matrices, the MNIST dataset (LeCun, 1998) and the reinforcement learning domain PuddleWorld (Sutton, 1995) demonstrating the usefulness of our approach."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset was used for experimental validation of the proposed algorithm.",
          "quote": "We complement our theoretical analysis with a series of experiments on synthetic matrices, the MNIST dataset (LeCun, 1998) and the reinforcement learning domain PuddleWorld (Sutton, 1995) demonstrating the usefulness of our approach."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The MNIST Database of Handwritten Digits",
          "justification": "The referenced version of the MNIST dataset paper is most likely 'The MNIST Database of Handwritten Digits' by LeCun et al.",
          "quote": "the MNIST dataset (LeCun, 1998)"
        }
      },
      {
        "name": {
          "value": "PuddleWorld",
          "justification": "PuddleWorld is used to demonstrate the capabilities of the proposed algorithm in the domain of reinforcement learning.",
          "quote": "We further demonstrate the effectiveness of our method for representation learning in reinforcement learning, specifically by learning a neural network-based approximation to the principal subspace of the successor measure (Blier et al., 2021) in the Puddle World domain (Sutton, 1995)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding",
          "justification": "The referenced version of the PuddleWorld paper is most likely 'Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding' by Sutton.",
          "quote": "Puddle World domain (Sutton, 1995)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TensorFlow",
          "justification": "The authors acknowledge the use of TensorFlow, a deep learning library, for their experiments.",
          "quote": "We would also like to thank the Python community (Van Rossum and Drake Jr, 1995; Oliphant, 2007) for developing tools that enabled this work, including NumPy (Oliphant, 2006; Walt et al., 2011; Harris et al., 2020), SciPy (Jones et al., 2001), Matplotlib (Hunter, 2007) and JAX (Bradbury et al., 2018)."
        },
        "aliases": [
          "TF"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
          "justification": "The referenced paper for TensorFlow is 'TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems' by Abadi et al.",
          "quote": " including NumPy (Oliphant, 2006; Walt et al., 2011; Harris et al., 2020), SciPy (Jones et al., 2001), Matplotlib (Hunter, 2007) and JAX (Bradbury et al., 2018)."
        }
      },
      {
        "name": {
          "value": "JAX",
          "justification": "JAX, a library for high-performance machine learning research, was utilized in this work.",
          "quote": "We would also like to thank the Python community (Van Rossum and Drake Jr, 1995; Oliphant, 2007) for developing tools that enabled this work, including NumPy (Oliphant, 2006; Walt et al., 2011; Harris et al., 2020), SciPy (Jones et al., 2001), Matplotlib (Hunter, 2007) and JAX (Bradbury et al., 2018)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "JAX: Composable transformations of Python+NumPy programs",
          "justification": "The referenced paper for JAX is 'JAX: Composable transformations of Python+NumPy programs' by Bradbury et al.",
          "quote": "including NumPy (Oliphant, 2006; Walt et al., 2011; Harris et al., 2020), SciPy (Jones et al., 2001), Matplotlib (Hunter, 2007) and JAX (Bradbury et al., 2018)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1693,
    "prompt_tokens": 17328,
    "total_tokens": 19021
  }
}