{
  "paper": "V04YoiX4Zq.txt",
  "words": 8420,
  "extractions": {
    "title": {
      "value": "Challenging the Foundations: Mining Hard Test Samples through Diffusion Generation",
      "justification": "Paper's title in the document.",
      "quote": "Challenging the Foundations: Mining Hard Test Samples through Diffusion Generation"
    },
    "description": "The paper proposes DiffusionExplorer, a framework that efficiently probes large vision foundation models for their vulnerabilities using diffusion generation. It generates test sets with novel combinations of objects and nuisances to expose model weaknesses, significantly reducing the accuracy of models like MiniGPT-4 and LLaVa. This suggests that generative models can be an effective source for creating challenging test data.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments and collects data to test diffusion-generated test samples on foundation models like MiniGPT-4 and LLaVa to evaluate their vulnerabilities.",
      "quote": "Experimental results show that our mined test samples are challenging to foundation models, such as MiniGPT-4 and LLaVa, significantly reducing their accuracy by 29.56% and 39.96%, respectively."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on evaluating and improving the robustness of large vision foundation models using new test generation techniques.",
        "quote": "This work proposes the first framework that mines hard test images of large foundation models through diffusion generation, suggesting generative models can be an effective data source for testing foundation models."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Robustness",
          "justification": "The paper investigates the vulnerabilities of large foundation models by creating challenging test sets with novel object and background combinations.",
          "quote": "We show that our framework can efficiently construct a test set with novel combinations of object and nuisance factors to expose the failures of foundation models."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Generative Models",
          "justification": "The paper uses diffusion-based generative models to create novel and challenging test samples for evaluating foundation models.",
          "quote": "We introduce DiffusionExplorer, a framework that is able to identify challenging test images for large foundation models using diffusion generation."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Vision-Language Models",
          "justification": "The paper tests the vulnerabilities of vision-language models like MiniGPT-4 and LLaVa using its novel test sets.",
          "quote": "Experiments show that our test set significantly reduces the test accuracy of new foundation models, suggesting that our test set poses a general challenge for large foundation models. For MiniGPT-4 and LLaVa, our test set reduces the accuracy by 29.56% and 39.96%, respectively."
        },
        "aliases": [
          "VLM"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DiffusionExplorer",
          "justification": "DiffusionExplorer is introduced in the paper as a framework to discover challenging test samples for large vision foundation models using diffusion generation techniques.",
          "quote": "We introduce DiffusionExplorer, a framework that is able to identify challenging test images for large foundation models using diffusion generation (Sohl-Dickstein et al., 2015; Ho et al., 2020)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The model is the main contribution of the paper.",
          "quote": "We introduce DiffusionExplorer, a framework that is able to identify challenging test images for large foundation models using diffusion generation."
        },
        "is_executed": {
          "value": 1,
          "justification": "The framework involves generating images and evaluating model performance on these images.",
          "quote": "Our work suggests that generative models can be viewed as an effective data source in finding the vulnerability of large vision foundation models."
        },
        "is_compared": {
          "value": 0,
          "justification": "The primary focus is to evaluate foundation models using DiffusionExplorer-generated data rather than comparing DiffusionExplorer to other models.",
          "quote": "We show that our framework can efficiently construct a test set with novel combinations of object and nuisance factors to expose the failures of foundation models."
        },
        "referenced_paper_title": {
          "value": "Denoising Diffusion Probabilistic Models",
          "justification": "The diffusion generation technique is based on the work presented in this referenced paper.",
          "quote": "We introduce DiffusionExplorer, a framework that is able to identify challenging test images for large foundation models using diffusion generation (Sohl-Dickstein et al., 2015; Ho et al., 2020)."
        }
      },
      {
        "name": {
          "value": "MiniGPT-4",
          "justification": "MiniGPT-4 is mentioned as one of the foundation models tested using the new framework.",
          "quote": "Experimental results show that our mined test samples are challenging to foundation models, such as MiniGPT-4 and LLaVa, significantly reducing their accuracy by 29.56% and 39.96%, respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used for evaluation purposes and is not contributed by the paper.",
          "quote": "Experimental results show that our mined test samples are challenging to foundation models, such as MiniGPT-4 and LLaVa, significantly reducing their accuracy by 29.56% and 39.96%, respectively."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was tested on the diffusion-generated data.",
          "quote": "Experimental results show that our mined test samples are challenging to foundation models, such as MiniGPT-4 and LLaVa."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of MiniGPT-4 is compared to its performance on other test sets.",
          "quote": "Our test set significantly reduces the performance of large foundation models, e.g., an accuracy decrease of 29.56% for MiniGPT-4."
        },
        "referenced_paper_title": {
          "value": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
          "justification": "The reference to the paper where MiniGPT-4 is introduced.",
          "quote": "Two followups LLaVa (Liu et al., 2023) and MiniGPT-4 (Zhu et al., 2023a) exhibit many capabilities similar to GPT-4 by visual instruction tuning and adopting a large language model Vicuna (Chiang et al., 2023), respectively."
        }
      },
      {
        "name": {
          "value": "LLaVa",
          "justification": "LLaVa is another foundation model evaluated with the proposed framework.",
          "quote": "Experimental results show that our mined test samples are challenging to foundation models, such as MiniGPT-4 and LLaVa, significantly reducing their accuracy by 29.56% and 39.96%, respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used for evaluation, not contributed by the paper.",
          "quote": "Experimental results show that our mined test samples are challenging to foundation models, such as MiniGPT-4 and LLaVa, significantly reducing their accuracy by 29.56% and 39.96%, respectively."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was tested on the diffusion-generated data.",
          "quote": "Experimental results show that our mined test samples are challenging to foundation models, such as MiniGPT-4 and LLaVa."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of LLaVa is compared to its performance on other datasets.",
          "quote": "Our test set significantly reduces the performance of large foundation models, e.g., an accuracy decrease of 39.96% for LLaVa."
        },
        "referenced_paper_title": {
          "value": "Visual Instruction Tuning",
          "justification": "The reference indicating the paper in which LLaVa was introduced.",
          "quote": "Two followups LLaVa (Liu et al., 2023) and MiniGPT-4 (Zhu et al., 2023a) exhibit many capabilities similar to GPT-4 by visual instruction tuning and adopting a large language model Vicuna (Chiang et al., 2023), respectively."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet-A",
          "justification": "The dataset is mentioned in the context of evaluating vision models by filtering out samples from online collections.",
          "quote": "Prior work studied how to collect hard test samples for vision models. ImageNet-A (Hendrycks et al., 2021) filters out samples from online image collections."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Natural Adversarial Examples",
          "justification": "The reference to the original paper about the ImageNet-A dataset.",
          "quote": "Prior work studied how to collect hard test samples for vision models. ImageNet-A (Hendrycks et al., 2021) filters out samples from online image collections."
        }
      },
      {
        "name": {
          "value": "Stylized-ImageNet",
          "justification": "This dataset is used to alter the styles of ImageNet images for evaluation purposes.",
          "quote": "Stylized-ImageNet (Geirhos et al., 2018) alters the styles of ImageNet images (Deng et al., 2009)."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",
          "justification": "The reference to the original paper introducing the Stylized-ImageNet.",
          "quote": "Stylized-ImageNet (Geirhos et al., 2018) alters the styles of ImageNet images (Deng et al., 2009)."
        }
      },
      {
        "name": {
          "value": "ObjectNet",
          "justification": "ObjectNet is mentioned as a dataset that controls backgrounds by manual collection, serving as a point of comparison for the proposed method.",
          "quote": "ObjectNet (Barbu et al., 2019) controls the backgrounds by manually collecting images in 4 different scenes. However, to achieve this, ObjectNet (Barbu et al., 2019) requires 5982 workers to collect images at their homes."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "ObjectNet: A Large-Scale Bias-Controlled Dataset for Pushing the Limits of Object Recognition Models",
          "justification": "The reference to the original paper introducing ObjectNet.",
          "quote": "ObjectNet (Barbu et al., 2019) controls the backgrounds by manually collecting images in 4 different scenes."
        }
      },
      {
        "name": {
          "value": "Broden dataset",
          "justification": "The paper uses backgrounds from the Broden dataset to combine them with objects for generating test sample images.",
          "quote": "We generate images using 113 categories from ImageNet and ObjectNet overlap and 468 backgrounds from the Broden dataset (Bau et al., 2017), resulting in a total of 52884 object and background combinations."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Network Dissection: Quantifying Interpretability of Deep Visual Representations",
          "justification": "The reference to the original paper about the Broden dataset.",
          "quote": "We generate images using 113 categories from ImageNet and ObjectNet overlap and 468 backgrounds from the Broden dataset (Bau et al., 2017), resulting in a total of 52884 object and background combinations."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stable Diffusion",
          "justification": "The paper uses Stable Diffusion for high-fidelity image synthesis to create images with novel combinations.",
          "quote": "We use the pioneering Stable Diffusion model (Rombach et al., 2022) to obtain images with novel combinations of objects and nuisances, which allows us to create images that follow our text specifications with high fidelity."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "High-Resolution Image Synthesis with Latent Diffusion Models",
          "justification": "The reference paper detailing the use of Stable Diffusion.",
          "quote": "We use the pioneering Stable Diffusion model(Rombach et al., 2022) to obtain images with novel combinations of objects and nuisances, which allows us to create images that follow our text specifications with high fidelity."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2406,
    "prompt_tokens": 14905,
    "total_tokens": 17311
  }
}