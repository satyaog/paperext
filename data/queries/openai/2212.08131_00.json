{
  "paper": "2212.08131.txt",
  "words": 8578,
  "extractions": {
    "title": {
      "value": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies",
      "justification": "The title of the paper is \"Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies\"",
      "quote": "Bridging the Gap Between Offline and Online Reinforcement Learning Evaluation Methodologies"
    },
    "description": "This paper discusses the limitations of current offline reinforcement learning (RL) evaluation methods and proposes a new sequential evaluation approach. The goal is to provide more insight into the data efficiency of RL algorithms and their robustness to changes in data distribution. Several existing offline RL algorithms are compared using this new approach.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments and comparisons of several existing offline reinforcement learning algorithms using the proposed sequential evaluation approach.",
      "quote": "We compare several existing offline RL algorithms using this approach and present insights from a variety of tasks and offline datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on evaluating reinforcement learning algorithms, specifically offline RL algorithms.",
        "quote": "Reinforcement learning (RL) has shown great promise with algorithms learning in environments with large state and action spaces purely from scalar reward signals."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Data Efficiency",
          "justification": "The paper aims to provide insights into the data efficiency of RL algorithms.",
          "quote": "Sequential evaluation provides valuable insights into the data efficiency of the learning process and the robustness of algorithms to distribution changes in the dataset while also harmonizing the visualization of the offline and online learning phases."
        },
        "aliases": [
          "Data Efficiency"
        ]
      },
      {
        "name": {
          "value": "Evaluation Methodologies",
          "justification": "The paper proposes a new sequential evaluation approach for offline RL algorithms.",
          "quote": "In this paper, we propose to evaluate algorithms as a function of available data instead of just reporting final performance or plotting learning curves over a number of gradient steps."
        },
        "aliases": [
          "Evaluation Methodologies"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Implicit Q Learning (IQL)",
          "justification": "The paper lists Implicit Q Learning (IQL) as one of the algorithms they evaluate.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Implicit Q Learning (IQL) (Kostrikov et al., 2022)."
        },
        "aliases": [
          "IQL"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Implicit Q Learning (IQL) is mentioned as an existing algorithm evaluated using the proposed method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Implicit Q Learning (IQL) (Kostrikov et al., 2022)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions that the algorithm was evaluated using the proposed sequential evaluation method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Implicit Q Learning (IQL) (Kostrikov et al., 2022)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of IQL is compared numerically with other algorithm.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Implicit Q Learning (IQL) (Kostrikov et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "Offline Reinforcement Learning with Implicit Q-Learning",
          "justification": "The referenced paper title for Implicit Q Learning is provided.",
          "quote": "Offline Reinforcement Learning with Implicit Q-Learning"
        }
      },
      {
        "name": {
          "value": "Conservative Q Learning (CQL)",
          "justification": "The paper lists Conservative Q Learning (CQL) as one of the algorithms they evaluate.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Conservative Q Learning (CQL) (Kumar et al., 2020)."
        },
        "aliases": [
          "CQL"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Conservative Q Learning (CQL) is mentioned as an existing algorithm evaluated using the proposed method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Conservative Q Learning (CQL) (Kumar et al., 2020)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions that the algorithm was evaluated using the proposed sequential evaluation method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Conservative Q Learning (CQL) (Kumar et al., 2020)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of CQL is compared numerically with other algorithm.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Conservative Q Learning (CQL) (Kumar et al., 2020)."
        },
        "referenced_paper_title": {
          "value": "Conservative Q-Learning for Offline Reinforcement Learning",
          "justification": "The referenced paper title for Conservative Q Learning is provided.",
          "quote": "Conservative Q-Learning for Offline Reinforcement Learning"
        }
      },
      {
        "name": {
          "value": "TD3+BC",
          "justification": "The paper lists TD3+BC as one of the algorithms they evaluate.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely TD3+BC (Fujimoto & Gu, 2021)."
        },
        "aliases": [
          "TD3+BC"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "TD3+BC is mentioned as an existing algorithm evaluated using the proposed method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely TD3+BC (Fujimoto & Gu, 2021)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions that the algorithm was evaluated using the proposed sequential evaluation method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely TD3+BC (Fujimoto & Gu, 2021)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of TD3+BC is compared numerically with other algorithm.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely TD3+BC (Fujimoto & Gu, 2021)."
        },
        "referenced_paper_title": {
          "value": "A Minimalist Approach to Offline Reinforcement Learning",
          "justification": "The referenced paper title for TD3+BC is provided.",
          "quote": "A Minimalist Approach to Offline Reinforcement Learning"
        }
      },
      {
        "name": {
          "value": "AWAC",
          "justification": "The paper lists AWAC as one of the algorithms they evaluate.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely AWAC (Nair et al., 2020)."
        },
        "aliases": [
          "AWAC"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "AWAC is mentioned as an existing algorithm evaluated using the proposed method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely AWAC (Nair et al., 2020)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions that the algorithm was evaluated using the proposed sequential evaluation method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely AWAC (Nair et al., 2020)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of AWAC is compared numerically with other algorithm.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely AWAC (Nair et al., 2020)."
        },
        "referenced_paper_title": {
          "value": "Accelerating Online Reinforcement Learning with Offline Datasets",
          "justification": "The referenced paper title for AWAC is provided.",
          "quote": "Accelerating Online Reinforcement Learning with Offline Datasets"
        }
      },
      {
        "name": {
          "value": "BCQ",
          "justification": "The paper lists BCQ as one of the algorithms they evaluate.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely BCQ (Fujimoto et al., 2019)."
        },
        "aliases": [
          "BCQ"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "BCQ is mentioned as an existing algorithm evaluated using the proposed method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely BCQ (Fujimoto et al., 2019)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions that the algorithm was evaluated using the proposed sequential evaluation method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely BCQ (Fujimoto et al., 2019)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of BCQ is compared numerically with other algorithm.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely BCQ (Fujimoto et al., 2019)."
        },
        "referenced_paper_title": {
          "value": "Off-Policy Deep Reinforcement Learning without Exploration",
          "justification": "The referenced paper title for BCQ is provided.",
          "quote": "Off-Policy Deep Reinforcement Learning without Exploration"
        }
      },
      {
        "name": {
          "value": "Decision Transformer (DT)",
          "justification": "The paper lists Decision Transformer (DT) as one of the algorithms they evaluate.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Decision Transformer (DT) (Chen et al., 2021)."
        },
        "aliases": [
          "DT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Decision Transformer (DT) is mentioned as an existing algorithm evaluated using the proposed method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Decision Transformer (DT) (Chen et al., 2021)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions that the algorithm was evaluated using the proposed sequential evaluation method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Decision Transformer (DT) (Chen et al., 2021)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Decision Transformer (DT) is compared numerically with other algorithm.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Decision Transformer (DT) (Chen et al., 2021)."
        },
        "referenced_paper_title": {
          "value": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
          "justification": "The referenced paper title for Decision Transformer is provided.",
          "quote": "Decision Transformer: Reinforcement Learning via Sequence Modeling"
        }
      },
      {
        "name": {
          "value": "Behavior Cloning (BC)",
          "justification": "The paper lists Behavior Cloning (BC) as one of the algorithms they evaluate.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Behavior Cloning (BC)."
        },
        "aliases": [
          "BC"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Behavior Cloning (BC) is mentioned as an existing algorithm evaluated using the proposed method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Behavior Cloning (BC)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions that the algorithm was evaluated using the proposed sequential evaluation method.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Behavior Cloning (BC)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Behavior Cloning (BC) is compared numerically with other algorithm.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Behavior Cloning (BC)."
        },
        "referenced_paper_title": {
          "value": "NA",
          "justification": "Behavior Cloning is a well-known baseline and may not have a singular reference paper.",
          "quote": "We evaluate several existing offline RL algorithms using the sequential approach, namely Behavior Cloning (BC)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "D4RL Benchmark",
          "justification": "The paper uses the D4RL Benchmark for evaluating the RL algorithms.",
          "quote": "These algorithms were evaluated on the D4RL benchmark (Fu et al., 2020), which consists of three environments: Halfcheetah-v2, Walker2d-v2 and Hopper-v2."
        },
        "aliases": [
          "D4RL"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
          "justification": "The referenced paper title for D4RL Benchmark is provided.",
          "quote": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning"
        }
      },
      {
        "name": {
          "value": "DeepMind Control Suite (DMC)",
          "justification": "The paper uses the DeepMind Control Suite (DMC) dataset for evaluating the RL algorithms.",
          "quote": "We also created a dataset from the DeepMind Control Suite (DMC) (Tassa et al., 2018) environments following the same procedure as outlined by the authors of D4RL."
        },
        "aliases": [
          "DMC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepMind Control Suite",
          "justification": "The referenced paper title for DeepMind Control Suite (DMC) is provided.",
          "quote": "DeepMind Control Suite"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "rliable",
          "justification": "The paper mentions using the rliable library for plotting interval estimates of normalized performance measures.",
          "quote": "We present complete training curves on all twelve datasets that were used in Fig. 11 and final performance in Table 2. In addition to the curves, we compare the algorithms at the end of training with scores aggregated across environments. This is done using the rliable (Agarwal et al., 2021) library to plot interval estimates of normalized performance measures such as median, mean, interquartile mean (IQM) and optimality gap."
        },
        "aliases": [
          "rliable"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep Reinforcement Learning at the Edge of the Statistical Precipice",
          "justification": "The referenced paper title for rliable is provided.",
          "quote": "Deep Reinforcement Learning at the Edge of the Statistical Precipice"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2849,
    "prompt_tokens": 17043,
    "total_tokens": 19892
  }
}