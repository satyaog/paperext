{
  "paper": "2311.01248.txt",
  "words": 11442,
  "extractions": {
    "title": {
      "value": "Multimodal and Force-Matched Imitation Learning with a See-Through Visuotactile Sensor",
      "justification": "The provided information is directly obtained from the title of the research paper.",
      "quote": "Multimodal and Force-Matched Imitation Learning with a See-Through Visuotactile Sensor"
    },
    "description": "This paper investigates the use of a multimodal visuotactile sensor in the context of imitation learning for robotic manipulation tasks. It introduces two algorithmic contributions: tactile force matching and learned mode switching, which aid in improving the performance of imitation learning by enhancing kinesthetic teaching and coupling visual and tactile feedback. The experiments demonstrate the effectiveness of these methods in various robotic door opening tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves conducting robotic manipulation experiments and compares the performance across different configurations, which indicates that it is an empirical study.",
      "quote": "We perform robotic manipulation experiments on four door opening tasks with a variety of observation and method configurations to study the utility of our proposed improvements and multimodal visuotactile sensing."
    },
    "primary_research_field": {
      "name": {
        "value": "Robotic Manipulation",
        "justification": "The paper is primarily focused on robotic manipulation and improving performance in contact-rich tasks through imitation learning and the use of a visuotactile sensor.",
        "quote": "This paper aims to learn robot policies for door opening that are more in line with human manipulation, by leveraging high-resolution visual and tactile feedback to control the contact interactions between the robot end-effector and the handle."
      },
      "aliases": [
        "Robot Manipulation",
        "Manipulation"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Imitation Learning",
          "justification": "The paper discusses multiple contributions to improve imitation learning in robotics, specifically through kinesthetic teaching and learned mode switching.",
          "quote": "In this paper, we investigate how to leverage visuotactile sensing for imitation learning (IL) on a real robotic platform for contact-rich manipulation tasks."
        },
        "aliases": [
          "IL"
        ]
      },
      {
        "name": {
          "value": "Tactile Sensing",
          "justification": "The research leverages a see-through visuotactile sensor for contact feedback in robotic manipulation, making tactile sensing a significant aspect of the study.",
          "quote": "A recently-introduced see-through-your-skin (STS) multimodal optical sensor variant combines visual sensing with tactile sensing by leveraging a transparent membrane and controllable lighting."
        },
        "aliases": [
          "Tactile Sensing",
          "Visuotactile Sensing"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Behavior Cloning",
          "justification": "Behavior cloning is mentioned as the method used for imitation learning, and it is trained using the demonstration data collected during the experiments.",
          "quote": "(5) We train policies using some or all of STS, wrist camera, and relative pose data with behavior cloning."
        },
        "aliases": [
          "BC"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Behavior Cloning is a well-known technique in imitation learning and is not introduced as a new model in this paper.",
          "quote": "Behavior cloning [20], in which supervised learning is carried out on the expert demonstration set"
        },
        "is_executed": {
          "value": 1,
          "justification": "Behavior Cloning was executed as part of the experiments with various sensor inputs and force matching.",
          "quote": "We train policies using some or all of STS, wrist camera, and relative pose data with behavior cloning."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Behavior Cloning is compared across different configurations, such as with and without force matching and mode switching.",
          "quote": "We perform robotic manipulation experiments on four door opening tasks with a variety of observation and method configurations to study the utility of our proposed improvements and multimodal visuotactile sensing."
        },
        "referenced_paper_title": {
          "value": "Behavioral Cloning",
          "justification": "Behavior Cloning is referenced in the context of imitation learning and supervised learning from demonstration sets.",
          "quote": "Behavior cloning [20], in which supervised learning is carried out on the expert demonstration set"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Expert Demonstration Dataset",
          "justification": "The dataset consists of expert demonstrations used to train the behavior cloning model.",
          "quote": "We collect one expert dataset BE for each task separately using kinesthetic teaching, where the expert physically pushes the robot to generate demonstrations."
        },
        "aliases": [
          "Expert Dataset",
          "Demonstration Dataset"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The dataset is created and used within the scope of this paper and does not reference an external paper.",
          "quote": "We collect one expert dataset BE for each task separately using kinesthetic teaching, where the expert physically pushes the robot to generate demonstrations."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly mentioned as the library used for training the models in the paper.",
          "quote": "We trained our policies in PyTorch with the Adam optimizer and a learning rate of 0.0003, halving the learning rate halfway through training."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "PyTorch is commonly referenced in the context of automatic differentiation and deep learning framework, which is applicable here for training neural networks.",
          "quote": "We trained our policies in PyTorch with the Adam optimizer and a learning rate of 0.0003, halving the learning rate halfway through training."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1091,
    "prompt_tokens": 18915,
    "total_tokens": 20006
  }
}