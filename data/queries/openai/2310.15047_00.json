{
  "paper": "2310.15047.txt",
  "words": 13577,
  "extractions": {
    "title": {
      "value": "Meta - (Out - Of - Context) Learning in Neural Networks",
      "justification": "The provided text mentions 'Meta - (Out- Of - Context) Learning in Neural Networks' as the main title of the paper.",
      "quote": "M ETA - ( OUT- OF - CONTEXT ) LEARNING IN NEURAL NETWORKS"
    },
    "description": "The paper introduces and explores the phenomenon of meta-out-of-context learning (meta-OCL) in large language models (LLMs) and other deep learning models. The authors conduct synthetic experiments to show that LLMs preferentially internalize information from reliable sources (like Wikipedia) over unreliable ones (like 4chan), even when both contain the same factual content. They illustrate meta-OCL in NLP and computer vision settings and propose two hypotheses to explain its emergence.",
    "type": {
      "value": "Empirical Study",
      "justification": "The research involves extensive experimentation with synthetic datasets and various models to establish the existence of meta-OCL in practice.",
      "quote": "Our experiments show that OCL and meta-OCL can be observed in a wide range of settings, including in transformer models without pretraining, as well as an image classification setting."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paperâ€™s focus on phenomena experienced by large language models and other neural networks situates it firmly within the field of Deep Learning.",
        "quote": "Our results suggest that meta-OCL leads LLMs to more readily 'internalize' the semantic content of text that is, or appears to be, broadly useful..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Interpretability and Optimization",
          "justification": "The work explores how models internalize information and proposes mechanisms tied to optimization processes, thereby relating to how models learn and interpret data.",
          "quote": "We propose two hypotheses for the emergence of metaOCL: one relying on the way models store knowledge in their parameters, and another suggesting that the implicit gradient alignment bias of gradient-descentbased optimizers may be responsible."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Pythia",
          "justification": "The Pythia suite of models was used to demonstrate the primary phenomenon of meta-OCL.",
          "quote": "Our experiments on LLMs in Section 2 span several sizes of language models from the Pythia suite (Biderman et al., 2023)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "T5",
          "justification": "The T5 model was another example used to demonstrate meta-OCL.",
          "quote": "We also replicate our results with models GPT-Neo (Black et al., 2021) and LLAMA2-7B (Touvron et al., 2023)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-Neo",
          "justification": "GPT-Neo was used alongside other models to confirm the presence of meta-OCL.",
          "quote": "We also replicate our results with models GPT-Neo (Black et al., 2021)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "LLAMA2-7B",
          "justification": "The LLaMA2-7B model was used to corroborate the results seen with other large language models.",
          "quote": "We also replicate our results with models GPT-Neo (Black et al., 2021) and LLAMA2-7B (Touvron et al., 2023)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ConvNeXt V2",
          "justification": "ConvNeXt V2 was used in computer vision experiments to demonstrate the broader applicability of meta-OCL.",
          "quote": "Is meta-OCL a phenomenon that holds more broadly for a wider class of model architectures and modalities? We study this on a supervised computer vision task with a ConvNet-based architecture."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Cross-Verified Database",
          "justification": "CVDB was used to create the synthetic question-answer pairs to test out-of-context learning.",
          "quote": "Our starting point is a dataset of facts about named entities, which we transform into QA pairs about each entity. Specifically, we start with the Cross-Verified database (CVDB)..."
        },
        "aliases": [
          "CVDB"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "T-REx",
          "justification": "The T-REx knowledge base was another source of facts used to generate QA pairs for testing out-of-context learning.",
          "quote": "We also investigate out-of-context learning on an analogous QA dataset based on the T-REx knowledge base (Elsahar et al., 2018)..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST dataset was adapted for a synthetic computer vision task to explore meta-OCL.",
          "quote": "We study this on a supervised computer vision task with a ConvNet-based architecture. Concretely, we construct an MNIST-based synthetic dataset with an analogous notion of QA and definition examples..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Transformers",
          "justification": "The Transformers library was used to fine-tune language models.",
          "quote": "We use the HuggingFace Transformers (Wolf et al., 2020) library to finetune the LLMs..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch served as the fundamental deep learning framework for model development and training.",
          "quote": "PyTorch served as the fundamental deep learning framework for model development and training."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1793,
    "prompt_tokens": 22948,
    "total_tokens": 24741
  }
}