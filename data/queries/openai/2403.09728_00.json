{
  "paper": "2403.09728.txt",
  "words": 14888,
  "extractions": {
    "title": {
      "value": "Simulating Weighted Automata over Sequences and Trees with Transformers",
      "justification": "The title is explicitly mentioned in the provided text.",
      "quote": "Simulating Weighted Automata over Sequences and Trees with Transformers"
    },
    "description": "The paper investigates the capabilities of transformers in simulating weighted finite automata (WFAs) and weighted tree automata (WTAs). It provides formal proofs for the ability of transformers to compactly simulate these automata and offers empirical evidence through synthetic experiments.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily focuses on proving the theoretical capabilities of transformers in simulating WFAs and WTAs, supported by empirical evidence.",
      "quote": "In this work, we show that transformers can simulate weighted finite automata (WFAs), a class of models which subsumes DFAs, as well as weighted tree automata (WTA), a generalization of weighted automata to tree structured inputs. We prove these claims formally and provide upper bounds on the sizes of the transformer models needed as a function of the number of states the target automata."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper discusses the use of transformers, which are widely used in the NLP community, although the application expands beyond traditional NLP tasks.",
        "quote": "Transformers are ubiquitous models in the natural language processing (NLP) community and have shown impressive empirical successes in the past few years."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Sequence Modeling",
          "justification": "The paper extensively discusses the simulation of sequential reasoning tasks using transformers, which aligns with the field of sequence modeling.",
          "quote": "These models do not process data sequentially, and yet outperform sequential neural models such as RNNs."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Transformer",
          "justification": "The paper extensively discusses the use of transformers for simulating automata.",
          "quote": "In this work, we show that transformers can simulate weighted finite automata (WFAs)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "training"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Pautomac",
          "justification": "The dataset is explicitly mentioned in the experimental section of the paper.",
          "quote": "We evaluate models on target WFAs taken from the Pautomac dataset (Verwer et al., 2014)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The framework is explicitly mentioned in the experimental details section of the supplementary material.",
          "quote": "For all experiments, we use the PyTorch TransformerEncoder implementation..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 716,
    "prompt_tokens": 24167,
    "total_tokens": 24883
  }
}