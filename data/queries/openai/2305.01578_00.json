{
  "paper": "2305.01578.txt",
  "words": 4605,
  "extractions": {
    "title": {
      "value": "SELF-SUPERVISED LEARNING FOR INFANT CRY ANALYSIS",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "SELF-SUPERVISED LEARNING FOR INFANT CRY ANALYSIS"
    },
    "description": "This paper explores self-supervised learning (SSL) for analyzing a unique database of infant cry recordings to detect neurological injury and identify cry triggers such as pain, hunger, and discomfort. The paper demonstrates that SSL pre-training, especially using SimCLR, performs significantly better than supervised pre-training for these tasks. It also explores SSL-based domain adaptation and compares various models and methodologies.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experiments with different models and methodologies (e.g., SSL vs. supervised pre-training) and involves measurements and comparisons of performance metrics.",
      "quote": "we experiment with self-supervised pre-training of a convolutional neural network on large audio datasets... We also show that using such SSL-based pre-training for adaptation to cry sounds decreases the need for labeled data of the overall system."
    },
    "primary_research_field": {
      "name": {
        "value": "Audio Classification",
        "justification": "The primary focus of the paper is on classifying audio recordings, specifically infant cries.",
        "quote": "we experiment with self-supervised pre-training of a convolutional neural network on large audio datasets... Specifically, we target cry-based detection of neurological injury as well as identification of cry triggers such as pain, hunger, and discomfort."
      },
      "aliases": [
        "Audio Classification",
        "Sound Classification"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper focuses on self-supervised learning for the task of infant cry analysis.",
          "quote": "we experiment with self-supervised pre-training of a convolutional neural network on large audio datasets... We show that pre-training with SSL contrastive loss (SimCLR) performs significantly better than supervised pre-training for both neuro injury and cry triggers."
        },
        "aliases": [
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The paper discusses the use of pre-trained models and their adaptation using self-supervised learning, which falls under transfer learning.",
          "quote": "We also show that using such SSL-based pre-training for adaptation to cry sounds decreases the need for labeled data of the overall system."
        },
        "aliases": [
          "Domain Adaptation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "The SimCLR model is explicitly mentioned as being used for self-supervised pre-training.",
          "quote": "We show that pre-training with SSL contrastive loss (SimCLR) performs significantly better than supervised pre-training for both neuro injury and cry triggers."
        },
        "aliases": [
          "SimCLR"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper references SimCLR as an existing method used for the experiments.",
          "quote": "SimCLR maximizes the similarity between modified (distorted) views of the same object. For audio, such distortion can be done, for example, by mixing random audio samples... SimCLR introduced in Computer Vision [18, 19] demonstrated good performance in multiple audio tasks [17, 20], including music analysis [21, 22]."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed as part of the experiments described in the paper.",
          "quote": "We show that pre-training with SSL contrastive loss (SimCLR) performs significantly better than supervised pre-training for both neuro injury and cry triggers."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of SimCLR is compared with other methods, such as supervised pre-training.",
          "quote": "We show that pre-training with SSL contrastive loss (SimCLR) performs significantly better than supervised pre-training for both neuro injury and cry triggers."
        },
        "referenced_paper_title": {
          "value": "A simple framework for contrastive learning of visual representations",
          "justification": "This is the referenced paper title for SimCLR, as originally introduced in the context of computer vision.",
          "quote": "Recently, a similarity-based contrastive learning method called SimCLR introduced in Computer Vision [18, 19] demonstrated good performance... Ting Chen et al., “A simple framework for contrastive learning of visual representations,” in ICML, 2020."
        }
      },
      {
        "name": {
          "value": "CNN14",
          "justification": "The CNN14 model is specifically mentioned and used as the backbone encoder in the study.",
          "quote": "As the backbone encoder, we adopt CNN14 introduced in [12]. The encoder is pre-trained on the VGGSound database as in [20]."
        },
        "aliases": [
          "CNN14"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is adopted from previous work and is not an original contribution of this paper.",
          "quote": "There are a few minor differences between the CNN14 proposed in [12] and the models that we adopted from [20]."
        },
        "is_executed": {
          "value": true,
          "justification": "The CNN14 model was executed as part of the experiments in the paper, including pre-training and fine-tuning stages.",
          "quote": "As the backbone encoder, we adopt CNN14 introduced in [12]. The encoder is pre-trained on the VGGSound database as in [20]."
        },
        "is_compared": {
          "value": true,
          "justification": "Different methods, including CNN14 with and without supervised/SSL pre-training, are compared in terms of performance metrics.",
          "quote": "We compare two identical CNN14 models pre-trained in a supervised and self-supervised manner on VGGSound...similarly for SimCLR, fine-tuning, and cry triggers."
        },
        "referenced_paper_title": {
          "value": "PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition",
          "justification": "The paper references the original work on CNN14 within the broader context of PANNs.",
          "quote": "As the backbone encoder, we adopt CNN14 introduced in [12]. The encoder is pre-trained on the VGGSound database as in [20]. Qiuqiang Kong et al., “PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition,” IEEE/ACM Trans. Audio, Speech and Lang. Proc., vol. 28, 2020."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "VGGSound",
          "justification": "The VGGSound dataset is explicitly mentioned as being used for pre-training CNN14 and other models in the experiments.",
          "quote": "First the CNN14 backbone is pre-trained on the VGGSound dataset using SimCLR. (Middle) The CNN14 backbone is further pre-trained via SSL using cry-specific datasets."
        },
        "aliases": [
          "VGGSound"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "VGGSound: A Large-scale Audio-Visual Dataset",
          "justification": "The referenced paper provides details about the VGGSound dataset.",
          "quote": "Honglie Chen et al., “VGGSound: A Large-scale Audio-Visual Dataset,” in ICASSP, 2020."
        }
      },
      {
        "name": {
          "value": "Chillanto",
          "justification": "The Chillanto dataset is mentioned as a commonly used dataset in machine learning research on pathology detection from cry sounds.",
          "quote": "Most machine learning (ML) research on pathology detection from cry sounds was done using the Baby Chillanto [11] database, which contains only six patients diagnosed with birth asphyxia."
        },
        "aliases": [
          "Baby Chillanto"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "A system for the processing of infant cry to recognize pathologies in recently born babies with neural networks",
          "justification": "The referenced paper provides details about creating and using the Chillanto dataset.",
          "quote": "Orion F Reyes-Galaviz and Carlos Alberto Reyes-Garcia, “A system for the processing of infant cry to recognize pathologies in recently born babies with neural networks,” in SPECOM, 2004."
        }
      },
      {
        "name": {
          "value": "Ubenwa",
          "justification": "The Ubenwa database, collected by Ubenwa Health, is used extensively for training and testing the models in the experiments.",
          "quote": "This study is based on a subset of a larger Ubenwa newborn cry clinical database collected from five hospitals in Nigeria, Brazil, and Canada since 2020 [24]."
        },
        "aliases": [
          "Ubenwa Health"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Ubenwa: Cry-based diagnosis of birth asphyxia",
          "justification": "The referenced paper provides details about the Ubenwa database and its use in research.",
          "quote": "CC Onu et al., “Ubenwa: Cry-based diagnosis of birth asphyxia,” NIPS Workshop on Machine Learning for the Developing World, 2017."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Adam",
          "justification": "The Adam optimizer is specifically mentioned as being used for model training during the experiments.",
          "quote": "We use Adam optimizer [35] with a learning rate reduced two times if validation loss does not improve for three epochs."
        },
        "aliases": [
          "Adam"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A method for stochastic optimization",
          "justification": "The referenced paper provides details about the Adam optimization algorithm.",
          "quote": "We use Adam optimizer [35] with a learning rate reduced two times if validation loss does not improve for three epochs. Diederik P Kingma and Jimmy Ba, “Adam: A method for stochastic optimization,” ICLR, 2015."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1896,
    "prompt_tokens": 8603,
    "total_tokens": 10499
  }
}