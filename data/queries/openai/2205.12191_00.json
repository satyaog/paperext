{
  "paper": "2205.12191.txt",
  "words": 16502,
  "extractions": {
    "title": {
      "value": "Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization",
      "justification": "The provided title is extracted from the beginning of the paper.",
      "quote": "Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization"
    },
    "description": "This paper investigates whether recent pretrained models for Visual Question Answering (VQA) can generalize to answer open-ended questions about images outside their training datasets, specifically focusing on their out-of-distribution (OOD) generalization capabilities. Through extensive experiments, the authors evaluate pretrained models on multiple VQA datasets, examining their performance and adaptability. They explore the impact of multimodal pretraining, generative vs. discriminative modeling, and the robustness of automatic evaluation metrics, among other factors.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts extensive experiments and evaluations on different VQA models and datasets, making it an empirical study.",
      "quote": "In this work, we extensively evaluate OOD generalization of current pretrained V&L models by conducting cross-dataset evaluations (without any adaptation to the test domain)."
    },
    "primary_research_field": {
      "name": {
        "value": "Visual Question Answering",
        "justification": "The research focuses on models that answer natural language questions based on visual content.",
        "quote": "Visual Question Answering (VQA) is the task of automatically answering natural language open-ended questions about images."
      },
      "aliases": [
        "VQA"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Out-of-Distribution Generalization",
          "justification": "The primary focus of the paper is on evaluating the OOD generalization capabilities of VQA models.",
          "quote": "To measure whether models learn to solve the task of VQA, we believe we need to examine their out-of-distribution (OOD) generalization capabilities: how they perform on examples drawn from a distribution other than that of the training set."
        },
        "aliases": [
          "OOD Generalization"
        ]
      },
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The paper evaluates the impact of multimodal pretraining on the generalization capabilities of VQA models.",
          "quote": "We find that while image–text pretraining is helpful in most OOD settings, it is not always more useful than in IID ones."
        },
        "aliases": [
          "Image-text Pretraining"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "VILBERT",
          "justification": "VILBERT is mentioned as one of the primary pretrained visual-linguistic models evaluated for OOD generalization.",
          "quote": "We evaluate the performance of two representative, widely-used pretrained models that have achieved strong performance in various V&L tasks in the last few years: V I LBERT (Lu et al., 2019) and ALBEF (Li et al., 2021a)."
        },
        "aliases": [
          "VILBERTDISC",
          "VILBERTGEN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "VILBERT is evaluated but not contributed by this paper. The paper references VILBERT as an existing model.",
          "quote": "We evaluate the performance of two representative, widely-used pretrained models that have achieved strong performance in various V&L tasks in the last few years: V I LBERT (Lu et al., 2019) and ALBEF (Li et al., 2021a)."
        },
        "is_executed": {
          "value": 1,
          "justification": "VILBERT is executed as part of the experiments in this paper to evaluate its performance on different datasets.",
          "quote": "Our implementation of the generative decoder follows that of TransformerDecoder available at https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py."
        },
        "is_compared": {
          "value": 1,
          "justification": "VILBERT is compared to other models in the paper to evaluate its performance.",
          "quote": "In Fig. 1, with different evaluation benchmarks grouped on the x-axis. First, across all models and for each benchmark, we see a notable drop in the VQA accuracy from the IID to the OOD setting."
        },
        "referenced_paper_title": {
          "value": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
          "justification": "The original VILBERT paper is referenced in the context of models evaluated.",
          "quote": "Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks. Advances in Neural Information Processing Systems, 32."
        }
      },
      {
        "name": {
          "value": "ALBEF",
          "justification": "ALBEF is mentioned as one of the primary pretrained visual-linguistic models evaluated for OOD generalization.",
          "quote": "We evaluate the performance of two representative, widely-used pretrained models that have achieved strong performance in various V&L tasks in the last few years: V I LBERT (Lu et al., 2019) and ALBEF (Li et al., 2021a)."
        },
        "aliases": [
          "ALBEFDISC",
          "ALBEFGEN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ALBEF is evaluated but not contributed by this paper. The paper references ALBEF as an existing model.",
          "quote": "We evaluate the performance of two representative, widely-used pretrained models that have achieved strong performance in various V&L tasks in the last few years: V I LBERT (Lu et al., 2019) and ALBEF (Li et al., 2021a)."
        },
        "is_executed": {
          "value": 1,
          "justification": "ALBEF is executed as part of the experiments in this paper to evaluate its performance on different datasets.",
          "quote": "Our analysis, we investigate the role of multimodal pretraining. V I LBERT was pretrained on 3M image–text pairs from Conceptual Captions (CC; Sharma et al. 2018)."
        },
        "is_compared": {
          "value": 1,
          "justification": "ALBEF is compared to other models in the paper to evaluate its performance.",
          "quote": "In Fig. 1, with different evaluation benchmarks grouped on the x-axis. First, across all models and for each benchmark, we see a notable drop in the VQA accuracy from the IID to the OOD setting."
        },
        "referenced_paper_title": {
          "value": "Align before Fuse: Vision and Language Representation Learning with Momentum Distillation",
          "justification": "The original ALBEF paper is referenced in the context of models evaluated.",
          "quote": "Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., & Hoi, S. C. H. (2021). Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. Advances in Neural Information Processing Systems, 34, 9694-9705."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "VQAV2",
          "justification": "VQAV2 is listed as one of the VQA benchmarks used for evaluation.",
          "quote": "We ground our analysis on five diverse VQA datasets: VQAV 2 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), V ISUAL G ENOME (VG; Krishna et al. 2017), V IZ W IZ (Gurari et al., 2018) and VQA-CP (Agrawal et al., 2018)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering",
          "justification": "The referenced title mentioned in the context of VQAV2 dataset.",
          "quote": "Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017). Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)."
        }
      },
      {
        "name": {
          "value": "GQA",
          "justification": "GQA is listed as one of the VQA benchmarks used for evaluation.",
          "quote": "We ground our analysis on five diverse VQA datasets: VQAV 2 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), V ISUAL G ENOME (VG; Krishna et al. 2017), V IZ W IZ (Gurari et al., 2018) and VQA-CP (Agrawal et al., 2018)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
          "justification": "The referenced title mentioned in the context of the GQA dataset.",
          "quote": "Hudson, D. A., & Manning, C. D. (2019). GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)."
        }
      },
      {
        "name": {
          "value": "Visual Genome",
          "justification": "Visual Genome is listed as one of the VQA benchmarks used for evaluation.",
          "quote": "We ground our analysis on five diverse VQA datasets: VQAV 2 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), V ISUAL G ENOME (VG; Krishna et al. 2017), V IZ W IZ (Gurari et al., 2018) and VQA-CP (Agrawal et al., 2018)."
        },
        "aliases": [
          "VG"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations",
          "justification": "The referenced title mentioned in the context of Visual Genome dataset.",
          "quote": "Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., & Fei-Fei, L. (2017). Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision, 123(1), 32-73."
        }
      },
      {
        "name": {
          "value": "VizWiz",
          "justification": "VizWiz is listed as one of the VQA benchmarks used for evaluation.",
          "quote": "We ground our analysis on five diverse VQA datasets: VQAV 2 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), V ISUAL G ENOME (VG; Krishna et al. 2017), V IZ W IZ (Gurari et al., 2018) and VQA-CP (Agrawal et al., 2018)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "VizWiz Grand Challenge: Answering Visual Questions From Blind People",
          "justification": "The referenced title mentioned in the context of VizWiz dataset.",
          "quote": "Gurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman, K., & Bigham, J. P. (2018). VizWiz Grand Challenge: Answering Visual Questions From Blind People. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)."
        }
      },
      {
        "name": {
          "value": "VQA-CP",
          "justification": "VQA-CP is listed as one of the VQA benchmarks used for evaluation.",
          "quote": "We ground our analysis on five diverse VQA datasets: VQAV 2 (Goyal et al., 2017), GQA (Hudson and Manning, 2019), V ISUAL G ENOME (VG; Krishna et al. 2017), V IZ W IZ (Gurari et al., 2018) and VQA-CP (Agrawal et al., 2018)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering",
          "justification": "The referenced title mentioned in the context of VQA-CP dataset.",
          "quote": "Agrawal, A., Batra, D., & Parikh, D. (2018). Don’t Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The authors used the PyTorch library for their implementation and evaluations.",
          "quote": "Our implementation of the generative decoder follows that of TransformerDecoder available at https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The original PyTorch paper is referenced in the context of the library used.",
          "quote": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., & Chintala, S. (2019). PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems, 32."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2774,
    "prompt_tokens": 29910,
    "total_tokens": 32684
  }
}