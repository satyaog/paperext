{
  "paper": "2303.06740.txt",
  "words": 4598,
  "extractions": {
    "title": {
      "value": "FINE-TUNING STRATEGIES FOR FASTER INFERENCE USING SPEECH SELF-SUPERVISED MODELS: A COMPARATIVE STUDY",
      "justification": "Title is explicitly mentioned at the beginning of the paper.",
      "quote": "FINE-TUNING STRATEGIES FOR FASTER INFERENCE USING SPEECH SELF-SUPERVISED MODELS: A COMPARATIVE STUDY"
    },
    "description": "The paper explores various fine-tuning strategies with the goal of reducing inference times in self-supervised speech models, particularly for automatic speech recognition (ASR) systems. Techniques such as layer dropping, early-exiting, and input sequence downsampling are adapted and benchmarked to investigate their efficiency and impact on ASR performance.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experiments and benchmarking of different techniques to evaluate their efficiency in reducing inference times for ASR tasks.",
      "quote": "This article explores different approaches that may be deployed during the fine-tuning to reduce the computations needed in the SSL encoder, leading to faster inferences. We adapt a number of existing techniques to common ASR settings and benchmark them, displaying performance drops and gains in inference times."
    },
    "primary_research_field": {
      "name": {
        "value": "Automatic Speech Recognition",
        "justification": "The paper primarily deals with improving the performance of Automatic Speech Recognition (ASR) using self-supervised models.",
        "quote": "Self-supervised learning (SSL) has allowed substantial progress in Automatic Speech Recognition (ASR) performance in low-resource settings."
      },
      "aliases": [
        "ASR"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The main focus of the paper is on fine-tuning self-supervised models to improve inference times for ASR.",
          "quote": "Self-supervised learning (SSL) has emerged as the main approach for leveraging unlabelled data to achieve significant performance improvements in a wide range of downstream tasks."
        },
        "aliases": [
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Model Optimization",
          "justification": "The paper discusses various optimization techniques like layer dropping, early-exiting, and input sequence downsampling to reduce computation in ASR models.",
          "quote": "This article considers three families of techniques originating from the recent SSL literature: i) layer dropping or replacement, ii) early-exiting, and iii) input sequence downsampling."
        },
        "aliases": [
          "Model Compression"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "WavLM Large",
          "justification": "The WavLM Large model is frequently mentioned and used as the primary SSL model in the experiments.",
          "quote": "We use the released pre-trained and non fine-tuned WavLM Large [6] as the SSL model, as it tops speech self-supervised learning benchmarks and exhibits resilience to noisy conditions."
        },
        "aliases": [
          "WavLM"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used but not contributed by the paper.",
          "quote": "We use the released pre-trained and non fine-tuned WavLM Large [6] as the SSL model, as it tops speech self-supervised learning benchmarks and exhibits resilience to noisy conditions."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed during the experiments conducted in the paper.",
          "quote": "We use the released pre-trained and non fine-tuned WavLM Large... In all the experiments of this section, we use the train-clean-100 split of LibriSpeech [16] as our training set, the dev-clean split for validation and finally the test-clean split for testing."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model is compared with various other techniques and configurations for reducing inference time.",
          "quote": "We use the released pre-trained and non fine-tuned WavLM Large... In all the experiments of this section, we use the train-clean-100 split of LibriSpeech [16] as our training set, the dev-clean split for validation and finally the test-clean split for testing."
        },
        "referenced_paper_title": {
          "value": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
          "justification": "This is the title of the reference paper for WavLM Large, as mentioned in the citations.",
          "quote": "We use the released pre-trained and non fine-tuned WavLM Large [6] as the SSL model, as it tops speech self-supervised learning benchmarks and exhibits resilience to noisy conditions."
        }
      },
      {
        "name": {
          "value": "DistilHuBERT",
          "justification": "DistilHuBERT is used as a baseline for comparison in the experiments.",
          "quote": "For comparison with baselines, we add two experiments using DistilHuBERT [7]."
        },
        "aliases": [
          "DistilHuBERT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used as a baseline but not contributed by the paper.",
          "quote": "For comparison with baselines, we add two experiments using DistilHuBERT [7]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed in the experiments as a baseline.",
          "quote": "For comparison with baselines, we add two experiments using DistilHuBERT [7]."
        },
        "is_compared": {
          "value": 1,
          "justification": "DistilHuBERT is compared with WavLM and other techniques for its performance in reducing inference time.",
          "quote": "For comparison with baselines, we add two experiments using DistilHuBERT [7]."
        },
        "referenced_paper_title": {
          "value": "DistilHuBERT: Speech Representation Learning by Layer-wise Distillation of Hidden-unit BERT",
          "justification": "This is the reference paper for the DistilHuBERT model.",
          "quote": "For comparison with baselines, we add two experiments using DistilHuBERT [7]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "LibriSpeech",
          "justification": "LibriSpeech dataset is used for training, validation, and testing in the experiments.",
          "quote": "In all the experiments of this section, we use the train-clean-100 split of LibriSpeech [16] as our training set, the dev-clean split for validation and finally the test-clean split for testing."
        },
        "aliases": [
          "LibriSpeech"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Librispeech: an ASR corpus based on public domain audio books",
          "justification": "This is the reference paper for the LibriSpeech dataset as mentioned in the references.",
          "quote": "In all the experiments of this section, we use the train-clean-100 split of LibriSpeech [16] as our training set, the dev-clean split for validation and finally the test-clean split for testing."
        }
      },
      {
        "name": {
          "value": "Wall Street Journal (WSJ)",
          "justification": "WSJ dataset is used to test the robustness of the methods.",
          "quote": "We tested the same methods with a 100-hour subset of the Wall Street Journal (WSJ) dataset [26]."
        },
        "aliases": [
          "WSJ"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The design for the Wall Street Journal-based CSR corpus",
          "justification": "This is the reference paper for the WSJ dataset as mentioned in the references.",
          "quote": "We tested the same methods with a 100-hour subset of the Wall Street Journal (WSJ) dataset [26]."
        }
      },
      {
        "name": {
          "value": "Buckeye",
          "justification": "Buckeye dataset is used in the robustness testing of the techniques.",
          "quote": "We also test the robustness of the approach to dataset size variation by reducing the fine-tuning dataset to LibriSpeech-10h train set in first experiment and training on a small spontaneous English dataset, the Buckeye corpus [27] containing 11 hours of data, in a final one."
        },
        "aliases": [
          "Buckeye"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Buckeye corpus of conversational speech: Labeling conventions and a test of transcriber reliability",
          "justification": "This is the reference paper for the Buckeye dataset as mentioned in the references.",
          "quote": "We also test the robustness of the approach to dataset size variation by reducing the fine-tuning dataset to LibriSpeech-10h train set in first experiment and training on a small spontaneous English dataset, the Buckeye corpus [27] containing 11 hours of data, in a final one."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "SpeechBrain library is explicitly mentioned as being used for the experiments.",
          "quote": "The SpeechBrain code base is released for replication and further advancements [14]."
        },
        "aliases": [
          "SpeechBrain"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Speechbrain: A general-purpose speech toolkit",
          "justification": "This is the reference paper for the SpeechBrain library as mentioned in the references.",
          "quote": "The SpeechBrain code base is released for replication and further advancements [14]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1798,
    "prompt_tokens": 9150,
    "total_tokens": 10948
  }
}