{
  "paper": "2305.07558.txt",
  "words": 15474,
  "extractions": {
    "title": {
      "value": "Measuring Progress in Fine-grained Vision-and-Language Understanding",
      "justification": "The title of the research paper is clearly stated at the beginning.",
      "quote": "Measuring Progress in Fine-grained Vision-and-Language Understanding"
    },
    "description": "The paper investigates the performance of vision-and-language (V&L) models on fine-grained tasks. It evaluates several recent models with innovations in image-text alignment and their baselines on a suite of fine-grained benchmarks. Through comprehensive analysis, it aims to identify the models' strengths and weaknesses in understanding fine-grained relationships, verbs, numbers, and other entities in images.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper conducts empirical evaluations of several vision-and-language models on various benchmarks to understand their performance on fine-grained tasks.",
      "quote": "To fill this gap, we analyse several recent models with innovations designed for a better image–text alignment and their corresponding baselines on a suite of fine-grained benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Vision-and-Language Understanding",
        "justification": "The primary focus of the paper is to better understand and quantify progress in fine-grained vision-and-language (V&L) understanding.",
        "quote": "To better understand and quantify progress in this direction, we investigate four competitive V&L models on four fine-grained benchmarks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Benchmarking",
          "justification": "The paper evaluates the performance of models on various benchmarks specifically designed for fine-grained understanding.",
          "quote": "To better understand and quantify progress in this direction, we investigate four competitive V&L models on four fine-grained benchmarks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Multimodal Alignment",
          "justification": "The paper focuses on the alignment between visual and textual modalities to understand fine-grained V&L capabilities.",
          "quote": "We evaluate models from four different model families trained with different amounts of pretraining data, as well as recent architectures that leverage frozen large language models (LLMs)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ALBEF",
          "justification": "ALBEF is evaluated as a coarse-grained baseline and is part of the models evaluated in the paper.",
          "quote": "We find that many recent models build on ALBEF (Singh et al., 2022; Yang et al., 2022; Hao et al., 2023) (which we also study as a coarse-grained baseline)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not a contribution of the paper; it has been used as a baseline for comparison.",
          "quote": "We find that many recent models build on ALBEF (Singh et al., 2022; Yang et al., 2022; Hao et al., 2023) (which we also study as a coarse-grained baseline)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the empirical evaluations in the paper.",
          "quote": "We evaluate the public models released by the authors on GCP."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance is compared with other models in the study.",
          "quote": "We compare two strong VLMs (ALBEF and BLIP) with two models with explicit object modelling (i.e., fine-grained; X-VLM and PEVL)."
        },
        "referenced_paper_title": {
          "value": "Align before fuse: Vision and language representation learning with momentum distillation",
          "justification": "This is the original paper where the ALBEF model was introduced.",
          "quote": "Li et al., 2021"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SVO-Probes",
          "justification": "The dataset focuses on verb understanding and includes over 48K image-sentence pairs.",
          "quote": "SVO-Probes (Hendricks and Nematzadeh, 2021) focuses on verb understanding: it tests whether a model can identify if an image matches a sentence, and includes negative images which differ on a specific part of speech (Subject, Verb, and Object)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Probing image-language transformers for verb understanding.",
          "justification": "This is the reference paper associated with the dataset SVO-Probes.",
          "quote": "SVO-Probes (Hendricks and Nematzadeh, 2021) focuses on verb understanding: it tests whether a model can identify if an image matches a sentence, and includes negative images which differ on a specific part of speech (Subject, Verb, and Object)."
        }
      },
      {
        "name": {
          "value": "VALSE",
          "justification": "VALSE consists of six tasks that involve various linguistic phenomena such as plurality, actions, and coreference.",
          "quote": "VALSE (Parcalabescu et al., 2022) consists of six tasks that cover basic linguistic phenomena, such as plurality, actions, and coreference."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "VALSE: A task-independent benchmark for vision and language models centered on linguistic phenomena.",
          "justification": "This is the reference paper associated with the dataset VALSE.",
          "quote": "VALSE (Parcalabescu et al., 2022) consists of six tasks that cover basic linguistic phenomena, such as plurality, actions, and coreference."
        }
      },
      {
        "name": {
          "value": "VSR",
          "justification": "VSR dataset tests for 65 types of visual spatial relationships and consists of image-sentence pairs.",
          "quote": "VSR (Liu et al., 2023) tests for 65 types of visual spatial relationships (e.g., under, in front of) grouped into seven categories (e.g., adjacency, orientation)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Visual spatial reasoning.",
          "justification": "This is the reference paper associated with the dataset VSR.",
          "quote": "VSR (Liu et al., 2023) tests for 65 types of visual spatial relationships (e.g., under, in front of) grouped into seven categories (e.g., adjacency, orientation)."
        }
      },
      {
        "name": {
          "value": "Winoground",
          "justification": "Winoground aims to test models’ compositional reasoning with two images and two captions containing the same set of words in different orders.",
          "quote": "Winoground (Thrush et al., 2022) is an expert-curated benchmark aiming to test models’ compositional reasoning. Given two images and two captions, the goal is to match them correctly; wherein both captions contain the same set of words, but in a different order."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Winoground: Probing vision and language models for visio-linguistic compositionality.",
          "justification": "This is the reference paper associated with the dataset Winoground.",
          "quote": "Winoground (Thrush et al., 2022) is an expert-curated benchmark aiming to test models’ compositional reasoning. Given two images and two captions, the goal is to match them correctly; wherein both captions contain the same set of words, but in a different order."
        }
      },
      {
        "name": {
          "value": "Flickr30K",
          "justification": "Flickr30K is used for evaluating zero-shot performance on coarse-grained retrieval.",
          "quote": "We also report zero-shot performance on coarse-grained retrieval in Flickr30K (Young et al., 2014) and COCO (Lin et al., 2014) in our analysis."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.",
          "justification": "This is the reference paper associated with the dataset Flickr30K.",
          "quote": "We also report zero-shot performance on coarse-grained retrieval in Flickr30K (Young et al., 2014) and COCO (Lin et al., 2014) in our analysis."
        }
      },
      {
        "name": {
          "value": "COCO",
          "justification": "COCO is used for evaluating zero-shot performance on coarse-grained retrieval.",
          "quote": "We also report zero-shot performance on coarse-grained retrieval in Flickr30K (Young et al., 2014) and COCO (Lin et al., 2014) in our analysis."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Microsoft COCO: Common objects in context.",
          "justification": "This is the reference paper associated with the dataset COCO.",
          "quote": "We also report zero-shot performance on coarse-grained retrieval in Flickr30K (Young et al., 2014) and COCO (Lin et al., 2014) in our analysis."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1789,
    "prompt_tokens": 32190,
    "total_tokens": 33979
  }
}