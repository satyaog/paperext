{
  "paper": "qbRHGQuEAJ.txt",
  "words": 9582,
  "extractions": {
    "title": {
      "value": "Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning",
      "justification": "This is the title of the paper.",
      "quote": "Re-Weighted Softmax Cross-Entropy to Control Forgetting in Federated Learning"
    },
    "description": "The paper addresses the issue of catastrophic forgetting in federated learning due to data heterogeneity across clients. It proposes modifying the cross-entropy objective on a per-client basis by re-weighting the softmax logits before computing the loss to mitigate client forgetting. This approach is tested empirically and shown to improve standard federated learning algorithms.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments using models and datasets to empirically validate the proposed method.",
      "quote": "In this section, we present the empirical results analyzing local client forgetting and our WSM approach."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The paper focuses on federated learning, a distributed machine learning paradigm.",
        "quote": "Federated Learning (FL) is a distributed machine learning paradigm in which a shared global model is learned from a decentralized set of data located at a number of independent client nodes."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The paper draws parallels between the catastrophic forgetting problem in continual learning and the client drift problem in federated learning.",
          "quote": "we are able to draw a connection between the catastrophic forgetting problem in CL and the client drift problem in FL."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet-18",
          "justification": "The model names are explicitly mentioned in the experiments section.",
          "quote": "Our primary evaluations train a ResNet-18."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model ResNet-18 is used for evaluation, not as a contribution.",
          "quote": "Our primary evaluations train a ResNet-18."
        },
        "is_executed": {
          "value": 1,
          "justification": "The ResNet-18 model is executed as part of the experiments.",
          "quote": "Our primary evaluations train a ResNet-18."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the ResNet-18 model is compared with and without the proposed WSM method.",
          "quote": "Table 1 demonstrates that WSM substantially improves performance for both CIFAR datasets with a 2.2% and 1.3% improvement for CIFAR-10 and CIFAR-100, respectively."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "ResNet-18 is part of the ResNet family presented in the paper 'Deep Residual Learning for Image Recognition' by He et al., 2016.",
          "quote": "Our primary evaluations train a ResNet-18."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The paper mentions that CIFAR-10 is used as an evaluation dataset.",
          "quote": "Table 1 shows model performance across a range of learning rates for CIFAR-10."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-10 dataset reference.",
          "quote": "Donald G. L. and Geoffrey E. H. Learning Multiple Layers of Features from Tiny Images."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "The paper mentions that CIFAR-100 is used as an evaluation dataset.",
          "quote": "Table 1 shows model performance across a range of learning rates for CIFAR-100."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-100 dataset reference.",
          "quote": "Donald G. L. and Geoffrey E. H. Learning Multiple Layers of Features from Tiny Images."
        }
      },
      {
        "name": {
          "value": "FEMNIST",
          "justification": "The paper mentions that FEMNIST is used as an evaluation dataset.",
          "quote": "Table 1 shows model performance across a range of learning rates for FEMNIST datasets."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "LEAF: A Benchmark for Federated Settings",
          "justification": "FEMNIST dataset reference.",
          "quote": "Caldas S., Wu P., Li T., Konecny J., McMahan H. B., Smith V., Talwalkar A. LEAF: A Benchmark for Federated Settings."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1836,
    "prompt_tokens": 45587,
    "total_tokens": 47423
  }
}