{
  "paper": "2310.18555.txt",
  "words": 14075,
  "extractions": {
    "title": {
      "value": "Group Robust Classification Without Any Group Information",
      "justification": "The title is taken directly from the paper.",
      "quote": "Group Robust Classification Without Any Group Information"
    },
    "description": "The paper introduces a method to achieve group robustness in classification without relying on group information during training and validation. The method leverages self-supervised learning (SSL) to pretrain a model, which is then used as a bias network to provide a proxy for the bias variable. This proxy is used for debiased training with logit adjustment and to define a validation criterion for robust model selection. Experiments on various synthetic and real-world tasks show that the approach is competitive with or outperforms state-of-the-art methods that require bias labels.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper performs empirical analysis on synthetic and real-world tasks to validate the proposed method.",
      "quote": "Our empirical analysis on synthetic and real-world tasks provides evidence that our approach overcomes the identified challenges and consistently enhances robust accuracy, attaining performance which is competitive with or outperforms that of state-of-the-art methods, which, conversely, rely on bias labels for validation."
    },
    "primary_research_field": {
      "name": {
        "value": "Robustness in Machine Learning",
        "justification": "The primary focus of the paper is on achieving robustness in machine learning classifiers against biased data without using group information.",
        "quote": "This study contends that current biasunsupervised approaches to group robustness continue to rely on group information to achieve optimal performance."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper investigates robust classification on various computer vision datasets, including MPI3D, CelebA, and CIFAR10.",
          "quote": "We introduce a benchmark consisting of systematic splits from the MPI3D (thereby named S MPI3D) image dataset."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Fairness in Machine Learning",
          "justification": "The paper addresses fairness issues by proposing a method for unbiased group-robust learning without explicit bias annotations, making it relevant to fairness in machine learning.",
          "quote": "To address these limitations, we propose a revised methodology for training and validating debiased models in an entirely bias-unsupervised manner."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet",
          "justification": "The ResNet model is used for training on the CIFAR10, MPI3D, CelebA, and Waterbirds datasets.",
          "quote": "For C CIFAR10, we train a 3-hidden layer MLP, while we use a ResNet18 [34] for C CIFAR10 and S MPI3D, and a ResNet50 for WATERBIRDS and C ELEBA."
        },
        "aliases": [
          "ResNet18",
          "ResNet50"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet is not a new model contributed by this paper; it is an existing architecture used in their experiments.",
          "quote": "For C CIFAR10, we train a 3-hidden layer MLP, while we use a ResNet18 [34] for C CIFAR10 and S MPI3D, and a ResNet50 for WATERBIRDS and C ELEBA."
        },
        "is_executed": {
          "value": 1,
          "justification": "The ResNet models were executed and used for empirical analysis in the experiments conducted in this paper.",
          "quote": "For C CIFAR10, we train a 3-hidden layer MLP, while we use a ResNet18 [34] for C CIFAR10 and S MPI3D, and a ResNet50 for WATERBIRDS and C ELEBA."
        },
        "is_compared": {
          "value": 1,
          "justification": "The ResNet models were compared numerically to other baseline models to demonstrate the effectiveness of the proposed method.",
          "quote": "In Table 1, we report the group-balanced accuracy on C MNIST and C CIFAR10, across different percentages of bias-conflicting examples in the training set. For C MNIST, we observe that our method performs overall competitively against L F F, DFA and LC, even though these baselines use bias annotations during model selection."
        },
        "referenced_paper_title": {
          "value": "Deep residual learning for image recognition",
          "justification": "The referenced paper is the original paper introducing the ResNet architecture.",
          "quote": "We use a ResNet18 [34] for C CIFAR10 and S MPI3D, and a ResNet50 for WATERBIRDS and C ELEBA."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MPI3D",
          "justification": "The paper introduces a systematic generalization task using the MPI3D dataset to evaluate the robustness of classifiers.",
          "quote": "To do so, we introduce a benchmark consisting of systematic splits from the MPI3D (thereby named S MPI3D) image dataset."
        },
        "aliases": [
          "S MPI3D"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset",
          "justification": "The referenced paper for the MPI3D dataset introduces it as a disentanglement dataset to study the transfer of inductive bias.",
          "quote": "In deep learning, this question was introduced for the first time by Lake and Baroni [46], who studied the systematicity of RNN models in sequence prediction tasks. Here, we raise the same question in classification. To do so, we introduce a benchmark consisting of systematic splits from the MPI3D (thereby named S MPI3D) image dataset [29]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The neural networks in this paper were implemented using PyTorch.",
          "quote": "Using PyTorch [59], all models in our experiments were implemented, leveraging its high-performance deep learning capabilities."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The referenced paper corresponds to the publication introducing PyTorch.",
          "quote": "Using PyTorch [59], all models in our experiments were implemented, leveraging its high-performance deep learning capabilities."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1244,
    "prompt_tokens": 25339,
    "total_tokens": 26583
  }
}