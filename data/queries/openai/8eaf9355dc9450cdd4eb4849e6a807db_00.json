{
  "paper": "8eaf9355dc9450cdd4eb4849e6a807db.txt",
  "words": 10139,
  "extractions": {
    "title": {
      "value": "GFETM: Genome Foundation-based Embedded Topic Model for scATAC-seq Modeling",
      "justification": "The title is clearly stated on the first page of the paper.",
      "quote": "GFETM: Genome Foundation-based Embedded Topic Model for scATAC-seq Modeling"
    },
    "description": "This paper introduces GFETM, an interpretable and transferable deep neural network framework that integrates Genome Foundation Models (GFM) and Embedded Topic Model (ETM) for analyzing single-cell ATAC-seq (scATAC-seq) data. The approach leverages DNA sequence embeddings from GFMs to enhance scATAC-seq data modeling. The framework aims to achieve state-of-the-art performance on various benchmark datasets and demonstrates generalizability across different batches, tissues, species, and omics.",
    "type": {
      "value": "Empirical",
      "justification": "The paper includes comprehensive experiments demonstrating the effectiveness of the proposed GFETM model on various benchmark datasets and its generalizability across multiple scenarios.",
      "quote": "Through comprehensive experiments, we demonstrate that GFETM offers state-of-the-art cell clustering performance in all benchmark datasets and exhibits scalability on large-scale atlas-level datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Genomics",
        "justification": "The paper focuses on methods for analyzing genomic data, specifically single-cell ATAC-seq, which is part of genomics research.",
        "quote": "Single-cell ATAC-seq (scATAC-seq) has emerged as a powerful technique for investigating open chromatin landscapes at the single-cell level."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The paper proposes a new deep learning framework (GFETM) for genomic data analysis.",
          "quote": "In this paper, we present Genome Foundation Embedded Topic Model (GFETM), an interpretable and transferable deep neural network framework that integrates GFM and Embedded Topic Model (ETM) to perform scATAC-seq data analysis."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Bioinformatics",
          "justification": "The paper falls under bioinformatics as it involves computational methods to model biological data (scATAC-seq).",
          "quote": "It is of vital significance to develop computational methods to model the emerging scATAC-seq datasets and distill biological insights."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The paper discusses the generalizability of the proposed framework across different datasets, which is a key topic in transfer learning.",
          "quote": "By aligning scATAC-seq peaks, we can perform zero or few-shot transfer learning across multiple datasets."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GFETM",
          "justification": "GFETM is the primary model introduced and discussed throughout the paper.",
          "quote": "In this paper, we present Genome Foundation Embedded Topic Model (GFETM), an interpretable and transferable deep neural network framework that integrates GFM and Embedded Topic Model (ETM) to perform scATAC-seq data analysis."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "GFETM is introduced as a novel framework developed by the authors.",
          "quote": "We present Genome Foundation Embedded Topic Model (GFETM), a novel and interpretable generative topic modeling framework enhanced by GFMs."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper discusses training and evaluating the GFETM model, which implies it was executed.",
          "quote": "Training of the encoder network is done by stochastic gradient descent."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of GFETM is compared with other baseline methods in the experiments section.",
          "quote": "We compared the performance of GFETM with the SOTA methods scBasset and PeakVI."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This is the primary model introduced in this paper, and there is no referenced paper title for it.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Buenrostro2018",
          "justification": "The Buenrostro2018 dataset is used as a benchmark dataset in the experiments.",
          "quote": "We used the Buenrostro2018 hematopoietic dataset [42] ... to benchmark the model performance."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Integrated single-cell analysis maps the continuous regulatory landscape of human hematopoietic differentiation",
          "justification": "The referenced paper for the Buenrostro2018 dataset is provided in the references section.",
          "quote": "The Buenrostro2018 dataset contains scATAC-seq data of human hematopoiesis of 10 hematopoietic cell types, with 2034 cells and 6 batches included in total."
        }
      },
      {
        "name": {
          "value": "10x multiome PBMC",
          "justification": "The 10x multiome PBMC dataset is used as a benchmark dataset in the experiments.",
          "quote": "We used ... 10x multiome PBMC dataset ... to benchmark the model performance."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no specific referenced paper title for the 10x multiome PBMC dataset provided in the text.",
          "quote": "10x multiome PBMC dataset ... contains cryopreserved human peripheral blood mononuclear cells (PBMCs) ... with 2714 cells included in total."
        }
      },
      {
        "name": {
          "value": "10x multiome mouse brain",
          "justification": "The 10x multiome mouse brain dataset is used as a benchmark dataset in the experiments.",
          "quote": "We used ... the 10x multiome mouse brain dataset ... to benchmark the model performance."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no specific referenced paper title for the 10x multiome mouse brain dataset provided in the text.",
          "quote": "10x multiome mouse brain dataset contains cells obtained from fresh embryonic E18 mouse brain, with 4878 cells in total."
        }
      },
      {
        "name": {
          "value": "Catlas-human",
          "justification": "The Catlas-human dataset is used for evaluating transfer learning in the experiments.",
          "quote": "To evaluate atlas-level integration and transfer learning, we used the Catlas-human dataset [43]."
        },
        "aliases": [
          "Cusanovich-Atlas Human",
          "Catlas-human"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "A single-cell atlas of chromatin accessibility in the human genome",
          "justification": "The referenced paper for the Catlas-human dataset is provided in the references section.",
          "quote": "Catlas-human dataset..."
        }
      },
      {
        "name": {
          "value": "Cusanovich-mouse",
          "justification": "The paper refers to using the Cusanovich-mouse dataset for large-scale evaluation and transfer learning experiments.",
          "quote": "To evaluate atlas-level integration and transfer learning, we used the Catlas-human dataset [43], and we used the Cusanovich-mouse dataset [7], which is a large scale multi-tissue mouse scATAC-seq data."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "A Single-Cell Atlas of In Vivo Mammalian Chromatin Accessibility",
          "justification": "The referenced paper for the Cusanovich-mouse dataset is provided in the references section.",
          "quote": "Dataset..."
        }
      },
      {
        "name": {
          "value": "Kidney diabetic",
          "justification": "The kidney diabetic dataset is used in a case study to identify marker peaks associated with kidney diabetes.",
          "quote": "In a case study of kidney diabetes, we identify cell-type-specific and disease-associated marker peaks via the GFETM-learned sequence embedded topics."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Multimodal single cell sequencing implicates chromatin accessibility and genetic background in diabetic kidney disease progression",
          "justification": "The referenced paper for the kidney diabetic dataset is provided for clarity.",
          "quote": "The kidney diabetic dataset..."
        }
      },
      {
        "name": {
          "value": "Myocardial",
          "justification": "The myocardial dataset is used for cross-omic transfer learning experiments.",
          "quote": "We performed zero-shot cross-omic transfer learning between scRNA-seq and scATAC-seq datasets on the same tissues and species. For example, on the myocardial dataset..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Spatial multi-omic map of human myocardial infarction",
          "justification": "The referenced paper for the myocardial dataset is provided in the references section.",
          "quote": "The myocardial dataset..."
        }
      },
      {
        "name": {
          "value": "Ovary",
          "justification": "The ovary dataset is utilized for cross-omic transfer learning experiments.",
          "quote": "We also performed zero-shot cross-omic transfer learning between scRNA-seq and scATAC-seq datasets on the same tissues and species. For example, we applied the method on the ovary dataset..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "A molecular atlas of the human postmenopausal fallopian tube and ovary from single-cell RNA and ATAC sequencing",
          "justification": "This is cited in the references section as the source for the ovary dataset.",
          "quote": "The ovary dataset ..."
        }
      },
      {
        "name": {
          "value": "NT-2.5b-multi-species",
          "justification": "This refers to one of several Nucleotide Transformers models which are evaluated for their performance in the paper.",
          "quote": "NT-2.5b-multi-species performs exceptionally well across all released Nucleotide Transformers [16] checkpoints, and it also demonstrates superior performance when its embeddings are utilized in ETM for cell clustering."
        },
        "aliases": [
          "Nucleotide Transformer 2.5 billion parameters, multi-species"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics | bioRxiv",
          "justification": "This refers to the original cited reference paper that discusses the Nucleotide Transformers model.",
          "quote": "[16] The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics | bioRxiv."
        }
      },
      {
        "name": {
          "value": "NT-500M-human-ref",
          "justification": "This refers to one among several Nucleotide Transformers models which were evaluated for their performance.",
          "quote": "Using NT-500M-human-ref outperforms NT-500M-1000g by a large margin..."
        },
        "aliases": [
          "Nucleotide Transformer 500 million parameters, human reference"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics | bioRxiv",
          "justification": "This is referring to the research paper that discusses the Nucleotide Transformers model.",
          "quote": "[16] The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics | bioRxiv."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "DNABERT",
          "justification": "The DNABERT is specifically mentioned as one of the pre-trained GFMs used in this study.",
          "quote": "Specifically, given input peak p containing the chromosome index and start/end position information, we extract the corresponding DNA sequence from the reference genome. The sequence is then tokenized into input tokens by specific tokenization algorithms in specific GFM. The input tokens are then fed into the pre-trained GFM to obtain the token embedding. The sequence embedding ρp for each peak is computed by average pooling the token embeddings. In this study, we use existing released GFMs including DNABERT [14], Nucleotide Transformers [16], DNABERT-2 [15] and HyenaDNA models [17]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome | Bioinformatics | Oxford Academic",
          "justification": "The cited source for DNABERT is provided in the references section.",
          "quote": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome | Bioinformatics | Oxford Academic"
        }
      },
      {
        "name": {
          "value": "DNABERT-2",
          "justification": "The DNABERT-2 model is explicitly mentioned as being used in the experiments.",
          "quote": "We extracted the sequence embeddings from the last layer of the pre-trained GFM for all the peaks in the dataset and concatenate the embeddings into the peak embeddings ρ ∈ RL×M in the ETM component. We experimented three strategies of leveraging these peak embeddings of GFM. The the first strategy, the peak embeddings from the last layer of GFMs were used as they are. In the second strategy, we use the peak embedding to initialize the model parameters ρ in the ETM component and jointly update it with the other ETM parameters during the training. We experimented with different GFMs and reported the performance from fixing and fine-tuning the peak embeddings in Fig.2.\n\nHowever, directly using the peak embedding as the model parameters result in too many learnable weights for large number of peaks, which is the case in scATAC-seq atlas. To further improve the model performance, we developed a joint-training and fine-tuning strategy for the ETM and GFM components, respectively. Specifically, we fine-tuned the last two transformer layers from GFM together with the ETM component. This allows the GFM and ETM to jointly optimize the ELBO objective function. Specifically, we modified Eq 4 to amortize the peak embedding learning on the GFM: r̂c,p = θc αGFM(sp ) + λbc ,p , rtasks are executed within a module named multi_tool_use.c,p = P ... exp(r̂c,p ) p′ exp(r̂c,p′ )... ...steps for the three nucleotide models referenced are presented in the paper: DNABERT -1, DNABERT-2, the aforementioned processes and results."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome",
          "justification": "The referenced paper for DNABERT-2 is provided within the context of evaluating multiple GFMs.",
          "quote": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome"
        }
      },
      {
        "name": {
          "value": "Nucleotide Transformers",
          "justification": "Nucleotide Transformers is one of the GFMs used in the study.",
          "quote": "We included different versions (i.e. kmer, length, size) of these GFMs for comprehensive evaluation. Specifically, DNABERT [14] is based on the BERT [22] architecture and the DNA sequences are tokenized using k-mer tokenization. DNABERT-2 [15] is also based on the BERT but uses more advanced techniques including Flash-Attention [23] and Attention with Linear Biases [24] to further improve the training efficiency and model performance. Byte Pair Encoding [25] is also used as the tokenization algorithm for DNA sequences tokenization. Nucleotide Transformers are also based on the BERT architecture and k-mer tokenization, but trained on not only human reference genome but also 3000 individual genomes from 1000 Genomes Projects as well as 850 species genomes with the largest parameter size up to 2.5B."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics | bioRxiv",
          "justification": "The referenced paper for Nucleotide Transformers is appropriately cited as one of the foundation models evaluated in the study.",
          "quote": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics | bioRxiv"
        }
      },
      {
        "name": {
          "value": "HyenaDNA",
          "justification": "HyenaDNA is specifically named as one of the GFMs used in this study.",
          "quote": "We use existing released GFMs including DNABERT [14], Nucleotide Transformers [16], DNABERT-2 [15], and HyenaDNA models [17]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
          "justification": "The referenced paper for HyenaDNA is included in the context of evaluating multiple GFMs.",
          "quote": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3315,
    "prompt_tokens": 20100,
    "total_tokens": 23415
  }
}