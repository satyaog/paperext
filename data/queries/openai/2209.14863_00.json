{
  "paper": "2209.14863.txt",
  "words": 22443,
  "extractions": {
    "title": {
      "value": "Neural Networks Efficiently Learn Low-Dimensional Representations with SGD",
      "justification": "The title of the paper is clearly stated at the beginning of the document.",
      "quote": "Neural Networks Efficiently Learn Low-Dimensional Representations with SGD"
    },
    "description": "The paper investigates the training of two-layer neural networks using Stochastic Gradient Descent (SGD), specifically focusing on learning low-dimensional representations. The key findings include proving that the first-layer weights of the neural network converge to the principal subspace spanned by the true model's vectors, establishing generalization error bounds, and demonstrating the efficiency of SGD in learning certain target functions with a sample complexity linear in the input dimension.",
    "type": {
      "value": "Theoretical study",
      "justification": "The paper primarily involves mathematical proofs and theoretical analysis to establish the properties and behavior of neural networks trained with SGD. It does not include empirical experiments or observational data.",
      "quote": "We prove that the first-layer weights of the NN converge to the k-dimensional principal subspace..."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper focuses on training neural networks and proving theoretical bounds related to their training and performance, which are core aspects of Machine Learning.",
        "quote": "The task of learning an unknown statistical (teacher) model using data is fundamental in many areas of learning theory."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper involves detailed analysis of optimization techniques used in training neural networks, particularly stochastic gradient descent (SGD).",
          "quote": "Our main result on SGD is presented in Section 3."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Theoretical Machine Learning",
          "justification": "The paper provides theoretical contributions to understanding how neural networks learn low-dimensional structures and converge to principal subspaces.",
          "quote": "This is in contrast to the known dâ„¦(p) sample requirement to learn any degree p polynomial in the kernel regime..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Neural Networks",
          "justification": "The study specifically deals with the properties and training dynamics of two-layer neural networks.",
          "quote": "We study the problem of training a two-layer neural network (NN) of arbitrary width..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Two-layer Neural Network",
          "justification": "The paper focuses on a two-layer neural network model for analysis and theoretical proofs related to training with SGD.",
          "quote": "We study the problem of training a two-layer neural network (NN) of arbitrary width..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The two-layer neural network model is not a novel contribution of this paper; it is a commonly used model in the field for theoretical analysis.",
          "quote": "In this paper, we demonstrate the emergence of low-complexity structures during the training procedure. More specifically, we consider training a two-layer student NN..."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper is theoretical in nature and does not involve actual execution or implementation of the model on hardware.",
          "quote": "The task of learning an unknown statistical (teacher) model using data is fundamental in many areas of learning theory."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance and behavior of the two-layer neural network are theoretically compared to kernel methods in the paper.",
          "quote": "...and it shows that NNs trained with SGD can outperform the neural tangent kernel at initialization."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "No specific reference paper mentioned for the two-layer neural network model.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 733,
    "prompt_tokens": 40454,
    "total_tokens": 41187
  }
}