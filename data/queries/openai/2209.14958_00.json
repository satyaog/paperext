{
  "paper": "2209.14958.txt",
  "words": 32877,
  "extractions": {
    "title": {
      "value": "Co-Writing Screenplays and Theatre Scripts with Language Models",
      "justification": "This is the exact title of the paper.",
      "quote": "Co-Writing Screenplays and Theatre Scripts with Language Models An Evaluation by Industry Professionals PIOTR MIROWSKI∗ and KORY W. MATHEWSON∗ , DeepMind, United Kingdom JAYLEN PITTMAN† , Stanford University, USA RICHARD EVANS, DeepMind, United Kingdom"
    },
    "description": "This paper introduces Dramatron, a system leveraging hierarchical language models to generate coherent scripts and screenplays. The evaluation includes a user study with industry professionals to understand the tool's usefulness and implications in co-creative endeavors.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents empirical evidence through a user study involving theatre and film professionals who interact with the Dramatron system.",
      "quote": "To evaluate Dramatron’s usability and capabilities, instead of relying on online crowd-sourced annotation and evaluation from non-expert raters, we engaged 15-experts in two-hour long user study sessions to co-write a script alongside Dramatron."
    },
    "primary_research_field": {
      "name": {
        "value": "Creative AI",
        "justification": "The research primarily focuses on using AI for creative applications such as screenplay and scriptwriting.",
        "quote": "This process results in greater story coherence than “flat” sequential text generation. Our method is, in spirit, similar to hierarchical neural story generation [37], but generates scripts that far surpass 1000 words. Hierarchical generation of stories can produce an entire script—sometimes tens of thousands of words—from a single user-provided summary of the central dramatic conflict, called the log line."
      },
      "aliases": [
        "Creative Artificial Intelligence",
        "AI-driven Creativity"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The core of the research involves using large language models for text generation.",
          "quote": "By building structural context via prompt chaining, Dramatron can generate coherent scripts and screenplays complete with title, characters, story beats, location descriptions, and dialogue."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Human-Computer Interaction",
          "justification": "The paper includes a user study to evaluate the interactive co-creative system, engaging professionals to interact with the tool.",
          "quote": "Finally, we discuss the suitability of Dramatron for co-creativity, ethical considerations—including plagiarism and bias—and participatory models for the design and deployment of such tools."
        },
        "aliases": [
          "HCI"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Dramatron",
          "justification": "Dramatron is the main system developed and evaluated in this paper, making it the primary model discussed.",
          "quote": "In this work, we present Dramatron, a system that uses LLMs to generate scripts and screenplays hierarchically through a method we call hierarchical story generation."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "The paper appears to be where Dramatron is first introduced and thoroughly discussed.",
          "quote": "In this work, we present Dramatron, a system that uses LLMs to generate scripts and screenplays hierarchically through a method we call hierarchical story generation."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was actively used during the user study sessions.",
          "quote": "Dramatron uses several hard-coded prompts (i.e. input prefixes) to guide the large language model. […] The user can intervene at any stage of the hierarchical generation."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper focuses on the usability and outputs of Dramatron rather than comparing it numerically to other models.",
          "quote": "Our methods can be used with any LLMs that accept an input prompt and then predict which tokens come next."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper title provided for Dramatron within the parent paper.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MassiveText",
          "justification": "The paper mentions the use of the MassiveText dataset for training the Chinchilla model, which is used by Dramatron.",
          "quote": "In this study, we employed the Chinchilla large language model (LLM) [48], represented as a neural network with 70B-parameters and that was trained on 1.4T tokens of the MassiveText dataset. As described by Rae et al. [84], that corpora contains 604M MassiveWeb documents, 4M Books, 361M questions and responses from C4, 1.1B News articles, 142M GitHub code entries, and 6M Wikipedia articles."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Training compute-optimal large language models",
          "justification": "The reference provides context on the usage and details of the MassiveText dataset.",
          "quote": "In this study, we employed the Chinchilla large language model (LLM) [48], represented as a neural network with 70B-parameters and that was trained on 1.4T tokens of the MassiveText dataset. As described by Rae et al. [84], that corpora contains 604M MassiveWeb documents, 4M Books, 361M questions and responses from C4, 1.1B News articles, 142M GitHub code entries, and 6M Wikipedia articles."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1307,
    "prompt_tokens": 50766,
    "total_tokens": 52073
  }
}