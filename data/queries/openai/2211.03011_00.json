{
  "paper": "2211.03011.txt",
  "words": 12665,
  "extractions": {
    "title": {
      "value": "On learning history based policies for controlling Markov decision processes",
      "justification": "Directly taken from the title of the paper",
      "quote": "On learning history based policies for controlling Markov decision processes"
    },
    "description": "This paper introduces a theoretical framework for studying and designing reinforcement learning algorithms that use history-based feature abstraction mappings to control Markov decision processes (MDPs). The authors develop a practical reinforcement learning algorithm based on this framework and evaluate it on continuous control tasks.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper primarily focuses on introducing a theoretical framework and analysis of history-based feature abstraction in reinforcement learning.",
      "quote": "In this paper, we introduce a theoretical framework for studying the behaviour of RL algorithms that learn to control an MDP using history-based feature abstraction mappings."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on history-based reinforcement learning algorithms and their application to control Markov decision processes.",
        "quote": "In this paper, we introduce a theoretical framework for studying the behaviour of RL algorithms that learn to control an MDP using history-based feature abstraction mappings."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Function Approximation",
          "justification": "The paper discusses function approximation techniques in the context of reinforcement learning.",
          "quote": "State abstraction and function approximation are vital components used by reinforcement learning (RL) algorithms to efficiently solve complex control problems when exact computations are intractable due to large state and action spaces."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Continuous Control",
          "justification": "The effectiveness of the proposed algorithm is numerically evaluated on continuous control tasks.",
          "quote": "Furthermore, we use this framework to design a practical RL algorithm and we numerically evaluate its effectiveness on a set of continuous control tasks."
        },
        "aliases": []
      }
    ],
    "models": [],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 376,
    "prompt_tokens": 24376,
    "total_tokens": 24752
  }
}