{
  "paper": "2310.14192.txt",
  "words": 8063,
  "extractions": {
    "title": {
      "value": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation",
      "justification": "The title of the paper is provided at the beginning of the text.",
      "quote": "PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation"
    },
    "description": "This paper proposes PromptMix, a two-step method aimed at generating challenging text augmentations near class boundaries and relabeling them to enhance label correctness. The method is used to facilitate the transfer of knowledge from large language models to smaller classifiers in few-shot or zero-shot text classification tasks.",
    "type": {
      "value": "Empirical",
      "justification": "The paper presents experiment results demonstrating the effectiveness of the proposed PromptMix method across several datasets and settings.",
      "quote": "We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERTbase and BERTbase."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper deals with tasks related to text classification and data augmentation, which are core areas of Natural Language Processing.",
        "quote": "Data augmentation is a widely used technique to address the problem of text classification when there is a limited amount of training data."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text Classification",
          "justification": "The primary task addressed in this paper is few-shot and zero-shot text classification.",
          "quote": "In this work, we focus on the task of few-shot text classification (Schick and Schütze, 2021; Alex et al., 2021; Bragg et al., 2021). Specifically, we explore zero-shot and 2-shot settings."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Data Augmentation",
          "justification": "The paper extensively discusses and proposes a new method for data augmentation to improve text classification.",
          "quote": "In this work, we propose a method to generate more helpful augmented data by utilizing the LLM’s abilities to follow instructions and perform few-shot classifications. Our specific PromptMix method consists of two steps: 1) generate challenging text augmentations near class boundaries..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Knowledge Distillation",
          "justification": "The paper focuses on transferring the knowledge from a large language model (like GPT-3.5-turbo) to smaller models (like DistilBERT and BERT) which is a typical agenda in knowledge distillation.",
          "quote": "First, we summarize that generating borderline examples and relabeling them improves knowledge transfer from a massive LLM like GPT3.5 into much smaller models like DistilBERT and BERT, even without abundant seed examples."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-3.5-turbo",
          "justification": "The paper uses GPT-3.5-turbo as the large language model for generating and relabeling augmented data.",
          "quote": "First, PromptMix instructs an LLM (in our case, GPT3.5-turbo) to generate new examples by mixing information from multiple classes."
        },
        "aliases": [
          "GPT-3.5"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-3.5-turbo is utilized but not contributed by the paper.",
          "quote": "First, PromptMix instructs an LLM (in our case, GPT3.5-turbo) to generate new examples by mixing information from multiple classes."
        },
        "is_executed": {
          "value": 1,
          "justification": "GPT-3.5-turbo was executed as part of the method to generate and relabel data.",
          "quote": "First, PromptMix instructs an LLM (in our case, GPT3.5-turbo) to generate new examples by mixing information from multiple classes."
        },
        "is_compared": {
          "value": 0,
          "justification": "GPT-3.5-turbo is used as a tool and not compared numerically to other models.",
          "quote": "First, PromptMix instructs an LLM (in our case, GPT3.5-turbo) to generate new examples by mixing information from multiple classes."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The referenced paper describes the development and capabilities of the GPT-3 family models.",
          "quote": "First, PromptMix instructs an LLM (in our case, GPT3.5-turbo) to generate new examples by mixing information from multiple classes."
        }
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "DistilBERT is one of the smaller models that the knowledge from GPT-3.5-turbo is distilled into.",
          "quote": "generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERTbase and BERTbase."
        },
        "aliases": [
          "DistilBERTbase"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "DistilBERT is utilized but not contributed by the paper.",
          "quote": "generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERTbase and BERTbase."
        },
        "is_executed": {
          "value": 1,
          "justification": "DistilBERT was fine-tuned and evaluated as part of the experiments.",
          "quote": "We find that training a classifier on these relabeled examples effectively transfers the knowledge of a massive LLM like GPT3.5 into much smaller models like BERT and DistilBERT"
        },
        "is_compared": {
          "value": 1,
          "justification": "DistilBERT is compared to BERT in the experiments conducted.",
          "quote": "We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERTbase and BERTbase."
        },
        "referenced_paper_title": {
          "value": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
          "justification": "The referenced paper presents the DistilBERT model which is used in this study.",
          "quote": "We find that training a classifier on these relabeled examples effectively transfers the knowledge of a massive LLM like GPT3.5 into much smaller models like BERT and DistilBERT"
        }
      },
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is another smaller model that the knowledge from GPT-3.5-turbo is distilled into.",
          "quote": "We find that training a classifier on these relabeled examples effectively transfers the knowledge of a massive LLM like GPT3.5 into much smaller models like BERT and DistilBERT"
        },
        "aliases": [
          "BERTbase"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "BERT is utilized but not contributed by the paper.",
          "quote": "We find that training a classifier on these relabeled examples effectively transfers the knowledge of a massive LLM like GPT3.5 into much smaller models like BERT and DistilBERT"
        },
        "is_executed": {
          "value": 1,
          "justification": "BERT was fine-tuned and evaluated as part of the experiments.",
          "quote": "We find that training a classifier on these relabeled examples effectively transfers the knowledge of a massive LLM like GPT3.5 into much smaller models like BERT and DistilBERT"
        },
        "is_compared": {
          "value": 1,
          "justification": "BERT is compared to DistilBERT in the experiments conducted.",
          "quote": "We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints. Our experiments show that generating and, crucially, relabeling borderline examples facilitates the transfer of knowledge of a massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like DistilBERTbase and BERTbase."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "The referenced paper presents the BERT model which is used in this study.",
          "quote": "We find that training a classifier on these relabeled examples effectively transfers the knowledge of a massive LLM like GPT3.5 into much smaller models like BERT and DistilBERT"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Banking77",
          "justification": "Banking77 is one of the datasets used for evaluation in the paper.",
          "quote": "We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints."
        },
        "aliases": [
          "B77"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Efficient Intent Detection with Dual Sentence Encoders",
          "justification": "This referenced paper introduces the Banking77 dataset.",
          "quote": "Banking77 (B77) (Casanueva et al., 2020) is a single-domain dataset with 77 banking-related classes, where the difference between multiple classes is nuanced."
        }
      },
      {
        "name": {
          "value": "TREC6",
          "justification": "TREC6 is one of the datasets used for evaluation in the paper.",
          "quote": "We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The TREC-8 Question Answering Track Evaluation",
          "justification": "This referenced paper describes the TREC6 dataset used in the study.",
          "quote": "TREC6 (Voorhees et al., 1999) is a question classification dataset with six broad classes of questions in English."
        }
      },
      {
        "name": {
          "value": "Subjectivity dataset",
          "justification": "Subjectivity (SUBJ) is one of the datasets used for evaluation in the paper.",
          "quote": "We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints."
        },
        "aliases": [
          "SUBJ"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts",
          "justification": "This referenced paper introduces the Subjectivity dataset.",
          "quote": "The subjectivity dataset (SUBJ) (Pang and Lee, 2004) contains movie reviews with objectivity labels."
        }
      },
      {
        "name": {
          "value": "Twitter Complaints",
          "justification": "Twitter Complaints is one of the datasets used for evaluation in the paper.",
          "quote": "We evaluate the proposed method in challenging 2-shot and zero-shot settings on four text classification datasets: Banking77, TREC6, Subjectivity (SUBJ), and Twitter Complaints."
        },
        "aliases": [
          "TC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatically identifying complaints in social media",
          "justification": "This referenced paper introduces the Twitter Complaints dataset.",
          "quote": "The twitter complaints dataset (TC) (Preoţiuc-Pietro et al., 2019) contains tweets annotated by whether they contain a complaint or not."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Huggingface Transformers",
          "justification": "The paper uses the Huggingface Transformers library for fine-tuning and evaluation of classifiers such as BERT and DistilBERT.",
          "quote": "Training. We fine-tune DistilBERTbase and BERTbase models for text classification by adding a linear layer on top of the [CLS] token (Wolf et al., 2019)."
        },
        "aliases": [
          "Transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "HuggingFace’s Transformers: State-of-the-art Natural Language Processing",
          "justification": "The referenced paper introduces the Transformers library used in this study.",
          "quote": "Training. We fine-tune DistilBERTbase and BERTbase models for text classification by adding a linear layer on top of the [CLS] token (Wolf et al., 2019)."
        }
      },
      {
        "name": {
          "value": "Sentence Transformers",
          "justification": "The paper uses the Sentence Transformers library for obtaining sentence embeddings that guide the relabeling process.",
          "quote": "To ensure a valid prediction, we retrieve the closest class in the dataset based on the cosine similarity of the SBERT embedding of the GPT-generated class and the ground-truth classes in the dataset. We do not include all the classes in the prompt because a) some datasets can have hundreds of classes that would not fit in the context size of the LLM, and b) we found in our preliminary experiments that long contexts degraded GPT’s classification ability."
        },
        "aliases": [
          "SBERT"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Sentence-BERT: Sentence embeddings using Siamese BERT-networks",
          "justification": "The referenced paper describes the development of the Sentence Transformers library used in this study.",
          "quote": "To ensure a valid prediction, we retrieve the closest class in the dataset based on the cosine similarity of the SBERT embedding of the GPT-generated class and the ground-truth classes in the dataset."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2773,
    "prompt_tokens": 15610,
    "total_tokens": 18383
  }
}