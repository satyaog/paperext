{
  "paper": "2309.06497.txt",
  "words": 19013,
  "extractions": {
    "title": {
      "value": "A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale",
      "justification": "The title is obtained directly from the paper's provided title.",
      "quote": "A Distributed Data-Parallel PyTorch Implementation of the Distributed Shampoo Optimizer for Training Neural Networks At-Scale"
    },
    "description": "This paper introduces a PyTorch implementation of the Distributed Shampoo optimizer, an advanced optimization algorithm designed for training neural networks at scale. It details the algorithm and optimizations made for efficient multi-GPU distributed data-parallel training, providing evidence of its performance improvements over standard methods.",
    "type": {
      "value": "empirical study",
      "justification": "The paper includes performance optimizations and empirical validation through experiments, illustrating the efficiency of the Distributed Shampoo algorithm.",
      "quote": "We validate our implementation by performing an ablation study on training ImageNet ResNet50, demonstrating Shampoo’s superiority against standard training recipes with minimal hyperparameter tuning."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on training neural networks using advanced optimization algorithms, which falls squarely within the domain of Deep Learning.",
        "quote": "Shampoo is an online and stochastic optimization algorithm belonging to the AdaGrad family of methods for training neural networks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization Algorithms",
          "justification": "The paper extensively discusses the Distributed Shampoo optimizer, detailing its implementation and performance enhancements for optimizing neural network training.",
          "quote": "The contribution of this paper is the description and design of a PyTorch implementation of the Distributed Shampoo algorithm."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet50",
          "justification": "ResNet50 is used to validate the implementation of the Distributed Shampoo optimizer, highlighting its performance improvements.",
          "quote": "We validate our implementation by performing an ablation study on training ImageNet ResNet50, demonstrating Shampoo’s superiority against standard training recipes with minimal hyperparameter tuning."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "training"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is used as the dataset for training the ResNet50 model to validate the optimizer's performance.",
          "quote": "We validate our implementation by performing an ablation study on training ImageNet ResNet50, demonstrating Shampoo’s superiority against standard training recipes with minimal hyperparameter tuning."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is the primary deep learning library used for implementing the Distributed Shampoo optimizer and conducting experiments.",
          "quote": "It is designed specifically for distributed data-parallel training using PyTorch’s DistributedDataParallel module."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 592,
    "prompt_tokens": 39809,
    "total_tokens": 40401
  }
}