{
  "paper": "2310.18807.txt",
  "words": 9765,
  "extractions": {
    "title": {
      "value": "OC-NMN: Object-centric Compositional Neural Module Network for Generative Visual Analogical Reasoning",
      "justification": "Title as mentioned in the provided content.",
      "quote": "OC-NMN: Object-centric Compositional Neural Module Network for Generative Visual Analogical Reasoning"
    },
    "description": "The paper presents OC-NMN, a modular neural network model designed for visual generative analogical reasoning. It introduces the Arith-MNIST dataset to evaluate the model's ability to generalize out-of-distribution by performing arithmetic operations on MNIST digits. Key innovations include leveraging object-centric inductive biases and a compositional data augmentation framework inspired by human imagination.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experimental evaluation, dataset creation, and comparison of model performance.",
      "quote": "In this section, we present the experimental results of applying OC-NMN to different splits of the Arith-MNIST dataset, and compare it against several baseline approaches."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper deals with neural networks, model generalization, and compositional data augmentation in the context of deep learning.",
        "quote": "Such capacity is not yet attained for machine learning systems. In this work, in the context of visual reasoning, we show how modularity can be leveraged to derive a compositional data augmentation framework inspired by imagination."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Visual Reasoning",
          "justification": "The focus of the paper is on models and datasets for visual generative analogical reasoning, which falls under the subfield of visual reasoning.",
          "quote": "Our method, denoted Object-centric Compositional Neural Module Network (OC-NMN), decomposes visual generative reasoning tasks into a series of primitives applied to objects without using a domain-specific language."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Neural Abstract Reasoner",
          "justification": "NAR is one of the models used for comparison in the experiments.",
          "quote": "we evaluate GPT-4 on the easy split and obtain an accuracy of 16 in the best case.\n\nIn this work, we take a step towards addressing the ARC challenge by designing a new and simpler generative benchmark, which we call Arith-MNIST."
        },
        "aliases": [
          "NAR"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DNC-GRU",
          "justification": "This model serves as one of the baselines for comparison in the experiments.",
          "quote": "The second baseline consists of a stack of Transformer encoder layers, and takes as input a set composed of the query slots, the controller output,and a CLS token from which we retrieve the final answer. We denote this model DNC-Transformer."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DNC-Transformer",
          "justification": "This model is another baseline used for comparison.",
          "quote": "We denote this model DNC-Transformer. All architectural details and hyperparameters are described in the Appendix."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "FLAN-T5",
          "justification": "FLAN-T5 is evaluated on the text equivalent tasks in the experiments.",
          "quote": "We also report the performance of a state-of-the-art language model baseline FLAN-T5 on the text equivalent tasks."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is used to evaluate generalization to unseen digit-color configurations.",
          "quote": "We evaluate GPT-4 on the easy split and obtain an accuracy of 16 in the best case. More details about the GPT-4 training can be found in the Appendix."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Abstract Reasoning Corpus",
          "justification": "The ARC dataset is specifically mentioned as a reference point for generative reasoning tasks.",
          "quote": "To that end, Chollet (2019) proposed a generative reasoning task, the Abstract Reasoning Corpus (ARC), where the model is given a few examples of input-output (I/O) pairs and has to understand the underlying common program that was applied to the inputs to obtain the outputs."
        },
        "aliases": [
          "ARC"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Differentiable Neural Computation",
          "justification": "DNC is specifically mentioned as the controller architecture for several models used in the paper.",
          "quote": "The controller moduleâ€™s architecture is the same for all the baselines considered (including our model) and corresponds to the Differentiable Neural Computer controller (Graves et al., 2016) proposed in Neural Abstract Reasoner (Kolev et al., 2020)."
        },
        "aliases": [
          "DNC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1298,
    "prompt_tokens": 16113,
    "total_tokens": 17411
  }
}