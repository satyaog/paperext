{
  "paper": "pxpbTdUEpD.txt",
  "words": 11637,
  "extractions": {
    "title": {
      "value": "The Stack: 3 TB of permissively licensed source code",
      "justification": "Title of the paper.",
      "quote": "The Stack:\n3 TB of permissively licensed source code"
    },
    "description": "This paper introduces The Stack, a 3.1 TB dataset containing permissively licensed source code in 30 programming languages. The authors describe the dataset construction, present a data governance plan, discuss limitations, and demonstrate the effectiveness of the dataset on text2code benchmarks by training 350M parameter decoders on various Python subsets. The dataset is publicly available for promoting open and responsible research on large language models (LLMs) for code.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents empirical results from training and evaluating 350M parameter decoders on benchmarks using the proposed dataset.",
      "quote": "we introduce The Stack, a 3.1 TB dataset... and show promising results on text2code benchmarks by training 350M-parameter decoders on different Python subsets."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on large language models (LLMs) and their applications, which fall under the domain of Natural Language Processing.",
        "quote": "One of the challenges faced by researchers working on code LLMs is the lack of openness and transparency around the development of these systems."
      },
      "aliases": [
        "NLP",
        "Computational Linguistics"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Code Synthesis",
          "justification": "The primary application and investigation of the dataset and models are within the scope of code generation and understanding.",
          "quote": "these large transformer models are pre-trained on large internet corpora and have shown impressive zero and few-shot performance on numerous NLP tasks... More recently, researchers have started exploring LLMs for coding applications."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Codex",
          "justification": "Codex is one of the models mentioned and evaluated for performance comparison in the study.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Codex is used for performance comparison and is not a contribution of this paper.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not indicate executing Codex; it only compares performance results.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "is_compared": {
          "value": 1,
          "justification": "Codex is compared with the models trained on The Stack dataset in the paper.",
          "quote": "We outperform these models by a large margin if we train on the all-license version of the dataset."
        },
        "referenced_paper_title": {
          "value": "Evaluating large language models trained on code",
          "justification": "This referenced paper details the Codex model, which is compared against in this study.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        }
      },
      {
        "name": {
          "value": "CodeGen",
          "justification": "CodeGen is one of the models mentioned and evaluated for performance comparison in the study.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CodeGen is used for performance comparison and is not a contribution of this paper.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not indicate executing CodeGen; it only compares performance results.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        },
        "is_compared": {
          "value": 1,
          "justification": "CodeGen is compared with the models trained on The Stack dataset in the paper.",
          "quote": "We outperform these models by a large margin if we train on the all-license version of the dataset."
        },
        "referenced_paper_title": {
          "value": "A conversational paradigm for program synthesis",
          "justification": "This referenced paper details the CodeGen model, which is compared against in this study.",
          "quote": "We show it is possible to reproduce text2code performance of Codex (Chen et al., 2021) and CodeGen (Nijkamp et al., 2022) by only using permissively licensed data."
        }
      },
      {
        "name": {
          "value": "InCoder",
          "justification": "InCoder is mentioned and evaluated for performance comparison in the study.",
          "quote": "We compare against CodeGen (Nijkamp et al., 2022), InCoder (Fried et al., 2022), and Codex (Chen et al., 2021) models of similar size."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "InCoder is used for performance comparison and is not a contribution of this paper.",
          "quote": "We compare against CodeGen (Nijkamp et al., 2022), InCoder (Fried et al., 2022), and Codex (Chen et al., 2021) models of similar size."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not indicate executing InCoder; it only compares performance results.",
          "quote": "We compare against CodeGen (Nijkamp et al., 2022), InCoder (Fried et al., 2022), and Codex (Chen et al., 2021) models of similar size."
        },
        "is_compared": {
          "value": 1,
          "justification": "InCoder is compared with the models trained on The Stack dataset in the paper.",
          "quote": "We compare against CodeGen (Nijkamp et al., 2022), InCoder (Fried et al., 2022), and Codex (Chen et al., 2021) models of similar size."
        },
        "referenced_paper_title": {
          "value": "Incoder: A generative model for code infilling and synthesis",
          "justification": "This referenced paper details the InCoder model, which is compared against in this study.",
          "quote": "We compare against CodeGen (Nijkamp et al., 2022), InCoder (Fried et al., 2022), and Codex (Chen et al., 2021) models of similar size."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "The Stack",
          "justification": "The dataset introduced in the paper is named The Stack.",
          "quote": "we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "None",
          "justification": "The Stack is a contribution of the paper itself and is not referenced from another paper.",
          "quote": "we introduce The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages."
        }
      },
      {
        "name": {
          "value": "HumanEval",
          "justification": "HumanEval is one of the benchmarks used to evaluate the performance of the trained models.",
          "quote": "it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Evaluating large language models trained on code",
          "justification": "The referenced paper details the HumanEval benchmark used for evaluation.",
          "quote": "it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data."
        }
      },
      {
        "name": {
          "value": "MBPP",
          "justification": "MBPP is one of the benchmarks used to evaluate the performance of the trained models.",
          "quote": "it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Program synthesis with large language models",
          "justification": "The referenced paper details the MBPP benchmark used for evaluation.",
          "quote": "it is possible to match previously reported HumanEval and MBPP performance using only permissively licensed data."
        }
      },
      {
        "name": {
          "value": "CodeSearchNet",
          "justification": "CodeSearchNet is mentioned as a resource used by other studies for pre-training code LLMs.",
          "quote": "Another frequently used resource is the CodeSearchNet corpus (Husain et al., 2019)."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "CodeSearchNet challenge: Evaluating the state of semantic code search",
          "justification": "The referenced paper details the CodeSearchNet corpus as mentioned in the study.",
          "quote": "Another frequently used resource is the CodeSearchNet corpus (Husain et al., 2019)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Megatron-LM",
          "justification": "Megatron-LM is used to train the models evaluated in this study.",
          "quote": "We use a fork of Megatron-LM (Shoeybi et al., 2019) for training."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Megatron-LM: Training multi-billion parameter language models using model parallelism",
          "justification": "The referenced paper details the Megatron-LM library used for training.",
          "quote": "We use a fork of Megatron-LM (Shoeybi et al., 2019) for training."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2092,
    "prompt_tokens": 23962,
    "total_tokens": 26054
  }
}