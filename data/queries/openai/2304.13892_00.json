{
  "paper": "2304.13892.txt",
  "words": 9076,
  "extractions": {
    "title": {
      "value": "Discovering Object-Centric Generalized Value Functions From Pixels",
      "justification": "The literal title as presented at the beginning and within the text of the paper.",
      "quote": "Discovering Object-Centric Generalized Value Functions From Pixels"
    },
    "description": "The paper introduces a method to automatically discover object-centric General Value Functions (GVFs) from pixel data and leverage these GVFs as features for reinforcement learning tasks. The approach aims to address challenges in learning good control policies from high-dimensional visual inputs by discovering meaningful features centered around objects, which facilitates fast adaptation in both stationary and non-stationary settings.",
    "type": {
      "value": "empirical study",
      "justification": "The paper involves empirical demonstrations of the proposed method through experiments on various reinforcement learning environments, comparing its performance against several baselines.",
      "quote": "We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary focus of the paper is on improving reinforcement learning by discovering object-centric GVFs and using them to learn better control policies.",
        "quote": "In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent ‘question’ functions and leveraging the subsequent learned general value functions for control."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The techniques and models developed and tested in the paper are specifically applied to and benefit from deep reinforcement learning frameworks.",
          "quote": "Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using handcrafted auxiliary tasks and pseudo rewards."
        },
        "aliases": [
          "Deep RL"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper leverages computer vision techniques, particularly for extracting and learning object-centric representations from pixel data.",
          "quote": "Learning control from high-dimensional input such as images is a complex problem relevant to many real world applications."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "OC-GVFs",
          "justification": "OC-GVFs (Object-Centric General Value Functions) is the main model proposed and developed in the paper to discover and leverage object-centric GVFs for reinforcement learning.",
          "quote": "We propose OC-GVFs: an end-to-end approach to automatically discover object-centric General Value Functions from pixels."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "OC-GVFs is a novel model introduced as the main contribution of the paper.",
          "quote": "We propose OC-GVFs: an end-to-end approach to automatically discover object-centric General Value Functions from pixels."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments indicated in the paper involve running the OC-GVFs model on GPUs to handle high-dimensional pixel data and extensive computations.",
          "quote": "All our experiments were run on a single V100 GPU."
        },
        "is_compared": {
          "value": 1,
          "justification": "The OC-GVFs model is compared against several baselines, including DDQN and existing approaches to GVF discovery.",
          "quote": "We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "OC-GVFs is the original model proposed in this paper and does not reference another paper for its development.",
          "quote": "We propose OC-GVFs: an end-to-end approach to automatically discover object-centric General Value Functions from pixels."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Collect Objects",
          "justification": "This is a customized environment used for evaluating the performance of the proposed method under both stationary and non-stationary conditions.",
          "quote": "Collect-objects Environment: is a customized version of the four-room gridworld environment similar to the one used in Veeriah et al.."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Veeriah et al., 2019",
          "justification": "The environment is similar to the one used in the referenced paper by Veeriah et al., 2019.",
          "quote": "Collect-objects Environment: is a customized version of the four-room gridworld environment similar to the one used in Veeriah et al.."
        }
      },
      {
        "name": {
          "value": "MiniGrid Dynamic Obstacles",
          "justification": "This environment is used to test the model’s performance in non-stationary settings where the agent has to avoid obstacles and reach a goal in a grid space.",
          "quote": "MiniGrid-Dynamic Obstacles: For the experiments on non-stationarity, we used the MiniGrid Dynamic Obstacles (Chevalier-Boisvert et al., 2018)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Chevalier-Boisvert et al., 2018",
          "justification": "The MiniGrid Dynamic Obstacles environment is referenced from Chevalier-Boisvert et al., 2018.",
          "quote": "MiniGrid-Dynamic Obstacles: For the experiments on non-stationarity, we used the MiniGrid Dynamic Obstacles (Chevalier-Boisvert et al., 2018)."
        }
      },
      {
        "name": {
          "value": "CoinRun",
          "justification": "This environment is part of the ProcGen benchmark and is used to test the model’s adaptation to new tasks and levels.",
          "quote": "CoinRun & StarPilot: are a part of procedurally generated environments called ProcGen (Cobbe et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Cobbe et al., 2019",
          "justification": "CoinRun is sourced from the ProcGen benchmark referenced in Cobbe et al., 2019.",
          "quote": "CoinRun & StarPilot: are a part of procedurally generated environments called ProcGen (Cobbe et al., 2019)."
        }
      },
      {
        "name": {
          "value": "StarPilot",
          "justification": "This environment is part of the ProcGen benchmark and is used to test the model’s adaptation to new tasks and levels.",
          "quote": "CoinRun & StarPilot: are a part of procedurally generated environments called ProcGen (Cobbe et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Cobbe et al., 2019",
          "justification": "StarPilot is sourced from the ProcGen benchmark referenced in Cobbe et al., 2019.",
          "quote": "CoinRun & StarPilot: are a part of procedurally generated environments called ProcGen (Cobbe et al., 2019)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1677,
    "prompt_tokens": 16237,
    "total_tokens": 17914
  }
}