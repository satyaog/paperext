{
  "paper": "2403.13086.txt",
  "words": 6513,
  "extractions": {
    "title": {
      "value": "Listenable Maps for Audio Classifiers",
      "justification": "The title of the paper is 'Listenable Maps for Audio Classifiers', as stated at the beginning and throughout the document.",
      "quote": "This paper contributes to this emerging field by introducing a novel method called Listenable Maps for Audio Classifiers (L-MAC)."
    },
    "description": "This paper introduces Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method designed to generate listenable and faithful interpretations for pretrained audio classifiers. L-MAC employs a decoder that produces binary masks over the spectrogram of the original input audio. These masks identify relevant portions of the audio that influenced the classifier's decision. The decoder is trained using a loss function that promotes high classifier confidence for the masked-in audio and low probability for the masked-out portions. Quantitative and qualitative evaluations demonstrate that L-MAC outperforms several existing methods, including gradient and masking-based techniques, in producing faithful and user-preferred interpretations.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments, user studies, and quantitative evaluations to demonstrate that L-MAC produces more faithful and listenable interpretations compared to existing methods.",
      "quote": "Quantitative evaluations on both in-domain and out-of-domain data demonstrate that L-MAC consistently produces more faithful interpretations than several gradient and masking-based methodologies. Furthermore, a user study confirms that, on average, users prefer the interpretations generated by the proposed technique."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper addresses deep learning techniques for audio signal processing, specifically focusing on improving interpretation methods for deep learning models in this domain.",
        "quote": "In recent years, deep learning models made significant strides in a variety of speech/audio applications, including sound event recognition, sound generation, speech recognition, speech separation, and many more."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Explainable AI",
          "justification": "The paper is focused on posthoc interpretation methods for pretrained machine learning models, making it a contribution to the field of Explainable AI (XAI).",
          "quote": "Explainable Machine Learning is a research area that aims to render the models transparent concerning their decision-making mechanisms. This paper contributes to this emerging field by introducing a novel method called Listenable Maps for Audio Classifiers (L-MAC)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "L-MAC",
          "justification": "The focus of the paper is to introduce and evaluate the L-MAC model as a posthoc interpretation method for audio classifiers.",
          "quote": "To address this issue, we introduce Listenable Maps for Audio Classifiers (L-MAC), a posthoc interpretation method that generates faithful and listenable interpretations."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CNN14",
          "justification": "The CNN14 model is used as the pretrained classifier on which L-MAC is evaluated and trained.",
          "quote": "In these experiments, we first train a CNN14 classifier (Kong et al., 2020) on the ESC-50 dataset (Piczak) augmented with WHAM! noise, to simulate real-world mixtures. The classifier is trained on folds 1, 2, and 3 and obtains 75% and 78% classification accuracy on folds 5 and 4, respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ESC-50",
          "justification": "ESC-50 dataset is used for training and evaluating the CNN14 classifier and L-MAC method.",
          "quote": "In these experiments, we first train a CNN14 classifier (Kong et al., 2020) on the ESC-50 dataset (Piczak) augmented with WHAM! noise, to simulate real-world mixtures."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "VGG-Sound",
          "justification": "The CNN14 classifier is pre-trained on the VGG-Sound dataset, as mentioned in the paper.",
          "quote": "The CNN14 classifier we employed has 12 2D convolutional layers and is pre-trained on the VGG-sound dataset (Chen et al., 2020a) using SimCLR (Chen et al., 2020b)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper references using Captum, a model interpretability library specifically designed for PyTorch.",
          "quote": "For L2I, we have reported results using relevance thresholds RT=0.2, 0.4, 0.6, and 0.8 (an important hyper-parameter for the L2I method). We have used the Captum implementations (Kokhlikyan et al., 2020) for the gradient-based methods and SHAP."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Captum",
          "justification": "The Captum library is used for implementing and comparing gradient-based interpretation methods.",
          "quote": "We have used the Captum implementations (Kokhlikyan et al., 2020) for the gradient-based methods and SHAP."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SHAP",
          "justification": "The SHAP library is used for comparing interpretation methods in the experiments.",
          "quote": "We have used the Captum implementations (Kokhlikyan et al., 2020) for the gradient-based methods and SHAP."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1231,
    "prompt_tokens": 12263,
    "total_tokens": 13494
  }
}