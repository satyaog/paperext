{
  "paper": "2310.08278.txt",
  "words": 12676,
  "extractions": {
    "title": {
      "value": "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting",
      "justification": "Based on the title and abstract, the main contribution is the Lag-Llama model for probabilistic time series forecasting.",
      "quote": "Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting"
    },
    "description": "This paper introduces Lag-Llama, a foundation model for univariate probabilistic time series forecasting. The model is based on a decoder-only transformer architecture that uses lags as covariates and is pretrained on a large corpus of diverse time series data from several domains. The paper demonstrates Lag-Llama's strong zero-shot generalization capabilities and its state-of-the-art performance when fine-tuned on new datasets.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves the implementation of a new model, pretraining it on a diverse corpus, and testing its performance on a variety of datasets, which are all empirical methods.",
      "quote": "We demonstrate the strong few-shot adaptation performance of Lag-Llama on previously unseen datasets, across varying fractions of data history being available. We investigate the diversity of the pretraining corpus used to train Lag-Llama, and present the scaling laws of Lag-Llama with respect to the pretraining data."
    },
    "primary_research_field": {
      "name": {
        "value": "Time Series Forecasting",
        "justification": "The entire paper focuses on developing and evaluating a model specifically for time series forecasting.",
        "quote": "Foundation models are an emerging paradigm of selfsupervised (or) unsupervised learning on large datasets (Bommasani et al., 2022)."
      },
      "aliases": [
        "TSF"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Probabilistic Forecasting",
          "justification": "The paper discusses and evaluates probabilistic modeling of time series data extensively.",
          "quote": "We present Lag-Llama, a foundation model for univariate probabilistic forecasting."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Transformer Models in Time Series",
          "justification": "The model presented is a transformer-based architecture adapted for time series forecasting.",
          "quote": "Lag-Llama is a general-purpose foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Lag-Llama",
          "justification": "Lag-Llama is the primary model developed and evaluated in this paper.",
          "quote": "We present Lag-Llama— a foundation model for probabilistic time series forecasting trained on a large collection of open time series data, and evaluated on unseen time series datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Lag-Llama is introduced by the authors of the paper.",
          "quote": "We present Lag-Llama— a foundation model for probabilistic time series forecasting..."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments and evaluations mention running the model in different configurations.",
          "quote": "We evaluate zero-shot performance of our pretrained Lag-Llama on the unseen datasets..."
        },
        "is_compared": {
          "value": true,
          "justification": "Lag-Llama is compared to several other models in different scenarios throughout the paper.",
          "quote": "Tab. 1 presents the results comparing the performance of supervised baselines trained on specific datasets to the pretrained Lag-Llama zero-shot performance on the unseen datasets, and to finetuned Lag-Llama on the respective unseen datasets."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper describes a new model rather than referencing an existing one.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Beijing PM2.5",
          "justification": "This dataset is used for evaluating the Lag-Llama model.",
          "quote": "On the exchange-rate dataset coming from an entirely new domain, exhibiting a new unseen frequency, Lag-Llama has comparable zero-shot performance, and when finetuned achieves performance similar to the state-of-the-art."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Developing Fine-Scale Geographic Estimates of Air Pollution Exposure Concentrations for Population Studies in Beijing; Stowell",
          "justification": "The paper discusses the use and importance of this dataset.",
          "quote": "Developing Fine-Scale Geographic Estimates of Air Pollution Exposure Concentrations for Population Studies in Beijing; Stowell"
        }
      },
      {
        "name": {
          "value": "ETT",
          "justification": "This dataset is used for evaluating the model's performance.",
          "quote": "We pretrain a foundation model on a large corpus of diverse datasets..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
          "justification": "The paper uses the ETT dataset extensively for model evaluation.",
          "quote": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting; Zhou"
        }
      },
      {
        "name": {
          "value": "Exchange Rate",
          "justification": "This is one of the datasets used to test the Lag-Llama model.",
          "quote": "On the exchange-rate dataset coming from an entirely new domain, exhibiting a new unseen frequency, Lag-Llama has comparable zero-shot performance."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Macroeconomic Time Series Data Filtering; Lütkepohl",
          "justification": "The paper benchmarks performance on this dataset.",
          "quote": "Macroeconomic Time Series Data Filtering; Lütkepohl"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "GluonTS",
          "justification": "GluonTS is mentioned as being a tool used in the experiments.",
          "quote": "GluonTS: Probabilistic and Neural Time Series Modeling in Python."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "GluonTS: Probabilistic and Neural Time Series Modeling in Python",
          "justification": "The paper references GluonTS for probabilistic and neural time series modeling.",
          "quote": "GluonTS: Probabilistic and Neural Time Series Modeling in Python"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1498,
    "prompt_tokens": 27335,
    "total_tokens": 28833
  }
}