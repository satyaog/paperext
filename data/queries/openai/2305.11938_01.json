{
  "paper": "2305.11938.txt",
  "words": 17798,
  "extractions": {
    "title": {
      "value": "XTREME-UP: A User-Centric Scarce-ata Benchmark for Under-Represented Languages",
      "justification": "The title is clearly mentioned at the beginning and in the header of the paper.",
      "quote": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages"
    },
    "description": "XTREME-UP is a benchmark designed to focus on the scarce-data scenario rather than zero-shot, evaluating the capabilities of language models across 88 under-represented languages in 9 key user-centric technologies.",
    "type": {
      "value": "empirical study",
      "justification": "The paper discusses the creation and evaluation of the XTREME-UP benchmark using various existing datasets and language models, which is indicative of empirical research.",
      "quote": "Our results highlight the limitations of current models on ULs, demonstrate the potential of language models (LMs) to improve user-centric applications, and show the benefit of byte-based approaches, among other findings."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on evaluating NLP capabilities such as ASR, OCR, MT, and more, across multiple languages.",
        "quote": "XTREME-UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks that are of general utility."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multilingual and Low-Resource Language Processing",
          "justification": "The focus is on under-represented languages and tasks that can be tackled using textual data, specifically for languages with scarce data.",
          "quote": "XTREME-UP focuses on under-represented languages and user-centric tasks, creating new data for under-represented tasks and languages."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "mT5",
          "justification": "mT5 is explicitly mentioned and evaluated across various tasks within the XTREME-UP benchmark.",
          "quote": "The data for each language is sub-sampled to emulate data sizes that can be realistically annotated within a reasonable time frame... We evaluate mT5-base (Xue et al., 2021)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ByT5",
          "justification": "ByT5 is discussed and evaluated within the XTREME-UP benchmark, particularly emphasizing the byte-based approaches.",
          "quote": "ByT5-base (Xue et al., 2022), a byte-based multilingual encoder-decoder model."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Flan-PaLM",
          "justification": "Flan-PaLM is highlighted as an in-context learning model evaluated in the XTREME-UP benchmark.",
          "quote": "For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of PaLM (Chowdhery et al., 2022)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "FLEURS",
          "justification": "FLEURS is used as a dataset for evaluating automatic speech recognition (ASR) in the XTREME-UP benchmark.",
          "quote": "We employ the FLEURS dataset (Conneau et al., 2023) consisting of recordings in 102 languages for sentences from FLORES-101 (Goyal et al., 2022)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Universal Dependencies",
          "justification": "Universal Dependencies is utilized for creating high-quality natural language data for the autocomplete task.",
          "quote": "We process high-quality natural language data from Universal Dependencies (de Marneffe et al., 2021)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Dakshina",
          "justification": "The Dakshina dataset is mentioned as part of the transliteration tasks within the XTREME-UP benchmark.",
          "quote": "Most of the data for the task comes from the romanized full-string subset of the Dakshina dataset (Roark et al., 2020)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "FLORES-101",
          "justification": "FLORES-101 is adapted for machine translation tasks in the XTREME-UP benchmark.",
          "quote": "The dataset is adapted from FLORES-101 (Goyal et al., 2022)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "TyDi QA",
          "justification": "TyDi QA is utilized in the question answering tasks within the XTREME-UP benchmark.",
          "quote": "In the in-language QA task, both the question and passage are from the TyDi QA dataset (Clark et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MasakhaNER",
          "justification": "MasakhaNER is used for named entity recognition (NER) tasks in XTREME-UP benchmark.",
          "quote": "We build on MasakhaNER (Adelani et al., 2021) and MasakhaNER 2.0 (Adelani et al., 2022)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "T5X",
          "justification": "T5X is mentioned as the infrastructure used for training models in the XTREME-UP benchmark.",
          "quote": "Models were trained using seqio and T5X (Roberts et al., 2022) on TPUs (Kumar et al., 2019; Pope et al., 2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "seqio",
          "justification": "seqio is used alongside T5X for training the models within the XTREME-UP benchmark",
          "quote": "Models were trained using seqio and T5X (Roberts et al., 2022) on TPUs (Kumar et al., 2019; Pope et al., 2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1379,
    "prompt_tokens": 31572,
    "total_tokens": 32951
  }
}