{
  "paper": "2305.18388.txt",
  "words": 14106,
  "extractions": {
    "title": {
      "value": "The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation",
      "justification": "Directly mentioned in the given research paper details",
      "quote": "The Statistical Benefits of Quantile Temporal-Difference Learning for Value Estimation"
    },
    "description": "This paper studies the application of Quantile Temporal-Difference (QTD) learning in the context of reinforcement learning, specifically for value estimation tasks. The paper provides theoretical analysis and empirical results demonstrating that QTD can offer superior performance for value estimation compared to classical TD learning in various scenarios, including those with high stochasticity.",
    "type": {
      "value": "Empirical",
      "justification": "This paper combines theoretical analysis with extensive empirical studies to validate the performance of QTD algorithms.",
      "quote": "We develop the theory of QTD for mean estimation in Section 4, establishing asymptotic guarantees on the quality of the value predictions that the algorithm makes. Second, we conduct further empirical investigations in Section 5, aiming to develop a more nuanced understanding of the relative performance of QTD and TD in practice."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The study is focused on policy evaluation and value estimation within the domain of reinforcement learning using temporal difference methods.",
        "quote": "We study the problem of temporal-difference based policy evaluation in reinforcement learning."
      },
      "aliases": [
        "Reinforcement Learning (RL)"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Distributional Reinforcement Learning",
          "justification": "The paper specifically addresses distributional RL algorithms including QTD, which aims to learn the full distribution of returns rather than just the mean.",
          "quote": "In particular, we analyse the use of a distributional reinforcement learning algorithm, quantile temporal-difference learning (QTD), for this task."
        },
        "aliases": [
          "Distributional RL",
          "Distributional Reinforcement Learning (DRL)"
        ]
      },
      {
        "name": {
          "value": "Policy Evaluation",
          "justification": "The focus is on value estimation methods which are a core part of policy evaluation in reinforcement learning.",
          "quote": "These findings have consequences for both theoreticians and practitioners. For the former, QTD represents a distinct fundamental approach to the problem of value prediction in RL, often with complementary performance to classical TD, and raises a range of open questions."
        },
        "aliases": [
          "Value Estimation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Quantile Temporal-Difference Learning (QTD)",
          "justification": "Quantile Temporal-Difference Learning (QTD) is the primary focus of this study, with the aim of evaluating its effectiveness in policy evaluation in reinforcement learning.",
          "quote": "In particular, we analyse the use of a distributional reinforcement learning algorithm, quantile temporal-difference learning (QTD), for this task."
        },
        "aliases": [
          "QTD"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper discusses QTD extensively as part of its core contributions, including theoretical foundations and empirical performance.",
          "quote": "Even in the tabular setting, there are many scenarios where quantile temporal-difference learning (QTD; Dabney et al., 2018b), a distributional RL algorithm which aims to learn quantiles of the return distribution, can more accurately estimate the expected return than classical temporal-difference learning (TD; Sutton, 1984; 1988) which predicts only the expected return."
        },
        "is_executed": {
          "value": 1,
          "justification": "The empirical evaluation involves executing the models to measure mean-squared errors in various environments, which implies model execution.",
          "quote": "We run both TD and QTD (using 128 quantiles) with a variety of learning rates, and measure the mean-squared error to the true value function after 1,000 updates via online interaction with the environments."
        },
        "is_compared": {
          "value": 1,
          "justification": "QTD performance is empirically compared to TD across a variety of environments and conditions in the paper.",
          "quote": "We run both TD and QTD (using 128 quantiles) with a variety of learning rates, and measure the mean-squared error to the true value function after 1,000 updates via online interaction with the environments."
        },
        "referenced_paper_title": {
          "value": "Distributional reinforcement learning with quantile regression",
          "justification": "The referenced paper by Dabney et al., 2018b is a foundational work on QTD, as highlighted in the main text.",
          "quote": "quantile temporal-difference learning (QTD; Dabney et al., 2018b), a distributional RL algorithm which aims to learn quantiles of the return distribution"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 922,
    "prompt_tokens": 24351,
    "total_tokens": 25273
  }
}