{
  "paper": "2310.09163.txt",
  "words": 16046,
  "extractions": {
    "title": {
      "value": "Jointly-Learned Exit and Inference for a Dynamic Neural Network: JEI-DNN",
      "justification": "The title captures the main focus of the research work, which is on designing a jointly learned system for dynamic neural networks.",
      "quote": "Jointly-Learned Exit and Inference for a Dynamic Neural Network: JEI-DNN"
    },
    "description": "The paper proposes a novel training procedure for dynamic neural networks called JEI-DNN that jointly optimizes gating mechanisms and intermediate inference modules. This joint optimization aims to address the train-test mismatch issue and improve uncertainty characterization, leading to significant performance improvements in classification tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts a series of experiments to validate the effectiveness of the proposed JEI-DNN method on various datasets and models, thus making it an empirical study.",
      "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 (Yuan et al., 2021) pretrained on the ImageNet dataset (Deng et al., 2009) which we then transfer-learn to the datasets: CIFAR10, CIFAR100, CIFAR100-LT (Krizhevsky, 2009) and SVHN (Netzer et al., 2011)."
    },
    "primary_research_field": {
      "name": {
        "value": "Dynamic Neural Networks",
        "justification": "The primary focus of the paper is on optimizing and improving dynamic neural networks through a novel joint learning technique.",
        "quote": "The paper focuses on early-exiting dynamic neural networks (EDNN) and proposes a novel learning procedure for the GMs and IMs given a fixed backbone network."
      },
      "aliases": [
        "Dynamic Neural Networks",
        "DNN"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Uncertainty Characterization",
          "justification": "One of the key contributions of the paper is improving uncertainty characterization capabilities.",
          "quote": "The benefits are threefold: 1) we close the training gap between IMs and GMs, which leads to better performance; 2) the architecture produces reliable uncertainty characterization in the form of conformal intervals and well-calibrated predicted probabilities."
        },
        "aliases": [
          "Uncertainty Characterization"
        ]
      },
      {
        "name": {
          "value": "Intermediate Inference Modules",
          "justification": "Optimizing Intermediate Inference Modules (IMs) is a significant part of the proposed method.",
          "quote": "Our approach involves joint training so it directly avoids train-test mismatch and provides good uncertainty characterization."
        },
        "aliases": [
          "IMs"
        ]
      },
      {
        "name": {
          "value": "Gating Mechanisms",
          "justification": "The paper addresses the training and optimization of Gating Mechanisms (GMs) in dynamic neural networks.",
          "quote": "The method introduces a new approach for modeling the probability of exiting at a particular inference module."
        },
        "aliases": [
          "GMs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "JEI-DNN",
          "justification": "It is the primary model proposed and validated in the paper.",
          "quote": "We introduce our method named: Jointly-Learned Exit and Inference for a Dynamic Neural Network (JEI-DNN)"
        },
        "aliases": [
          "JEI-DNN"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is a novel contribution proposed by the paper.",
          "quote": "We introduce our method named: Jointly-Learned Exit and Inference for a Dynamic Neural Network (JEI-DNN)"
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed and validated through experiments.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 (Yuan et al., 2021) pretrained on the ImageNet dataset (Deng et al., 2009) which we then transfer-learn to the datasets: CIFAR10, CIFAR100, CIFAR100-LT (Krizhevsky, 2009) and SVHN (Netzer et al., 2011)."
        },
        "is_compared": {
          "value": true,
          "justification": "The JEI-DNN model is empirically compared against other models.",
          "quote": "We compare our proposal with the following baselines: BoostedNet (Yu et al., 2023) and L2W-DEN (Han et al., 2022b) are state-of-the-art benchmarks with architecture-agnostic training procedures for general-purpose networks."
        },
        "referenced_paper_title": {
          "value": "Jointly-Learned Exit and Inference for a Dynamic Neural Network: JEI-DNN",
          "justification": "The referenced paper title is the same as the primary paper title.",
          "quote": "Jointly-Learned Exit and Inference for a Dynamic Neural Network: JEI-DNN"
        }
      },
      {
        "name": {
          "value": "T2T-ViT-7",
          "justification": "The model is utilized in the experimentation to validate the proposed method.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 (Yuan et al., 2021) pretrained on the ImageNet dataset (Deng et al., 2009)"
        },
        "aliases": [
          "T2T-ViT-7"
        ],
        "is_contributed": {
          "value": false,
          "justification": "T2T-ViT-7 is not a contribution of this paper but a pre-trained model utilized for experiments.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 (Yuan et al., 2021) pretrained on the ImageNet dataset (Deng et al., 2009)"
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed as part of the experimental validations.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 (Yuan et al., 2021) pretrained on the ImageNet dataset."
        },
        "is_compared": {
          "value": false,
          "justification": "The primary comparison is between JEI-DNN and other state-of-the-art models.",
          "quote": "We compare our proposal with the following baselines: BoostedNet (Yu et al., 2023) and L2W-DEN (Han et al., 2022b) are state-of-the-art benchmarks."
        },
        "referenced_paper_title": {
          "value": "Training vision transformers from scratch on imagenet",
          "justification": "The referenced paper provides the detail about the T2T-ViT-7 and T2T-ViT-14 models.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 (Yuan et al., 2021) pretrained on the ImageNet dataset."
        }
      },
      {
        "name": {
          "value": "T2T-ViT-14",
          "justification": "The model is utilized in the experimentation to validate the proposed method.",
          "quote": "We use the vision transformers T2T-ViT-14."
        },
        "aliases": [
          "T2T-ViT-14"
        ],
        "is_contributed": {
          "value": false,
          "justification": "T2T-ViT-14 is not a contribution of this paper but a pre-trained model utilized for experiments.",
          "quote": "We use the vision transformers T2T-ViT-14."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed as part of the experimental validations.",
          "quote": "We use the vision transformers T2T-ViT-14 pretrained on the ImageNet dataset."
        },
        "is_compared": {
          "value": false,
          "justification": "The primary comparison is between JEI-DNN and other state-of-the-art models.",
          "quote": "We compare our proposal with the following baselines: BoostedNet (Yu et al., 2023) and L2W-DEN (Han et al., 2022b) are state-of-the-art benchmarks."
        },
        "referenced_paper_title": {
          "value": "Training vision transformers from scratch on imagenet",
          "justification": "The referenced paper provides the detail about the T2T-ViT-7 and T2T-ViT-14 models.",
          "quote": "We use the vision transformers T2T-ViT-14 pretrained on the ImageNet dataset."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "The ImageNet dataset is used as part of the experimental setup.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 (Yuan et al., 2021) pretrained on the ImageNet dataset."
        },
        "aliases": [
          "ImageNet"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "The referenced paper provides the detail about the ImageNet dataset.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 (Yuan et al., 2021) pretrained on the ImageNet dataset (Deng et al., 2009)"
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The CIFAR-10 dataset is used as part of the experimental setup.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 pretrained on the ImageNet dataset which we then transfer-learn to the datasets: CIFAR10."
        },
        "aliases": [
          "CIFAR-10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The referenced paper provides the detail about the CIFAR-10 dataset.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 pretrained on the ImageNet dataset which we then transfer-learn to the datasets: CIFAR10."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "The CIFAR-100 dataset is used as part of the experimental setup.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 pretrained on the ImageNet dataset which we then transfer-learn to the datasets: CIFAR100."
        },
        "aliases": [
          "CIFAR-100"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The referenced paper provides the detail about the CIFAR-100 dataset.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 pretrained on the ImageNet dataset which we then transfer-learn to the datasets: CIFAR100."
        }
      },
      {
        "name": {
          "value": "CIFAR-100-LT",
          "justification": "The CIFAR-100-LT dataset is used as part of the experimental setup.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 pretrained on the ImageNet dataset which we then transfer-learn to the datasets: CIFAR100-LT."
        },
        "aliases": [
          "CIFAR-100-LT"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The referenced paper provides the detail about the CIFAR-100 dataset and its long-tail version CIFAR-100-LT.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 pretrained on the ImageNet dataset which we then transfer-learn to the datasets: CIFAR100-LT."
        }
      },
      {
        "name": {
          "value": "SVHN",
          "justification": "The SVHN dataset is used as part of the experimental setup.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 pretrained on the ImageNet dataset which we then transfer-learn to the datasets: SVHN."
        },
        "aliases": [
          "SVHN"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Reading Digits in Natural Images with Unsupervised Feature Learning",
          "justification": "The referenced paper provides the detail about the SVHN dataset.",
          "quote": "We use the vision transformers T2T-ViT-7 and T2T-ViT-14 pretrained on the ImageNet dataset which we then transfer-learn to the datasets: SVHN."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3002,
    "prompt_tokens": 28292,
    "total_tokens": 31294
  }
}