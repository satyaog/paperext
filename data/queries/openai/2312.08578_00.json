{
  "paper": "2312.08578.txt",
  "words": 13198,
  "extractions": {
    "title": {
      "value": "A Picture is Worth More Than 0 Text Tokens: Evaluating CLIP-Style Models on Dense Captions",
      "justification": "This is the title mentioned at the beginning of the paper.",
      "quote": "A Picture is Worth More Than 0 Text Tokens: Evaluating CLIP-Style Models on Dense Captions"
    },
    "description": "The paper introduces the Densely Captioned Images (DCI) dataset containing 7805 images with dense and mask-aligned descriptions. It evaluates vision-language models (VLMs) for fine-grained vision-language understanding using a summarized version of the dataset (sDCI) to fit into CLIP’s 77 token limit.",
    "type": {
      "value": "empirical study",
      "justification": "The paper conducts empirical evaluations of vision-language models using the new dataset and compares their performances.",
      "quote": "We demonstrate how to leverage this dataset to evaluate VLMs in two ways after summarizing captions to fit into CLIP’s 77 token limit, both with a negatives-based test as well as a novel matching task..."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper is primarily centered around evaluating vision-language models and creating a new image-caption dataset, which falls under Computer Vision.",
        "quote": "In this paper, we introduce the Densely Captioned Images dataset, a collection of 7805 images with dense and mask-aligned descriptions averaging above 1000 words each."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper also deals significantly with text processing, summarizing long captions to fit within the constraints of models like CLIP.",
          "quote": "To show the value of dense and highly-aligned image-text pairs, we collect the Densely Captioned Images (DCI) dataset..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Vision-Language Models",
          "justification": "The models evaluated in the paper and the tasks defined fall under the purview of Vision-Language Models.",
          "quote": "We evaluate vision-language models (VLMs) understanding of image content with a novel task..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is one of the main models evaluated and fine-tuned in this study.",
          "quote": "Lastly, we finetune CLIP using sDCI and show significant improvements over the baseline despite a small training set."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CLIP is an existing model used for evaluation purposes rather than a contribution made by this paper.",
          "quote": "Lastly, we finetune CLIP using sDCI and show significant improvements over the baseline despite a small training set."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions fine-tuning and evaluating CLIP.",
          "quote": "Lastly, we finetune CLIP using sDCI and show significant improvements over the baseline despite a small training set."
        },
        "is_compared": {
          "value": 1,
          "justification": "CLIP is numerically compared to NegCLIP, BLIP, Flava, and X-VLM models in various tasks.",
          "quote": "We compare in Table 2 the sDCI performances given by different state-of-the-art models: CLIP, NegCLIP, BLIP, Flava, X-VLM."
        },
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models from Natural Language Supervision",
          "justification": "This is the referenced paper for the CLIP model as per the usual citation format in machine learning papers.",
          "quote": "CLIP (Radford et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Densely Captioned Images (DCI)",
          "justification": "The DCI dataset is introduced and used extensively in the paper.",
          "quote": "To show the value of dense and highly-aligned image-text pairs, we collect the Densely Captioned Images (DCI) dataset, containing 7805 natural images human-annotated with mask-aligned descriptions averaging above 1000 words each."
        },
        "aliases": [
          "DCI"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "A Picture is Worth More Than 0 Text Tokens: Evaluating CLIP-Style Models on Dense Captions",
          "justification": "This is the current paper in which the dataset is introduced.",
          "quote": "To show the value of dense and highly-aligned image-text pairs, we collect the Densely Captioned Images (DCI) dataset, containing 7805 natural images human-annotated with mask-aligned descriptions averaging above 1000 words each."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Mephisto",
          "justification": "Mephisto was used to host the annotation tasks for the dataset.",
          "quote": "We use Mephisto (Urbanek and Ringshia, 2023) to host our task, pay crowdworkers to provide annotations on the dataset, and additionally run qualification steps."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Mephisto: A framework for portable, reproducible, and iterative crowdsourcing",
          "justification": "This is the paper referenced for Mephisto as per the citation in the current paper.",
          "quote": "Urbanek and Ringshia, 2023"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1078,
    "prompt_tokens": 25399,
    "total_tokens": 26477
  }
}