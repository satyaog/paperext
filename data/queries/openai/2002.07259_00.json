{
  "paper": "2002.07259.txt",
  "words": 9147,
  "extractions": {
    "title": {
      "value": "Identifying Critical Neurons in ANN Architectures using Mixed Integer Programming",
      "justification": "Title from the paper",
      "quote": "Identifying Critical Neurons in ANN Architectures using Mixed Integer Programming"
    },
    "description": "This paper introduces a mixed integer program (MIP) to assign importance scores to neurons in deep neural network architectures based on their impact on the main learning task of the network. The approach aims to minimize the number of critical neurons needed to maintain the overall accuracy of the trained model and generalizes across multiple datasets by retraining the network weights. The proposed MIP formulation is applied to fully-connected and convolutional layers, which demonstrates the efficiency of the method in pruning neural networks with only marginal losses in accuracy.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results and computational experiments to demonstrate the effectiveness of the proposed MIP framework in pruning neural networks and maintaining accuracy across different datasets.",
      "quote": "Sec. 4 provides computational experiments, and Sec. 5 summarizes our findings."
    },
    "primary_research_field": {
      "name": {
        "value": "Neural Network Pruning",
        "justification": "The paper focuses on pruning neural networks by identifying critical neurons using mixed integer programming, which falls under the sub-field of neural network pruning.",
        "quote": "In this context, pruning neurons from an over-parameterized neural model has been an active research area."
      },
      "aliases": [
        "Neural Network Compression"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Optimization",
          "justification": "The research involves optimizing neural network models by pruning neurons to enhance computational efficiency without significant loss of accuracy.",
          "quote": "The motivation to use such approach comes from the existence of powerful techniques to solve MIPs efficiently in practice, and consequently, to allow the scalability of this procedure to large ReLU neural models."
        },
        "aliases": [
          "Neural Network Optimization"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "VGG-16",
          "justification": "The paper uses the VGG-16 model to demonstrate the scalability and efficiency of the proposed MIP technique in neuron pruning.",
          "quote": "Furthermore, we enhance our approach such that the importance scores computation is also efficient for very deep neural models, like VGG-16."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The VGG-16 model is not a new contribution but an existing one used to demonstrate the proposed method.",
          "quote": "VGG-16 [20]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The VGG-16 model was executed to demonstrate the applicability of the proposed MIP methodology in pruning neural networks.",
          "quote": "Furthermore, we enhance our approach such that the importance scores computation is also efficient for very deep neural models, like VGG-16."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the pruned VGG-16 model against its unpruned version in terms of accuracy.",
          "quote": "Finally, we compare our masking methodology to [28], a framework used to compute connections sensitivity, and to create a sparsified sub-network based on the input dataset and model initialization."
        },
        "referenced_paper_title": {
          "value": "Very Deep Convolutional Networks for Large-scale Image Recognition",
          "justification": "This is the original paper where the VGG-16 model is introduced.",
          "quote": "VGG-16 [20]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The CIFAR-10 dataset is used in the experiments to validate the proposed MIP-based neuron pruning methodology.",
          "quote": "Decoupled greedy learning [51] was used to train each VGG-16â€™s layer using a small auxiliary network, and the neuron importance score was computed independently on each auxiliary network; then we fine-tuned the generated masks for 1 epoch to propagate error across them. Decoupled training of each layer allowed us to represent deep models using the MIP formulation and to parallelize the computation per layer; see appendix for details about decoupled greedy learning."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the original paper that introduced the CIFAR-10 dataset.",
          "quote": "CIFAR-10 [48]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The PyTorch library is mentioned as part of the computational tools used in the experiments.",
          "quote": "Computational Environment The experiments were performed in an Intel(R) Xeon(R) CPU @ 2.30GHz with 12 GB RAM and Tesla k80 using Mosek 9.1.11 [52] solver on top of CVXPY [53, 54] and PyTorch 1.3.1 [55]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the original paper that describes the PyTorch library.",
          "quote": "PyTorch 1.3.1 [55]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1024,
    "prompt_tokens": 16041,
    "total_tokens": 17065
  }
}