{
  "paper": "2308.04014.txt",
  "words": 6508,
  "extractions": {
    "title": {
      "value": "Continual Pre-Training of Large Language Models: How to (re)warm your model?",
      "justification": "The title is given at the beginning of the paper.",
      "quote": "Continual Pre-Training of Large Language Models: How to (re)warm your model?"
    },
    "description": "This paper examines the effect of different warm-up strategies in the continual pre-training of large language models (LLMs). The authors' hypothesis is that re-increasing the learning rate can improve compute efficiency when training on new datasets. The paper uses Pythia 410M architecture and data from the Pile and SlimPajama datasets for their experiments.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments to understand the effect of various warm-up strategies during the continual pre-training of large language models.",
      "quote": "In this work, we examine the effect of different warm-up strategies."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on the continual pre-training of large language models, which is a topic under Natural Language Processing.",
        "quote": "Large pre-trained models have enabled massive performance improvements for many downstream tasks in vision and language."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The primary focus of the paper is on continual pre-training of language models.",
          "quote": "A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e., updating pre-trained models with new data instead of re-training them from scratch."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Pythia 410M",
          "justification": "The Pythia 410M model architecture is explicitly mentioned as the model used for the experiments.",
          "quote": "We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not a new contribution but rather used for experimental purposes.",
          "quote": "We conduct all experiments on the Pythia 410M language model architecture."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was actively used in experiments.",
          "quote": "We conduct all experiments on the Pythia 410M language model architecture and evaluate performance through validation perplexity."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was compared under various conditions as part of the study.",
          "quote": "Our results show that while rewarming models first increases the loss on upstream and downstream data, in the longer run, it improves the downstream performance."
        },
        "referenced_paper_title": {
          "value": "Pythia: A suite for analyzing large language models across training and scaling",
          "justification": "The paper references the Pythia model and its attributes, including pre-training.",
          "quote": "Model â€“ We use the 410M Pythia pre-trained on the Pile (Biderman et al., 2023), i.e., GPT-NeoX (Black et al., 2022) models."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "The Pile",
          "justification": "The Pile is referred to as the upstream dataset used for pre-training.",
          "quote": "We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
          "justification": "The paper mentions this dataset as a reference for pre-training.",
          "quote": "In our setup, the upstream (or pre-training) dataset is the Pile (Gao et al., 2020)."
        }
      },
      {
        "name": {
          "value": "SlimPajama",
          "justification": "SlimPajama is used as the downstream dataset in the experiments.",
          "quote": "We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SlimPajama: A 627B Token Cleaned and Deduplicated Version of RedPajama",
          "justification": "The dataset is mentioned as part of the new data used for continual pre-training.",
          "quote": "In our setup, the upstream (or pre-training) dataset is the Pile (Gao et al., 2020). The downstream (or fine-tuning) dataset is SlimPajama (Soboleva et al., 2023)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions PyTorch as the framework used for the model.",
          "quote": "GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic Differentiation in PyTorch",
          "justification": "The paper references PyTorch, the library used for developing the models.",
          "quote": "GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch."
        }
      },
      {
        "name": {
          "value": "HuggingFace Transformers",
          "justification": "The HuggingFace Transformers library is explicitly mentioned.",
          "quote": "Transformers: State-of-the-Art Natural Language Processing."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-Art Natural Language Processing",
          "justification": "The Transformers library from HuggingFace is mentioned as part of the tools used.",
          "quote": "Transformers: State-of-the-Art Natural Language Processing."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1467,
    "prompt_tokens": 13715,
    "total_tokens": 15182
  }
}