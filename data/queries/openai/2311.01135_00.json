{
  "paper": "2311.01135.txt",
  "words": 7264,
  "extractions": {
    "title": {
      "value": "Generating QM1B with PySCFIPU",
      "justification": "The title is explicitly mentioned at the beginning of the paper.",
      "quote": "\"Generating QM1B with PySCFIPU\""
    },
    "description": "This paper introduces a new data generator called PySCFIPU, which utilizes Intelligence Processing Units (IPUs) to create a large quantum chemistry dataset called QM1B. The dataset consists of one billion training examples, aiming to overcome the limitations of previous DFT datasets. The paper validates the dataset's effectiveness by training a baseline neural network model (SchNet), showing performance improvements with increased training data.",
    "type": {
      "value": "Empirical",
      "justification": "The focus of the paper is on the creation of a large dataset and the empirical validation of its usefulness using a baseline neural network model.",
      "quote": "In this paper, we take a first step towards utilising hardware accelerators by introducing the data generator PySCFIPU using Intelligence Processing Units (IPUs). This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms. We demonstrate that a simple baseline neural network (SchNet 9M) improves its performance by simply increasing the amount of training data without additional inductive biases."
    },
    "primary_research_field": {
      "name": {
        "value": "Molecular Machine Learning",
        "justification": "The paper is centered on generating a large quantum chemistry dataset and its implications for molecular machine learning.",
        "quote": "Foundation models have transformed natural language processing (NLP) (1) and computer vision (CV) (2), but have not yet been demonstrated in the important field of molecular machine learning."
      },
      "aliases": [
        "Molecular ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Quantum Chemistry",
          "justification": "The dataset generated is focused on quantum chemistry properties using Density Functional Theory (DFT).",
          "quote": "However, current molecular datasets are limited to 100k-20M training examples, that are seemingly insufficient to train foundation models."
        },
        "aliases": [
          "QC"
        ]
      },
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The paper involves training and evaluating a neural network model using the generated dataset.",
          "quote": "To investigate whether more training data improves neural networks, we trained a baseline SchNet with 9M parameters on differently sized subsets of QM1B."
        },
        "aliases": [
          "DL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SchNet",
          "justification": "The model is specifically named and used for performance evaluation in the study.",
          "quote": "To investigate whether more training data improves neural networks, we trained a baseline SchNet with 9M parameters on differently sized subsets of QM1B."
        },
        "aliases": [
          "SchNet 9M"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model used was SchNet, which already existed prior to this study.",
          "quote": "For demonstration purposes, we chose to scale one of the simplest models, SchNet (50), to investigate how far performance can be improved by simply scaling training data without incorporating additional architectural innovations."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions the training and evaluation of the SchNet model.",
          "quote": "To investigate whether more training data improves neural networks, we trained a baseline SchNet with 9M parameters on differently sized subsets of QM1B."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the SchNet model was evaluated against different dataset sizes.",
          "quote": "To investigate whether more training data improves neural networks, we trained a baseline SchNet with 9M parameters on differently sized subsets of QM1B."
        },
        "referenced_paper_title": {
          "value": "SchNet â€“ A deep learning architecture for molecules and materials",
          "justification": "The reference is directly associated with the description of the SchNet model.",
          "quote": "SchNet (50)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "QM1B",
          "justification": "The dataset QM1B is the main contribution of this paper.",
          "quote": "This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Generating QM1B with PySCFIPU",
          "justification": "The dataset is directly created and introduced in this paper.",
          "quote": "This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms."
        }
      },
      {
        "name": {
          "value": "QM9",
          "justification": "The QM9 dataset is used for fine-tuning in the study.",
          "quote": "After pre-training SchNet on QM1B we fine-tuned SchNet on QM9."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Quantum Chemistry Structures and Properties of 134 Kilo Molecules",
          "justification": "The reference is directly associated with the description of the QM9 dataset.",
          "quote": "QM9 (6; 7)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PySCFIPU",
          "justification": "PySCFIPU is a key contribution of the paper for generating the QM1B dataset.",
          "quote": "This allowed us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Generating QM1B with PySCFIPU",
          "justification": "PySCFIPU is introduced and detailed in this paper.",
          "quote": "We introduce PySCFIPU, a DFT data generator which utilises Intelligence Processing Units (IPUs(10)) to accelerate molecular property dataset generation."
        }
      },
      {
        "name": {
          "value": "JAX",
          "justification": "JAX is used as the backend for porting computations to IPUs.",
          "quote": "To utilise IPUs we ported PySCF from NumPy to JAX (61), which can target IPUs by using IPU TensorFlow XLA backend (62)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "JAX: Composable transformations of Python+NumPy programs, 2018.",
          "justification": "The reference is directly associated with how JAX is used in the implementation.",
          "quote": "61. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: Composable transformations of Python+NumPy programs, 2018."
        }
      },
      {
        "name": {
          "value": "RDKit",
          "justification": "RDKit is used for generating conformers in the dataset.",
          "quote": "Hydrogen atoms were added by using RDKIT resulting in 0 to 11 Hydrogen atoms."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "RDKit: A Software Suite For Cheminformatics, Computational Chemistry, and Predictive Modeling.",
          "justification": "The reference is directly associated with the usage of RDKit for generating conformers.",
          "quote": "69. Greg Landrum et al. RDKit: A Software Suite For Cheminformatics, Computational Chemistry, and Predictive Modeling. Greg Landrum, 2013."
        }
      },
      {
        "name": {
          "value": "PySCF",
          "justification": "PySCF is the base library that was ported to create PySCFIPU.",
          "quote": "We found IPUs allowed us to speed up DFT data generation due to two reasons: IPUs have 940MB on-chip memory with 12-65TB/s bandwidth, enough to perform small DFT computations without relying on off-chip RAM with < 3TB/s bandwidth."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "PySCF: the Python-based simulations of chemistry framework. Wiley Interdisciplinary Reviews: Computational Molecular Science, 2018.",
          "justification": "PySCF is detailed as the base on which PySCFIPU was built.",
          "quote": "24. Qiming Sun, Timothy C Berkelbach, Nick S Blunt, George H Booth, Sheng Guo, Zhendong Li, Junzi Liu, James D McClain, Elvira R Sayfutyarova, Sandeep Sharma, et al. PySCF: the Python-based simulations of chemistry framework. Wiley Interdisciplinary Reviews: Computational Molecular Science, 2018."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2082,
    "prompt_tokens": 14086,
    "total_tokens": 16168
  }
}