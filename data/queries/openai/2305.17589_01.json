{
  "paper": "2305.17589.txt",
  "words": 10495,
  "extractions": {
    "title": {
      "value": "Graph Inductive Biases in Transformers without Message Passing",
      "justification": "This is the exact title of the paper provided by the user and matches the content reviewed.",
      "quote": "Graph Inductive Biases in Transformers without Message Passing"
    },
    "description": "The paper proposes GRIT (Graph Inductive bias Transformer) that incorporates graph inductive biases without using traditional message-passing modules. It aims to address limitations of message-passing Graph Neural Networks and achieve state-of-the-art performance on various graph datasets.",
    "type": {
      "value": "empirical study",
      "justification": "The paper includes detailed experimental results, benchmarking against other models, and empirical evaluations to demonstrate the effectiveness of the proposed model.",
      "quote": "Along with theoretical justification, we provide ample empirical evidence to demonstrate the effectiveness of our design choices. GRIT achieves state-of-the-art empirical performance across a variety of graph learning benchmarks, both small and large-scale."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on advancements in transformer architectures and graph neural networks, which are central topics in the field of deep learning.",
        "quote": "Transformers for graph data are increasingly widely studied and successful in numerous learning tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Graph Neural Networks",
          "justification": "The paper is centered around innovative techniques for Graph Transformers and their application to graph-based learning tasks, making it specific to the sub-field of Graph Neural Networks within deep learning.",
          "quote": "On the other hand, Graph Transformers without message-passing often perform poorly on smaller datasets, where inductive biases are more important."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GRIT",
          "justification": "GRIT is the central model that the paper introduces and evaluates.",
          "quote": "To bridge this gap, we propose the Graph Inductive bias Transformer (GRIT) â€” a new Graph Transformer that incorporates graph inductive biases without using message passing."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ZINC",
          "justification": "The ZINC dataset is used to benchmark the model's performance.",
          "quote": "On the small ZINC dataset (12,000 graphs) (Dwivedi et al., 2022a), GNNs that rely on message-passing take up the top spots on the leaderboards."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PCQM4Mv2",
          "justification": "The PCQM4Mv2 dataset is used to evaluate the model on large-scale graph regression tasks.",
          "quote": "For the large PCQM4MV2 dataset (about 3,700,000 graphs) (Hu et al., 2021), Graph Transformers take up the top spots."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is used to benchmark the model's performance on graph-based node classification tasks.",
          "quote": "We show that our model has the best mean performance for four of the five datasets with statistically significant improvement... MNIST"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CIFAR10",
          "justification": "The CIFAR10 dataset is used as part of the evaluation of the model on graph-based learning tasks.",
          "quote": "We show that our model has the best mean performance for four of the five datasets with statistically significant improvement... CIFAR10"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PATTERN",
          "justification": "The PATTERN dataset is involved in demonstrating the model's empirical performance.",
          "quote": "We show that our model has the best mean performance for four of the five datasets with statistically significant improvement... PATTERN"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CLUSTER",
          "justification": "The CLUSTER dataset is used to evaluate the model's performance in node classification tasks.",
          "quote": "We show that our model has the best mean performance for four of the five datasets with statistically significant improvement... CLUSTER"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Peptides-func",
          "justification": "Peptides-func is used to evaluate the model on long-range graph benchmarks.",
          "quote": "Next, we evaluate our method on the recently proposed Long-Range Graph Benchmark (LRGB). We conduct experiments on the two peptide graph benchmarks from LRGB, namely Peptides-func and Peptides-struct."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Peptides-struct",
          "justification": "Peptides-struct is used for evaluating the model on structural properties in graphs.",
          "quote": "Next, we evaluate our method on the recently proposed Long-Range Graph Benchmark (LRGB). We conduct experiments on the two peptide graph benchmarks from LRGB, namely Peptides-func and Peptides-struct."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ZINC-full",
          "justification": "ZINC-full is a larger version of the ZINC dataset used for comprehensive evaluation.",
          "quote": "We also test our model on the ZINCfull dataset (Irwin et al., 2012), which is the full version of ZINC that has 250,000 graphs."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1349,
    "prompt_tokens": 19187,
    "total_tokens": 20536
  }
}