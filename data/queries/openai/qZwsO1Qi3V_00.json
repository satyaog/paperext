{
  "paper": "qZwsO1Qi3V.txt",
  "words": 10130,
  "extractions": {
    "title": {
      "value": "Syntactic Substitutability as Unsupervised Dependency Syntax",
      "justification": "This is the title of the research paper provided by the user.",
      "quote": "Syntactic Substitutability as Unsupervised Dependency Syntax Jasper Jian 1 and Siva Reddy 2, 3"
    },
    "description": "This paper presents a method for inducing syntactic structures from language models using the concept of syntactic substitutability without additional training. The method improves parsing accuracy and consistency with underlying syntactic representations.",
    "type": {
      "value": "empirical",
      "justification": "The paper provides experimental results from multiple datasets to validate the proposed method.",
      "quote": "We demonstrate that our method, Syntactic Substitutability as Unsupervised Dependency Syntax (SSUD) leads to improvements in dependency parsing accuracy."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus of the paper is on extracting and analyzing syntactic dependencies from language models, which is a core task in Natural Language Processing.",
        "quote": "In recent years, large pretrained language models (LLMs), like BERT (Devlin et al., 2019), have led to impressive performance gains across many natural language processing tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Dependency Parsing",
          "justification": "The main contribution of the paper is a novel method for dependency parsing using the concept of syntactic substitutability.",
          "quote": "We demonstrate that our method, Syntactic Substitutability as Unsupervised Dependency Syntax (SSUD) leads to improvements in dependency parsing accuracy."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Transformers",
          "justification": "The experiments in the paper heavily rely on Transformer-based models like BERT for extracting syntactic information.",
          "quote": "Previous work has made use of attention distributions from transformer-based LMs."
        },
        "aliases": [
          "Transformer Models"
        ]
      },
      {
        "name": {
          "value": "Syntax",
          "justification": "The paper explores syntactic structures and aims to understand how they can be extracted from language models.",
          "quote": "This paper presents a method for inducing syntactic structures from language models using the concept of syntactic substitutability without additional training."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is used in the experiments to test the proposed method for extracting syntactic dependencies.",
          "quote": "In recent years, large pretrained language models (LLMs), like BERT (Devlin et al., 2019), have led to impressive performance gains across many natural language processing tasks."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BERT is utilized in the research; it is not a contribution of this paper.",
          "quote": "The language model investigated here is BERT (Devlin et al., 2019), a transformer-based language model."
        },
        "is_executed": {
          "value": true,
          "justification": "BERT was used to generate attention distributions for syntactic parsing.",
          "quote": "We wish to extract a tree-shaped syntactic dependency structure ts for a sentence s from the mechanisms or representations of an LLM... We propose attention distributions of self-attention heads as candidate scores."
        },
        "is_compared": {
          "value": true,
          "justification": "BERT's performance in syntactic parsing is compared to other methods and models.",
          "quote": "In Table 3, we provide comparisons to other previously proposed methods. We see that SSUD is competitive with other reported UUAS scores."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "This is the reference paper where BERT was introduced.",
          "quote": "The language model investigated here is BERT (Devlin et al., 2019), a transformer-based language model."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "WSJ10",
          "justification": "WSJ10 is used to evaluate the performance of the proposed method.",
          "quote": "We assess our method using two gold-standard English dependency parsing datasets: (1) the sentence length ≤ 10 test split (section 23) of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993) annotated with Stanford Dependencies (de Marneffe et al., 2006) (WSJ10; 389 sentences)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Generating typed dependency parses from phrase structure parses",
          "justification": "This is the reference paper where the Stanford Dependencies annotation used in WSJ10 was described.",
          "quote": "annotated with Stanford Dependencies (de Marneffe et al., 2006) (WSJ10; 389 sentences)."
        }
      },
      {
        "name": {
          "value": "EN-PUD",
          "justification": "EN-PUD is used to evaluate the performance of the proposed method.",
          "quote": "We assess our method using two gold-standard English dependency parsing datasets: (2) the English section of the Parallel Universal Dependencies dataset annotated with Universal Dependencies (Nivre et al., 2020) (EN-PUD; 1000 sentences)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Universal Dependencies v2: An evergrowing multilingual treebank collection",
          "justification": "This is the reference paper where the Universal Dependencies annotations used in EN-PUD were described.",
          "quote": "the English section of the Parallel Universal Dependencies dataset annotated with Universal Dependencies (Nivre et al., 2020) (ENPUD; 1000 sentences)."
        }
      },
      {
        "name": {
          "value": "Marvin and Linzen Dataset",
          "justification": "This dataset is used to test long-distance subject-verb agreement constructions.",
          "quote": "In this experiment, we use data from Marvin and Linzen (2018) to control for the syntactic structures being evaluated. Specifically, we look at more challenging long-distance subject-verb agreement constructions."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Targeted syntactic evaluation of language models",
          "justification": "This is the reference paper where the Marvin and Linzen dataset was described.",
          "quote": "we use data from Marvin and Linzen (2018) to control for the syntactic structures being evaluated."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stanza",
          "justification": "Stanza is used to tag parts of speech in the sentences.",
          "quote": "In order to do so, we use Stanza’s Universal POS tagger (Qi et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Stanza: A Python natural language processing toolkit for many human languages",
          "justification": "This is the reference paper where the Stanza library was described.",
          "quote": "In order to do so, we use Stanza’s Universal POS tagger (Qi et al., 2020)."
        }
      },
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments likely used a deep learning framework for implementation.",
          "quote": "The SSUD experiments can be reproduced with a GPU with 2GB of memory, and a CPU with 24GB of memory."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the reference paper where the PyTorch library was described.",
          "quote": "The SSUD experiments can be reproduced with a GPU with 2GB of memory, and a CPU with 24GB of memory."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3296,
    "prompt_tokens": 39307,
    "total_tokens": 42603
  }
}