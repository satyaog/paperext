{
  "paper": "2212.12653.txt",
  "words": 7198,
  "extractions": {
    "title": {
      "value": "Hyperspherical Quantization: Toward Smaller and More Accurate Models",
      "justification": "This title clearly represents the focus of the paper.",
      "quote": "Hyperspherical Quantization: Toward Smaller and More Accurate Models"
    },
    "description": "The paper proposes an efficient framework for ternary quantization called Hyperspherical Quantization (HQ), to produce smaller and more accurate compressed models. It integrates hyperspherical learning, pruning, and reinitialization to reduce the cosine distance between full-precision and ternary weights.",
    "type": {
      "value": "Empirical",
      "justification": "The paper reports on experiments and results, including compression ratios and accuracy metrics.",
      "quote": "We propose an efficient framework for ternary quantization to produce smaller and more accurate compressed models. By integrating hyperspherical learning, pruning and reinitialization, our proposed Hyperspherical Quantization (HQ) method reduces the cosine distance between the full-precision and ternary weights."
    },
    "primary_research_field": {
      "name": {
        "value": "Model Compression",
        "justification": "The primary focus of the research is on compressing deep learning models to make them smaller and more efficient.",
        "quote": "Hyperspherical Quantization (HQ) method reduces the cosine distance between the full-precision and ternary weights, thus reducing the bias of the straight-through gradient estimator during ternary quantization. Compared with existing work at similar compression levels (∼30×, ∼40×), our method significantly improves the test accuracy and reduces the model size."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Quantization",
          "justification": "The method proposed revolves around the concept of quantizing model weights.",
          "quote": "By integrating hyperspherical learning, pruning and reinitialization, our proposed Hyperspherical Quantization (HQ) method reduces the cosine distance between the full-precision and ternary weights."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Pruning",
          "justification": "The method also involves pruning as a key component to reduce model size and maintain accuracy.",
          "quote": "We first pre-train a DNN model with a hyperspherical learning method to preserve the direction information of the model weights, then apply our proposed approach to push the full-precision weights close to their ternary counterparts, and lastly, we combine the straight-through estimator (STE) with a gradually increased threshold to fulfill the ternary quantization process."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet-18",
          "justification": "ResNet-18 is specifically mentioned as one of the models used for demonstrating the efficacy of the HQ method.",
          "quote": "For example, on ImageNet, our method can compress a ResNet-18 model from 45 MB to 939 KB (48× compressed) while the accuracy is only 4% lower than the original accuracy."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper does not claim the creation of ResNet-18, rather it uses it as a baseline model for experiments.",
          "quote": "For example, on ImageNet, our method can compress a ResNet-18 model from 45 MB to 939 KB (48× compressed) while the accuracy is only 4% lower than the original accuracy."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments.",
          "quote": "For image classification, the batch size is 128. The weight decay is 0.0001, and the momentum of stochastic gradient descent (SGD) is 0.9. We use the cosine annealing schedule with restarts every 10 epochs to adjust the learning rates."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper reports specific compression ratios and accuracy metrics for ResNet-18 compared to other models and methods.",
          "quote": "For example, on ImageNet, our method can compress a ResNet-18 model from 45 MB to 939 KB (48× compressed) while the accuracy is only 4% lower than the original accuracy. It is the best result among the existing results (43×, 6.4% accuracy drop)."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "This is the original paper introducing ResNet-18.",
          "quote": "For example, on ImageNet, our method can compress a ResNet-18 model from 45 MB to 939 KB (48× compressed) while the accuracy is only 4% lower than the original accuracy."
        }
      },
      {
        "name": {
          "value": "ResNet-50",
          "justification": "ResNet-50 is another model used to demonstrate the efficacy of the HQ method.",
          "quote": "It compresses ResNet-18 from 45 MB to 1.28MB (35× compressed) while maintaining high accuracy (67.03% vs. 69.7% of the original model), and compress ResNet-50 from 99 MB to 3.1MB (32× compressed) with an accuracy of 74.7% (vs. 76.15% of the original model)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper does not claim the creation of ResNet-50, rather it uses it as a baseline model for experiments.",
          "quote": "It compresses ResNet-18 from 45 MB to 1.28MB (35× compressed) while maintaining high accuracy (67.03% vs. 69.7% of the original model), and compress ResNet-50 from 99 MB to 3.1MB (32× compressed) with an accuracy of 74.7% (vs. 76.15% of the original model)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments.",
          "quote": "For image classification, the batch size is 128. The weight decay is 0.0001, and the momentum of stochastic gradient descent (SGD) is 0.9. We use the cosine annealing schedule with restarts every 10 epochs to adjust the learning rates."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper reports specific compression ratios and accuracy metrics for ResNet-50 compared to other models and methods.",
          "quote": "It compresses ResNet-18 from 45 MB to 1.28MB (35× compressed) while maintaining high accuracy (67.03% vs. 69.7% of the original model), and compress ResNet-50 from 99 MB to 3.1MB (32× compressed) with an accuracy of 74.7% (vs. 76.15% of the original model)."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "This is the original paper introducing ResNet-50.",
          "quote": "It compresses ResNet-18 from 45 MB to 1.28MB (35× compressed) while maintaining high accuracy (67.03% vs. 69.7% of the original model), and compress ResNet-50 from 99 MB to 3.1MB (32× compressed) with an accuracy of 74.7% (vs. 76.15% of the original model)."
        }
      },
      {
        "name": {
          "value": "MobileNetV2",
          "justification": "MobileNetV2 is specifically mentioned as one of the models used for demonstrating the efficacy of the HQ method.",
          "quote": "For MobileNetV2, our method performs better at above 15× compression level."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper does not claim the creation of MobileNetV2, rather it uses it as a baseline model for experiments.",
          "quote": "For MobileNetV2, our method performs better at above 15× compression level."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments.",
          "quote": "The pre-trained weights are provided by the PyTorch zoo and Detectron2."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper reports specific compression ratios and accuracy metrics for MobileNetV2 compared to other models and methods.",
          "quote": "For MobileNetV2, our method performs better at above 15× compression level. Quantizing the pointwise layer of MobileNet leads to significant accuracy loss."
        },
        "referenced_paper_title": {
          "value": "MobileNetV2: Inverted Residuals and Linear Bottlenecks",
          "justification": "This is the original paper introducing MobileNetV2.",
          "quote": "For MobileNetV2, our method performs better at above 15× compression level."
        }
      },
      {
        "name": {
          "value": "Mask R-CNN",
          "justification": "Mask R-CNN is specifically mentioned as one of the models used for demonstrating the efficacy of the HQ method for object detection.",
          "quote": "Similar to previous work, we test our method on the Mask R-CNN architecture with ResNet-50 backbone to verify its generalizability."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper does not claim the creation of Mask R-CNN, rather it uses it as a baseline model for experiments.",
          "quote": "Similar to previous work, we test our method on the Mask R-CNN architecture with ResNet-50 backbone to verify its generalizability."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments.",
          "quote": "Similar to previous work, we test our method on the Mask R-CNN architecture with ResNet-50 backbone to verify its generalizability."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper reports results for Mask R-CNN, including Average Precision (AP) with bounding box (bb) and mask (mk) compared to other models.",
          "quote": "As shown in Table 3, compared to ABGD and PQF, our method gives a higher compression ratio and a similar or better recognition result."
        },
        "referenced_paper_title": {
          "value": "Mask R-CNN",
          "justification": "This is the original paper introducing Mask R-CNN.",
          "quote": "Similar to previous work, we test our method on the Mask R-CNN architecture with ResNet-50 backbone to verify its generalizability."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is mentioned as a dataset used to evaluate the performance of the models.",
          "quote": "For image classification, our method significantly outperforms existing works in terms of the size-accuracy trade-off of DNN models. For example, on ImageNet, our method can compress a ResNet-18 model from 45 MB to 939 KB (48× compressed) while the accuracy is only 4% lower than the original accuracy."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "This is the original paper introducing the ImageNet dataset.",
          "quote": "For image classification, our method significantly outperforms existing works in terms of the size-accuracy trade-off of DNN models. For example, on ImageNet, our method can compress a ResNet-18 model from 45 MB to 939 KB (48× compressed) while the accuracy is only 4% lower than the original accuracy."
        }
      },
      {
        "name": {
          "value": "MS COCO",
          "justification": "MS COCO is mentioned as a dataset used to evaluate the performance of the models for object detection.",
          "quote": "For object detection, we use the MS COCO dataset and Mask R-CNN."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Microsoft COCO: Common Objects in Context",
          "justification": "This is the original paper introducing the MS COCO dataset.",
          "quote": "For object detection, we use the MS COCO dataset and Mask R-CNN."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly mentioned as the framework used for implementing the models and running the experiments.",
          "quote": "The pre-trained weights are provided by the PyTorch zoo and Detectron2."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the original paper introducing PyTorch.",
          "quote": "The pre-trained weights are provided by the PyTorch zoo and Detectron2."
        }
      },
      {
        "name": {
          "value": "Detectron2",
          "justification": "Detectron2 is explicitly mentioned as the framework used for implementing the models and running the experiments.",
          "quote": "The pre-trained weights are provided by the PyTorch zoo and Detectron2."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Detectron2",
          "justification": "This is the original paper introducing Detectron2.",
          "quote": "The pre-trained weights are provided by the PyTorch zoo and Detectron2."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3151,
    "prompt_tokens": 14689,
    "total_tokens": 17840
  }
}