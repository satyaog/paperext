{
  "paper": "2404.16020.txt",
  "words": 14795,
  "extractions": {
    "title": {
      "value": "Universal Adversarial Triggers Are Not Universal",
      "justification": "Exact title as mentioned in the paper.",
      "quote": "Universal Adversarial Triggers Are Not Universal"
    },
    "description": "This paper investigates the universality of adversarial triggers across different language models. It highlights the inconsistency in the transferability of these triggers between models aligned by preference optimization (APO) and those aligned by fine-tuning (AFT). The study also reveals that AFT models, although seemingly safe, are more susceptible to adversarial triggers compared to APO models.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results based on empirical investigations across 13 open models and compares the robustness of different alignment strategies.",
      "quote": "In this paper, we concretely show that such adversarial triggers are not universal. We extensively investigate trigger transfer amongst 13 open models and observe inconsistent transfer."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The focus of the paper is on studying the behavior of language models in the context of adversarial triggers.",
        "quote": "These triggers are believed to be universally transferable, i.e., a trigger optimized on one model can jailbreak other models."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Adversarial Attacks",
          "justification": "The main subject of the paper is investigating the transferability of adversarial triggers across different models.",
          "quote": "Recent work has developed optimization procedures to find token sequences, called adversarial triggers, which can elicit unsafe responses from aligned language models."
        },
        "aliases": [
          "Adversarial Triggers"
        ]
      },
      {
        "name": {
          "value": "Large Language Models",
          "justification": "The paper discusses the behavior and alignment of large language models when subjected to adversarial triggers.",
          "quote": "These triggers are believed to be universally transferable, i.e., a trigger optimized on one model can jailbreak other models."
        },
        "aliases": [
          "LLMs"
        ]
      },
      {
        "name": {
          "value": "Model Robustness",
          "justification": "The paper investigates the robustness of models aligned through different methods against adversarial triggers.",
          "quote": "Our experiments further reveal a significant difference in robustness to adversarial triggers between models Aligned by Preference Optimization (APO) and models Aligned by Fine-Tuning (AFT)."
        },
        "aliases": [
          "Model Safety",
          "Adversarial Robustness"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Vicuna-7B",
          "justification": "Vicuna-7B was one of the models experimented with in the study.",
          "quote": "We evaluate three model ensembles that Zou et al. (2023) showed could produce triggers transferable to GPT-3.5, GPT-4, and other models: 1) Vicuna-7B; 2) Vicuna-7B/13B;"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Vicuna-7B is not a new contribution but one of the existing models studied for adversarial trigger transfer.",
          "quote": "We evaluate three model ensembles that Zou et al. (2023) showed could produce triggers transferable to GPT-3.5, GPT-4, and other models: 1) Vicuna-7B; 2) Vicuna-7B/13B;"
        },
        "is_executed": {
          "value": 1,
          "justification": "Vicuna-7B was one of the models on which adversarial triggers were executed.",
          "quote": "We evaluate three model ensembles that Zou et al. (2023) showed could produce triggers transferable to GPT-3.5, GPT-4, and other models: 1) Vicuna-7B; 2) Vicuna-7B/13B;"
        },
        "is_compared": {
          "value": 1,
          "justification": "Vicuna-7B was compared with other models in terms of robustness to adversarial triggers.",
          "quote": "One popular attack (Zou et al., 2023) uses a gradient-guided search to find token sequences, referred to as adversarial triggers, which can elicit harmful responses when appended to user inputs."
        },
        "referenced_paper_title": {
          "value": "Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality, March 2023",
          "justification": "The model Vicuna-7B was discussed with reference to another paper.",
          "quote": "Vicuna-7B"
        }
      },
      {
        "name": {
          "value": "Llama2-7B-Chat",
          "justification": "Llama2-7B-Chat was one of the models experimented with in the study.",
          "quote": "For instance, on Llama2-13B-Chat, we obtain an average ∆ASR of 4.0 across three triggers. Across all nine ensembles, we again observe no consistent transfer to the other models."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Llama2-7B-Chat is not a new contribution but one of the existing models studied for adversarial trigger transfer.",
          "quote": "For instance, on Llama2-13B-Chat, we obtain an average ∆ASR of 4.0 across three triggers. Across all nine ensembles, we again observe no consistent transfer to the other models."
        },
        "is_executed": {
          "value": 1,
          "justification": "Llama2-7B-Chat was one of the models on which adversarial triggers were executed.",
          "quote": "For instance, on Llama2-13B-Chat, we obtain an average ∆ASR of 4.0 across three triggers. Across all nine ensembles, we again observe no consistent transfer to the other models."
        },
        "is_compared": {
          "value": 1,
          "justification": "Llama2-7B-Chat was compared with other models in terms of robustness to adversarial triggers.",
          "quote": "For instance, on Llama2-13B-Chat, we obtain an average ∆ASR of 4.0 across three triggers. Across all nine ensembles, we again observe no consistent transfer to the other models."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open Foundation and Fine-Tuned Chat Models, July 2023",
          "justification": "The model Llama2-7B-Chat was discussed with reference to another paper.",
          "quote": "Llama2-7B-Chat"
        }
      },
      {
        "name": {
          "value": "Starling-7B-β",
          "justification": "Starling-7B-β was one of the models experimented with in the study.",
          "quote": "Notably, we obtain a mean ∆ASR of zero for Gemma-7B-Chat, Llama2-7B-Chat, and Starling-7B-β."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Starling-7B-β is not a new contribution but one of the existing models studied for adversarial trigger transfer.",
          "quote": "Notably, we obtain a mean ∆ASR of zero for Gemma-7B-Chat, Llama2-7B-Chat, and Starling-7B-β."
        },
        "is_executed": {
          "value": 1,
          "justification": "Starling-7B-β was one of the models on which adversarial triggers were executed.",
          "quote": "Notably, we obtain a mean ∆ASR of zero for Gemma-7B-Chat, Llama2-7B-Chat, and Starling-7B-β."
        },
        "is_compared": {
          "value": 1,
          "justification": "Starling-7B-β was compared with other models in terms of robustness to adversarial triggers.",
          "quote": "Notably, we obtain a mean ∆ASR of zero for Gemma-7B-Chat, Llama2-7B-Chat, and Starling-7B-β."
        },
        "referenced_paper_title": {
          "value": "Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF, November 2023",
          "justification": "The model Starling-7B-β was discussed with reference to another paper.",
          "quote": "Starling-7B-β"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AdvBench",
          "justification": "AdvBench was the primary dataset used for evaluating adversarial triggers.",
          "quote": "Dataset. We use examples from AdvBench (Zou et al., 2023) for trigger optimization and evaluation."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Universal and Transferable Adversarial Attacks on Aligned Language Models, July 2023",
          "justification": "The dataset AdvBench was discussed with reference to another paper.",
          "quote": "Dataset. We use examples from AdvBench (Zou et al., 2023) for trigger optimization and evaluation."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2153,
    "prompt_tokens": 34015,
    "total_tokens": 36168
  }
}