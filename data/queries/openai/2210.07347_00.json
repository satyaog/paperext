{
  "paper": "2210.07347.txt",
  "words": 14523,
  "extractions": {
    "title": {
      "value": "DisentangLEMENT OF COrrELATED FACTORS VIA HAUSDORFF FACTORIZED SUPPORT",
      "justification": "Title mentioned at the beginning of the provided text.",
      "quote": "DISENTANGLEMENT OF CORRELATED FACTORS VIA HAUSDORFF FACTORIZED SUPPORT"
    },
    "description": "The paper addresses the challenge of learning disentangled representations in deep learning, specifically focusing on the limitations of existing methods that assume statistical independence of factors. The authors propose a novel criterion called Hausdorff Factorized Support (HFS), which relaxes this assumption by encouraging pairwise factorized support using a Hausdorff distance. This novel approach shows significant improvements across various benchmarks and settings with correlated factors.",
    "type": {
      "value": "empirical",
      "justification": "The study presents empirical results showing the effectiveness of the proposed HFS method in various benchmark experiments. The performance improvements and numerical comparisons are key indicators of empirical research.",
      "quote": "We show that the use of HFS consistently facilitates disentanglement and recovery of ground-truth factors across a variety of correlation settings and benchmarks, even under severe training correlations and correlation shifts, with in parts over +60% in relative improvement over existing disentanglement methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Representation Learning",
        "justification": "The primary focus is on learning disentangled representations for deep learning models, which falls under Representation Learning.",
        "quote": "Disentangled representation learning (Bengio et al., 2013; Higgins et al., 2018) is a promising path to facilitate reliable generalization to in- and out-of-distribution downstream tasks (Bengio et al., 2013; Higgins et al., 2018; Milbich et al., 2020; Dittadi et al., 2021; Horan et al., 2021), on top of being more interpretable and fair (Locatello et al., 2019a; Träuble et al., 2021)."
      },
      "aliases": [
        "Disentangled Representation Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generalization",
          "justification": "The paper emphasizes the goal of generalizing across distribution shifts as a major aspect of its contributions.",
          "quote": "A grand goal in deep learning research is to learn representations capable of generalizing across distribution shifts."
        },
        "aliases": [
          "Robust Generalization"
        ]
      },
      {
        "name": {
          "value": "Unsupervised Learning",
          "justification": "The focus on unsupervised disentanglement methods and their challenges indicates a significant component of unsupervised learning.",
          "quote": "But fully unsupervised disentanglement – our focus in this study – is in theory impossible to achieve in the general unconstrained nonlinear case."
        },
        "aliases": [
          "Unsupervised Disentanglement"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "β-VAE",
          "justification": "The β-VAE is frequently mentioned as a baseline model compared against the proposed HFS method.",
          "quote": "standard unsupervised disentanglement methods (s.a. Higgins et al. (2017); Kim & Mnih (2018); Chen et al. (2018))"
        },
        "aliases": [
          "Beta-VAE"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The β-VAE model is used as a baseline for comparison but is not contributed by this paper.",
          "quote": "standard unsupervised disentanglement methods (s.a. Higgins et al. (2017); Kim & Mnih (2018); Chen et al. (2018))"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical comparisons of β-VAE's performance with and without the proposed HFS method, indicating the model was executed as part of the experiments.",
          "quote": "On downstream classification tasks, we improve generalization to more severe distribution shifts and sample efficiency (§4.2, Fig. 2)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The β-VAE model's performance is compared with the proposed HFS method and other baselines throughout the paper.",
          "quote": "Tab. 1 shows HFS consistently improving disentanglement (as measured by DCI-D) by up to +61% over standard methods (β/TC/Factor/Annealed-VAE, c.f. §4.1)."
        },
        "referenced_paper_title": {
          "value": "beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
          "justification": "This is the original paper introducing β-VAE, which is referenced for comparison in the current study.",
          "quote": "Higgins et al. (2017)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Shapes3D",
          "justification": "Listed as one of the benchmarks for evaluating HFS and existing disentanglement methods.",
          "quote": "Across large-scale experiments on standard disentanglement benchmarks and novel extensions with correlated factors, HFS consistently facilitates disentanglement."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Disentangling by Factorising",
          "justification": "Paper where Shapes3D is extensively used and evaluated in the context of disentanglement methods.",
          "quote": "Kim & Mnih (2018)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The implementation and experiments for the study are conducted using the PyTorch framework.",
          "quote": "We implement all our experiments using the PyTorch framework Paszke et al. (2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This study leverages the PyTorch library for its experiments.",
          "quote": "Paszke et al. (2019)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1154,
    "prompt_tokens": 30133,
    "total_tokens": 31287
  }
}