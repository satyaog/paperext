{
  "paper": "2310.16046.txt",
  "words": 11781,
  "extractions": {
    "title": {
      "value": "A Unified, Scalable Framework for Neural Population Decoding",
      "justification": "This is the title of the paper being analyzed.",
      "quote": "A Unified, Scalable Framework for Neural Population Decoding"
    },
    "description": "This paper introduces a scalable framework called POYO for modeling neural population dynamics using large-scale neural recordings. The framework includes a novel tokenization method and a PerceiverIO architecture which enable the integration of multiple neural datasets for robust decoding. Additionally, it discusses the benefits of multi-session training and develops two large-scale pretrained models, POYO-1 and POYO-mp.",
    "type": {
      "value": "Empirical",
      "justification": "The paper includes experiments and results which evaluate the proposed framework.",
      "quote": "In this section, we demonstrate the promise of our approach for large-scale training and examine the benefit of scaling in neural population decoding."
    },
    "primary_research_field": {
      "name": {
        "value": "Neural decoding",
        "justification": "The paper is primarily focused on developing a framework for decoding neural population dynamics.",
        "quote": "Our ability to use deep learning approaches to decipher neural activity would likely benefit from greater scale, in terms of both model size and datasets."
      },
      "aliases": [
        "Neuroscience",
        "Computational Neuroscience"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Transformers",
          "justification": "The framework makes extensive use of transformer architectures for modeling neural data.",
          "quote": "The transformer architecture [14], originally introduced in the context of natural language processing (NLP), has shown remarkable flexibility and effectiveness in various domains, especially in the presence of large and diverse datasets."
        },
        "aliases": [
          "Attention mechanisms"
        ]
      },
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper discusses learning latent representations of neural population activity.",
          "quote": "Utilizing this architecture and training framework, we construct a large-scale multi-session model trained on large datasets from seven nonhuman primates, spanning over 158 different sessions of recording from over 27,373 neural units and over 100 hours of recordings."
        },
        "aliases": [
          "Latent Space"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "POYO-1",
          "justification": "POYO-1 is one of the large pretrained models introduced in the paper.",
          "quote": "We build two large pretrained models (POYO-1, POYO-mp) that can be fine-tuned on new sessions and across recordings from different animals and new behavioral tasks."
        },
        "aliases": [
          "POYO"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "POYO-1 is introduced and developed as part of this research.",
          "quote": "We build two large pretrained models (POYO-1, POYO-mp) that can be fine-tuned on new sessions and across recordings from different animals and new behavioral tasks."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper discusses executing and evaluating the performance of POYO-1 across different tasks and datasets.",
          "quote": "We demonstrate that through pretraining on large amounts of data, we can transfer with very few samples (few-shot learning) and thus improve overall brain decoding performance."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of POYO-1 is compared with other models and baselines.",
          "quote": "The single-session model is the mean over 100 different models."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "POYO-1 is a new model introduced in this current study.",
          "quote": "We build two large pretrained models (POYO-1, POYO-mp) that can be fine-tuned on new sessions and across recordings from different animals and new behavioral tasks."
        }
      },
      {
        "name": {
          "value": "POYO-mp",
          "justification": "POYO-mp is also one of the large pretrained models introduced in the paper.",
          "quote": "We build two large pretrained models (POYO-1, POYO-mp) that can be fine-tuned on new sessions and across recordings from different animals and new behavioral tasks."
        },
        "aliases": [
          "POYO"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "POYO-mp is introduced and developed as part of this research.",
          "quote": "We build two large pretrained models (POYO-1, POYO-mp) that can be fine-tuned on new sessions and across recordings from different animals and new behavioral tasks."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper discusses executing and evaluating the performance of POYO-mp across different tasks and datasets.",
          "quote": "We demonstrate that through pretraining on large amounts of data, we can transfer with very few samples (few-shot learning) and thus improve overall brain decoding performance."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of POYO-mp is compared with other models and baselines.",
          "quote": "The single-session model is the mean over 100 different models."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "POYO-mp is a new model introduced in this current study.",
          "quote": "We build two large pretrained models (POYO-1, POYO-mp) that can be fine-tuned on new sessions and across recordings from different animals and new behavioral tasks."
        }
      },
      {
        "name": {
          "value": "PerceiverIO",
          "justification": "The PerceiverIO backbone is used for building the latent tokenization of neural population activities.",
          "quote": "We then employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural population activities."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "PerceiverIO is not a new model developed in this paper; it is referenced as part of the architecture.",
          "quote": "We then employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural population activities."
        },
        "is_executed": {
          "value": 0,
          "justification": "The implementation specifics of PerceiverIO in the experiments are not detailed.",
          "quote": "We then employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural population activities."
        },
        "is_compared": {
          "value": 0,
          "justification": "PerceiverIO is not directly compared with other models; rather, it is used as a component in the POYO framework.",
          "quote": "We then employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural population activities."
        },
        "referenced_paper_title": {
          "value": "Perceiver IO: A general architecture for structured inputs & outputs",
          "justification": "The referenced paper for PerceiverIO is cited in the text.",
          "quote": "We then employ cross-attention and a PerceiverIO backbone to further construct a latent tokenization of neural population activities."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Neural Latents Benchmark",
          "justification": "The NLB datasets are used as part of the evaluation setup for the models developed in the paper.",
          "quote": "To understand how well our pre-trained model performs on data collected from new animals performing novel tasks with different equipment (example: touch screen vs. manipulandum), we applied our pretrained model to the MC-Maze (Monkey L) and MC-RTT (Monkey I) datasets from the NLB (Table 3)."
        },
        "aliases": [
          "NLB"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Neural Latents Benchmark ‘21: Evaluating latent variable models of neural population activity",
          "justification": "The referenced paper for Neural Latents Benchmark is cited in the text.",
          "quote": "Neural Latents Benchmark ‘21: Evaluating latent variable models of neural population activity"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "LAMB optimizer",
          "justification": "The LAMB optimizer is used for training the models.",
          "quote": "The model is trained using the LAMB optimizer [28] with weight decay."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes",
          "justification": "The referenced paper for the LAMB optimizer is cited in the text.",
          "quote": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2014,
    "prompt_tokens": 19451,
    "total_tokens": 21465
  }
}