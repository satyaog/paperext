{
  "paper": "2306.10998.txt",
  "words": 7814,
  "extractions": {
    "title": {
      "value": "RepoFusion: Training Code Models to Understand Your Repository",
      "justification": "This is the title of the paper present at the beginning of the document.",
      "quote": "RepoFusion: Training Code Models to Understand Your Repository"
    },
    "description": "This paper introduces RepoFusion, a framework designed to train code models to incorporate repository context, improving the models' performance in tasks like single-line code completion. Experiments show significant improvements over larger pre-trained models, and the authors also introduce Stack-Repo, a dataset of 200 Java repositories. The paper includes extensive ablation studies to understand the impact of various design choices.",
    "type": {
      "value": "Empirical",
      "justification": "The paper presents empirical results from experiments including performance evaluations, ablation studies, and comparisons with other models.",
      "quote": "Experiments on single-line code completion show that our models trained with repository context significantly outperform much larger code models as CodeGen-16B-multi (∼ 73× larger) and closely match the performance of the ∼ 70× larger StarCoderBase model that was trained with the Fill-in-the-Middle objective."
    },
    "primary_research_field": {
      "name": {
        "value": "Software Engineering",
        "justification": "The paper focuses on improving code completion and programming tasks by leveraging repository context, which is a part of software engineering.",
        "quote": "In this work, we extend this idea and propose RepoFusion, a framework to train models to incorporate relevant repository context."
      },
      "aliases": [
        "Software Engineering"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The research involves training and evaluating language models for code, which falls under Natural Language Processing (NLP).",
          "quote": "Large Language Models (LLMs) of code [32, 7, 11, 34, 24, 19, 2] have gained significant popularity."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Computer Programming",
          "justification": "The primary application of the research is in code completion and understanding, which are core areas of computer programming.",
          "quote": "Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository."
        },
        "aliases": [
          "Coding"
        ]
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The paper involves training machine learning models to understand and predict code based on the repository context.",
          "quote": "We carry out extensive ablation studies to investigate the impact of design choices such as context type, number of contexts, context length, and initialization within our framework."
        },
        "aliases": [
          "ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "RepoFusion",
          "justification": "RepoFusion is the primary model proposed and evaluated in the paper.",
          "quote": "In this work, we extend this idea and propose RepoFusion, a framework to train models to incorporate relevant repository context."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "RepoFusion is introduced in this paper as a novel approach to incorporating repository context into code models.",
          "quote": "In this work, we extend this idea and propose RepoFusion, a framework to train models to incorporate relevant repository context."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments and ablation studies involving RepoFusion were executed, indicating its implementation and testing.",
          "quote": "We carry out extensive ablation studies to investigate the impact of design choices such as context type, number of contexts, context length, and initialization within our framework."
        },
        "is_compared": {
          "value": 1,
          "justification": "RepoFusion was compared numerically to other models like CodeGen-16B-multi and StarCoderBase in the experiments.",
          "quote": "Experiments on single-line code completion show that our models trained with repository context significantly outperform much larger code models as CodeGen-16B-multi (∼ 73× larger) and closely match the performance of the ∼ 70× larger StarCoderBase model that was trained with the Fill-in-the-Middle objective."
        },
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "RepoFusion is a new model proposed in this paper, so there is no reference paper.",
          "quote": "In this work, we extend this idea and propose RepoFusion, a framework to train models to incorporate relevant repository context."
        }
      },
      {
        "name": {
          "value": "CodeT5",
          "justification": "CodeT5 is used as a base model and also fine-tuned for comparisons in the experiments.",
          "quote": "We use the 220M parameter CodeT5-base [34] encoder-decoder model as our base code model for RepoFusion."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CodeT5 is not a model contributed by this paper; it is used as a baseline and for fine-tuning.",
          "quote": "We use the 220M parameter CodeT5-base [34] encoder-decoder model as our base code model for RepoFusion."
        },
        "is_executed": {
          "value": 1,
          "justification": "CodeT5 was executed as part of the experimental comparisons and fine-tuning processes.",
          "quote": "The finetuning helps a lot.\nTable 7: Completion success rate on the test set for pretrained CodeT5.\nPretrained 2.42 (0.04)"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of CodeT5 is compared with RepoFusion and other models in the study.",
          "quote": "Table 2: Completion success rate on the test set for different methods."
        },
        "referenced_paper_title": {
          "value": "CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
          "justification": "The reference for CodeT5 used in the paper is \"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation\" by Wang et al., 2021.",
          "quote": "CodeT5-base [34] encoder-decoder model"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "The Stack",
          "justification": "The Stack is the modified version used for repository selection in the training process.",
          "quote": "In this work, we build upon a modified version of The Stack V1.1 [17]."
        },
        "aliases": [
          "The Stack V1.1"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Stack: 3 TB of permissively licensed source code",
          "justification": "The referenced paper for The Stack is given by Kocetkov et al., 2022.",
          "quote": "The modified version 5 consists of near-deduplicated code repositories with permissive licenses from GitHub."
        }
      },
      {
        "name": {
          "value": "Stack-Repo",
          "justification": "Stack-Repo is the new dataset introduced in this paper consisting of 200 Java repositories.",
          "quote": "Lastly, we release Stack-Repo, a dataset of 200 Java repositories with permissive licenses and near-deduplicated files that are augmented with three types of repository contexts."
        },
        "aliases": [],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "Stack-Repo is a new dataset introduced for the purpose of this paper; thus, it has no prior reference.",
          "quote": "Lastly, we release Stack-Repo, a dataset of 200 Java repositories"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Rank-BM25",
          "justification": "Rank-BM25 was employed to score and retrieve repo contexts based on similarity.",
          "quote": "The BM25 repo contexts were obtained using the Okapi BM25 implementation with default parameters given by the pip package rank-bm25 0.2.2"
        },
        "aliases": [
          "BM25"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "A probabilistic model of information retrieval: development and comparative experiments - part 1",
          "justification": "The referenced paper for BM25 is by Jones et al., 2000.",
          "quote": "The BM25 repo contexts were obtained using the Okapi BM25 implementation with default parameters given by the pip package rank-bm25 0.2.2 [14]."
        }
      },
      {
        "name": {
          "value": "CodeBERT",
          "justification": "Pre-trained CodeBERT was used to obtain representations for Random-NN repo contexts.",
          "quote": "Random NN repo contexts used the procedure followed by Shrivastava et al. [31] using CodeBERT [10] to obtain the representations"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
          "justification": "The referenced paper for CodeBERT is \"CodeBERT: A Pre-Trained Model for Programming and Natural Languages\" by Feng et al., 2020.",
          "quote": "Random NN repo contexts used the procedure followed by Shrivastava et al. [31] using CodeBERT [10] to obtain the representations (See Appendix C.3 for details)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1794,
    "prompt_tokens": 13952,
    "total_tokens": 15746
  }
}