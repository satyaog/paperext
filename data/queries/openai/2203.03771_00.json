{
  "paper": "2203.03771.txt",
  "words": 9708,
  "extractions": {
    "title": {
      "value": "Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions",
      "justification": "The title of the paper is 'S TATIC P REDICTION OF RUNTIME E RRORS BY L EARNING TO E XECUTE P ROGRAMS WITH E XTERNAL R ESOURCE D ESCRIPTIONS'. Capitalization and formatting were standardized.",
      "quote": "S TATIC P REDICTION OF RUNTIME E RRORS BY L EARNING TO E XECUTE P ROGRAMS WITH E XTERNAL R ESOURCE D ESCRIPTIONS"
    },
    "description": "This paper introduces a dataset and task for predicting runtime errors in Python programs in a static setting, where program execution is not possible. The authors develop an interpreter-inspired architecture that simulates program execution and models exception handling to address this task. The effectiveness of this approach is demonstrated against baseline models like Transformers and LSTMs.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a new dataset and task, conducts experiments with various models, including a novel interpreter-inspired architecture, and evaluates their performance on the proposed task. This makes it an empirical study.",
      "quote": "we introduce a real-world dataset and task for predicting runtime errors... We perform evaluations comparing these interpreter-inspired architectures against Transformer, LSTM, and GGNN baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Programming Languages",
        "justification": "The primary focus is on using machine learning models to predict runtime errors in code, specifically Python programs. This places it within the domain of Programming Languages as it deals with code analysis and error prediction.",
        "quote": "We investigate applying neural machine learning methods to the static analysis of source code for the prediction of runtime errors."
      },
      "aliases": [
        "Code Analysis",
        "Error Prediction"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The paper uses machine learning models such as Transformers and LSTMs as baselines and introduces a novel model to predict runtime errors, making it relevant to Machine Learning.",
          "quote": "we observe that it is a challenging task for generic models like Transformers."
        },
        "aliases": [
          "Neural Networks"
        ]
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper utilizes natural language descriptions of external resources to improve the predictions of the machine learning models, making it relevant to Natural Language Processing.",
          "quote": "Each program relies on an external resource, the stdin input stream, and we pair the programs with a natural language description of the behavior of the stream."
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Transformer",
          "justification": "The paper uses Transformer models as one of the baselines for comparison against the proposed interpreter-inspired architecture.",
          "quote": "We perform evaluations comparing these interpreter-inspired architectures against Transformer, LSTM, and GGNN baselines."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The Transformer model was used as a baseline and was not a contribution of this research.",
          "quote": "We perform evaluations comparing these interpreter-inspired architectures against Transformer, LSTM, and GGNN baselines."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Transformers were trained and evaluated on the dataset to measure their performance on the task.",
          "quote": "We perform evaluations comparing these interpreter-inspired architectures against Transformer, LSTM, and GGNN baselines."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the Transformer model was compared to that of other models in the study.",
          "quote": "We perform evaluations comparing these interpreter-inspired architectures against Transformer, LSTM, and GGNN baselines."
        },
        "referenced_paper_title": {
          "value": "Attention Is All You Need",
          "justification": "The Transformer model is based on the architecture introduced in the paper 'Attention Is All You Need'.",
          "quote": "Attention Is All You Need"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Python Runtime Errors",
          "justification": "The dataset introduced in the paper is named 'Python Runtime Errors'. It consists of runtime error information for Python 3 programs.",
          "quote": "Our dataset, Python Runtime Errors, consists of 2.4 million Python 3 programs written by competitive programmers selected from Project CodeNet (Puri et al., 2021)."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Project CodeNet",
          "justification": "The dataset was constructed using submissions from Project CodeNet, as referenced in the paper.",
          "quote": "Our dataset, Python Runtime Errors, consists of 2.4 million Python 3 programs written by competitive programmers selected from Project CodeNet (Puri et al., 2021)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 929,
    "prompt_tokens": 16855,
    "total_tokens": 17784
  }
}