{
  "paper": "WFYbBOEOtv.txt",
  "words": 11532,
  "extractions": {
    "title": {
      "value": "V-JEPA: Latent Video Prediction for Visual Representation Learning",
      "justification": "This is the title of the paper.",
      "quote": "V-JEPA: Latent Video Prediction for Visual Representation Learning"
    },
    "description": "The paper introduces V-JEPA, a self-supervised learning method that predicts masked spatio-temporal regions in a learned representation space of videos, to produce visual features that excel in various downstream tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents empirical results demonstrating the effectiveness of the V-JEPA model on various benchmark datasets.",
      "quote": "Our latent video prediction strategy produces visual features that can be applied to various downstream image and video tasks without adaption of the model’s parameters, achieving 82.1% on Kinetics-400 and 71.2% on Something-Something-v2..."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on visual representation learning from video data.",
        "quote": "We introduce V-JEPA, a method for self-supervised learning from video that predicts masked spatio-temporal regions in a learned representation space."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper proposes a method for self-supervised learning from videos.",
          "quote": "We introduce V-JEPA, a method for self-supervised learning from video that predicts masked spatio-temporal regions in a learned representation space."
        },
        "aliases": [
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Video Representation Learning",
          "justification": "The method focuses on learning useful representations from video data.",
          "quote": "Our latent video prediction strategy produces visual features that can be applied to various downstream image and video tasks without adaption of the model’s parameters."
        },
        "aliases": [
          "VRL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "V-JEPA",
          "justification": "V-JEPA is the main model introduced by the paper.",
          "quote": "We introduce V-JEPA, a method for self-supervised learning from video that predicts masked spatio-temporal regions in a learned representation space."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The model is introduced as the main contribution of the paper.",
          "quote": "We introduce V-JEPA, a method for self-supervised learning from video that predicts masked spatio-temporal regions in a learned representation space."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes results from experiments with V-JEPA showing its performance on various tasks.",
          "quote": "Our latent video prediction strategy produces visual features that can be applied to various downstream image and video tasks without adaption of the model’s parameters."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of V-JEPA is compared with other state-of-the-art models in the paper.",
          "quote": "We achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This model is introduced in the current paper and therefore does not have a reference paper.",
          "quote": "We introduce V-JEPA, a method for self-supervised learning from video that predicts masked spatio-temporal regions in a learned representation space."
        }
      },
      {
        "name": {
          "value": "VideoMAE",
          "justification": "The paper compares the performance of V-JEPA with the VideoMAE model.",
          "quote": "On visual tasks that require a semantic temporal understanding of a scene, such as action classification on SSv2...we find V-JEPA to provide a significant boost in frozen evaluation over the largest state-of-the-art image and video models, such as VideoMAE."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "This model was referenced, not contributed by the authors of the paper.",
          "quote": "On visual tasks that require a semantic temporal understanding of a scene, such as action classification on SSv2...we find V-JEPA to provide a significant boost in frozen evaluation over the largest state-of-the-art image and video models, such as VideoMAE."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that VideoMAE was executed in this paper.",
          "quote": "On visual tasks that require a semantic temporal understanding of a scene, such as action classification on SSv2...we find V-JEPA to provide a significant boost in frozen evaluation over the largest state-of-the-art image and video models, such as VideoMAE."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares V-JEPA's performance against VideoMAE.",
          "quote": "On visual tasks that require a semantic temporal understanding of a scene, such as action classification on SSv2...we find V-JEPA to provide a significant boost in frozen evaluation over the largest state-of-the-art image and video models, such as VideoMAE."
        },
        "referenced_paper_title": {
          "value": "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training",
          "justification": "The paper provides the title of the VideoMAE reference paper.",
          "quote": "VideoMAE (Tong et al., 2022) directly extend the masked autoencoder approach to spatio-temporal volumes by training an encoder-decoder model to predict masked spatio-temporal voxels."
        }
      },
      {
        "name": {
          "value": "DINOv2",
          "justification": "The paper compares the performance of V-JEPA with the DINOv2 model.",
          "quote": "Specifically, we achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "This model was referenced, not contributed by the authors of the paper.",
          "quote": "Specifically, we achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that DINOv2 was executed in this paper.",
          "quote": "Specifically, we achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares V-JEPA's performance against DINOv2.",
          "quote": "Specifically, we achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "referenced_paper_title": {
          "value": "DINOv2: Learning Robust Visual Features without Supervision",
          "justification": "The paper provides the title of the DINOv2 reference paper.",
          "quote": "The DINOv2 method of Oquab et al. (2023) combines an invariance-based loss with a masked-image modelling loss and is currently the most competitive instantiation of self-supervised learning with (static) images, scaled to a model with over 1.1B parameters trained on a curated dataset of 142M images."
        }
      },
      {
        "name": {
          "value": "OpenCLIP",
          "justification": "The paper compares the performance of V-JEPA with the OpenCLIP model.",
          "quote": "Specifically, we achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "This model was referenced, not contributed by the authors of the paper.",
          "quote": "Specifically, we achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no indication that OpenCLIP was executed in this paper.",
          "quote": "Specifically, we achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares V-JEPA's performance against OpenCLIP.",
          "quote": "Specifically, we achieve 71.2% top-1 frozen evaluation on Something-Something-v2, surpassing VideoMAE, DINOv2 and OpenCLIP by +10, +21, and +32 points respectively..."
        },
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models from Natural Language Supervision",
          "justification": "The paper provides the title of the OpenCLIP reference paper.",
          "quote": "OpenCLIP (Radford et al., 2021) trains a video encoder to predict the representation of video captions computed by a text encoder."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Kinetics-400",
          "justification": "The paper evaluates the V-JEPA model on the Kinetics-400 dataset.",
          "quote": "Our latent video prediction strategy produces visual features that can be applied to various downstream image and video tasks without adaption of the model’s parameters, achieving 82.1% on Kinetics-400..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Kinetics Human Action Video Dataset",
          "justification": "The referenced paper title for the Kinetics-400 dataset is provided.",
          "quote": "On tasks that require good image-level features, such as Kinetics and ImageNet, we observe that V-JEPA is an effective pretraining strategy."
        }
      },
      {
        "name": {
          "value": "Something-Something-v2",
          "justification": "The paper evaluates the V-JEPA model on the Something-Something-v2 dataset.",
          "quote": "We obtain the first unsupervised video model to achieve 82.1% top-1 frozen evaluation accuracy on Kinetics, without any fine-tuning, outperforming the previous best video model, VideoMAE, by +4.2 points. Similarly, V-JEPA obtains 77.9% top-1 on ImageNet, without any image fine-tuning."
        },
        "aliases": [
          "SSv2"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The 'Something Something' Video Database for Learning and Evaluating Visual Common Sense",
          "justification": "The referenced paper title for the Something-Something-v2 dataset is provided.",
          "quote": "On visual tasks that require a semantic temporal understanding of a scene, such as action classification on SSv2 or action localization on AVA, we find V-JEPA to provide a significant boost in frozen evaluation..."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "The paper evaluates the V-JEPA model on the ImageNet dataset.",
          "quote": "...V-JEPA trains a visual encoder by predicting masked spatio-temporal regions in a learned latent space. On visual tasks that require a semantic temporal understanding of a scene, such as action classification on SSv2..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "The referenced paper title for the ImageNet dataset is provided.",
          "quote": "Specifically, we achieve 77.9% top-1 on ImageNet, without any image fine-tuning."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is commonly used for implementing such models and is likely used in this work given its prevalence in the field.",
          "quote": "N/A"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "No specific reference is made to PyTorch in the paper.",
          "quote": "N/A"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2513,
    "prompt_tokens": 21622,
    "total_tokens": 24135
  }
}