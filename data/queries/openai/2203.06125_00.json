{
  "paper": "2203.06125.txt",
  "words": 16047,
  "extractions": {
    "title": {
      "value": "Protein Representation Learning by Geometric Structure Pretraining",
      "justification": "It is the exact title of the paper mentioned at the beginning of the document.",
      "quote": "Protein Representation Learning by Geometric Structure Pretraining"
    },
    "description": "This paper proposes methods to pretrain protein representations according to their 3D structures, leveraging multiview contrastive learning and self-prediction tasks. The proposed methods outperform or match state-of-the-art sequence-based methods and do so with significantly less pretraining data.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experimental results on various benchmarks to demonstrate the effectiveness of the proposed methods, indicating that it is primarily empirical.",
      "quote": "Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data."
    },
    "primary_research_field": {
      "name": {
        "value": "Protein Representation Learning",
        "justification": "The paper focuses on learning effective representations of proteins using their geometric structures.",
        "quote": "Learning effective protein representations is critical in a variety of tasks in biology..."
      },
      "aliases": [
        "Protein Representation Learning",
        "Protein Embedding"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Contrastive Learning",
          "justification": "The paper uses multiview contrastive learning as a method for pretraining protein representations.",
          "quote": "We further introduce a geometric pretraining method to learn the protein structure encoder based on the popular contrastive learning framework."
        },
        "aliases": [
          "Contrastive Learning",
          "Contrastive Pretraining"
        ]
      },
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper introduces pretraining methods that include self-prediction tasks, aligning with the concepts of self-supervised learning.",
          "quote": "Simultaeneously, we propose a suite of straightforward baselines based on self-prediction."
        },
        "aliases": [
          "Self-Supervised Learning",
          "Self-Prediction"
        ]
      },
      {
        "name": {
          "value": "Graph Neural Networks",
          "justification": "The study extensively uses graph neural networks (GNNs) for encoding protein structures.",
          "quote": "We propose a simple yet effective structure-based encoder called GeomEtry-Aware Relational Graph Neural Network (GearNet), which encodes spatial information by adding different types of sequential or structural edges and then performs relational message passing on protein residue graphs."
        },
        "aliases": [
          "Graph Neural Networks",
          "GNN"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "GearNet",
          "justification": "The model is introduced in the paper as a key component.",
          "quote": "We propose a simple yet effective structure-based encoder called GeomEtry-Aware Relational Graph Neural Network (GearNet)."
        },
        "aliases": [
          "GearNet"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is a contribution of the paper.",
          "quote": "We propose a simple yet effective structure-based encoder called GeomEtry-Aware Relational Graph Neural Network (GearNet)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experiments conducted in the study.",
          "quote": "Our implementation is available at https://github.com/DeepGraphLearning/GearNet."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model is compared against other methods in various experimental benchmarks.",
          "quote": "Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data."
        },
        "referenced_paper_title": {
          "value": "Protein Representation Learning by Geometric Structure Pretraining",
          "justification": "The current paper is the reference for this model.",
          "quote": "We propose a simple yet effective structure-based encoder called GeomEtry-Aware Relational Graph Neural Network (GearNet)."
        }
      },
      {
        "name": {
          "value": "GearNet-Edge",
          "justification": "The model is an enhanced version of GearNet introduced in the paper.",
          "quote": "We propose a variant of GearNet enhanced with an edge message passing layer, named as GearNet-Edge."
        },
        "aliases": [
          "GearNet-Edge"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is a contribution of the paper.",
          "quote": "We propose a variant of GearNet enhanced with an edge message passing layer, named as GearNet-Edge."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experiments conducted in the study.",
          "quote": "Our implementation is available at https://github.com/DeepGraphLearning/GearNet."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model is compared against other methods in various experimental benchmarks.",
          "quote": "Experimental results on both function prediction and fold classification tasks show that our proposed pretraining methods outperform or are on par with the state-of-the-art sequence-based methods, while using much less pretraining data."
        },
        "referenced_paper_title": {
          "value": "Protein Representation Learning by Geometric Structure Pretraining",
          "justification": "The current paper is the reference for this model.",
          "quote": "We propose a variant of GearNet enhanced with an edge message passing layer, named as GearNet-Edge."
        }
      },
      {
        "name": {
          "value": "GearNet-IEConv",
          "justification": "The model is GearNet enhanced with an IEConv layer, as described in the paper.",
          "quote": "As we find that the IEConv layer is important for predicting fold labels, we also enhance our model by incorporating this as an additional layer."
        },
        "aliases": [
          "GearNet-IEConv"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is a contribution of the paper.",
          "quote": "As we find that the IEConv layer is important for predicting fold labels, we also enhance our model by incorporating this as an additional layer."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experiments conducted in the study.",
          "quote": "The models are trained for 200 epochs on EC and GO prediction and for 300 epochs on fold and reaction classification."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model is compared against other methods in various experimental benchmarks.",
          "quote": "Experimental results on several benchmarks verify our GearNet augmented with edge message passing can consistently outperform existing protein encoders on most tasks in a supervised setting."
        },
        "referenced_paper_title": {
          "value": "Protein Representation Learning by Geometric Structure Pretraining",
          "justification": "The current paper is the reference for this model.",
          "quote": "As we find that the IEConv layer is important for predicting fold labels, we also enhance our model by incorporating this as an additional layer."
        }
      },
      {
        "name": {
          "value": "GearNet-Edge-IEConv",
          "justification": "This model combines the enhancements of GearNet-Edge and IEConv layers, as described in the paper.",
          "quote": "We also enhance our model by incorporating this as an additional layer, referred as GearNet-Edge-IEConv."
        },
        "aliases": [
          "GearNet-Edge-IEConv"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The model is a contribution of the paper.",
          "quote": "We also enhance our model by incorporating this as an additional layer, referred as GearNet-Edge-IEConv."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experiments conducted in the study.",
          "quote": "The models are trained for 200 epochs on EC and GO prediction and for 300 epochs on fold and reaction classification."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model is compared against other methods in various experimental benchmarks.",
          "quote": "Experimental results on several benchmarks verify our GearNet augmented with edge message passing can consistently outperform existing protein encoders on most tasks in a supervised setting."
        },
        "referenced_paper_title": {
          "value": "Protein Representation Learning by Geometric Structure Pretraining",
          "justification": "The current paper is the reference for this model.",
          "quote": "We also enhance our model by incorporating this as an additional layer, referred as GearNet-Edge-IEConv."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AlphaFoldDB",
          "justification": "The dataset is mentioned as being used for pretraining in the paper.",
          "quote": "We use AlphaFoldDB v1 and v2 (Varadi et al., 2021) for pretraining, which is the largest protein structure database before March, 2022."
        },
        "aliases": [
          "AlphaFold Database",
          "AlphaFoldDB"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "AlphaFold Protein Structure Database: Massively Expanding the Structural Coverage of Protein-Sequence Space with High-accuracy Models",
          "justification": "The original paper describing the AlphaFoldDB is referenced in this research.",
          "quote": "We use AlphaFoldDB v1 and v2 (Varadi et al., 2021) for pretraining, which is the largest protein structure database before March, 2022."
        }
      },
      {
        "name": {
          "value": "Protein Data Bank (PDB)",
          "justification": "The dataset is used for comparisons and referenced as a source of experimentally-determined protein structures.",
          "quote": "For example, there are 182K experimentally-determined structures in the Protein Data Bank (PDB) (Berman et al., 2000) vs 47M protein sequences in Pfam (Mistry et al., 2021) and vs 10M annotated images in ImageNet (Russakovsky et al., 2015)."
        },
        "aliases": [
          "PDB"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Protein Data Bank",
          "justification": "The original paper describing the Protein Data Bank is referenced in this research.",
          "quote": "For example, there are 182K experimentally-determined structures in the Protein Data Bank (PDB) (Berman et al., 2000) vs 47M protein sequences in Pfam (Mistry et al., 2021) and vs 10M annotated images in ImageNet (Russakovsky et al., 2015)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper implicitly mentions using PyTorch for model implementation as indicated by the available link to the code, which is hosted on GitHub and typically relies on PyTorch.",
          "quote": "Our implementation is available at https://github.com/DeepGraphLearning/GearNet."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High Performance Deep Learning Library",
          "justification": "PyTorch is commonly used for deep learning research and its use is implied by the GitHub repository link provided.",
          "quote": "Our implementation is available at https://github.com/DeepGraphLearning/GearNet."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2153,
    "prompt_tokens": 29257,
    "total_tokens": 31410
  }
}