{
  "paper": "KoFOg41haE.txt",
  "words": 22718,
  "extractions": {
    "title": {
      "value": "StarCoder: may the source be with you!",
      "justification": "This is the title of the presented research paper.",
      "quote": "StarCoder: may the source be with you!"
    },
    "description": "The paper describes StarCoder and StarCoderBase, Large Language Models for code, developed by the BigCode community. These models are designed to support multiple programming languages and have been evaluated on their performance, including comparisons with other models and considerations for responsible model development and data governance.",
    "type": {
      "value": "Empirical",
      "justification": "The paper includes extensive evaluation of the models, describing performance metrics, comparison with other models, and practical applications, indicating empirical research.",
      "quote": "We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus is on Large Language Models for code which is a domain within Natural Language Processing.",
        "quote": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Code Generation",
          "justification": "The paper focuses on generating code using large language models.",
          "quote": "introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Model Evaluation",
          "justification": "The study conducts evaluations by comparing the model's performance against other models and benchmarks.",
          "quote": "We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "StarCoder",
          "justification": "StarCoder is explicitly mentioned as a model created and evaluated within the paper.",
          "quote": "BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "StarCoder was introduced and evaluated as part of this paper's contributions.",
          "quote": "BigCode community...introduces StarCoder."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper describes the evaluation and comparison of the model's performance.",
          "quote": "These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed."
        },
        "is_compared": {
          "value": 1,
          "justification": "StarCoder's performance is evaluated and compared to other models such as OpenAI's models.",
          "quote": "We perform the most comprehensive evaluation of Code LLMs to date...matches or outperforms the OpenAI code-cushman-001 model."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "StarCoder itself, not referenced in another paper, is a contribution of this paper.",
          "quote": "introduces StarCoder and StarCoderBase."
        }
      },
      {
        "name": {
          "value": "StarCoderBase",
          "justification": "StarCoderBase is another model introduced and evaluated within this paper.",
          "quote": "BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "StarCoderBase was introduced and evaluated as part of this paper's contributions.",
          "quote": "BigCode community...introduces StarCoderBase."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper describes the evaluation and comparison of the model's performance.",
          "quote": "These benchmarks are further described in Section 6.) To give an indication of the amount of data removed by decontamination, Python is the language with the highest number of matches, with 558 files removed."
        },
        "is_compared": {
          "value": 1,
          "justification": "StarCoderBase's performance is evaluated and compared to other models such as OpenAI's models.",
          "quote": "We perform the most comprehensive evaluation of Code LLMs to date...matches or outperforms the OpenAI code-cushman-001 model."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "StarCoderBase itself, not referenced in another paper, is a contribution of this paper.",
          "quote": "introduces StarCoder and StarCoderBase."
        }
      },
      {
        "name": {
          "value": "code-cushman-001",
          "justification": "code-cushman-001 is mentioned as a benchmark model for comparison with StarCoder and StarCoderBase.",
          "quote": "StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "code-cushman-001 is not a contribution of this paper; it is a referenced benchmark model used for comparison.",
          "quote": "matches or outperforms the OpenAI code-cushman-001 model."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not specify execution details of code-cushman-001.",
          "quote": "matches or outperforms the OpenAI code-cushman-001 model."
        },
        "is_compared": {
          "value": 1,
          "justification": "code-cushman-001 is used as benchmark for comparisons in the paper.",
          "quote": "matches or outperforms the OpenAI code-cushman-001 model."
        },
        "referenced_paper_title": {
          "value": "Evaluating large language models trained on code",
          "justification": "The performance comparison against the code-cushman-001 model references OpenAI's evaluation paper.",
          "quote": "Evaluating large language models trained on code."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "The Stack v1.2",
          "justification": "The Stack v1.2 is mentioned as the primary dataset for training StarCoderBase and StarCoder models.",
          "quote": "The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Stack: 3 TB of permissively licensed source code",
          "justification": "The dataset's reference title is given alongside its description in the paper.",
          "quote": "The Stack (Kocetkov et al., 2022), a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face Tokenizers",
          "justification": "The paper mentions using the Hugging Face Tokenizers library for creating the tokenizer used in the models.",
          "quote": "we use the Hugging Face Tokenizers library (MOI et al., 2022) to train a byte-level Byte-Pair-Encoding with a vocabulary size of 49,152 tokensâ€”including the sentinel tokens from table 10."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rust 0.13.2",
          "justification": "This is the specific version of the Hugging Face Tokenizers library referenced in the paper.",
          "quote": "Hugging Face Tokenizers library (MOI et al., 2022)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1564,
    "prompt_tokens": 42881,
    "total_tokens": 44445
  }
}