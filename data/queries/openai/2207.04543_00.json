{
  "paper": "2207.04543.txt",
  "words": 12774,
  "extractions": {
    "title": {
      "value": "CHALLENGING COMMON ASSUMPTIONS ABOUT CATASTROPHIC FORGETTING",
      "justification": "This is the title of the paper as it appears in the provided text.",
      "quote": "CHALLENGING COMMON ASSUMPTIONS ABOUT CATASTROPHIC FORGETTING"
    },
    "description": "This paper investigates knowledge retention and accumulation in deep neural networks, challenging the prevailing belief regarding catastrophic forgetting in continual learning. The authors propose a framework called SCoLe (Scaling Continual Learning) to study how knowledge accumulation evolves when data reoccurs over long sequences of tasks. They present empirical evidence that stochastic gradient descent (SGD)-trained models can retain and accumulate knowledge even under non-stationary distributions, contrary to the classical understanding of catastrophic forgetting.",
    "type": {
      "value": "Empirical Study",
      "justification": "This paper presents empirical evaluations and experimental results on knowledge retention and accumulation using various deep learning models and datasets.",
      "quote": "We propose a new framework, SCoLe (Scaling Continual Learning), to investigate KA and discover that catastrophic forgetting has a limited effect on DNNs trained with SGD. When trained on long sequences with data sparsely re-occurring, the overall accuracy improves, which might be counter-intuitive given the CF phenomenon. We empirically investigate KA in DNNs under various data occurrence frequencies and propose simple and scalable strategies to increase knowledge accumulation in DNNs."
    },
    "primary_research_field": {
      "name": {
        "value": "Continual Learning",
        "justification": "The paper focuses on continual learning, specifically investigating knowledge retention and accumulation over long sequences of tasks.",
        "quote": "Building learning agents that can progressively learn and accumulate knowledge is the core goal of the continual learning (CL) research field."
      },
      "aliases": [
        "CL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Catastrophic Forgetting",
          "justification": "The paper challenges traditional views on catastrophic forgetting (CF) by demonstrating how knowledge accumulation can mitigate its impact.",
          "quote": "Unfortunately, training a model on new data usually compromises the performance on past data. In the CL literature, this effect is referred to as catastrophic forgetting (CF)."
        },
        "aliases": [
          "CF"
        ]
      },
      {
        "name": {
          "value": "Knowledge Accumulation",
          "justification": "Knowledge accumulation (KA) is a central concept in this study, as the paper investigates how models retain and build upon learned knowledge.",
          "quote": "We propose the SCoLe framework to study knowledge accumulation (KA) and discover that catastrophic forgetting has a limited effect on DNNs trained with SGD."
        },
        "aliases": [
          "KA"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SCoLe (Scaling Continual Learning)",
          "justification": "SCoLe is a framework proposed by the authors to study knowledge accumulation and investigate the effects of catastrophic forgetting in DNNs.",
          "quote": "We propose the SCoLe framework to study knowledge accumulation (KA) and discover that catastrophic forgetting has a limited effect on DNNs trained with SGD."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The SCoLe framework is introduced and developed within the scope of this paper as a contribution to the continual learning field.",
          "quote": "We propose the SCoLe framework to study knowledge accumulation"
        },
        "is_executed": {
          "value": 1,
          "justification": "The framework is empirically tested and executed within the scope of this paper.",
          "quote": "We empirically investigate KA in DNNs under various data occurrence frequencies and propose simple and scalable strategies to increase knowledge accumulation in DNNs."
        },
        "is_compared": {
          "value": 1,
          "justification": "The framework is compared to traditional continual learning benchmarks involving catastrophic forgetting.",
          "quote": "We propose a new framework, SCoLe (Scaling Continual Learning), to investigate KA and discover that catastrophic forgetting has a limited effect on DNNs trained with SGD."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This framework is an original contribution of the current paper, so there is no referenced paper title.",
          "quote": "We propose a new framework, SCoLe (Scaling Continual Learning)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is one of the datasets used for experimentation in the study.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "MNIST is a well-known dataset and not introduced by a single reference paper in the context of this study.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        }
      },
      {
        "name": {
          "value": "Fashion-MNIST",
          "justification": "Fashion-MNIST is another dataset used in the experiments conducted in this paper.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [
          "Fashion MNIST"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Fashion-MNIST is a known dataset and is not introduced by a single reference paper in the context of this study.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        }
      },
      {
        "name": {
          "value": "KMNIST",
          "justification": "KMNIST is listed as one of the datasets utilized for testing the knowledge accumulation in DNNs.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "KMNIST is a known dataset and is not introduced by a single reference paper in the context of this study.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        }
      },
      {
        "name": {
          "value": "CIFAR10",
          "justification": "The paper lists CIFAR10 as one of the datasets for evaluating their methods for continual learning and knowledge accumulation.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "CIFAR10 is a known dataset widely used in research and is not introduced by a single reference paper in the context of this study.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        }
      },
      {
        "name": {
          "value": "CIFAR100",
          "justification": "The CIFAR100 dataset is involved in the empirical analysis performed in the paper.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "CIFAR100 is a known dataset and is not introduced by a single reference paper in the context of this study.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        }
      },
      {
        "name": {
          "value": "Tiny-ImageNet200",
          "justification": "Tiny-ImageNet200 is one of the datasets mentioned explicitly in the experimentation part of the paper.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [
          "Tiny-ImageNet"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Tiny-ImageNet is a known dataset and is not introduced by a single reference paper in the context of this study.",
          "quote": "We propose an experimentation framework “SCoLe” to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper specifies the usage of PyTorch for training models in their experimental setup.",
          "quote": "By default, we kept the optimizer’s hyper-parameters of PyTorch (Paszke et al., 2019) and train a small convolutional neural network."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The paper mentions using the default hyper-parameters of PyTorch and provides a reference to its publication.",
          "quote": "By default, we kept the optimizer’s hyper-parameters of PyTorch (Paszke et al., 2019)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2188,
    "prompt_tokens": 21917,
    "total_tokens": 24105
  }
}