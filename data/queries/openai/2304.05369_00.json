{
  "paper": "2304.05369.txt",
  "words": 8436,
  "extractions": {
    "title": {
      "value": "A surprisingly simple technique to control the pretraining bias for better transfer: Expand or Narrow your representation",
      "justification": "This is the title of the paper provided.",
      "quote": "A surprisingly simple technique to control the pretraining bias for better transfer: Expand or Narrow your representation"
    },
    "description": "This paper studies how modifying the dimensionality of the backbone representation in Self-Supervised Learning (SSL) models can control pretraining bias and improve transfer performance across various downstream tasks. The study finds that tweaking the size of the last block of the backbone (either expanding or narrowing) significantly impacts the robustness and effectiveness of the learned representations in both SSL and supervised settings.",
    "type": {
      "value": "Empirical",
      "justification": "The paper presents experimental results from various models, projector architectures, and datasets to support its findings.",
      "quote": "In this paper we study a simple generic way to further improve robustness with respect to task misalignment and the pretraining bias occurring in SSL – including the bias due to an implicit uniform prior... Our main contributions highlight that: • Training SimCLR with a very small linear projector (32 neurons) can lead to competitive results on ImageNet."
    },
    "primary_research_field": {
      "name": {
        "value": "Self-Supervised Learning",
        "justification": "The paper focuses on improving techniques for Self-Supervised Learning models to control pretraining bias and improve transfer to downstream tasks.",
        "quote": "The self-supervised learning (SSL) paradigm aims at learning representations by using 'pretext tasks'... In this paper we study a simple generic way to further improve robustness with respect to task misalignment and the pretraining bias occurring in SSL."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The paper addresses how SSL models can better transfer their learned representations to downstream tasks.",
          "quote": "A commonly used trick in SSL, shown to make deep networks more robust to such bias, is the addition of a small projector...This trick can mitigate the detrimental effect of the pretraining bias only to some degree... In this paper we study a simple generic way to further improve robustness with respect to task misalignment and the pretraining bias occurring in SSL."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Supervised Learning",
          "justification": "The paper also evaluates the impact of modifying backbone dimensions on models pretrained using supervised learning.",
          "quote": "It significantly improves downstream transfer performance for both Self-Supervised and Supervised pretrained models."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "SimCLR is one of the SSL methods studied and experimented with in this paper.",
          "quote": "In this paper we study a simple generic way to further improve robustness with respect to task misalignment and the pretraining bias occurring in SSL... We will show that merely expanding or narrowing the backbone dimension... allows to significantly improve transfer performance... Training SimCLR with a very small linear projector (32 neurons) can lead to competitive results on ImageNet."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "SimCLR is not introduced by this paper; it is an existing SSL method used for experimentation.",
          "quote": "Several works have studied the impact of the projector architecture in SSL. Some methods like SimCLR (Chen et al., 2020) achieve similar performance when using small or large projector embeddings..."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments using SimCLR were conducted as part of the study.",
          "quote": "For our experiments with linear projectors, we use a regular nonlinear Resnet50 (He et al., 2016) backbone.... We choose to increase the size of the bottleneck before pooling in order for the network to be able to retain more spatial information about the input image. This method can be applied on both supervised and self-supervised architectures, since it is performed on the backbone common to both... Table 1. Changing the linear projector dimension for a fixed backbone dimension (32768) on ImageNet and Inat with SimCLR."
        },
        "is_compared": {
          "value": 1,
          "justification": "SimCLR's performance with varying projector and backbone dimensions is compared to other SSL and supervised models in the study.",
          "quote": "We train SimCLR, VICReg, Byol and a supervised model on various backbone representation sizes."
        },
        "referenced_paper_title": {
          "value": "A simple framework for contrastive learning of visual representations",
          "justification": "This is the referenced paper that introduced SimCLR.",
          "quote": "Some methods like SimCLR (Chen et al., 2020) achieve similar performance when using small or large projector embeddings..."
        }
      },
      {
        "name": {
          "value": "VICReg",
          "justification": "VICReg is another SSL method studied and experimented with in this paper.",
          "quote": "In this paper we study a simple generic way to further improve robustness with respect to task misalignment and the pretraining bias occurring in SSL... We will show that merely expanding or narrowing the backbone dimension... allows to significantly improve transfer performance... Several works have studied the impact of the projector architecture in SSL. Some methods like SimCLR achieve similar performance when using small or large projector embeddings, while methods like VICReg are sensitive to the projector’s dimensionality (Garrido et al., 2022)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "VICReg is not introduced by this paper; it is an existing SSL method used for experimentation.",
          "quote": "Several works have studied the impact of the projector architecture in SSL... while methods like VICReg are sensitive to the projector’s dimensionality (Garrido et al., 2022)."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments using VICReg were conducted as part of the study.",
          "quote": "For our experiments with linear projectors, we use a regular nonlinear Resnet50 (He et al., 2016) backbone.... We choose to increase the size of the bottleneck before pooling in order for the network to be able to retain more spatial information about the input image. This method can be applied on both supervised and self-supervised architectures, since it is performed on the backbone... We train SimCLR, VICReg, Byol and a supervised model on various backbone representation sizes."
        },
        "is_compared": {
          "value": 1,
          "justification": "VICReg's performance with varying projector and backbone dimensions is compared to other SSL and supervised models in the study.",
          "quote": "We train SimCLR, VICReg, Byol and a supervised model on various backbone representation sizes. In Figure 4, we present the ImageNet top-1 validation accuracy for several methods across backbone dimensions D, using a linear probing evaluation."
        },
        "referenced_paper_title": {
          "value": "On the duality between contrastive and non-contrastive self-supervised learning",
          "justification": "This is the referenced paper for VICReg.",
          "quote": "Several works have studied the impact of the projector architecture in SSL... while methods like VICReg are sensitive to the projector’s dimensionality (Garrido et al., 2022)."
        }
      },
      {
        "name": {
          "value": "Byol",
          "justification": "Byol is another SSL method studied and experimented with in this paper.",
          "quote": "Methods like VICReg are sensitive to the projector’s dimensionality... Dubois et al. (2022) observed that larger backbone representations lead to better linear probe performance when using CISSL. We generalize this result to SimCLR, VICReg, Byol and the supervised setting and shed light on the importance of wider representations to mitigate the pretraining bias."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Byol is not introduced by this paper; it is an existing SSL method used for experimentation.",
          "quote": "We generalize this result to SimCLR, VICReg, Byol and the supervised setting and shed light on the importance of wider representations to mitigate the pretraining bias."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments using Byol were conducted as part of the study.",
          "quote": "We train SimCLR, VICReg, Byol and a supervised model on various backbone representation sizes."
        },
        "is_compared": {
          "value": 1,
          "justification": "Byol's performance with varying projector and backbone dimensions is compared to other SSL and supervised models in the study.",
          "quote": "In Figure 4, we present the ImageNet top-1 validation accuracy for several methods across backbone dimensions D, using a linear probing evaluation."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper does not explicitly mention the referenced title for Byol.",
          "quote": "While the paper mentions Byol, it does not explicitly reference its paper title."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is one of the key datasets used in pretraining and evaluation of the models in this study.",
          "quote": "Despite having shown great success on relatively class-balanced datasets such as ImageNet-1k (Deng et al., 2009)... In Figure 4, we present the ImageNet top-1 validation accuracy for several methods across backbone dimensions D, using a linear probing evaluation."
        },
        "aliases": [
          "ImageNet-1k"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "This is the standard reference paper for the ImageNet dataset.",
          "quote": "ImageNet-1k (Deng et al., 2009)"
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is another dataset used in the study for experimenting with the robustness and performance of SSL methods.",
          "quote": "To verify that this insight translates experimentally to a realistic model... Instead of sampling uniformly from the CIFAR10 dataset, we sample images belonging to only two different classes per mini batch."
        },
        "aliases": [
          "CIFAR10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the referenced paper that introduced the CIFAR-10 dataset.",
          "quote": "CIFAR10 (Krizhevsky, 2009)"
        }
      },
      {
        "name": {
          "value": "iNaturalist 2018",
          "justification": "iNaturalist 2018 is used to showcase how models trained with different backbone dimensions perform on long-tailed datasets.",
          "quote": "ImageNet-1k (Deng et al., 2009), Inat (Horn et al., 2018)... We evaluate how SimCLR trained on different backbone representations performs on another long-tailed dataset: Inat18."
        },
        "aliases": [
          "Inat18"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The iNaturalist Species Classification and Detection Dataset",
          "justification": "This is the referenced paper for the iNaturalist 2018 dataset.",
          "quote": "Inat (Horn et al., 2018)"
        }
      },
      {
        "name": {
          "value": "ImageNet-9",
          "justification": "ImageNet-9 is used for robustness evaluation.",
          "quote": "To verify such hypothesis, we ran an experiment on ImageNet-9 in Table 5 that show that indeed a representation that is smaller than the number of class is more robust to variation in the background."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The referenced title for ImageNet-9 is not explicitly mentioned.",
          "quote": "While the paper mentions ImageNet-9, it does not explicitly reference its paper title."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2778,
    "prompt_tokens": 15865,
    "total_tokens": 18643
  }
}