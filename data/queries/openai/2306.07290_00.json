{
  "paper": "2306.07290.txt",
  "words": 5466,
  "extractions": {
    "title": {
      "value": "Value function estimation using conditional diffusion models for control",
      "justification": "The title is directly obtained from the provided research paper.",
      "quote": "Value function estimation using conditional diffusion models for control"
    },
    "description": "This paper introduces the Diffused Value Function (DVF) algorithm, which leverages conditional diffusion models to estimate the value function in reinforcement learning tasks. The DVF can operate on state sequences without requiring action or reward labels, effectively using low-quality data to estimate future states and value functions for optimal control. The approach shows promising results particularly in challenging robotic tasks and offline reinforcement learning settings.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper demonstrates the application of the DVF algorithm on multiple robotics benchmarks, providing qualitative and quantitative results.",
      "quote": "We highlight the strengths of DVF both qualitatively and quantitatively on challenging robotic tasks, and show how generative models can be used to accelerate tabula rasa learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The focus of the research is on estimating value functions for reinforcement learning tasks using diffusion models.",
        "quote": "We propose a simple algorithm called Diffused Value Function (DVF), which learns a joint multi-step model of the environment-robot interaction dynamics using a diffusion model."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Robotics",
          "justification": "The paper emphasizes the application of the DVF algorithm on challenging robotics benchmarks.",
          "quote": "We highlight the strengths of DVF both qualitatively and quantitatively on challenging robotic tasks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Generative Models",
          "justification": "The paper leverages conditional diffusion models, which are a type of generative model, to estimate future states and value functions.",
          "quote": "We show how DVF can be used to efficiently capture the state visitation measure for multiple controllers, and show promising qualitative and quantitative results on challenging robotics benchmarks."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Diffused Value Function (DVF)",
          "justification": "DVF is an algorithm proposed in the paper to estimate the value function using conditional diffusion models.",
          "quote": "We propose a simple algorithm called Diffused Value Function (DVF), which learns a joint multi-step model of the environment-robot interaction dynamics using a diffusion model."
        },
        "aliases": [
          "DVF"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "DVF is novel and introduced by the authors of the paper.",
          "quote": "We propose a simple algorithm called Diffused Value Function (DVF)..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The DVF algorithm is implemented and evaluated on multiple benchmarks as discussed in the experiments.",
          "quote": "We highlight the strengths of DVF both qualitatively and quantitatively on challenging robotic tasks..."
        },
        "is_compared": {
          "value": 1,
          "justification": "DVF is compared to other reinforcement learning algorithms such as behavior cloning and Conservative Q-learning in the experiments.",
          "quote": "We compare DVF to behavior cloning and Conservative Q-learning [27], two strong offline RL baselines."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no referenced paper title for DVF because it is a novel contribution by the authors.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Maze 2d",
          "justification": "The paper examines the qualitative behavior of the diffusion model of DVF on tasks in the Maze 2d environment.",
          "quote": "We examine the qualitative behavior of the diffusion model of DVF on a simple locomotion task inside mazes of various shapes, as introduced in the D4RL offline suite [25]."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
          "justification": "The dataset Maze 2d is part of the D4RL offline suite referenced in the paper.",
          "quote": "We examine the qualitative behavior of the diffusion model of DVF on a simple locomotion task inside mazes of various shapes, as introduced in the D4RL offline suite [25]."
        }
      },
      {
        "name": {
          "value": "PyBullet",
          "justification": "The PyBullet environment is used in the experiments to evaluate the performance of DVF against other RL baselines.",
          "quote": "Our final set of experiments consists in ablations performed on offline data collected from classical PyBullet environments5 , as opposed to D4RL..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The reference to the PyBullet environment used in the experiments does not specify a particular paper title.",
          "quote": "Our final set of experiments consists in ablations performed on offline data collected from classical PyBullet environments5 , as opposed to D4RL..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Soft Actor-Critic (SAC)",
          "justification": "SAC is used in the PyBullet dataset for data collection by training a policy.",
          "quote": "Medium dataset contains data collected by medium-level policy and mixed contains data from SAC [20] training."
        },
        "aliases": [
          "SAC"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
          "justification": "The referenced paper for SAC is cited in the research paper's references.",
          "quote": "Medium dataset contains data collected by medium-level policy and mixed contains data from SAC [20] training."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1104,
    "prompt_tokens": 10406,
    "total_tokens": 11510
  }
}