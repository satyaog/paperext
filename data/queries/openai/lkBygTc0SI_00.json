{
  "paper": "lkBygTc0SI.txt",
  "words": 11263,
  "extractions": {
    "title": {
      "value": "Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning",
      "justification": "This is the title of the paper as provided by the user.",
      "quote": "Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning"
    },
    "description": "This paper performs a systematic study of the unintended memorization of image-specific information in Self-Supervised Learning (SSL) models – referred to as déjà vu memorization. The authors show that SSL models can memorize specific foreground objects from training images when presented only with background crops. They quantify this memorization across different SSL algorithms, model architectures, and training criteria, and propose methods to mitigate this privacy risk.",
    "type": {
      "value": "empirical",
      "justification": "The paper systematically studies the phenomenon of déjà vu memorization through experiments and quantitative analysis.",
      "quote": "In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models..."
    },
    "primary_research_field": {
      "name": {
        "value": "Self-Supervised Learning",
        "justification": "The primary research field is Self-Supervised Learning as the study revolves around the functionalities and risks associated with SSL models.",
        "quote": "Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another."
      },
      "aliases": [
        "SSL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Privacy in Machine Learning",
          "justification": "The study focuses on privacy risks arising from unintended memorization in SSL models.",
          "quote": "Our study of déjà vu memorization reveals previously unknown privacy risks in SSL models..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "SimCLR is one of the SSL models studied in the paper to evaluate déjà vu memorization.",
          "quote": "Many SSL algorithms rely on joint-embedding architectures (e.g., SimCLR [11], Barlow Twins [33], VICReg [2] and Dino [10])..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "SimCLR was not contributed by this paper, but it was one of the models analyzed.",
          "quote": "Many SSL algorithms rely on joint-embedding architectures (e.g., SimCLR [11], Barlow Twins [33], VICReg [2] and Dino [10])..."
        },
        "is_executed": {
          "value": 1,
          "justification": "SimCLR was executed as part of the experimentation in this paper.",
          "quote": "We test déjà vu memorization on a variety of popular SSL algorithms, with a focus on VICReg [2]. These algorithms produce two embeddings given an input image..."
        },
        "is_compared": {
          "value": 1,
          "justification": "SimCLR was compared numerically to other models within the scope of the paper.",
          "quote": "For both VICReg and supervised models, inferring the class of crop(Ai ) using KNNB (dashed line) through correlation achieves a reasonable accuracy that is significantly above chance level."
        },
        "referenced_paper_title": {
          "value": "A simple framework for contrastive learning of visual representations",
          "justification": "This is the reference paper for SimCLR mentioned in the paper.",
          "quote": "SimCLR [11]"
        }
      },
      {
        "name": {
          "value": "Barlow Twins",
          "justification": "Barlow Twins is among the SSL models analyzed for unintended memorization.",
          "quote": "Many SSL algorithms rely on joint-embedding architectures (e.g., SimCLR [11], Barlow Twins [33], VICReg [2] and Dino [10])..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Barlow Twins was not introduced by this paper, but it was part of the analysis conducted.",
          "quote": "Many SSL algorithms rely on joint-embedding architectures (e.g., SimCLR [11], Barlow Twins [33], VICReg [2] and Dino [10])..."
        },
        "is_executed": {
          "value": 1,
          "justification": "Barlow Twins was run in the experiments conducted in the paper.",
          "quote": "Model Training: We use PyTorch [23] with FFCV-SSL [5]. All models are trained for 1000 epochs..."
        },
        "is_compared": {
          "value": 1,
          "justification": "Barlow Twins was compared numerically to other SSL models as part of the analysis.",
          "quote": "For VICReg, the inference accuracy using KNNA (solid red line) is significantly higher...indy."
        },
        "referenced_paper_title": {
          "value": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "justification": "The Barlow Twins model is referenced in the paper.",
          "quote": "Barlow Twins [33]"
        }
      },
      {
        "name": {
          "value": "VICReg",
          "justification": "The VICReg model is central to the analysis conducted in the study.",
          "quote": "We test déjà vu memorization on a variety of popular SSL algorithms, with a focus on VICReg [2]."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "VICReg was not proposed in this paper, but it was a major focus of the study.",
          "quote": "We test déjà vu memorization on a variety of popular SSL algorithms, with a focus on VICReg [2]."
        },
        "is_executed": {
          "value": 1,
          "justification": "VICReg was used in the experiments to evaluate déjà vu memorization.",
          "quote": "We test déjà vu memorization on a variety of popular SSL algorithms, with a focus on VICReg [2]."
        },
        "is_compared": {
          "value": 1,
          "justification": "VICReg was numerically compared to other SSL models in this paper.",
          "quote": " For VICReg, inference accuracy on the 1% most confident examples is nearly 95%..."
        },
        "referenced_paper_title": {
          "value": "Variance-invariance-covariance regularization for self-supervised learning",
          "justification": "This is a reference paper for the VICReg model mentioned in the study.",
          "quote": "VICReg [2]"
        }
      },
      {
        "name": {
          "value": "Dino",
          "justification": "Dino is one of the SSL models evaluated in the paper.",
          "quote": "Many SSL algorithms rely on joint-embedding architectures (e.g., SimCLR [11], Barlow Twins [33], VICReg [2] and Dino [10])..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Dino was not developed in this paper but was included in the evaluation.",
          "quote": "Many SSL algorithms rely on joint-embedding architectures (e.g., SimCLR [11], Barlow Twins [33], VICReg [2] and Dino [10])..."
        },
        "is_executed": {
          "value": 1,
          "justification": "Dino was executed in the experiments mentioned in the paper.",
          "quote": "We test déjà vu memorization on a variety of popular SSL algorithms..."
        },
        "is_compared": {
          "value": 1,
          "justification": "Dino was compared numerically to other SSL models in this research paper.",
          "quote": "The accuracy of label inference using the target model (trained on A) vs. the reference model (trained on B)..."
        },
        "referenced_paper_title": {
          "value": "Unsupervised learning of visual features by contrasting cluster assignments",
          "justification": "This is the reference paper for Dino that is cited in the study.",
          "quote": "Dino [10]"
        }
      },
      {
        "name": {
          "value": "Byol",
          "justification": "Byol is included in the analysis of SSL models for memorization.",
          "quote": "Criteria Supervised Byol[18] SimCLR[11] Dino[10] Barlow T.[33] VICReg[2]"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Byol was not developed in this study but was part of the comparative analysis.",
          "quote": "Criteria Supervised Byol[18]"
        },
        "is_executed": {
          "value": 1,
          "justification": "Byol was executed in the scope of the experiments in the paper.",
          "quote": "We tested these models using the same framework as VICReg..."
        },
        "is_compared": {
          "value": 1,
          "justification": "Byol was numerically compared to other models in the paper.",
          "quote": "Comparison of déjà vu memorization for VICReg, Barlow Twins, Dino, Byol, SimCLR, and a supervised model."
        },
        "referenced_paper_title": {
          "value": "Bootstrap your own latent: A new approach to self-supervised learning",
          "justification": "This is the reference paper for Byol mentioned in the study.",
          "quote": "Byol [18]"
        }
      },
      {
        "name": {
          "value": "Supervised",
          "justification": "The Supervised baseline model is included in the paper for comparative purposes.",
          "quote": "Comparison of déjà vu memorization for VICReg, Barlow Twins, Dino, Byol, SimCLR, and a supervised model."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The Supervised model is a baseline model and was not proposed by this paper.",
          "quote": "Supervised [4]"
        },
        "is_executed": {
          "value": 1,
          "justification": "The Supervised model was executed for comparison with SSL models.",
          "quote": "We also evaluate embeddings produced by a supervised model CLFA trained on A."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Supervised model is compared to other SSL models as a baseline.",
          "quote": "We compare with a correlation baseline using the reference model: KNNB ’s label inference accuracy..."
        },
        "referenced_paper_title": {
          "value": "On the importance of difficulty calibration in membership inference attacks",
          "justification": "This is the referenced paper for the Supervised model used as a baseline.",
          "quote": "Supervised [4]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "The ImageNet dataset is used to evaluate déjà vu memorization in SSL models.",
          "quote": "We focus on testing déjà vu memorization for SSL models trained on the ImageNet dataset [13]4 ."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "This is the referenced paper for the ImageNet dataset used in the study.",
          "quote": "ImageNet [13]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is mentioned as the library used for model training.",
          "quote": "Model Training: We use PyTorch [23] with FFCV-SSL [5]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the referenced paper for PyTorch mentioned in the study.",
          "quote": "PyTorch [23]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3120,
    "prompt_tokens": 37684,
    "total_tokens": 40804
  }
}