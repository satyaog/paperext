{
  "paper": "2310.00429.txt",
  "words": 12684,
  "extractions": {
    "title": {
      "value": "On the Stability of Iterative Retraining of Generative Models on Their Own Data",
      "justification": "This is the title listed on the first page of the provided paper.",
      "quote": "ON THE STABILITY OF ITERATIVE RETRAINING OF GENERATIVE MODELS ON THEIR OWN DATA"
    },
    "description": "This paper examines the iterative retraining of generative models on datasets that mix real and synthetic data generated from the models themselves. It develops a theoretical framework for understanding the stability of such retraining, proving the existence of fixed points and providing empirical validation through experiments on various models and datasets.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper primarily focuses on developing a theoretical framework and proving theorems regarding the stability of iterative retraining of generative models. Empirical validation is also provided, but the main contribution is theoretical.",
      "quote": "We develop a framework to rigorously study the impact of training generative models on mixed datasets...We first prove the stability of iterative training...We then prove in Theorem 2 that..."
    },
    "primary_research_field": {
      "name": {
        "value": "Generative Models",
        "justification": "The paper focuses on generative models and their ability to retrain on their own synthetic data.",
        "quote": "Deep generative models have made tremendous progress in modeling complex data..."
      },
      "aliases": [
        "Generative Modeling",
        "Generative Adversarial Networks",
        "GANs"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Diffusion Models",
          "justification": "The paper specifically includes diffusion models as part of its empirical studies and theoretical discussions.",
          "quote": "...we show the stability of iterative training on...state-of-the-art diffusion models..."
        },
        "aliases": [
          "DDPM",
          "Denoising Diffusion Probabilistic Models"
        ]
      },
      {
        "name": {
          "value": "Normalizing Flows",
          "justification": "Normalizing flows are mentioned as one of the types of generative models empirically validated in the paper.",
          "quote": "...we empirically validate our theory on both synthetic and natural images by iteratively training normalizing flows..."
        },
        "aliases": [
          "Continuous Normalizing Flows"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Denoising Diffusion Probabilistic Models",
          "justification": "This model is empirically validated in the study.",
          "quote": "Denoising Diffusion Probabilistic Models (DDPM, Ho et al. 2020)"
        },
        "aliases": [
          "DDPM"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The DDPM model is referenced but not contributed by this paper.",
          "quote": "Denoising Diffusion Probabilistic Models (DDPM, Ho et al. 2020)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper conducts experiments using the DDPM model.",
          "quote": "We empirically validate our theory on both synthetic and natural images by iteratively training normalizing flows and state-of-the-art diffusion models on CIFAR10 and FFHQ."
        },
        "is_compared": {
          "value": 1,
          "justification": "The DDPM model's performance is compared against other models in the experiments.",
          "quote": "Moreover, we provide theoretical (Proposition 1) and empirical (Figure 1) evidence..."
        },
        "referenced_paper_title": {
          "value": "Denoising Diffusion Probabilistic Models",
          "justification": "This is the referenced paper title for the DDPM model.",
          "quote": "Denoising Diffusion Probabilistic Models (DDPM, Ho et al. 2020)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR10",
          "justification": "The CIFAR10 dataset is used for empirical validation in the paper's experiments.",
          "quote": "We empirically validate our theory...by iteratively training...on CIFAR10."
        },
        "aliases": [
          "CIFAR-10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the original paper where the CIFAR10 dataset was introduced.",
          "quote": "CIFAR-10 (Krizhevsky and Hinton, 2009)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "torch-cfm",
          "justification": "The 'torch-cfm' library is explicitly mentioned as the implementation used for normalizing flows.",
          "quote": "we relied on the original codebases torch-cfm (https://github.com/atong01/conditional-flow-matching)..."
        },
        "aliases": [
          "conditional-flow-matching"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Improving and Generalizing Flow-based Generative Models with Minibatch Optimal Transport",
          "justification": "This is the referenced paper title that aligns with the conditional flow matching technique and its implementation.",
          "quote": "OTCFM (Tong et al. 2023), https://github.com/atong01/conditional-flow-matching..."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 968,
    "prompt_tokens": 23777,
    "total_tokens": 24745
  }
}