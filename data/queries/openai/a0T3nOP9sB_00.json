{
  "paper": "a0T3nOP9sB.txt",
  "words": 10764,
  "extractions": {
    "title": {
      "value": "Adaptive patch foraging in deep reinforcement learning agents",
      "justification": "Title of the paper",
      "quote": "Adaptive patch foraging in deep reinforcement learning agents"
    },
    "description": "This paper investigates how deep reinforcement learning (RL) agents adaptively solve the patch foraging problem, which is a key behavioral optimization challenge studied in biology. It aims to understand whether agents can learn optimal foraging strategies akin to biological ones and if their internal dynamics reflect those of non-human primates. The study demonstrates that RL agents are capable of learning adaptive foraging strategies, achieve near-optimal foraging performance considering temporal discounting, and exhibit internal dynamics similar to neural recordings from foraging primates.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental evaluation and results involving deep reinforcement learning agents in a 3D patch foraging environment. The focus is on investigating and demonstrating capabilities through empirical methods rather than theoretical analysis.",
      "quote": "This paper is an empirical investigation into the emergence of complex patch foraging behavior..."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper revolves around the use of deep reinforcement learning to solve the patch foraging problem and compare it to biological foraging strategies.",
        "quote": "Here, we investigate deep reinforcement learning agents in an ecological patch foraging task."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Behavioral Ecology",
          "justification": "The study draws parallels between artificial agent behaviors and biological foraging strategies, grounding its experiments in behavioral ecology theories.",
          "quote": "Patch foraging is one of the most heavily studied behavioral optimization challenges in biology."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Neuroscience",
          "justification": "The paper investigates the internal dynamics of the RL agents and compares them to neural recordings from non-human primates, bridging the study with neuroscience.",
          "quote": "Finally, we show emergent internal dynamics in these agents that resemble single-cell recordings from foraging non-human primates."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Recurrent Deep Reinforcement Learning Agents",
          "justification": "This term describes the overall architecture of the agents studied in the paper, which include recurrent neural networks and RL algorithms.",
          "quote": "demonstrate that the current deep reinforcement learning agents learn to adaptively trade off travel time and patch reward in patterns similar to biological foragers"
        },
        "aliases": [
          "RL Agents"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The study investigates the behavior of these specific RL agents in patch foraging tasks, contributing insights and findings in this area.",
          "quote": "Here, we investigate deep reinforcement learning agents in an ecological patch foraging task."
        },
        "is_executed": {
          "value": true,
          "justification": "The RL agents were implemented and trained within simulation environments, utilizing computational resources for execution.",
          "quote": "Agents received a reward of zero on each step they were outside of both patches... Experience was saved in an experience buffer."
        },
        "is_compared": {
          "value": true,
          "justification": "The study compares the performance of the RL agents to optimal foraging behaviors and the Marginal Value Theorem (MVT).",
          "quote": "Here, we take each agent's mean patch leaving time relative to the MVT across patch distance environments, and test if these 12 values..."
        },
        "referenced_paper_title": {
          "value": "Maximum a posteriori policy optimisation",
          "justification": "This paper refers to the reinforcement learning algorithm used to train the agents.",
          "quote": "We use a state-of-the-art continuous control deep reinforcement learning (RL) algorithm for training our agents: maximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018)."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "Maximum a posteriori policy optimization (MPO)",
          "justification": "The paper uses this RL algorithm for training the agents.",
          "quote": "We use a state-of-the-art continuous control deep reinforcement learning (RL) algorithm for training our agents: maximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018)."
        },
        "aliases": [
          "MPO"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Maximum a posteriori policy optimisation",
          "justification": "This is the reference paper for the MPO algorithm used in the study.",
          "quote": "Maximum a posteriori policy optimization (MPO; Abdolmaleki et al., 2018)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 890,
    "prompt_tokens": 17690,
    "total_tokens": 18580
  }
}