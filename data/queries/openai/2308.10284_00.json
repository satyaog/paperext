{
  "paper": "2308.10284.txt",
  "words": 9791,
  "extractions": {
    "title": {
      "value": "T OWARDS F EW- SHOT C OORDINATION : R EVISITING A D - HOC TEAMPLAY CHALLENGE IN THE GAME OF H ANABI",
      "justification": "This is the full title that appears prominently at the start of the paper.",
      "quote": "T OWARDS F EW- SHOT C OORDINATION : R EVISITING A D - HOC TEAMPLAY CHALLENGE IN THE GAME OF H ANABI"
    },
    "description": "This paper revisits the ad-hoc teamplay challenge in the cooperative multi-agent reinforcement learning (MARL) setting using the game Hanabi. The authors propose a new framework to evaluate few-shot coordination (FSC) by defining an adaptation regret metric to measure the adaptability of MARL methods. They conduct experiments with several state-of-the-art MARL algorithms, analyze their adaptability, and study the influence of different hyper-parameters on adaptability.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves the creation of a framework, conducting experiments with multiple MARL algorithms, and analyzing the empirical results.",
      "quote": "In this work, we show empirically that state-of-the-art ZSC algorithms have poor performance when paired with agents trained with different learning methods, and they require millions of interaction samples to adapt to these new partners."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The research primarily deals with multi-agent reinforcement learning (MARL) and proposes a framework for evaluating MARL algorithms.",
        "quote": "Reinforcement Learning (RL) provides a general and scalable framework to model this challenging partially observable, non-stationary, multi-agent learning problem."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Agent Systems",
          "justification": "The paper specifically addresses issues in cooperative multi-agent reinforcement learning and proposes solutions for zero-shot coordination (ZSC) and few-shot coordination (FSC).",
          "quote": "Cooperative Multi-agent Reinforcement Learning (MARL) algorithms with Zero-Shot Coordination (ZSC) have gained significant attention in recent years."
        },
        "aliases": [
          "MAS"
        ]
      },
      {
        "name": {
          "value": "Hyper-Parameter Optimization",
          "justification": "The paper discusses the impact of different hyper-parameters on the adaptability of MARL algorithms.",
          "quote": "Our experiments show that two categories of hyper-parameters controlling the training data diversity and optimization process have a significant impact on the adaptability of Hanabi agents."
        },
        "aliases": [
          "HPO"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Independent Q-Learning (IQL)",
          "justification": "IQL is identified and evaluated in the experiments as one of the baseline MARL algorithms.",
          "quote": "However, we also discovered that naive Independent Q-Learning (IQL)(Tan, 1993) agents adapted to some of the partners as quickly as the SOTA Off-Belief Learning (OBL) algorithm, which is known to be excellent at ZSC."
        },
        "aliases": [
          "IQL"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Independent Q-Learning is not the main contribution of this paper; it is used as a baseline for comparison.",
          "quote": "However, we also discovered that naive Independent Q-Learning (IQL)(Tan, 1993) agents adapted to some of the partners as quickly as the SOTA Off-Belief Learning (OBL) algorithm, which is known to be excellent at ZSC."
        },
        "is_executed": {
          "value": true,
          "justification": "The models were executed as part of the experimental evaluation of adaptability to new partners.",
          "quote": "To adapt to a partner independently trained with a different algorithm or architecture, they require millions of episodes, which is several orders of magnitude higher than the amount of data needed for few-shot learning in supervised learning."
        },
        "is_compared": {
          "value": true,
          "justification": "IQL was compared experimentally to other state-of-the-art MARL algorithms like OBL.",
          "quote": "After evaluating several SOTA algorithms using our framework, our experiments reveal that naive Independent Q-Learning (IQL) agents in most cases adapt as quickly as the SOTA ZSC algorithm Off-Belief Learning (OBL)."
        },
        "referenced_paper_title": {
          "value": "Multi-agent reinforcement learning: Independent vs. cooperative agents",
          "justification": "This referenced paper is by Tan (1993) and is cited as the original work introducing Independent Q-Learning.",
          "quote": "However, we also discovered that naive Independent Q-Learning (IQL)(Tan, 1993) agents adapted to some of the partners as quickly as the SOTA Off-Belief Learning (OBL) algorithm."
        }
      },
      {
        "name": {
          "value": "Off-Belief Learning (OBL)",
          "justification": "OBL is characterized as a state-of-the-art algorithm for zero-shot coordination and is extensively studied in the paper's experiments.",
          "quote": "However, we also discovered that naive Independent Q-Learning (IQL)(Tan, 1993) agents adapted to some of the partners as quickly as the SOTA Off-Belief Learning (OBL) algorithm, which is known to be excellent at ZSC."
        },
        "aliases": [
          "OBL"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Off-Belief Learning is not the main contribution of this paper; it is used as a state-of-the-art algorithm for benchmark evaluation.",
          "quote": "However, we also discovered that naive Independent Q-Learning (IQL)(Tan, 1993) agents adapted to some of the partners as quickly as the SOTA Off-Belief Learning (OBL) algorithm, which is known to be excellent at ZSC."
        },
        "is_executed": {
          "value": true,
          "justification": "OBL was executed as part of the experimental evaluation of adaptability to new partners.",
          "quote": "To adapt to a partner independently trained with a different algorithm or architecture, they require millions of episodes, which is several orders of magnitude higher than the amount of data needed for few-shot learning in supervised learning."
        },
        "is_compared": {
          "value": true,
          "justification": "OBL was compared experimentally to other MARL algorithms like IQL.",
          "quote": "Our experiments show that two categories of hyper-parameters controlling the training data diversity and optimization process have a significant impact on the adaptability of Hanabi agents."
        },
        "referenced_paper_title": {
          "value": "Off-belief learning",
          "justification": "This referenced paper by Hu et al. (2021) is cited as the original work introducing Off-Belief Learning.",
          "quote": "Off-Belief Learning (OBL) (Hu et al., 2021) algorithm, which is known to be excellent at ZSC."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Hanabi",
          "justification": "The Hanabi game is used as a popular cooperative multi-agent game benchmark to evaluate the MARL methods proposed in this paper.",
          "quote": "In particular, we created a diverse set of pre-trained agents and defined a new metric called adaptation regret that measures the agentâ€™s ability to efficiently adapt and improve its coordination performance when paired with some held-out pool of partners on top of its ZSC performance."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Hanabi challenge: A new frontier for ai research",
          "justification": "The dataset reference is Bard et al. (2020), which is cited in the paper as the source defining Hanabi as a benchmark.",
          "quote": " Hanabi, a partially-observable cooperative multi-agent benchmark, has been a particularly popular game in recent years to study MARL in a cooperative setting."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Recurrent Replay Distributed Deep Q-Networks (R2D2)",
          "justification": "R2D2 is explicitly mentioned as a foundational library for implementing deep Q-learning in the partially observable setting used in this paper.",
          "quote": "We also pre-train agents with Value Decomposition Networks (VDN) algorithm (Sunehag et al., 2017) that learns a joint-action Q-function that consists of the sum of per-agent Q-values to allow for off-policy learning in the multi-agent setting. R2D2: In single-agent deep Q-learning (Mnih et al., 2015)..."
        },
        "aliases": [
          "R2D2"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Recurrent experience replay in distributed reinforcement learning",
          "justification": "This referenced paper by Kapturowski et al. (2019) is cited as the original work describing the Recurrent Replay Distributed Deep Q-Networks (R2D2) used in the paper's experiments.",
          "quote": "We also pre-train agents with Value Decomposition Networks (VDN) algorithm (Sunehag et al., 2017) that learns a joint-action Q-function that consists of the sum of per-agent Q-values to allow for off-policy learning in the multi-agent setting. R2D2: In single-agent deep Q-learning (Mnih et al., 2015)..."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1820,
    "prompt_tokens": 17692,
    "total_tokens": 19512
  }
}