{
  "paper": "2110.15137.txt",
  "words": 7295,
  "extractions": {
    "title": {
      "value": "PAC-Bayesian Learning of Aggregated Binary Activated Neural Networks with Probabilities over Representations",
      "justification": "Extracted from the title",
      "quote": "PAC-Bayesian Learning of Aggregated Binary Activated Neural Networks with Probabilities over Representations"
    },
    "description": "This paper focuses on the learning of neural networks with non-differentiable activation functions through considering a probability distribution over parameters. It leverages a PAC-Bayesian framework to provide tight generalization bounds and a unique learning algorithm for binary activated neural networks with real-valued parameters.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimentation and results from applying the proposed PAC-Bayesian approach and learning algorithms to binary classification datasets and compares them to existing models.",
      "quote": "We evaluated our proposed approach ABNet by following the experimental framework of Letarte, Germain, Guedj, and Laviolette [10], on the same six binary classification datasets"
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper deals with deep neural networks, learning algorithms, and improvement of their generalization through a PAC-Bayesian framework.",
        "quote": "Considering a probability distribution over parameters is known as an efficient strategy to learn a neural network with non-differentiable activation functions."
      },
      "aliases": [
        "Deep Neural Networks",
        "DNN"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Binary Neural Networks",
          "justification": "The paper focuses on learning and optimizing binary activated neural networks.",
          "quote": "Our work starts from one possible simplification, obtained by considering the binary activation function, meaning that each neuron outputs only one bit of information instead of the many bits needed to represent a real number."
        },
        "aliases": [
          "Binary Activated Networks",
          "Binary Networks"
        ]
      },
      {
        "name": {
          "value": "Model Aggregation",
          "justification": "The proposed approach is described as aggregating binary activated networks.",
          "quote": "We consider a distribution over BAMs, which we call an aggregation of BAMs."
        },
        "aliases": [
          "Network Aggregation"
        ]
      },
      {
        "name": {
          "value": "PAC-Bayesian Theory",
          "justification": "The theoretical framework used for deriving learning bounds and optimizing the model stems from PAC-Bayesian theory.",
          "quote": "Our work is motivated by the analysis of Letarte, Germain, Guedj, and Laviolette [10] and Biggs and Guedj [11], rooted in the PAC-Bayes theory."
        },
        "aliases": [
          "PAC-Bayes"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ABNet",
          "justification": "ABNet stands for Aggregation of Binary activated Networks and is the primary model proposed by the paper.",
          "quote": "The previous formulas lead to what stands as the forward propagation process of our new neural network, which we name ABNet for Aggregation of Binary activated Networks."
        },
        "aliases": [
          "Aggregation of Binary activated Networks"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "ABNet is the novel model introduced by the authors.",
          "quote": "The previous formulas lead to what stands as the forward propagation process of our new neural network, which we name ABNet for Aggregation of Binary activated Networks."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model underwent empirical testing and evaluation.",
          "quote": "We evaluated our proposed approach ABNet by following the experimental framework"
        },
        "is_compared": {
          "value": 1,
          "justification": "ABNet is compared empirically to other models and frameworks like PBGNet.",
          "quote": "We evaluated our proposed approach ABNet by following the experimental framework of Letarte, Germain, Guedj, and Laviolette [10]"
        },
        "referenced_paper_title": {
          "value": "Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks",
          "justification": "The referenced paper provides the basis for the PAC-Bayes theory used in this work.",
          "quote": "We evaluated our proposed approach ABNet by following the experimental framework of Letarte, Germain, Guedj, and Laviolette [10]"
        }
      },
      {
        "name": {
          "value": "PBGNet",
          "justification": "We evaluated our proposed approach ABNet by following the experimental framework of Letarte, Germain, Guedj, and Laviolette [10]",
          "quote": "we compare the Maximum-A-Posteriori (MAP) networks of both aggregated methods to three algorithms of the literature for learning neural networks with binary weights and/or activations: Expectation Backpropagation [9] (EBP) with real-valued weights and binary activations..."
        },
        "aliases": [
          "PAC-Bayesian Guided Network"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is referenced as a comparative baseline, not a contribution of this paper.",
          "quote": "We further extend the analysis to expose the dichotomy between the first layers versus the others."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model underwent empirical evaluation to benchmark ABNet.",
          "quote": "These observations are elaborated upon in the results section where empirical performance of PBGNet is reported along ABNet."
        },
        "is_compared": {
          "value": 1,
          "justification": "The empirical performance of PBGNet is compared to ABNet and other models.",
          "quote": "We evaluated our proposed approach ABNet by following the experimental framework of Letarte, Germain, Guedj, and Laviolette [10]"
        },
        "referenced_paper_title": {
          "value": "Dichotomize and Generalize: PAC-Bayesian Binary Activated Deep Neural Networks",
          "justification": "The referenced paper provides the basis for the PAC-Bayes theory used in this work.",
          "quote": "We evaluated our proposed approach ABNet by following the experimental framework of Letarte, Germain, Guedj, and Laviolette [10]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ads",
          "justification": "This dataset is used for empirical evaluation of ABNet and other models.",
          "quote": "Results correspond to means and standard deviations over 5 repetitions. Results of empirical evaluation on the ads dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "UCI Machine Learning Repository",
          "justification": "The paper lists the source repository as UCI.",
          "quote": "ads and adult from the UCI repository [16]"
        }
      },
      {
        "name": {
          "value": "adult",
          "justification": "This dataset is used for empirical evaluation of ABNet and other models.",
          "quote": "Results correspond to means and standard deviations over 5 repetitions. Results of empirical evaluation on the adult dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "UCI Machine Learning Repository",
          "justification": "The paper lists the source repository as UCI.",
          "quote": "ads and adult from the UCI repository [16]"
        }
      },
      {
        "name": {
          "value": "mnist",
          "justification": "This dataset is used for empirical evaluation of ABNet and other models.",
          "quote": "We evaluated our proposed approach ABNet by following the experimental framework of Letarte, Germain, Guedj, and Laviolette [10]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-based learning applied to document recognition",
          "justification": "The paper references the original MNIST dataset's creator.",
          "quote": "We evaluated our proposed approach ABNet by following the experimental framework of Letarte, Germain, Guedj, and Laviolette [10], on the same six binary classification datasets: ads and adult from the UCI repository, along with four MNIST binary variants mnistLH"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1851,
    "prompt_tokens": 13900,
    "total_tokens": 15751
  }
}