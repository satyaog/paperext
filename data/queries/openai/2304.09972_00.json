{
  "paper": "2304.09972.txt",
  "words": 10443,
  "extractions": {
    "title": {
      "value": "MasakhaNEWS: News Topic Classification for African languages",
      "justification": "This is the title of the paper as given in the provided text.",
      "quote": "MasakhaNEWS: News Topic Classification for African languages"
    },
    "description": "This paper develops the largest dataset for news topic classification covering 16 African languages and evaluates a set of baseline models using classical machine learning and fine-tuned language models. It also explores alternatives to full fine-tuning for zero-shot and few-shot learning.",
    "type": {
      "value": "empirical study",
      "justification": "The paper involves the creation of a dataset and empirical evaluation of various models and methods on that dataset, which characterizes it as an empirical study.",
      "quote": "We provide and evaluate a set of baseline models by training classical machine learning models and fine-tuning several language models... Our evaluation in a few-shot setting, shows that..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The study focuses on news topic classification, a core task in Natural Language Processing (NLP), specifically for African languages.",
        "quote": "Despite representing roughly a fifth of the world population, African languages are underrepresented in NLP research... In this paper, we develop MasakhaNEWS—the largest dataset for news topic classification covering 16 languages widely spoken in Africa."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text Classification",
          "justification": "The primary focus of the paper is on classifying news articles into different categories, which is a task in text classification.",
          "quote": "News topic classification is a text classification task in NLP that involves categorizing news articles into different categories like sports, business, entertainment, and politics."
        },
        "aliases": [
          "News Topic Classification"
        ]
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The paper evaluates baseline models using classical machine learning methods alongside deep learning approaches.",
          "quote": "We provide and evaluate a set of baseline models by training classical machine learning models and fine-tuning several language models."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The study explores various alternatives to full fine-tuning of language models for zero-shot and few-shot learning, which are applications of transfer learning.",
          "quote": "Furthermore, we explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning."
        },
        "aliases": [
          "Zero-shot Learning",
          "Few-shot Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "MAD-X",
          "justification": "MAD-X is mentioned as one of the alternative methods to full fine-tuning of language models in the paper.",
          "quote": "such as cross-lingual parameter-efficient fine-tuning (MAD-X)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "MAD-X is not introduced as a new model in this paper but is used as a method.",
          "quote": "such as cross-lingual parameter-efficient fine-tuning (MAD-X)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper evaluates MAD-X, indicating that it was run as part of the study.",
          "quote": "We explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning, such as cross-lingual parameter-efficient fine-tuning (MAD-X)."
        },
        "is_compared": {
          "value": 1,
          "justification": "MAD-X is compared to other models in the paper.",
          "quote": "Our evaluation shows that cross-lingual zero-shot transfer from a source language with same domain and task (i.e FINE-TUNE & MAD-X), gives superior result (+11 F1) than PET, SetFit, and GPT-3.5-TURBO."
        },
        "referenced_paper_title": {
          "value": "MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer",
          "justification": "This paper is cited when mentioning MAD-X.",
          "quote": "MAD-X (Pfeiffer et al., 2020)"
        }
      },
      {
        "name": {
          "value": "PET",
          "justification": "PET is mentioned as one of the alternative methods to full fine-tuning of language models in the paper.",
          "quote": "such as pattern exploiting training (PET)"
        },
        "aliases": [
          "Pattern Exploiting Training"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "PET is not introduced as a new model in this paper but is used as a method.",
          "quote": "such as pattern exploiting training (PET)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper evaluates PET, indicating that it was run as part of the study.",
          "quote": "We explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning, such as pattern exploiting training (PET)."
        },
        "is_compared": {
          "value": 1,
          "justification": "PET is compared to other models in the paper.",
          "quote": "Our evaluation shows that cross-lingual zero-shot transfer from a source language with same domain and task (i.e FINE-TUNE & MAD-X), gives superior result (+11 F1) than PET, SetFit, and GPT-3.5-TURBO."
        },
        "referenced_paper_title": {
          "value": "Exploiting cloze-questions for few-shot text classification and natural language inference",
          "justification": "This paper is cited when mentioning PET.",
          "quote": "Pattern exploiting training (PET) (Schick and Schütze, 2021a)"
        }
      },
      {
        "name": {
          "value": "GPT-3.5 Turbo",
          "justification": "GPT-3.5 Turbo is mentioned as one of the models evaluated for news topic classification.",
          "quote": "prompting language models (ChatGPT)"
        },
        "aliases": [
          "ChatGPT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-3.5 Turbo is not introduced as a new model in this paper but is used as a method.",
          "quote": "prompting language models (ChatGPT)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper evaluates GPT-3.5 Turbo, indicating that it was run as part of the study.",
          "quote": "Our evaluation in a zero-shot setting shows the potential of prompting ChatGPT for news topic classification for low-resource African languages."
        },
        "is_compared": {
          "value": 1,
          "justification": "GPT-3.5 Turbo is compared to other models in the paper.",
          "quote": "Our evaluation shows that cross-lingual zero-shot transfer from a source language with same domain and task (i.e FINE-TUNE & MAD-X), gives superior results (+11 F1) than PET, SetFit, and GPT-3.5-Turbo."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "This paper discusses GPT-3, which GPT-3.5-Turbo builds upon.",
          "quote": "Models such as GPT3 (Brown et al., 2020) and T5 (Raffel et al., 2020) are able to learn more structural and semantic relationships between words and have shown impressive results even in multilingual scenarios when tuned for different tasks (Chung et al., 2022; Muennighoff et al., 2023)."
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is mentioned as one of the models evaluated for news topic classification.",
          "quote": "However, GPT-4 was able to overcome this challenge for non-Latin script with impressive performance matching the result of cross-lingual transfer experiments from a related African language."
        },
        "aliases": [
          "ChatGPT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-4 is not introduced as a new model in this paper but is used as a method.",
          "quote": "However, GPT-4 was able to overcome this challenge for non-Latin script with impressive performance matching the result of cross-lingual transfer experiments from a related African language."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper evaluates GPT-4, indicating that it was run as part of the study.",
          "quote": "Our evaluation shows that cross-lingual zero-shot transfer from a source language with same domain and task (i.e FINE-TUNE & MAD-X), gives superior results (+11 F1) than PET, SetFit, and GPT-3.5-Turbo."
        },
        "is_compared": {
          "value": 1,
          "justification": "GPT-4 is compared to other models in the paper.",
          "quote": "Our evaluation shows that cross-lingual zero-shot transfer from a source language with same domain and task (i.e FINE-TUNE & MAD-X), gives superior results (+11 F1) than PET, SetFit, and GPT-3.5-Turbo."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "This paper discusses GPT-3, which GPT-4 builds upon.",
          "quote": "Models such as GPT3 (Brown et al., 2020) and T5 (Raffel et al., 2020) are able to learn more structural and semantic relationships between words and have shown impressive results even in multilingual scenarios when tuned for different tasks (Chung et al., 2022; Muennighoff et al., 2023)."
        }
      },
      {
        "name": {
          "value": "SetFit",
          "justification": "SetFit is mentioned as one of the alternative methods to full fine-tuning of language models in the paper.",
          "quote": "prompt-free sentence transformer fine-tuning (SetFit and the co:here embedding API)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "SetFit is not introduced as a new model in this paper but is used as a method.",
          "quote": "prompt-free sentence transformer fine-tuning (SetFit and the co:here embedding API)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper evaluates SetFit, indicating that it was run as part of the study.",
          "quote": "We explore several alternatives to full fine-tuning of language models that are better suited for zero-shot and few-shot learning, such as prompt-free sentence transformer fine-tuning (SetFit) and the co:here embedding API."
        },
        "is_compared": {
          "value": 1,
          "justification": "SetFit is compared to other models in the paper.",
          "quote": "Our evaluation shows that cross-lingual zero-shot transfer from a source language with same domain and task (i.e FINE-TUNE & MAD-X), gives superior result (+11 F1) than PET, SetFit, and GPT-3.5-Turbo."
        },
        "referenced_paper_title": {
          "value": "Efficient Few-Shot Learning Without Prompts",
          "justification": "This paper is cited when mentioning SetFit.",
          "quote": "prompt-free, sentence transformer fine-tuning (SetFit) (Tunstall et al., 2022a)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MasakhaNEWS",
          "justification": "The dataset created and discussed extensively in the paper is named MasakhaNEWS.",
          "quote": "we develop MasakhaNEWS—the largest dataset for news topic classification covering 16 languages widely spoken in Africa."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "MasakhaNEWS: News Topic Classification for African languages",
          "justification": "This is the current paper that introduces and details the dataset.",
          "quote": "In this paper, we develop MasakhaNEWS—the largest dataset for news topic classification covering 16 languages widely spoken in Africa."
        }
      },
      {
        "name": {
          "value": "BBC News",
          "justification": "The BBC News dataset is one of the sources for creating the MasakhaNEWS dataset.",
          "quote": "The data used in this research study were sourced from multiple reputable news outlets. The collection process involved crawling the British Broadcasting Corporation (BBC) and Voice of America (VOA) websites."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The data used in this research study were sourced from multiple reputable news outlets.",
          "justification": "The paper does not specify a title for this reference.",
          "quote": "The data used in this research study were sourced from multiple reputable news outlets."
        }
      },
      {
        "name": {
          "value": "Voice of America (VOA)",
          "justification": "The VOA dataset is one of the sources for creating the MasakhaNEWS dataset.",
          "quote": "The data used in this research study were sourced from multiple reputable news outlets. The collection process involved crawling the British Broadcasting Corporation (BBC) and Voice of America (VOA) websites."
        },
        "aliases": [
          "VOA"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The data used in this research study were sourced from multiple reputable news outlets.",
          "justification": "The paper does not specify a title for this reference.",
          "quote": "The data used in this research study were sourced from multiple reputable news outlets."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 5867,
    "prompt_tokens": 48239,
    "total_tokens": 54106
  }
}