{
  "paper": "2210.14891.txt",
  "words": 15751,
  "extractions": {
    "title": {
      "value": "Broken Neural Scaling Laws",
      "justification": "The title is provided at the beginning of the document",
      "quote": "B ROKEN N EURAL S CALING L AWS"
    },
    "description": "This research presents a smoothly broken power law functional form, named broken neural scaling law (BNSL), which accurately models and extrapolates the scaling behaviors of deep neural networks across various architectures and tasks. The study demonstrates that BNSL performs well in predicting scaling behavior, particularly in cases with non-monotonic transitions and sharp inflection points.",
    "type": {
      "value": "Empirical",
      "justification": "The study involves extensive empirical evaluation across various architectures and tasks to demonstrate the accuracy of the proposed scaling law.",
      "quote": "An extensive empirical evaluation demonstrates that BNSL accurately model & extrapolate the scaling behaviors for various architectures & for each of various tasks within a large & diverse set of upstream & downstream tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning Scaling Laws",
        "justification": "The focus of the paper is on discovering and validating scaling laws for deep neural networks.",
        "quote": "We present a smoothly broken power law functional form (referred to by us as a broken neural scaling law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Evaluation",
          "justification": "The paper evaluates the proposed scaling law across various models and tasks.",
          "quote": "An extensive empirical evaluation demonstrates that BNSL accurately model & extrapolate the scaling behaviors for various architectures & for each of various tasks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Artificial Intelligence Safety",
          "justification": "The paper discusses the implications of accurately predicting scaling behavior for AI safety.",
          "quote": "Additionally, this functional form accurately models & extrapolates scaling behavior that other functional forms are incapable of expressing such as the non-monotonic transitions present in the scaling behavior of phenomena such as double descent & the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "AlphaZero",
          "justification": "The paper mentions using AlphaZero to demonstrate the accuracy of BNSL in modeling and extrapolating scaling behaviors in reinforcement learning.",
          "quote": "We show that BNSL accurately models and extrapolates the scaling behaviors of various multi-agent and single-agent reinforcement learning algorithms trained in various environments. In the top left plot and top middle plot and top right plot of Figure 3, BNSL accurately models and extrapolates the scaling behavior of the AlphaZero algorithm."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "AlphaZero is used as a benchmark model in the study.",
          "quote": "We show that BNSL accurately models and extrapolates the scaling behaviors of various multi-agent and single-agent reinforcement learning algorithms trained in various environments. In the top left plot and top middle plot and top right plot of Figure 3, BNSL accurately models and extrapolates the scaling behavior of the AlphaZero algorithm"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results, indicating that AlphaZero was executed to gather data for the analysis.",
          "quote": "In the top left plot and top middle plot and top right plot of Figure 3, BNSL accurately models and extrapolates the scaling behavior of the AlphaZero algorithm"
        },
        "is_compared": {
          "value": 1,
          "justification": "AlphaZero's scaling behavior is used to validate the effectiveness of BNSL in comparison to other models.",
          "quote": "We show that BNSL accurately models and extrapolates the scaling behaviors of various multi-agent and single-agent reinforcement learning algorithms trained in various environments."
        },
        "referenced_paper_title": {
          "value": "Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm",
          "justification": "AlphaZero's original paper is on mastering board games through self-play.",
          "quote": "In the top left plot and top middle plot and top right plot of Figure 3, BNSL accurately models and extrapolates the scaling behavior of the AlphaZero algorithm (Silver et al., 2017)."
        }
      },
      {
        "name": {
          "value": "GPT-3",
          "justification": "The paper includes experiments involving the scaling behavior of large language models, such as GPT-3, in downstream tasks.",
          "quote": "In Section A.15 (and also Figure 2), we additionally show that BNSL yields accurate extrapolations of performance (to scales that are over an order of magnitude larger than the maximum (along the x-axis) of the points used for fitting) on large-scale downstream language tasks when number of model parameters is on the x-axis."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-3 is used as a benchmark model in the study.",
          "quote": "In Section A.15 (and also Figure 2), we additionally show that BNSL yields accurate extrapolations of performance (to scales that are over an order of magnitude larger than the maximum (along the x-axis) of the points used for fitting) on large-scale downstream language tasks when number of model parameters is on the x-axis."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results, indicating that GPT-3 was executed to gather data for the analysis.",
          "quote": "Experimental data obtained from Table H.1 of the GPT-3 arXiv paper (Brown et al., 2020)."
        },
        "is_compared": {
          "value": 1,
          "justification": "GPT-3's scaling behavior is used to validate the effectiveness of BNSL in comparison to other models.",
          "quote": "To view all plots of the BNSL on each of these tasks, see Figures 37, 39, 38 in Appendix A.36. To view plots of M1, M2, M3, and M4 on these tasks, see Figure 8 of Alabdulmohsin et al. (2022)."
        },
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "GPT-3's original paper focused on its capabilities as a language model.",
          "quote": "Experimental data obtained from Table H.1 of the GPT-3 arXiv paper (Brown et al., 2020)."
        }
      },
      {
        "name": {
          "value": "ViT (Vision Transformer)",
          "justification": "The paper includes experiments involving the scaling behavior of ViT in downstream vision tasks.",
          "quote": "The upstream task is supervised pretraining of ViT (Dosovitskiy et al., 2020) models on subsets of JFT-300M (Sun et al., 2017). The Downstream Task is 20-shot ImageNet classification."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ViT is used as a benchmark model in the study.",
          "quote": "The upstream task is supervised pretraining of ViT (Dosovitskiy et al., 2020) models on subsets of JFT-300M (Sun et al., 2017)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results, indicating that ViT was executed to gather data for the analysis.",
          "quote": "The upstream task is supervised pretraining of ViT (Dosovitskiy et al., 2020) models on subsets of JFT-300M (Sun et al., 2017)."
        },
        "is_compared": {
          "value": 1,
          "justification": "ViT's scaling behavior is used to validate the effectiveness of BNSL in comparison to other models.",
          "quote": "To view plots of M1, M2, M3, M4 on each of these tasks, see Appendix A.4 of Alabdulmohsin et al. (2022)."
        },
        "referenced_paper_title": {
          "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "justification": "ViT's original paper focused on its capability in image recognition.",
          "quote": "The upstream task is supervised pretraining of ViT (Dosovitskiy et al., 2020) models on subsets of JFT-300M (Sun et al., 2017)."
        }
      },
      {
        "name": {
          "value": "ResNet",
          "justification": "The paper includes experiments involving the scaling behavior of ResNet models in various tasks.",
          "quote": "The architectures in this set include ResNets, Transformers, MLPs, MLP-Mixers, RNNs, GNNs, U-Nets, Ensembles (& Non-Ensembles), MoE (& Non-MoE) models, & Sparse Pruned (& Non-Sparse Unpruned) models."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet is used as a benchmark model in the study.",
          "quote": "The architectures in this set include ResNets, Transformers, MLPs, MLP-Mixers, RNNs, GNNs, U-Nets, Ensembles (& Non-Ensembles), MoE (& Non-MoE) models, & Sparse Pruned (& Non-Sparse Unpruned) models."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results, indicating that ResNet was executed to gather data for the analysis.",
          "quote": "Experimental data obtained from the ResNet-34 CIFAR-100 results in Figure 1 left of Ko et al. (2023)."
        },
        "is_compared": {
          "value": 1,
          "justification": "ResNet's scaling behavior is used to validate the effectiveness of BNSL in comparison to other models.",
          "quote": "The architectures in this set include ResNets, Transformers, MLPs, MLP-Mixers, RNNs, GNNs, U-Nets, Ensembles (& Non-Ensembles), MoE (& Non-MoE) models, & Sparse Pruned (& Non-Sparse Unpruned) models."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "ResNet's original paper focused on its capability in image recognition.",
          "quote": "The architectures in this set include ResNets, Transformers."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "JFT-300M",
          "justification": "The paper includes experiments involving JFT-300M dataset for pretraining models.",
          "quote": "The upstream task is supervised pretraining of ViT (Dosovitskiy et al., 2020) models on subsets of JFT-300M (Sun et al., 2017)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
          "justification": "JFT-300M is discussed in the context of large-scale vision pretraining datasets.",
          "quote": "The upstream task is supervised pretraining of ViT (Dosovitskiy et al., 2020) models on subsets of JFT-300M (Sun et al., 2017)."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "The paper includes experiments involving ImageNet dataset for evaluating vision models.",
          "quote": "The Downstream Task is 20-shot ImageNet classification."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "ImageNet's original paper is focused on providing a large-scale image dataset for evaluation.",
          "quote": "The Downstream Task is 20-shot ImageNet classification."
        }
      },
      {
        "name": {
          "value": "BIG-Bench (BB)",
          "justification": "The paper includes experiments involving BIG-Bench dataset for evaluating language models.",
          "quote": "In this large-scale language subset of the benchmark, the tasks that are evaluated are error rates on each of the various downstream tasks from the BIG-Bench (BB) (Srivastava et al., 2022) benchmark."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models",
          "justification": "BIG-Bench's original paper provides a benchmark for evaluating language models across various tasks.",
          "quote": "In this large-scale language subset of the benchmark, the tasks that are evaluated are error rates on each of the various downstream tasks from the BIG-Bench (BB) (Srivastava et al., 2022) benchmark."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "The paper includes experiments involving CIFAR-100 dataset for evaluating vision models.",
          "quote": "The downstream tasks are: Birds 200 (Welinder et al., 2010), Caltech101 (Fei-Fei et al., 2004), CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-100's original paper is focused on providing a dataset for evaluating models on image classification.",
          "quote": "The downstream tasks are: Birds 200 (Welinder et al., 2010), Caltech101 (Fei-Fei et al., 2004), CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009)"
        }
      },
      {
        "name": {
          "value": "Caltech101",
          "justification": "The paper includes experiments involving Caltech101 dataset for evaluating vision models.",
          "quote": "The downstream tasks are: Birds 200 (Welinder et al., 2010), Caltech101 (Fei-Fei et al., 2004), CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories",
          "justification": "Caltech101's original paper is focused on providing a dataset for evaluating models on image classification.",
          "quote": "The downstream tasks are: Birds 200 (Welinder et al., 2010), Caltech101 (Fei-Fei et al., 2004), CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SciPy",
          "justification": "SciPy library is explicitly mentioned as the tool used for curve-fitting in the experiments.",
          "quote": "In our experiments, SciPy curve-fitting library (Virtanen et al., 2020) was used."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SciPy 1.0: fundamental algorithms for scientific computing in Python",
          "justification": "SciPy's original paper is focused on describing the capabilities and use cases of the SciPy library.",
          "quote": "In our experiments, SciPy curve-fitting library (Virtanen et al., 2020) was used."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3123,
    "prompt_tokens": 32457,
    "total_tokens": 35580
  }
}