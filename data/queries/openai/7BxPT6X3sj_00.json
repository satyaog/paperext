{
  "paper": "7BxPT6X3sj.txt",
  "words": 15935,
  "extractions": {
    "title": {
      "value": "Flexible Phase Dynamics for Bio-Plausible Contrastive Learning",
      "justification": "This is the given title of the paper.",
      "quote": "Flexible Phase Dynamics for Bio-Plausible Contrastive Learning"
    },
    "description": "This paper investigates alternative training dynamics for Contrastive Learning (CL) to make them more bio-plausible. The authors propose modifications to traditional CL algorithms that relax rigid training dynamics, making them temporally local and stochastic. The paper provides theoretical foundations and empirical results demonstrating that these relaxed dynamics can still achieve effective learning, thus expanding the range of systems capable of implementing CL, including neuromorphic and biological systems.",
    "type": {
      "value": "theoretical",
      "justification": "The focus of the paper is on theoretical foundations, mathematical proofs, and proposed modifications to existing CL algorithms rather than solely on empirical evaluations.",
      "quote": "Thanks to a set of general theorems corroborated by numerical experiments across several CL models, our results provide theoretical foundations for the study and development of CL methods for biological and neuromorphic neural networks."
    },
    "primary_research_field": {
      "name": {
        "value": "Contrastive Learning",
        "justification": "The paper specifically explores various aspects of contrastive learning (CL) algorithms, proposing modifications to their training dynamics.",
        "quote": "Here, we investigate alternative training dynamics for CL, to better understand the breadth of systems capable of implementing it."
      },
      "aliases": [
        "CL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neuromorphic Computing",
          "justification": "The paper discusses the relevance of modified CL algorithms for neuromorphic systems.",
          "quote": "This paper investigates alternative training dynamics for Contrastive Learning (CL) to make them more bio-plausible."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Neuroscience",
          "justification": "The paper aims to make CL algorithms more applicable as normative models in neuroscience.",
          "quote": "CL has been proposed as a normative model for biological learning..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Boltzmann Machine",
          "justification": "The paper mentions the Boltzmann machine as an example of an equilibrium-based CL method and compares its training dynamics with other methods.",
          "quote": "For example, in a Boltzmann machine (Ackley et al., 1985) the positive phase calculates network states conditioned on data while the negative phase calculates internally generated states free from data conditioning."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The Boltzmann Machine was not contributed by this paper but is used as a comparative model.",
          "quote": "For example, the Boltzmann machine (Ackley et al., 1985)."
        },
        "is_executed": {
          "value": 0,
          "justification": "It is not mentioned whether the Boltzmann Machine was executed on GPU or CPU in this paper.",
          "quote": ""
        },
        "is_compared": {
          "value": 1,
          "justification": "The Boltzmann Machine is used as a reference point for the new training dynamics proposed in this paper.",
          "quote": "We propose an importance sampling-inspired approach to estimating the gradient that makes two-term CL methods temporally local and show it is unbiased (see Appendix §§A.1)."
        },
        "referenced_paper_title": {
          "value": "A learning algorithm for Boltzmann machines",
          "justification": "The referenced paper for the Boltzmann Machine model is mentioned in the text.",
          "quote": "For example, in a Boltzmann machine (Ackley et al., 1985)..."
        }
      },
      {
        "name": {
          "value": "Equilibrium Propagation",
          "justification": "Equilibrium Propagation is another CL method discussed in the paper, particularly regarding its training dynamics.",
          "quote": "Equilibrium propagation (Scellier & Bengio, 2017); Hinton, 2022)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The Equilibrium Propagation model is cited and discussed but not contributed by this paper.",
          "quote": "Equilibrium propagation (Scellier & Bengio, 2017)."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not specify whether Equilibrium Propagation was executed on GPU or CPU.",
          "quote": ""
        },
        "is_compared": {
          "value": 1,
          "justification": "Equilibrium Propagation is compared against the proposed methods in the paper.",
          "quote": "We propose an importance sampling-inspired approach to estimating the gradient that makes two-term CL methods temporally local and show it is unbiased (see Appendix §§A.1)."
        },
        "referenced_paper_title": {
          "value": "Equilibrium propagation: Bridging the gap between energy-based models and backpropagation",
          "justification": "The referenced paper for the Equilibrium Propagation model is cited in the text.",
          "quote": "Equilibrium propagation (Scellier & Bengio, 2017)."
        }
      },
      {
        "name": {
          "value": "Contrastive Predictive Coding (CPC)",
          "justification": "The paper discusses CPC as a relevant non-equilibrium CL method.",
          "quote": "The first class originates from Contrastive Predictive Coding (CPC) (Oord et al., 2018), which contrasts positive samples of the future conditioned on the present with negative samples taken at random from the analyzed time series."
        },
        "aliases": [
          "CPC"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "CPC is cited but not contributed by this paper.",
          "quote": "The first class originates from Contrastive Predictive Coding (CPC) (Oord et al., 2018)."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not specify whether CPC was executed on GPU or CPU.",
          "quote": ""
        },
        "is_compared": {
          "value": 1,
          "justification": "CPC is discussed in comparison to the proposed methods for achieving temporal locality.",
          "quote": "The first class originates from Contrastive Predictive Coding (CPC) (Oord et al., 2018), which contrasts positive samples of the future conditioned on the present with negative samples taken at random from the analyzed time series."
        },
        "referenced_paper_title": {
          "value": "Representation learning with contrastive predictive coding",
          "justification": "The referenced paper for the CPC model is cited in the text.",
          "quote": "The first class originates from Contrastive Predictive Coding (CPC) (Oord et al., 2018)."
        }
      },
      {
        "name": {
          "value": "Forward-Forward (FF) Algorithm",
          "justification": "The paper proposes modifications to the FF algorithm to achieve temporal locality and bio-plausibility.",
          "quote": "The second class encompasses the Forward-Forward algorithm, and related works, that do binary classification of positive from negative samples at each layer to achieve learning without backpropagation (Hinton, 2022; Ororbia & Mali, 2023)."
        },
        "aliases": [
          "FF"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "Modifications to the FF algorithm are contributed by this paper.",
          "quote": "We empirically evaluated the proposed algorithms by using them to modify two different CL methods: maximum likelihood learning in energy based models (LeCun et al., 2006) and the recently proposed, noise contrastive estimation-inspired (Gutmann & Hyvärinen, 2010) Forward-Forward (FF) algorithm (Hinton, 2022)."
        },
        "is_executed": {
          "value": 0,
          "justification": "The execution environment (GPU or CPU) for the FF algorithm is not mentioned.",
          "quote": ""
        },
        "is_compared": {
          "value": 1,
          "justification": "The modified FF algorithm is compared with other methods in the experiments.",
          "quote": "We empirically evaluated the proposed algorithms by using them to modify two different CL methods: maximum likelihood learning...and the recently proposed, noise contrastive estimation-inspired (Gutmann & Hyvärinen, 2010) Forward-Forward (FF) algorithm (Hinton, 2022)."
        },
        "referenced_paper_title": {
          "value": "The forward-forward algorithm: Some preliminary investigations",
          "justification": "The referenced paper for the FF algorithm is cited in the text.",
          "quote": "The second class encompasses the Forward-Forward algorithm, and related works, that do binary classification of positive from negative samples at each layer to achieve learning without backpropagation (Hinton, 2022)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is one of the datasets used to test the modified CL methods proposed in the paper.",
          "quote": "Testing was performed on the binarized MNIST (bMNIST) and the Bars And Stripes (BAS) (Fischer & Igel, 2014) datasets for the RBM."
        },
        "aliases": [
          "bMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-Based Learning Applied to Document Recognition",
          "justification": "The referenced paper for the MNIST dataset is cited in the text.",
          "quote": "Testing was performed on the binarized MNIST (bMNIST) and the Bars And Stripes (BAS) (Fischer & Igel, 2014) datasets for the RBM."
        }
      },
      {
        "name": {
          "value": "Bars and Stripes",
          "justification": "Bars and Stripes is another dataset used to test the new methods proposed in the paper.",
          "quote": "Testing was performed on the binarized MNIST (bMNIST) and the Bars And Stripes (BAS) (Fischer & Igel, 2014) datasets for the RBM."
        },
        "aliases": [
          "BAS"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Training restricted Boltzmann machines: An introduction",
          "justification": "The referenced paper for the Bars and Stripes dataset is cited in the text.",
          "quote": "Testing was performed on the binarized MNIST (bMNIST) and the Bars And Stripes (BAS) (Fischer & Igel, 2014) datasets for the RBM."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "ADAM",
          "justification": "ADAM is used as the optimization algorithm for training the forward-forward network in the experiments.",
          "quote": "we used ADAM (Kingma & Ba, 2014) to train the forward-forward network."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A method for stochastic optimization",
          "justification": "The referenced paper for ADAM is cited in the text.",
          "quote": "we used ADAM (Kingma & Ba, 2014) to train the forward-forward network."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2100,
    "prompt_tokens": 25455,
    "total_tokens": 27555
  }
}