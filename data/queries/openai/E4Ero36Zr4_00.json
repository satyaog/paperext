{
  "paper": "E4Ero36Zr4.txt",
  "words": 11428,
  "extractions": {
    "title": {
      "value": "RETHINKING TEACHER-STUDENT CURRICULUM LEARNING THROUGH THE COOPERATIVE MECHANICS OF EXPERIENCE",
      "justification": "The title of the paper is \"RETHINKING TEACHER-STUDENT CURRICULUM LEARNING THROUGH THE COOPERATIVE MECHANICS OF EXPERIENCE\".",
      "quote": "RETHINKING TEACHER-STUDENT CURRICULUM LEARNING THROUGH THE COOPERATIVE MECHANICS OF EXPERIENCE"
    },
    "description": "The paper proposes a novel data-centric perspective to analyze Teacher-Student Curriculum Learning (TSCL) by leveraging cooperative game theory to describe how experiences presented by the teacher to the learner influence the performance of the curriculum. The aim is to provide a deeper exploration of TSCL and its broader applicability in machine learning.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper presents empirical studies to evaluate the game-theoretic interpretation of TSCL through experiments in supervised learning, reinforcement learning, and classical games.",
      "quote": "Through experiments covering supervised learning, reinforcement learning, and classical games, we estimate the cooperative values of experiences and use value-proportional curriculum mechanisms to construct curricula, even in cases where TSCL struggles."
    },
    "primary_research_field": {
      "name": {
        "value": "Curriculum Learning",
        "justification": "The primary focus of the paper is on Teacher-Student Curriculum Learning (TSCL) and its underlying mechanics.",
        "quote": "Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework that draws inspiration from human cultural transmission and learning."
      },
      "aliases": [
        "TSCL",
        "Teacher-Student Curriculum Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Cooperative Game Theory",
          "justification": "The paper leverages concepts from cooperative game theory to analyze and reinterpret TSCL components.",
          "quote": "We leverage cooperative game theory to describe how the composition of the set of experiences presented by the teacher to the learner, as well as their order, influences the performance of the curriculum that are found by TSCL approaches."
        },
        "aliases": [
          "Game Theory"
        ]
      },
      {
        "name": {
          "value": "Data Valuation",
          "justification": "The paper draws inspiration from data valuation to analyze units of experience and their impact on the learning process.",
          "quote": "We draw inspiration from work on feature attribution, data valuation and explainability, and leverage tools from cooperative game theory to analyze how the compositions of these units impact teacher-student interactions."
        },
        "aliases": [
          "Value of Data"
        ]
      },
      {
        "name": {
          "value": "Explainability",
          "justification": "The paper uses ideas from feature attribution and explainability to develop their data-centric perspective on TSCL.",
          "quote": "Our approach draws inspiration from work on feature attribution, data valuation and explainability."
        },
        "aliases": [
          "Model Interpretability"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "PPO",
          "justification": "The PPO model is used in the experiments related to reinforcement learning settings, including MINIGRID-ROOMS and Adversarial Sparse Iterated Prisoner’s Dilemma (A-SIPD).",
          "quote": "As a learner algorithm, we used PPO (Schulman et al., 2017) with an interaction budget of K = 500, 000 steps, and estimated, from the outcome of these simulations, the Nowak & Radzik values (Sec. 2, Eq. 2), conditioned on the every environment, and on a uniform distribution over all, as evaluation targets."
        },
        "aliases": [
          "Proximal Policy Optimization"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The PPO algorithm is not proposed by the paper; it is used as a learning algorithm in their experimental setups.",
          "quote": "As a learner algorithm, we used PPO (Schulman et al., 2017) with an interaction budget of K = 500, 000 steps, and estimated, from the outcome of these simulations, the Nowak & Radzik values (Sec. 2, Eq. 2), conditioned on the every environment, and on a uniform distribution over all, as evaluation targets."
        },
        "is_executed": {
          "value": 1,
          "justification": "The PPO model was executed as part of the experiments in the paper.",
          "quote": "As a learner algorithm, we used PPO (Schulman et al., 2017) with an interaction budget of K = 500, 000 steps, and estimated, from the outcome of these simulations, the Nowak & Radzik values (Sec. 2, Eq. 2), conditioned on the every environment, and on a uniform distribution over all, as evaluation targets."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not focus on comparing the performance of PPO to other models but rather uses it within their setup to evaluate TSCL.",
          "quote": "To do so, we build an experimental setting that evaluates the prospect of cooperation among units of experience in problems spanning supervised learning (SL), reinforcement learning (RL), and classical games (Sec. 5)"
        },
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "The PPO algorithm is referenced to the paper titled “Proximal Policy Optimization Algorithms” by Schulman et al.",
          "quote": "As a learner algorithm, we used PPO (Schulman et al., 2017) with an interaction budget of K = 500, 000 steps, and estimated, from the outcome of these simulations, the Nowak & Radzik values (Sec. 2, Eq. 2), conditioned on the every environment, and on a uniform distribution over all, as evaluation targets."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST dataset is used in the experiments, particularly in supervised learning tasks to evaluate units' values.",
          "quote": "For example, in the MNIST dataset (LeCun & Cortes, 2010), there may be ten units of experience, namely, classes ZERO, ONE, TWO, . . . , NINE."
        },
        "aliases": [
          "Modified National Institute of Standards and Technology database"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MNIST handwritten digit database",
          "justification": "The MNIST dataset is referenced to the paper titled “MNIST handwritten digit database” by LeCun & Cortes.",
          "quote": "For example, in the MNIST dataset (LeCun & Cortes, 2010), there may be ten units of experience, namely, classes ZERO, ONE, TWO, . . . , NINE."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 dataset is used in the experiments, particularly in supervised learning tasks to evaluate units' values.",
          "quote": "We applied the same general procedure to CIFAR10 (Krizhevsky, 2009)."
        },
        "aliases": [
          "Canadian Institute for Advanced Research 10"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The CIFAR-10 dataset is referenced to the paper titled “Learning Multiple Layers of Features from Tiny Images” by Krizhevsky.",
          "quote": "We applied the same general procedure to CIFAR10 (Krizhevsky, 2009)."
        }
      },
      {
        "name": {
          "value": "MINIGRID",
          "justification": "MINIGRID-ROOMS environments are used to evaluate the game-theoretic interpretation in reinforcement learning settings.",
          "quote": "We investigate the MINIGRID-ROOMS set of three environments, namely, TWO ROOMS, FOUR ROOMS, and SIX ROOMS for which it is folk knowledge that an optimal curriculum exists."
        },
        "aliases": [
          "MINIGRID-ROOMS"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Minimalistic Gridworld Environment for Gymnasium",
          "justification": "The MINIGRID-ROOMS environments are referenced to the associated repository titled \"Minimalistic Gridworld Environment for Gymnasium.\"",
          "quote": "We investigate the MINIGRID-ROOMS set of three environments, namely, TWO ROOMS, FOUR ROOMS, and SIX ROOMS for which it is folk knowledge that an optimal curriculum exists."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is mentioned as the library used for model architecture and implementation in the experiments.",
          "quote": "All models and architectures are implemented with PyTorch (Paszke et al., 2019)."
        },
        "aliases": [
          "torch"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The PyTorch library is referenced to the paper titled \"PyTorch: An Imperative Style, High-Performance Deep Learning Library” by Paszke et al.",
          "quote": "All models and architectures are implemented with PyTorch (Paszke et al., 2019)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1774,
    "prompt_tokens": 23859,
    "total_tokens": 25633
  }
}