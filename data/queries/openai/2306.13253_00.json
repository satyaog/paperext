{
  "paper": "2306.13253.txt",
  "words": 20573,
  "extractions": {
    "title": {
      "value": "Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok",
      "justification": "It's the exact title shown at the beginning of the paper.",
      "quote": "Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok"
    },
    "description": "This paper introduces a low-cost method to predict the phenomenon of grokking in neural networks, where perfect generalization occurs after extended periods of training. The method involves analyzing the spectral signature of learning curves in early epochs to determine the likelihood of future grokking.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves conducting experiments to validate the proposed method and analyze the causes of grokking.",
      "quote": "In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on neural networks, which fall under the domain of deep learning.",
        "quote": "Despite the recent growth of theoretical studies and empirical successes of neural networks (Graves et al., 2013; He et al., 2015; Krizhevsky et al., 2012; Silver et al., 2016), understanding why such networks find generalizable solutions in over-parameterized regimes, where the number of learnable parameters is much larger than the number of training samples, remains an open question."
      },
      "aliases": [
        "Neural Networks"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The study is heavily focused on understanding and improving the optimization process in neural networks.",
          "quote": "The grokking phenomenon opens the way to new studies concerning the structure of the minimum found by Stochastic Gradient Descent (SGD), and how networks behave in the neighbourhood of SGD training convergence."
        },
        "aliases": [
          "SGD",
          "Stochastic Gradient Descent"
        ]
      },
      {
        "name": {
          "value": "Generalization",
          "justification": "The primary concern of the paper is to predict when neural networks will generalize perfectly after a significant training period.",
          "quote": "Recently, Power et al. (2022) have shown through a phenomenon they named grokking that long after severe overfitting, validation accuracy sometimes suddenly begins to increase from chance level to perfect generalization."
        },
        "aliases": [
          "Overfitting"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Transformer",
          "justification": "The experiments in the paper use a transformer model.",
          "quote": "We study the learning curves of a transformer network (Vaswani et al., 2017) trained on arithmetic data in settings with and without grokking (section 2)."
        },
        "aliases": [
          "Attention"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The transformer model is not introduced in this paper; it was referenced from 'Attention is All You Need' by Vaswani et al. (2017).",
          "quote": "We study the learning curves of a transformer network (Vaswani et al., 2017)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments involve training instances of the transformer model.",
          "quote": "The training is performed by maximizing the likelihood under the direct autoregressive factorization."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of transformers is compared under different conditions regarding grokking.",
          "quote": "We propose a low-cost method that can predict grokking long before it occurs."
        },
        "referenced_paper_title": {
          "value": "Attention is All You Need",
          "justification": "The referenced paper for the Transformer model is 'Attention is All You Need' by Vaswani et al., 2017.",
          "quote": "We study the learning curves of a transformer network (Vaswani et al., 2017)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Modular Arithmetic Dataset",
          "justification": "The training is performed on a dataset consisting of arithmetic operations modulo a prime number.",
          "quote": "The task is to predict (a ◦ b) mod q for any pair of numbers (a, b) ∈ [p]², with [p] = {0, . . . , p − 1}."
        },
        "aliases": [
          "Arithmetic Data"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "none",
          "justification": "No specific paper is referenced for this dataset; it appears to be constructed explicitly for this study.",
          "quote": "The task is to predict (a ◦ b) mod q for any pair of numbers (a, b) ∈ [p]², with [p] = {0, . . . , p − 1}."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1179,
    "prompt_tokens": 46659,
    "total_tokens": 47838
  }
}