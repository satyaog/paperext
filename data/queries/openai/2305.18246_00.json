{
  "paper": "2305.18246.txt",
  "words": 23756,
  "extractions": {
    "title": {
      "value": "Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo",
      "justification": "This title concisely describes the main focus of the paper which is on reinforcement learning and the use of Langevin Monte Carlo for efficient exploration.",
      "quote": "Efficient Exploration in Reinforcement Learning via Langevin Monte Carlo"
    },
    "description": "This paper presents a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning. It introduces Langevin Monte Carlo to directly sample the Q function from its posterior distribution, as opposed to the conventional need for Gaussian approximation.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper not only provides theoretical analysis but also presents empirical evaluation on various tasks including Atari games.",
      "quote": "We apply this approach to deep RL, by using Adam optimizer to perform gradient updates. Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper's main focus is on improving exploration strategies in reinforcement learning using Langevin Monte Carlo.",
        "quote": "We present a scalable and effective exploration strategy based on Thompson sampling for reinforcement learning (RL)."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Bayesian Methods",
          "justification": "The paper utilizes a Bayesian approach by sampling from posterior distributions to achieve exploration in RL.",
          "quote": "Algorithms based on Langevin dynamics are widely used for training neural networks in Bayesian settings."
        },
        "aliases": [
          "Bayesian Approaches"
        ]
      },
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The practical implementation of the proposed methods in Atari games places this work in the realm of deep reinforcement learning.",
          "quote": "We apply this approach to deep RL, by using Adam optimizer to perform gradient updates."
        },
        "aliases": [
          "Deep RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Langevin Monte Carlo Least-Squares Value Iteration",
          "justification": "The main algorithm proposed in the paper is named Langevin Monte Carlo Least-Squares Value Iteration.",
          "quote": "We propose a practical and efficient online RL algorithm, Langevin Monte Carlo Least-Squares Value Iteration (LMC-LSVI)"
        },
        "aliases": [
          "LMC-LSVI"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "LMC-LSVI is a novel contribution introduced by the authors in this paper.",
          "quote": "We propose a practical and efficient online RL algorithm, Langevin Monte Carlo Least-Squares Value Iteration (LMC-LSVI)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The algorithm is executed and its performance is empirically evaluated in the paper.",
          "quote": "We apply this approach to deep RL, by using Adam optimizer to perform gradient updates."
        },
        "is_compared": {
          "value": 1,
          "justification": "LMC-LSVI is compared to several state-of-the-art algorithms in the experiments.",
          "quote": "Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The proposed algorithm does not seem to be based on a specific existing model and hence no referenced paper title is applicable.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Adam Langevin Monte Carlo Deep Q-Network",
          "justification": "A specific variant of LMC-LSVI tailored for Deep Q-Networks is proposed in the paper.",
          "quote": "we also propose Adam Langevin Monte Carlo Deep Q-Network (Adam LMCDQN), a preconditioned variant of LMC-LSVI based on the Adam optimizer"
        },
        "aliases": [
          "Adam LMCDQN"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "Adam LMCDQN is explicitly developed by the authors as a new approach.",
          "quote": "we also propose Adam Langevin Monte Carlo Deep Q-Network (Adam LMCDQN), a preconditioned variant of LMC-LSVI based on the Adam optimizer"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical results of Adam LMCDQN on several Atari games.",
          "quote": "In experiments on both N -chain and challenging Atari environments that require deep exploration, Adam LMCDQN performs similarly or better than state-of-the-art exploration approaches in deep RL."
        },
        "is_compared": {
          "value": 1,
          "justification": "Adam LMCDQN is compared numerically with state-of-the-art algorithms.",
          "quote": "Adam LMCDQN performs similarly or better than state-of-the-art exploration approaches in deep RL."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "Adam LMCDQN is a novel contribution and thus isn't based on a specific referenced paper.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari57",
          "justification": "The Atari57 suite is explicitly mentioned as one of the benchmarks used for evaluation.",
          "quote": "Our approach achieves better or similar results compared with state-of-the-art deep RL algorithms on several challenging exploration tasks from the Atari57 suite."
        },
        "aliases": [
          "Atari"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
          "justification": "The Atari benchmark is a well-known dataset commonly used in RL research, referenced as Bellemare et al., 2013.",
          "quote": "In experiments on both N -chain (Osband et al., 2016b) and challenging Atari environments (Bellemare et al., 2013)"
        }
      },
      {
        "name": {
          "value": "N-Chain",
          "justification": "The N-Chain environment is used in the experiments, particularly for demonstrating deep exploration.",
          "quote": "In our experiments, we consider N to be 25, 50, 75, or 100. For each chain length, we train different algorithms for 105 steps across 20 seeds."
        },
        "aliases": [
          "NChain"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "(Deep Exploration via Bootstrapped DQN, Osband et al. 2016)",
          "justification": "The N-Chain environment is referenced to Osband et al., 2016.",
          "quote": "For each chain length, we train different algorithms for 105 steps across 20 seeds. [...] Bootstrapped DQN"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1574,
    "prompt_tokens": 46832,
    "total_tokens": 48406
  }
}