{
  "paper": "2309.10954.txt",
  "words": 7266,
  "extractions": {
    "title": {
      "value": "In-Context Learning for Text Classification with Many Labels",
      "justification": "The title directly matches the one provided in the research paper.",
      "quote": "In-Context Learning for Text Classification with Many Labels"
    },
    "description": "This paper investigates the use of in-context learning (ICL) with large language models (LLMs) for text classification tasks with many labels. The approach leverages a pre-trained dense retrieval model to dynamically fetch relevant subsets of label examples and fit them into the limited context window of the LLM. The authors demonstrate state-of-the-art performance on multiple datasets without fine-tuning the models, and they explore the impact of various factors such as model size and number of in-context examples.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments and analyzes performance using multiple datasets and models, which is indicative of empirical research.",
      "quote": "We evaluate LLMs in this setting with three intent classification datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on text classification, which is a core task within the Natural Language Processing field.",
        "quote": "In-context learning (ICL) using large language models (LLMs) has recently exploded in popularity."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text Classification",
          "justification": "The study specifically targets text classification tasks, including intent classification and fine-grained sentiment analysis.",
          "quote": "In this work, we study whether ICL can handle challenging classification tasks with many possible labels."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "OPT",
          "justification": "OPT is one of the large language models evaluated in the study.",
          "quote": "Experiments are done using the LLaMA models (Touvron et al., 2023) and the OPT models (Zhang et al., 2022) as LLMs."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "LLaMA",
          "justification": "LLaMA is another large language model evaluated in the study.",
          "quote": "Experiments are done using the LLaMA models (Touvron et al., 2023) and the OPT models (Zhang et al., 2022) as LLMs."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DeBERTa-v2-XXLarge",
          "justification": "DeBERTa-v2-XXLarge is used as a baseline model in the comparative experiments.",
          "quote": "We compare the performance achieved against adapter-based fine-tuning of MLM models (DeBERTa-v2-XXLarge with the “Pfeiffer” bottleneck-style adapter)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SBERT",
          "justification": "SBERT is used as a retrieval model to fetch relevant examples for in-context learning.",
          "quote": "The model we use is a contrastively trained model which has been pre-trained on a massive generic dataset of text pairs."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "BANKING77",
          "justification": "BANKING77 is one of the intent classification datasets used for evaluation in the study.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "HWU64",
          "justification": "HWU64 is one of the intent classification datasets used for evaluation in the study.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CLINC150",
          "justification": "CLINC150 is one of the intent classification datasets used for evaluation in the study.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GoEmotions",
          "justification": "GoEmotions is a fine-grained sentiment classification dataset used for evaluation in the study.",
          "quote": "We evaluate LLMs in this setting with three intent classification datasets: BANKING77, HWU64, and CLINC150, as well as one fine-grained sentiment classification dataset: GoEmotions."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "AdapterHub",
          "justification": "AdapterHub is used to implement the bottleneck-style adapter for the DeBERTa-v2-XXLarge model.",
          "quote": "DeBERTa-v2-XXLarge with the “Pfeiffer” bottleneck-style adapter (Pfeiffer et al., 2020b) implemented with AdapterHub."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SentenceTransformers",
          "justification": "SentenceTransformers library is used to load the pre-trained retrieval model for the experiments.",
          "quote": "Specific retrieval model: For our sentence encoder/retriever, we use the SentenceTransformers library (Reimers and Gurevych, 2019a), and use the pre-trained “all-mpnet-base-v2” model."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SetFit",
          "justification": "SetFit is another framework used for contrastive fine-tuning in the comparative experiments.",
          "quote": "The SetFit results are based on contrastively tuning the same pre-trained model trained by Microsoft through the Setfit library."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1304,
    "prompt_tokens": 12823,
    "total_tokens": 14127
  }
}