{
  "paper": "2310.04680.txt",
  "words": 12267,
  "extractions": {
    "title": {
      "value": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning",
      "justification": "The title is taken directly from the paper header which clearly states it.",
      "quote": "The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning"
    },
    "description": "This paper studies the effects of scaling down large language models using weight pruning and dense scaling on two critical capabilities: fact recall from pre-training and in-context learning.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves empirical investigation and experiments to assess the effects of scaling on language models.",
      "quote": "By curating a suite of tasks that help disentangle these two capabilities, we find a striking difference in how these two abilities evolve due to scaling."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on the capabilities of Large Language Models (LLMs) in processing and recalling text-based information.",
        "quote": "How does scaling the number of parameters in large language models (LLMs) affect their core capabilities?"
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Pruning",
          "justification": "The paper studies the effects of pruning (removing weights) on maintaining the performance of language models.",
          "quote": "We investigate pruning as one possible technique to (down-)scale LLMs."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Language Model Scaling",
          "justification": "The paper evaluates the impact of scaling model size on LLM capabilities.",
          "quote": "We study the effects of scaling the number of parameters in an LLM on two fundamentally dichotomous capabilities."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "OPT-13B",
          "justification": "This model was used to evaluate the impact of model pruning and scaling on language model capabilities.",
          "quote": "we evaluate the two largest models that fit in our hardware setup – OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model was used as part of the research but was not newly developed in the paper.",
          "quote": "From the OPT family, we evaluate the two largest models that fit in our hardware setup – OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed for empirical evaluation in the research.",
          "quote": "we evaluate the two largest models that fit in our hardware setup – OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model was compared against other models for performance evaluation.",
          "quote": "TriviaQA dataset for fact recall evaluation (left) and linear classification dataset for ICL evaluation (right). We plot their average performance in red."
        },
        "referenced_paper_title": {
          "value": "OPT: Open Pre-trained Transformer Language Models",
          "justification": "The referenced paper is mentioned in the context of evaluating OPT models.",
          "quote": "From the OPT family, we evaluate the two largest models that fit in our hardware setup – OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively."
        }
      },
      {
        "name": {
          "value": "OPT-30B",
          "justification": "This model was used to evaluate the impact of model pruning and scaling on language model capabilities.",
          "quote": "we evaluate the two largest models that fit in our hardware setup – OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model was used as part of the research but was not newly developed in the paper.",
          "quote": "From the OPT family, we evaluate the two largest models that fit in our hardware setup – OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed for empirical evaluation in the research.",
          "quote": "we evaluate the two largest models that fit in our hardware setup – OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model was compared against other models for performance evaluation.",
          "quote": "TriviaQA dataset for fact recall evaluation (left) and linear classification dataset for ICL evaluation (right). We plot their average performance in red."
        },
        "referenced_paper_title": {
          "value": "OPT: Open Pre-trained Transformer Language Models",
          "justification": "The referenced paper is mentioned in the context of evaluating OPT models.",
          "quote": "From the OPT family, we evaluate the two largest models that fit in our hardware setup – OPT-13B and OPT-30B, with 13 and 30 billion parameters, respectively."
        }
      },
      {
        "name": {
          "value": "LLaMA-13B",
          "justification": "This model was used to evaluate the impact of model pruning and scaling on language model capabilities.",
          "quote": "From the LLaMA family, we evaluate LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model was used as part of the research but was not newly developed in the paper.",
          "quote": "From the LLaMA family, we evaluate LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed for empirical evaluation in the research.",
          "quote": "From the LLaMA family, we evaluate LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model was compared against other models for performance evaluation.",
          "quote": "TriviaQA dataset for fact recall evaluation (left) and linear classification dataset for ICL evaluation (right). We plot their average performance in red."
        },
        "referenced_paper_title": {
          "value": "LLaMA: Open and Efficient Foundation Language Models",
          "justification": "The referenced paper is mentioned in the context of evaluating LLaMA models.",
          "quote": "From the LLaMA family, we evaluate the two largest models that fit in our hardware setup – LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively."
        }
      },
      {
        "name": {
          "value": "LLaMA-33B",
          "justification": "This model was used to evaluate the impact of model pruning and scaling on language model capabilities.",
          "quote": "From the LLaMA family, we evaluate LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model was used as part of the research but was not newly developed in the paper.",
          "quote": "From the LLaMA family, we evaluate the two largest models that fit in our hardware setup – LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed for empirical evaluation in the research.",
          "quote": "From the LLaMA family, we evaluate LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model was compared against other models for performance evaluation.",
          "quote": "TriviaQA dataset for fact recall evaluation (left) and linear classification dataset for ICL evaluation (right). We plot their average performance in red."
        },
        "referenced_paper_title": {
          "value": "LLaMA: Open and Efficient Foundation Language Models",
          "justification": "The referenced paper is mentioned in the context of evaluating LLaMA models.",
          "quote": "From the LLaMA family, we evaluate the two largest models that fit in our hardware setup – LLaMA-13B and LLaMA-33B, with 13 and 33 billion parameters, respectively."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "TriviaQA",
          "justification": "The dataset was used to evaluate fact recall capabilities in the research.",
          "quote": "using closed-book QA benchmarks. In this setup, the answer to each question should be generated by recalling facts from the pre-training data."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
          "justification": "The referenced paper is directly cited when discussing TriviaQA dataset.",
          "quote": "TriviaQA. Joshi et al. (2017) developed the TriviaQA dataset with questions and supporting evidence."
        }
      },
      {
        "name": {
          "value": "DisentQA",
          "justification": "The dataset was used to evaluate the effect of in-context learning with overriding contexts.",
          "quote": "To solve the overriding QA task, the model must rely on in-context information rather than in-weights information from the pre-training data. We use the DisentQA dataset with synthetic contexts for this setup."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "DisentQA: Disentangling Parametric and Contextual Knowledge with Counterfactual Question Answering",
          "justification": "The referenced paper is directly cited when discussing DisentQA dataset.",
          "quote": "Neeman et al. (2022) constructed the DissentQA dataset from the NaturalQuestions dataset."
        }
      },
      {
        "name": {
          "value": "WebQuestions",
          "justification": "The dataset was used to evaluate fact recall capabilities in the research.",
          "quote": "WebQuestions. Berant et al. (2013) collected question-answer pairs from the Freebase knowledge database. We use its test set consisting of 2032 questions."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Semantic Parsing on Freebase from Question-Answer Pairs",
          "justification": "The referenced paper is directly cited when discussing WebQuestions dataset.",
          "quote": "WebQuestions. Berant et al. (2013) collected question-answer pairs from the Freebase knowledge database. We use its test set consisting of 2032 questions."
        }
      },
      {
        "name": {
          "value": "NaturalQuestions",
          "justification": "The dataset was used to evaluate the capability of extracting relevant information from the context.",
          "quote": "NaturalQuestions. Kwiatkowski et al. (2019) compiled the NaturalQuestions dataset from Google search queries."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Natural Questions: A Benchmark for Question Answering Research",
          "justification": "The referenced paper is directly cited when discussing NaturalQuestions dataset.",
          "quote": "NaturalQuestions. Kwiatkowski et al. (2019) compiled the NaturalQuestions dataset from Google search queries."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The library was used to conduct model evaluations and experiments.",
          "quote": "We perform our evaluations using TPU v3 running PyTorch (Paszke et al., 2019) with XLA backend."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The referenced paper is directly cited when discussing the use of PyTorch.",
          "quote": "We perform our evaluations using TPU v3 running PyTorch (Paszke et al., 2019) with XLA backend."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2371,
    "prompt_tokens": 22951,
    "total_tokens": 25322
  }
}