{
  "paper": "2210.13774.txt",
  "words": 14324,
  "extractions": {
    "title": {
      "value": "From Points to Functions: Infinite-Dimensional Representations in Diffusion Models",
      "justification": "Title at the beginning of the paper.",
      "quote": "FROM POINTS TO FUNCTIONS: INFINITE-DIMENSIONAL REPRESENTATIONS IN DIFFUSION MODELS"
    },
    "description": "This paper studies how to leverage diffusion models to provide infinite-dimensional representations for better downstream task performance. Instead of focusing on a single point in the trajectory, the paper suggests using the whole trajectory, and introduces methods to combine information from different timesteps for improved results.",
    "type": {
      "value": "Empirical",
      "justification": "The paper involves experimentation and analysis of diffusion models performance across different datasets.",
      "quote": "We introduce an attention and recurrence based modules that “learn to mix” information content of various time-steps such that the resultant representation leads to superior performance in downstream tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Representation Learning",
        "justification": "The primary focus of the paper is on representation learning using diffusion models.",
        "quote": "A lot of the progress in Machine Learning hinges on learning good representations of the data."
      },
      "aliases": [
        ""
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Diffusion Models",
          "justification": "The secondary focus is specifically on diffusion models as the mechanism for learning representations.",
          "quote": "using diffusion based models to obtain unbounded representations"
        },
        "aliases": [
          ""
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Variance Exploding SDE",
          "justification": "Specifically mentioned in the paper as the diffusion process used.",
          "quote": "we consider the Variance Exploding SDE (Song et al., 2021) for this diffusion process"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The Variance Exploding SDE is not presented as a novel model by the authors of the paper.",
          "quote": "(Song et al., 2021)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is used in experiments within the paper.",
          "quote": "we consider the Variance Exploding SDE (Song et al., 2021) for this diffusion process"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance is analyzed and compared with other baselines in the paper.",
          "quote": "We compare against MLP models at different points on the trajectory and RNN/Transformer models using the entire trajectory."
        },
        "referenced_paper_title": {
          "value": "Score-based generative modeling through stochastic differential equations",
          "justification": "The referenced paper where the model originally appeared.",
          "quote": "(Song et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR10",
          "justification": "Used as an example dataset for evaluating the performance of diffusion models.",
          "quote": "diffusion-based models...until convergence to an unstructured distribution (eg. N (0, I)) called, in this context, the prior distribution (eg. CIFAR10)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Cifar-10 (canadian institute for advanced research)",
          "justification": "The referenced paper where the CIFAR10 dataset originally appeared.",
          "quote": "(Krizhevsky et al., a)"
        }
      },
      {
        "name": {
          "value": "CIFAR100",
          "justification": "Used to evaluate the performance of proposed methods in the paper.",
          "quote": "We evaluate the MLP, RNN and Transformer based downstream models on diffusion systems with...also non-probabilistic ones (DRL). In Figure 1, we see the performance of these different setups for the following datasets: CIFAR10, CIFAR100 and Mini-ImageNet."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Cifar-100 (canadian institute for advanced research)",
          "justification": "The referenced paper where the CIFAR100 dataset originally appeared.",
          "quote": "(Krizhevsky et al., b)"
        }
      },
      {
        "name": {
          "value": "Mini-ImageNet",
          "justification": "Serves as a benchmark dataset for performance comparison in the paper.",
          "quote": "We evaluate the MLP, RNN and Transformer based downstream models on diffusion systems with...also non-probabilistic ones (DRL). In Figure 1, we see the performance of these different setups for the following datasets: CIFAR10, CIFAR100 and Mini-ImageNet."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Matching networks for one shot learning",
          "justification": "The referenced paper where the Mini-ImageNet dataset originally appeared.",
          "quote": "(Vinyals et al., 2016)"
        }
      },
      {
        "name": {
          "value": "CelebA",
          "justification": "Used to evaluate semantic feature learning along the trajectory in more real-world settings.",
          "quote": "Finally, we conduct experiments on the CelebA dataset (Liu et al., 2015), which is a large-scale dataset that consists of images of celebrities as well as multiple binary labels for each corresponding to the different attributes."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep learning face attributes in the wild",
          "justification": "The referenced paper where the CelebA dataset originally appeared.",
          "quote": "(Liu et al., 2015)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1060,
    "prompt_tokens": 39265,
    "total_tokens": 40325
  }
}