{
  "paper": "2305.17198.txt",
  "words": 12285,
  "extractions": {
    "title": {
      "value": "A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem",
      "justification": "The title is clearly stated on the first page of the paper.",
      "quote": "A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem"
    },
    "description": "This paper addresses the offline multi-agent reinforcement learning coordination problem by identifying the deficiencies of current model-free methods and proposing a model-based solution called MOMA-PPO. The new method generates synthetic interaction data, allowing agents to coordinate effectively and fine-tune their policies, significantly outperforming existing methods.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experiments and numerical comparisons with baselines to validate the proposed MOMA-PPO model.",
      "quote": "Across tasks and domains, MOMA-PPO significantly outperforms the offline MARL baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Multi-Agent Reinforcement Learning",
        "justification": "The primary focus of the paper is on multi-agent reinforcement learning, specifically coordination in offline settings.",
        "quote": "While these algorithms should leverage offline data when available, doing so gives rise to what we call the offline coordination problem."
      },
      "aliases": [
        "MARL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model-Based Reinforcement Learning",
          "justification": "The proposed solution, MOMA-PPO, emphasizes the use of model-based approaches for generating synthetic data.",
          "quote": "To address this setback, we emphasize the importance of inter-agent interactions and propose the very first model-based offline MARL method."
        },
        "aliases": [
          "MBRL"
        ]
      },
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "Deep learning techniques are used to train the policies and value functions in the studied MARL problems.",
          "quote": "Using deep reinforcement learning techniques, MOMA-PPO..."
        },
        "aliases": [
          "DRL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "MOMA-PPO",
          "justification": "MOMA-PPO is the novel model introduced and evaluated in this paper.",
          "quote": "We propose MOMA-PPO, a simple model-based approach that generates synthetic interactions."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The paper introduces MOMA-PPO as a new contribution to the field.",
          "quote": "Our resulting algorithm, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO)..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper presents results from experiments where the model was trained and evaluated, implying its execution.",
          "quote": "Using the world model to train the agents‚Äô policies."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of MOMA-PPO against several baseline models in different tasks.",
          "quote": "Our method is able to coordinate teams of offline learners and significantly outperforms model-free alternatives."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper title for MOMA-PPO as it is the primary contribution of this research.",
          "quote": "Our resulting algorithm, Model-based Offline Multi-Agent Proximal Policy Optimization (MOMA-PPO)..."
        }
      },
      {
        "name": {
          "value": "MAPPO",
          "justification": "MAPPO is one of the algorithms utilized to compare performance and for foundational techniques in the proposed MOMA-PPO.",
          "quote": "Here, we use MAPPO, a CTDE multi-agent version of PPO (Schulman et al., 2017)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "MAPPO is a pre-existing model referenced and used in the research, not a contribution of this paper.",
          "quote": "Here, we use MAPPO, a CTDE multi-agent version of PPO (Schulman et al., 2017)."
        },
        "is_executed": {
          "value": 1,
          "justification": "MAPPO was used to collect data and benchmark against MOMA-PPO, implying it was executed as part of the study.",
          "quote": "The offline dataset is collected as follows: in the first stage, we train online MAPPO on the fully observable two-agent Reacher task."
        },
        "is_compared": {
          "value": 1,
          "justification": "MAPPO was used as a baseline for performance comparison in the experiments.",
          "quote": "We compare with a large and varied array of baselines including MAPPO."
        },
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "MAPPO is based on PPO, which is referenced in the paper by Schulman et al., 2017.",
          "quote": "Here, we use MAPPO, a CTDE multi-agent version of PPO (Schulman et al., 2017)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Iterated Coordination Game Dataset",
          "justification": "The dataset is specifically designed for the Iterated Coordination Game to illustrate strategy agreement challenges.",
          "quote": "Table 1: Policies used to collect the datasets in the Iterated Coordination Game and resulting average scores of the datasets"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper title for the Iterated Coordination Game Dataset.",
          "quote": "in the Iterated Coordination Game"
        }
      },
      {
        "name": {
          "value": "Two-agent Reacher Dataset",
          "justification": "A dataset created from a mixture of expert demonstrations collected using MAPPO for the two-agent reacher environment.",
          "quote": "We generated two-agent reacher mixture-of-expert dataset using MAPPO."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper title for the two-agent reacher dataset as it is created for this study.",
          "quote": "We used a mixture-of-expert dataset using MAPPO."
        }
      },
      {
        "name": {
          "value": "Multi-Agent MuJoCo (MAMuJoCo) Datasets",
          "justification": "Multi-Agent MuJoCo datasets were referenced and used for training and evaluating multi-agent tasks under different conditions.",
          "quote": "We show that it allows for offline coordination even in complex Multi-Agent MuJoCo (MAMuJoCo) tasks with partial observability and learned world model."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MAMuJoCo: Benchmarking Multi-Agent Continuous Control",
          "justification": "The paper refers to MAMuJoCo datasets and tasks derived from the referenced work.",
          "quote": "Multi-Agent MuJoCo (MAMuJoCo) [45] tasks with partial observability and learned world model."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is commonly used for deep learning research and is implied as the primary library used for implementing and training the neural networks in the study.",
          "quote": "...we adapt the generalized advantage estimation [49] to account for the timeouts ùúÅùë° and ensure that there is no accumulation across rollouts while computing returns."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "The usage of PyTorch is standard for implementing algorithms and related tasks as referenced.",
          "quote": "Automatic differentiation in PyTorch"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1427,
    "prompt_tokens": 24086,
    "total_tokens": 25513
  }
}