{
  "paper": "2206.05825.txt",
  "words": 21748,
  "extractions": {
    "title": {
      "value": "A Unified Approach to Reinforcement Learning, Quantal Response Equilibria, and Two Player Zero Sum Games",
      "justification": "This is the complete title as mentioned in the provided text.",
      "quote": "A U NIFIED A PPROACH TO R EINFORCEMENT L EARN ING , Q UANTAL R ESPONSE E QUILIBRIA , AND T WO P LAYER Z ERO -S UM G AMES"
    },
    "description": "The paper introduces the Magnetic Mirror Descent algorithm, which unifies reinforcement learning, quantal response equilibria, and solving for Nash equilibria in two-player zero-sum games. The algorithm achieves linear convergence for extensive-form QREs and shows competitive performance in deep RL settings.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments to evaluate the convergence and performance of the proposed Magnetic Mirror Descent algorithm in various benchmarks and deep RL tasks.",
      "quote": "Our empirical contribution investigates MMD as a last iterate (regularized) equilibrium approximation algorithm across a variety of 2p0s benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary focus is on using the Magnetic Mirror Descent algorithm for reinforcement learning and solving equilibria in two-player zero-sum games.",
        "quote": "Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Game Theory",
          "justification": "The algorithm is applied to solve problems in game theory, specifically Nash equilibria and quantal response equilibria in two-player zero-sum games.",
          "quote": "solving for quantal response equilibria (McKelvey & Palfrey, 1995) (i.e., entropy regularized Nash equilibria) in extensive-form games (EFGs)"
        },
        "aliases": [
          "GT"
        ]
      },
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The paper tests the algorithm's performance in deep reinforcement learning tasks such as 3x3 Dark Hex and Phantom Tic-Tac-Toe.",
          "quote": "we examine MMD as a multi-agent deep RL algorithm for 3x3 Abrupt Dark Hex and Phantom Tic-Tac-Toe"
        },
        "aliases": [
          "Deep RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Magnetic Mirror Descent",
          "justification": "The Magnetic Mirror Descent (MMD) algorithm is the primary model introduced and evaluated in this paper.",
          "quote": "we call magnetic mirror descent"
        },
        "aliases": [
          "MMD"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The Magnetic Mirror Descent algorithm is the main contribution of the paper.",
          "quote": "Our contribution is demonstrating the virtues of magnetic mirror descent as both an equilibrium solver and as an approach to reinforcement learning in two-player zero-sum games."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper presents empirical evaluations of the algorithm, indicating that it was executed.",
          "quote": "Our empirical contribution investigates MMD"
        },
        "is_compared": {
          "value": 1,
          "justification": "The algorithm's performance is compared to other models in various benchmarks such as CFR.",
          "quote": "we show competitive performance with counterfactual regret minimization (CFR) (Zinkevich et al., 2007)"
        },
        "referenced_paper_title": {
          "value": "Mirror descent and nonlinear projected subgradient methods for convex optimization",
          "justification": "The Magnetic Mirror Descent algorithm builds upon the concept of mirror descent from the referenced paper.",
          "quote": "MMD is an extension of mirror descent (Beck & Teboulle, 2003; Nemirovsky & Yudin, 1983)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "3x3 Abrupt Dark Hex",
          "justification": "The 3x3 Abrupt Dark Hex game is used as a benchmark for testing the MMD algorithm in a deep RL setting.",
          "quote": "MMD as a multi-agent deep RL algorithm for 3x3 Abrupt Dark Hex"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Second Scientific American Book of Mathematical Puzzles and Diversions",
          "justification": "The dataset/game is based on the Dark Hex variant described in the referenced book.",
          "quote": "3x3 Abrupt Dark Hex"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "OpenSpiel",
          "justification": "The OpenSpiel library is used for implementing some of the games and experiments discussed in the paper.",
          "quote": "we used games implemented in OpenSpiel"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "OpenSpiel: A framework for reinforcement learning in games",
          "justification": "The OpenSpiel library is mentioned as a key resource used for game implementations in the paper.",
          "quote": "implemented in OpenSpiel (Lanctot et al., 2019)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 988,
    "prompt_tokens": 39145,
    "total_tokens": 40133
  }
}