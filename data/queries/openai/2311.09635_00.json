{
  "paper": "2311.09635.txt",
  "words": 10985,
  "extractions": {
    "title": {
      "value": "Evaluating In-Context Learning of Libraries for Code Generation",
      "justification": "This is the title explicitly stated at the top of the paper.",
      "quote": "Evaluating In-Context Learning of Libraries for Code Generation"
    },
    "description": "This paper systematically evaluates the capability of various large language models (LLMs) to generate code based on libraries defined in-context. It examines different models and types of supervisory data provided in-context, such as demonstrations, descriptions, and implementations, across diverse LLMs and tasks. The models' ability to adapt to novel libraries and learn new programming languages in-context is also studied, which has important implications for using LLMs in dynamic coding environments.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts a systematic evaluation of models across different tasks and scenarios, providing empirical evidence about their abilities and limitations.",
      "quote": "In this work, we take a broader approach by systematically evaluating a diverse array of LLMs across three scenarios reflecting varying levels of domain specialization to understand their abilities and limitations in generating code based on libraries defined in-context."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on the language models' capability to generate code, which falls under the domain of Natural Language Processing, especially contextual understanding and generation.",
        "quote": "We show that models like GPT-4 can learn new libraries from natural language descriptions or raw code implementations of the library functions just as effectively as they can using demonstrations."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Code Generation",
          "justification": "The paper explicitly deals with the ability of LLMs to generate code based on given libraries and from natural language descriptions.",
          "quote": "Contemporary Large Language Models (LLMs) exhibit a high degree of code generation and comprehension capability."
        },
        "aliases": [
          "Program Synthesis"
        ]
      },
      {
        "name": {
          "value": "In-context Learning",
          "justification": "The focus of the study is on how LLMs can learn and adapt to new libraries and languages from the context provided in the input.",
          "quote": "In-context learning (i.e., learning from instructions and examples in the prompt) has emerged as the preferred approach for adapting LLMs to tasks and domains not seen during training."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Model Evaluation",
          "justification": "The research involves evaluating the performance of various models across different scenarios and types of supervisory data.",
          "quote": "We describe our evaluation framework and summarize our findings below."
        },
        "aliases": [
          "Benchmarking"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Llama-2",
          "justification": "Llama-2 is one of the models evaluated in the study to understand its ability in in-context learning for code generation.",
          "quote": "even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Llama-2 is used for evaluation and not introduced or modified in the research.",
          "quote": "even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed to evaluate its performance in the given tasks.",
          "quote": "In Section 3, we observed that no model except GPT-4 could match the demonstrations performance with just descriptions or implementation."
        },
        "is_compared": {
          "value": 1,
          "justification": "The study compares Llama-2 with other models like GPT-4 and StarCoder in the context of code generation.",
          "quote": "We observe that the data on which the models have been pretrained on influences the choice of supervision that best suits them. For instance, LLaMA models, which have been primarily trained on text with comparatively lesser code pretraining (Touvron et al., 2023), show a much higher ability to learn from descriptions compared to code implementations."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
          "justification": "This is the referenced paper for Llama-2 as mentioned in the citations (Touvron et al., 2023).",
          "quote": "For instance, LLaMA models, which have been primarily trained on text with comparatively lesser code pretraining (Touvron et al., 2023)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GQA",
          "justification": "GQA is one of the datasets used to evaluate the models' capabilities in the vision-language tasks.",
          "quote": "The examples in this figure are from the GQA dataset (Hudson and Manning, 2019) and the functions are from the VisProg (Gupta and Kembhavi, 2023) library."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering",
          "justification": "This is the referenced paper for the GQA dataset as cited in the research (Hudson and Manning, 2019).",
          "quote": "The examples in this figure are from the GQA dataset (Hudson and Manning, 2019) and the functions are from the VisProg (Gupta and Kembhavi, 2023) library."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used as a library for model implementation as indicated in the implementation details.",
          "quote": "Our code is implemented in PyTorch (Paszke et al., 2019) and makes use of the HuggingFace Transformers library (Wolf et al., 2020)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the referenced paper for the PyTorch library as cited in the research (Paszke et al., 2019).",
          "quote": "Our code is implemented in PyTorch (Paszke et al., 2019) and makes use of the HuggingFace Transformers library (Wolf et al., 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1258,
    "prompt_tokens": 19290,
    "total_tokens": 20548
  }
}