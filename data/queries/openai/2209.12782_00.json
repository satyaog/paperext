{
  "paper": "2209.12782.txt",
  "words": 11171,
  "extractions": {
    "title": {
      "value": "Learning GFlowNets From Partial Episodes For Improved Convergence And Stability",
      "justification": "The title is clearly mentioned at the top of the text and within the document header.",
      "quote": "Learning GFlowNets From Partial Episodes For Improved Convergence And Stability"
    },
    "description": "This paper proposes a new training objective for Generative Flow Networks (GFlowNets) called subtrajectory balance (SubTB(ùúÜ)), which improves convergence speed and stability by learning from partial action subsequences of varying lengths. The efficacy of SubTB(ùúÜ) is demonstrated through experiments in various synthetic and real-world environments, showing better performance compared to existing GFlowNet objectives.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results on synthetic and real-world tasks to support the proposed training objective, indicating that it is an empirical study.",
      "quote": "Experiments on two synthetic and four real-world domains support three empirical claims"
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper involves Generative Flow Networks (GFlowNets) which are highly related to reinforcement learning techniques.",
        "quote": "Inspired by the TD(ùúÜ) algorithm in reinforcement learning, we introduce subtrajectory balance or SubTB(ùúÜ), a GFlowNet training objective that can learn from partial action subsequences of varying lengths."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generative Models",
          "justification": "Generative Flow Networks (GFlowNets) are discussed as generative models that construct objects in a target space.",
          "quote": "Generative flow networks (GFlowNets; Bengio et al., 2021a) are generative models that construct objects lying in a target space X by taking sequences of actions sampled from a learned policy."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GFlowNets",
          "justification": "Generative Flow Networks (GFlowNets) are the primary model discussed and improved upon in this paper.",
          "quote": "Generative flow networks (GFlowNets) are a family of algorithms for training a sequential sampler of discrete objects under an unnormalized target density and have been successfully used for various probabilistic modeling tasks."
        },
        "aliases": [
          "Generative Flow Networks"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Generative Flow Networks were not originally contributed by this paper but are used and improved upon.",
          "quote": "Generative flow networks (GFlowNets; Bengio et al., 2021a) are generative models that construct objects lying in a target space X by taking sequences of actions sampled from a learned policy."
        },
        "is_executed": {
          "value": 1,
          "justification": "GFlowNets were executed as part of the experiments in the paper.",
          "quote": "Experiments on two synthetic and four real-world domains support three empirical claims"
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of GFlowNets against other models when trained with the SubTB(ùúÜ) objective.",
          "quote": "We show that SubTB(ùúÜ) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before."
        },
        "referenced_paper_title": {
          "value": "GFlowNet foundations",
          "justification": "This referenced paper is foundational in the discussion of GFlowNets.",
          "quote": "A deeper introduction is given in Bengio et al. (2021b)."
        }
      },
      {
        "name": {
          "value": "SubTB (Subtrajectory Balance)",
          "justification": "SubTB is the new training objective introduced by this paper.",
          "quote": "Inspired by the TD(ùúÜ) algorithm in reinforcement learning, we introduce subtrajectory balance or SubTB(ùúÜ), a GFlowNet training objective that can learn from partial action subsequences of varying lengths."
        },
        "aliases": [
          "SubTB(ùúÜ)"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "This is an original contribution of the paper.",
          "quote": "We propose a new learning objective for GFlowNets, called subtrajectory balance (SubTB, or SubTB(ùúÜ) when its hyperparameter ùúÜ is specified)."
        },
        "is_executed": {
          "value": 1,
          "justification": "SubTB was tested experimentally as part of the paper's contributions.",
          "quote": "Experiments on two synthetic and four real-world domains support three empirical claims"
        },
        "is_compared": {
          "value": 1,
          "justification": "The SubTB method is compared with existing GFlowNet training objectives.",
          "quote": "We show that SubTB(ùúÜ) accelerates sampler convergence in previously studied and new environments and enables training GFlowNets in environments with longer action sequences and sparser reward landscapes than what was possible before."
        },
        "referenced_paper_title": {
          "value": "GFlowNet foundations",
          "justification": "The foundation for GFlowNets was laid in this paper.",
          "quote": "A deeper introduction is given in Bengio et al. (2021b)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "DBAASP",
          "justification": "The DBAASP database is used in the antimicrobial peptide generation task.",
          "quote": "We take 6438 known AMP sequences and 9522 non-AMP sequences from the DBAASP database Pirtskhalava et al. (2021)."
        },
        "aliases": [
          "Database of Antimicrobial Activity and Structure of Peptides"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "DBAASP v3: Database of Antimicrobial/Cytotoxic Activity and Structure of Peptides as a Resource for Development of New Therapeutics",
          "justification": "The database's version and purpose are detailed.",
          "quote": "Database of antimicrobial/cytotoxic activity and structure of peptides as a resource for development of new therapeutics. Nucleic Acids Research, 49(D1):D288‚ÄìD297, 2021."
        }
      },
      {
        "name": {
          "value": "Fluorescence Dataset",
          "justification": "A dataset of proteins with fluorescence scores is used in the protein generation task.",
          "quote": "We consider the task of generating protein sequences with fluorescence properties (Trabucco et al., 2022) to evaluate SubTB(ùúÜ) in settings with longer trajectories. In this task, sequences have a fixed length of 237, and the size of the state space is 20237 . The proxy reward function ùëÖ(ùë•) is trained on a dataset of proteins with their fluorescence scores from Sarkisyan et al. (2016)."
        },
        "aliases": [
          "GFP Dataset"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Local fitness landscape of the green fluorescent protein",
          "justification": "The paper that provides the fluorescence scores for the proteins.",
          "quote": "Sequences have a fixed length of 237, and the size of the state space is 20237. The proxy reward function ùëÖ(ùë•) is trained on a dataset of proteins with their fluorescence scores from Sarkisyan et al. (2016)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The PyTorch deep learning library is implicitly referenced as being used for experiments due to its common use in the cited works.",
          "quote": "Experiments on two synthetic and four real-world domains support three empirical claims"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "The Transformer model, which is typically implemented using PyTorch, is referenced.",
          "quote": "The policy for all methods is parameterized by a Transformer (Vaswani et al., 2017) with 3 layers, dimension 64, and 8 attention heads."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1582,
    "prompt_tokens": 22912,
    "total_tokens": 24494
  }
}