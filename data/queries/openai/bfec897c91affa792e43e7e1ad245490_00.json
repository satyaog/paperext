{
  "paper": "bfec897c91affa792e43e7e1ad245490.txt",
  "words": 11316,
  "extractions": {
    "title": {
      "value": "Cracking the Code of Cascading Disparity Towards Marginalized Communities",
      "justification": "The title is clearly stated multiple times in the input, including at the beginning and throughout the text.",
      "quote": "Position: Cracking the Code of Cascading Disparity Towards Marginalized Communities"
    },
    "description": "This position paper discusses the interconnected disparities that affect marginalized communities due to biases in foundation models. It highlights the cascading disparity phenomenon and analyzes the sources of disparities throughout the lifecycle of foundation models, while proposing actions to mitigate these disparities.",
    "type": {
      "value": "theoretical",
      "justification": "The paper does not present empirical experiments or data analysis but rather discusses theoretical concepts, definitions, and proposed solutions to the stated problems.",
      "quote": "In this position paper, we discuss that disparities towards marginalized communities – performance, representation, privacy, robustness, interpretability and safety – are not isolated concerns but rather interconnected elements of a cascading disparity phenomenon."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The primary focus is on disparities in machine learning models, particularly foundation models, affecting marginalized communities.",
        "quote": "Foundation models with their ability to learn and adapt across various domains, are rapidly transforming the landscape of AI."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Fairness in AI",
          "justification": "The paper discusses bias and fairness extensively as it relates to how foundation models impact marginalized communities.",
          "quote": "The rise of foundation models holds immense promise for advancing AI, but this progress may amplify existing risks and inequalities, leaving marginalized communities behind."
        },
        "aliases": [
          "AI Fairness"
        ]
      },
      {
        "name": {
          "value": "Ethical AI",
          "justification": "The ethical implications of AI models on marginalized communities are a major aspect of this discussion, relating to the fairness and disparities highlighted.",
          "quote": "Addressing the multifaceted problem of disparities in foundation models requires a holistic sociotechnical approach that goes beyond isolated technical fixes."
        },
        "aliases": [
          "Ethical Artificial Intelligence"
        ]
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "StereoSet",
          "justification": "StereoSet is mentioned as an evaluation benchmark in the paper, used to demonstrate biases in foundation models.",
          "quote": "Moreover, standard evaluation benchmarks, such as StereoSet (Nadeem et al., 2020), often focus on surface-level assessments of foundation models using non-robust prompting metrics."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Stereoset: Measuring stereotypical bias in pretrained language models.",
          "justification": "The referenced paper is cited for StereoSet, which is used to evaluate bias in foundation models.",
          "quote": "such as StereoSet (Nadeem et al., 2020)..."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 552,
    "prompt_tokens": 19494,
    "total_tokens": 20046,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}