{
  "paper": "2305.17446.txt",
  "words": 7111,
  "extractions": {
    "title": {
      "value": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models",
      "justification": "The values is taken from the title of the paper",
      "quote": "Fine-tuning Happens in Tiny Subspaces: Exploring Intrinsic Task-specific Subspaces of Pre-trained Language Models"
    },
    "description": "This paper explores the concept of intrinsic task-specific subspaces in pre-trained language models (PLMs). It investigates if fine-tuning PLMs occurs in smaller, task-specific subspaces, making use of parameter optimization trajectories to uncover these subspaces. The findings suggest that effective fine-tuning can happen within these smaller subspaces, significantly reducing the number of parameters needed while maintaining performance. Additionally, the paper discusses the emergence of outlier dimensions during fine-tuning, which appear crucial to task-specific knowledge.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts extensive empirical experiments on the GLUE benchmark using BERT and RoBERTa models, involving both transductive and inductive settings to validate its hypotheses.",
      "quote": "We conduct extensive experiments on the GLUE benchmark using BERT and RoBERTa models to support our claims."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on fine-tuning pre-trained language models and evaluates its claims on the GLUE benchmark, which is a standard for natural language processing tasks.",
        "quote": "We conduct extensive experiments on the GLUE benchmark using BERT and RoBERTa models to support our claims."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The paper explores intrinsic subspace fine-tuning, which is a technique within transfer learning. It specifically looks at fine-tuning pre-trained language models for various downstream tasks.",
          "quote": "We propose a method to uncover the subspaces by finding the principal directions of the fine-tuning trajectory."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Model Compression",
          "justification": "The paper's discussion on reducing the number of parameters required for effective fine-tuning falls under model compression.",
          "quote": "A key finding is that PLMs can be effectively fine-tuned in the subspace with a small number of free parameters."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "The BERT model is one of the main pre-trained language models used for experiments in the paper.",
          "quote": "We conduct extensive experiments on the GLUE benchmark using BERT and RoBERTa models to support our claims."
        },
        "aliases": [
          "Bidirectional Encoder Representations from Transformers"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses existing BERT model for its experiments and does not propose BERT as a new model.",
          "quote": "We conduct extensive experiments on the GLUE benchmark using BERT and RoBERTa models to support our claims."
        },
        "is_executed": {
          "value": 1,
          "justification": "The BERT model was fine-tuned and tested in the experiments conducted in the paper.",
          "quote": "We conduct extensive experiments on the GLUE benchmark using BERT and RoBERTa models to support our claims."
        },
        "is_compared": {
          "value": 1,
          "justification": "BERT model's performance is compared with RoBERTa and other settings like random subspaces to highlight its effectiveness in intrinsic subspaces.",
          "quote": "We conduct extensive experiments on the GLUE benchmark using BERT and RoBERTa models to support our claims."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "This is the reference paper for the BERT model as used in the experiments.",
          "quote": "(Devlin et al., 2019)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GLUE Benchmark",
          "justification": "The paper conducts extensive experiments on the GLUE benchmark using BERT and RoBERTa models to test the effectiveness of intrinsic subspaces.",
          "quote": "We conduct extensive experiments on the GLUE benchmark using BERT and RoBERTa models to support our claims."
        },
        "aliases": [
          "General Language Understanding Evaluation"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
          "justification": "This is the reference paper for the GLUE benchmark dataset used in the experiments.",
          "quote": "(Wang et al., 2018; Warstadt et al., 2019; Socher et al., 2013; Dolan and Brockett, 2005; Cer et al., 2017; Williams et al., 2018; Rajpurkar et al., 2016)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Transformers",
          "justification": "The paper uses the HuggingFace Transformers toolkit for the implementation of its experiments.",
          "quote": "Our implementation is based on HuggingFace’s Transformers toolkit (Wolf et al., 2020)."
        },
        "aliases": [
          "HuggingFace Transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-art natural language processing",
          "justification": "This is the reference paper for the Transformers library used in the experiments.",
          "quote": "Our implementation is based on HuggingFace’s Transformers toolkit (Wolf et al., 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1079,
    "prompt_tokens": 14537,
    "total_tokens": 15616
  }
}