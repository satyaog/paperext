{
  "paper": "tuxCm2h5JL.txt",
  "words": 6051,
  "extractions": {
    "title": {
      "value": "Cognitive Models as Simulators: Using Cognitive Models to Tap into Implicit Human Feedback",
      "justification": "Title was extracted directly from the research paper.",
      "quote": "Cognitive Models as Simulators: Using Cognitive Models to Tap into Implicit Human Feedback"
    },
    "description": "This paper explores the use of cognitive models as simulators to provide AI systems with implicit human feedback. Instead of interacting with real humans, AI systems interact with cognitive models, making the training process safer, cheaper, and faster. The study focuses on the context of learning fair behavior in the Ultimatum Game by training reinforcement learning (RL) agents to respond to various emotional states of a simulated responder.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results from simulations and discusses the implications of those results.",
      "quote": "To train RL Proposers, we leverage the broad framework of multi-armed bandits in reinforcement learning (Katehakis and Veinott, 1987; Gittins, 1979), and adopt the well-known Thompson Sampling method (Thompson, 1933)."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on training RL agents using cognitive models as simulators.",
        "quote": "We show that our reinforcement learning (RL) agents learn to exhibit differential, rationally-justified behaviors under various emotional states of their UG counterpart."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Cognitive Modeling",
          "justification": "The study leverages cognitive models to simulate human feedback.",
          "quote": "We substantiate the idea of cognitive models as simulators, which is to have AI systems interact with, and collect feedback from, cognitive models instead of humans, thereby making the training process safer, cheaper, and faster."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Emotion Research in AI",
          "justification": "The paper involves AI systems interacting with cognitive models of emotional states.",
          "quote": "As a case study, we adopt the Ultimatum game (UG), a canonical task in behavioral and brain sciences for studying fairness."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Fairness in AI",
          "justification": "The trained RL agents learn to exhibit fair behavior based on the emotional states of the simulated responder.",
          "quote": "Focusing on emotions as a form of implicit human feedback, we leverage this idea in the context of learning a fair behavior toward a counterpart exhibiting various emotional states."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Sample-based Expected Utility (SbEU)",
          "justification": "SbEU is the cognitive model used in the study to simulate feedback for the RL agents.",
          "quote": "Nobandegani et al.’s cognitive model rests on two main assumptions. First, UG Responder uses SbEU to estimate the expected-utility gap between their expectation and the offer, i.e., E[u(offer) − u(expectation)], where u(·) denotes Responder’s utility function."
        },
        "aliases": [
          "SbEU"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SbEU was not contributed by this paper but used in the experiments.",
          "quote": "Recently, Nobandegani et al. (2020) presented a cognitive model of UG Responder, called sample-based expected utility (SbEU)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was used for simulating feedback in the scope of the experiments.",
          "quote": "We train RL Proposers, using Thompson Sampling (see Algorithm 1), to learn how to interact with SbEU Responders experiencing positive or negative emotional states."
        },
        "is_compared": {
          "value": 0,
          "justification": "No numerical comparisons to other models were mentioned.",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "A resource-rational, process-level account of the St. Petersburg paradox",
          "justification": "This is a referenced paper that discusses the SbEU model.",
          "quote": "Recently, Nobandegani et al. (2020) presented a cognitive model of UG Responder, called sample-based expected utility (SbEU)."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1025,
    "prompt_tokens": 11844,
    "total_tokens": 12869
  }
}