{
  "paper": "g78QqvhnDU.txt",
  "words": 11891,
  "extractions": {
    "title": {
      "value": "Prioritizing Samples in Reinforcement Learning with Reducible Loss",
      "justification": "Title extracted from the provided research paper.",
      "quote": "Prioritizing Samples in Reinforcement Learning with Reducible Loss"
    },
    "description": "The paper proposes a method to prioritize samples in reinforcement learning based on their learn-ability, defined by the steady decrease in training loss over time. The method aims to prioritize samples that are most informative and avoid those that are noisy or hard to learn, improving the robustness and efficiency of the training process.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper provides empirical evidence and experiments to demonstrate the effectiveness and robustness of the proposed method across several domains.",
      "quote": "We empirically show that across multiple domains our method is more robust than random sampling and also better than just prioritizing with respect to the training loss."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on prioritizing samples in reinforcement learning by proposing a novel method to improve learning efficiency based on reducible loss.",
        "quote": "In this paper, we propose a method to prioritize samples based on how much we can learn from a sample... We empirically show that across multiple domains our method is more robust than random sampling."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The paper builds on deep reinforcement learning methods like DQN and SAC to implement and test the proposed prioritization scheme.",
          "quote": "One of the landmarks in the space of online RL learning has been Deep Q Networks (DQN) [Mnih et al., 2015]..."
        },
        "aliases": [
          "DRL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Deep Q Network",
          "justification": "Deep Q Network (DQN) is referenced as a significant prior method in online reinforcement learning which the paper aims to improve upon.",
          "quote": "One of the landmarks in the space of online RL learning has been Deep Q Networks (DQN)..."
        },
        "aliases": [
          "DQN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "DQN is an existing method referenced in the paper, not newly contributed.",
          "quote": "One of the landmarks in the space of online RL learning has been Deep Q Networks (DQN)..."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not specify that DQN was executed anew, but references it as a background method.",
          "quote": "One of the landmarks in the space of online RL learning has been Deep Q Networks (DQN)..."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not provide numerical comparisons with DQN; it rather builds on concepts established by DQN.",
          "quote": "One of the landmarks in the space of online RL learning has been Deep Q Networks (DQN)..."
        },
        "referenced_paper_title": {
          "value": "Human-level control through deep reinforcement learning",
          "justification": "This is the original paper where Deep Q Network (DQN) was introduced and is referenced in the current paper.",
          "quote": "One of the landmarks in the space of online RL learning has been Deep Q Networks (DQN) [Mnih et al., 2015]..."
        }
      },
      {
        "name": {
          "value": "Soft Actor Critic",
          "justification": "Soft Actor Critic (SAC) is used as a base algorithm in some of the experiments for the proposed method.",
          "quote": "Soft Actor Critic (SAC) [Haarnoja et al., 2018] is used as the base off-policy algorithm to which we add ReLo."
        },
        "aliases": [
          "SAC"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SAC is an existing method used for experimentation, not newly contributed.",
          "quote": "Soft Actor Critic (SAC) [Haarnoja et al., 2018] is used as the base off-policy algorithm to which we add ReLo."
        },
        "is_executed": {
          "value": 1,
          "justification": "SAC is executed as part of the experiments to evaluate the proposed prioritization method based on reducible loss.",
          "quote": "Soft Actor Critic (SAC) [Haarnoja et al., 2018] is used as the base off-policy algorithm to which we add ReLo."
        },
        "is_compared": {
          "value": 1,
          "justification": "SAC is numerically compared to the proposed method during the experiments on various benchmarks.",
          "quote": "We also include SAC with PER to showcase the differences in performance characteristics of PER and ReLo. The results are given in Figs. 1a."
        },
        "referenced_paper_title": {
          "value": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
          "justification": "This is the original paper introducing Soft Actor Critic, which is used as a base algorithm in the research.",
          "quote": "Soft Actor Critic (SAC) [Haarnoja et al., 2018] is used as the base off-policy algorithm to which we add ReLo."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "DeepMind Control Suite",
          "justification": "The DeepMind Control Suite is used for empirical evaluations in the experiments.",
          "quote": "We demonstrate the performance of our approach empirically on the DeepMind Control Suite [Tassa et al., 2018]..."
        },
        "aliases": [
          "DMC"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "DeepMind Control Suite",
          "justification": "The referenced paper is the original introduction of the DeepMind Control Suite benchmark.",
          "quote": "We demonstrate the performance of our approach empirically on the DeepMind Control Suite [Tassa et al., 2018]..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used as the deep learning framework for implementing the experiments in the research paper.",
          "quote": "We build our experiments on top of existing implementations of SAC, DQN and Rainbow... Software: Pytorch: 1.10.0"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper doesn't reference an original paper for PyTorch usage.",
          "quote": "We build our experiments on top of existing implementations of SAC, DQN and Rainbow... Software: Pytorch: 1.10.0"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1278,
    "prompt_tokens": 21054,
    "total_tokens": 22332
  }
}