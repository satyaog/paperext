{
  "paper": "7ZcyRF7Y3S.txt",
  "words": 16385,
  "extractions": {
    "title": {
      "value": "Synergies Between Disentanglement and Sparsity: A Multi-Task Learning Perspective",
      "justification": "The title of the paper is clear from the provided text.",
      "quote": "Synergies Between Disentanglement and Sparsity: A Multi-Task Learning Perspective"
    },
    "description": "The paper investigates the synergies between disentanglement and sparsity in the context of multi-task learning, providing theoretical and empirical evidence on how disentangled representations coupled with sparse base-predictors improve generalization.",
    "type": {
      "value": "Theoretical",
      "justification": "The paper provides theoretical results, such as a novel identifiability result, and supports these with empirical validation.",
      "quote": "In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations."
    },
    "primary_research_field": {
      "name": {
        "value": "Representation Learning",
        "justification": "The paper primarily contributes to the field of Representation Learning by investigating disentangled representations and their benefits for downstream tasks.",
        "quote": "In this work, we provide evidence that disentangled representations coupled with sparse base-predictors improve generalization."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Task Learning",
          "justification": "The paper focuses on disentangled representations and sparsity within the context of multi-task learning.",
          "quote": "In the context of multi-task learning, we prove a new identifiability result that provides conditions under which maximally sparse base-predictors yield disentangled representations."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Meta-Learning",
          "justification": "The paper explores a meta-learning version of their algorithm, showing competitive results on few-shot classification benchmarks.",
          "quote": "Finally, we explore a meta-learning version of this algorithm based on group Lasso multiclass SVM base-predictors, for which we derive a tractable dual formulation. It obtains competitive results on standard few-shot classification benchmarks."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Group Lasso SVM",
          "justification": "The paper proposes a meta-learning algorithm based on group Lasso multiclass SVM base-predictors.",
          "quote": "Inspired by this identifiability result, we enhance an existing method (Lee et al., 2019), where the base-learners are now group-sparse SVMs."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The paper introduces the group Lasso SVM as a new method for meta-learning tasks.",
          "quote": "we enhance an existing method (Lee et al., 2019), where the base-learners are now group-sparse SVMs."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed and its performance was evaluated on standard few-shot classification benchmarks.",
          "quote": "It obtains competitive results on standard few-shot classification benchmarks."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the Group Lasso SVM was compared to other methods in the few-shot classification benchmarks.",
          "quote": "We show that this new meta-learning algorithm achieves competitive performance on the miniImageNet benchmark (Vinyals et al., 2016), while only using a fraction of the learned representation."
        },
        "referenced_paper_title": {
          "value": "Meta-learning with differentiable convex optimization",
          "justification": "The group Lasso SVM model is inspired by and enhances the method from Lee et al., 2019.",
          "quote": "Inspired by this identifiability result, we enhance an existing method (Lee et al., 2019), where the base-learners are now group-sparse SVMs."
        }
      },
      {
        "name": {
          "value": "Sparsity-Promoting Bi-Level Optimization",
          "justification": "The paper proposes this model for learning disentangled representations in a multi-task learning context.",
          "quote": "We propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "This model is a new contribution introduced by the paper",
          "quote": "We propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed and its effectiveness was validated theoretically and empirically",
          "quote": "We validate our theory by showing our approach can indeed disentangle latent factors on tasks constructed from the 3D Shapes dataset (Burgess & Kim, 2018)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the effectiveness of using disentangled representations with sparse base-predictors versus entangled representations.",
          "quote": "In this section, we compare the generalization performance of entangled and disentangled representations on sparse downstream tasks."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "There is no single reference paper for this original model introduced in the paper.",
          "quote": "We propose a practical approach to learn disentangled representations based on a sparsity-promoting bi-level optimization problem."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "3D Shapes",
          "justification": "The 3D Shapes dataset is used to validate the theory proposed in the paper.",
          "quote": "We validate our theory by showing our approach can indeed disentangle latent factors on tasks constructed from the 3D Shapes dataset (Burgess & Kim, 2018)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "3d shapes dataset",
          "justification": "The dataset is credited to Burgess & Kim, 2018",
          "quote": "We validate our theory by showing our approach can indeed disentangle latent factors on tasks constructed from the 3D Shapes dataset (Burgess & Kim, 2018)."
        }
      },
      {
        "name": {
          "value": "miniImageNet",
          "justification": "The miniImageNet dataset is used to evaluate the meta-learning algorithm proposed in the paper.",
          "quote": "We show that this new meta-learning algorithm achieves competitive performance on the miniImageNet benchmark (Vinyals et al., 2016), while only using a fraction of the learned representation."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Matching networks for one shot learning",
          "justification": "The dataset is credited to Vinyals et al., 2016.",
          "quote": "We show that this new meta-learning algorithm achieves competitive performance on the miniImageNet benchmark (Vinyals et al., 2016), while only using a fraction of the learned representation."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions the usage of common libraries like PyTorch for implementing the models.",
          "quote": "Our implementation builds on the PyTorch framework."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "PyTorch standard citation as per the usage context.",
          "quote": "Our implementation builds on the PyTorch framework."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1408,
    "prompt_tokens": 30768,
    "total_tokens": 32176
  }
}