{
  "paper": "NEtI9o7gtPL.txt",
  "words": 10429,
  "extractions": {
    "title": {
      "value": "Investigating Multi-Task Pretraining and Generalization in Reinforcement Learning",
      "justification": "The title is displayed prominently at the beginning of the paper.",
      "quote": "Investigating Multi-Task Pretraining and Generalization in Reinforcement Learning"
    },
    "description": "This paper studies the generalization capabilities of the actor-critic method, IMPALA, in reinforcement learning through multi-task pretraining using Atari 2600 game variants. The study explores how pretraining on multiple variants of a game affects fine-tuning on unseen variants and discusses the influence of model capacity and pretraining data on generalization performance.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results and empirical data on the performance of multi-task reinforcement learning models.",
      "quote": "To fill this gap, we investigate the generalization capabilities of a popular actor-critic method, IMPALA (Espeholt et al., 2018)."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The study focuses on designing RL agents capable of learning multiple tasks and quickly adapting to new tasks.",
        "quote": "Deep reinforcement learning (RL) has achieved remarkable successes in complex single-task settings."
      },
      "aliases": [
        "RL",
        "Reinforcement Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Task Learning",
          "justification": "The paper discusses multi-task pretraining and its effects on generalization capabilities in reinforcement learning.",
          "quote": "Our understanding of multi-task training and generalization in RL remains limited."
        },
        "aliases": [
          "Multi-Task RL"
        ]
      },
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The study involves transferring learned knowledge from pretrained models to new, unseen variants of Atari games.",
          "quote": "We find that, given a fixed amount of pretraining data, agents trained with more variations are able to generalize better."
        },
        "aliases": [
          "Transfer Learning in RL"
        ]
      },
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The paper leverages deep learning architectures in conjunction with reinforcement learning techniques.",
          "quote": "We find that it is possible to train high capacity networks such as residual networks (He et al., 2016), with tens of millions of parameters, using online RL."
        },
        "aliases": [
          "Deep RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "IMPALA",
          "justification": "IMPALA is the primary model investigated in the paper for its generalization and multi-task learning capabilities.",
          "quote": "To fill this gap, we investigate the generalization capabilities of a popular actor-critic method, IMPALA (Espeholt et al., 2018)."
        },
        "aliases": [
          "Importance Weighted Actor-Learner Architecture"
        ],
        "is_contributed": {
          "value": false,
          "justification": "IMPALA was not originally developed in this paper.",
          "quote": "To fill this gap, we investigate the generalization capabilities of a popular actor-critic method, IMPALA (Espeholt et al., 2018)."
        },
        "is_executed": {
          "value": true,
          "justification": "IMPALA was executed as part of the empirical experiments in the paper.",
          "quote": "We use an efficient implementation of IMPALA (Espeholt et al., 2019; Hessel et al., 2021) that runs on TPUs."
        },
        "is_compared": {
          "value": true,
          "justification": "IMPALA's performance was compared with other network architectures in the experiments.",
          "quote": "Our results so far hold for the network introduced with the IMPALA algorithm (Espeholt et al., 2018)."
        },
        "referenced_paper_title": {
          "value": "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures",
          "justification": "The paper referenced for the IMPALA model.",
          "quote": "IMPALA (Espeholt et al., 2018)"
        }
      },
      {
        "name": {
          "value": "Residual Networks (He et al., 2016)",
          "justification": "Residual networks were used to study the effect of model capacity in the context of multi-task reinforcement learning.",
          "quote": "We use a more efficient algorithm, IMPALA (Espeholt et al., 2018) instead of DQN, much larger networks than the decade old 3-layer convolutional neural networks used by DQN (Mnih et al., 2013) and pretrain our agents using multiple variants of a game as opposed to just a single one."
        },
        "aliases": [
          "ResNet"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Residual Networks were not originally developed in this paper.",
          "quote": "We use a more efficient algorithm, IMPALA (Espeholt et al., 2018) instead of DQN, much larger networks than the decade old 3-layer convolutional neural networks used by DQN (Mnih et al., 2013) and pretrain our agents using multiple variants of a game as opposed to just a single one."
        },
        "is_executed": {
          "value": true,
          "justification": "Residual networks were executed as part of the empirical experiments.",
          "quote": "For experiments that analyze the impact of network capacity, we also use large ResNets (Espeholt et al., 2018)."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of Residual Networks was compared to the IMPALA network as part of the experiments.",
          "quote": "When training only on test environments, increasing the network size noticeably degrades performance in both games."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "The referenced paper for the Residual Networks model.",
          "quote": "with tens of millions of parameters, using online RL. We find that increased representation capabilities from such networks are essential to reach peak performance in the multi-task regime."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari 2600",
          "justification": "The paper utilizes various variants of Atari 2600 games for its multi-task pretraining and generalization experiments.",
          "quote": "We build on previous work that has advocated for the use of modes and difficulties of Atari 2600 games as a challenging benchmark for transfer learning in RL"
        },
        "aliases": [
          "Atari"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
          "justification": "The primary reference for the Atari 2600 dataset.",
          "quote": "The Arcade Learning Environment (Bellemare et al., 2013)"
        }
      },
      {
        "name": {
          "value": "Arcade Learning Environment (ALE)",
          "justification": "The study employs ALE as a benchmark for evaluating generalization and fine-tuning in reinforcement learning.",
          "quote": "In this paper, we propose to take a closer look at multi-task RL pretraining and generalization on the ALE, one of the most widely used deep RL benchmark."
        },
        "aliases": [
          "ALE"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
          "justification": "The reference paper for the Arcade Learning Environment dataset.",
          "quote": "The Arcade Learning Environment (Bellemare et al., 2013)"
        }
      },
      {
        "name": {
          "value": "Procgen",
          "justification": "The Procgen benchmark is used to discuss transfer and generalization capabilities in reinforcement learning, although not a primary focus of experiments.",
          "quote": "Zero-shot generalization is the most desirable but also most difficult kind of generalization to achieve as it does not allow for any interaction with the target environment. In the context of RL generalization benchmarks with procedurally generated environments, such as Procgen (Cobbe et al., 2020a), zero-shot transfer only emerges after interacting with hundreds of training environments."
        },
        "aliases": [
          "Procedural Generation Benchmark"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Leveraging procedural generation to benchmark reinforcement learning",
          "justification": "The reference paper for the Procgen benchmark.",
          "quote": "In the context of RL generalization benchmarks with procedurally generated environments, such as Procgen (Cobbe et al., 2020a)"
        }
      },
      {
        "name": {
          "value": "Meta-World",
          "justification": "The Meta-World benchmark is referenced in the context of generalization and transfer learning in RL.",
          "quote": "The major benefit of the ALE compared to other generalization benchmarks (Nichol et al., 2018; Yu et al., 2020)"
        },
        "aliases": [
          "Meta-World Benchmark"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
          "justification": "The reference paper for the Meta-World benchmark.",
          "quote": "The major benefit of the ALE compared to other generalization benchmarks (Nichol et al., 2018; Yu et al., 2020)"
        }
      },
      {
        "name": {
          "value": "Multi-Task and Meta Reinforcement Learning",
          "justification": "This benchmark is mentioned in relation to generalization capabilities in reinforcement learning.",
          "quote": "Recent work has also looked at the ALE in the context of generalization, transfer learning and continual learning (Farebrother et al., 2018; Rusu et al., 2022)."
        },
        "aliases": [
          "Multi-Task and Meta RL Benchmark"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
          "justification": "The reference paper for the Multi-Task and Meta Reinforcement Learning benchmark.",
          "quote": "Recent work has also looked at the ALE in the context of generalization, transfer learning and continual learning (Farebrother et al., 2018; Rusu et al., 2022)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TensorFlow",
          "justification": "The paper mentions using efficient implementations that are typically built with TensorFlow for executing RL models on TPUs.",
          "quote": "We use an efficient implementation of IMPALA (Espeholt et al., 2019; Hessel et al., 2021) that runs on TPUs."
        },
        "aliases": [
          "TF"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
          "justification": "The corresponding reference paper for TensorFlow as used in the implementation of the experiments.",
          "quote": "efficient implementation of IMPALA (Espeholt et al., 2019; Hessel et al., 2021) that runs on TPUs."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2081,
    "prompt_tokens": 21019,
    "total_tokens": 23100
  }
}