{
  "paper": "2305.13088.txt",
  "words": 8322,
  "extractions": {
    "title": {
      "value": "Should We Attend More or Less? Modulating Attention for Fairness",
      "justification": "This title correctly reflects the content and focus of the research paper.",
      "quote": "Should We Attend More or Less? Modulating Attention for Fairness"
    },
    "description": "This paper investigates the role of attention in propagating social biases in language models and proposes a novel method to modulate attention weights post-training to improve fairness.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experimental results from several text classification and generation tasks to validate their proposed method.",
      "quote": "Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper addresses bias in transformer-based pre-trained language models, which falls under the domain of Natural Language Processing.",
        "quote": "Recent advancements in transformer-based pre-trained language models, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), GPT-2 (Radford et al., 2019), GPT-3 (Brown et al., 2020) and XLNet (Yang et al., 2019), have led to the emergence of new state-of-the-art models in a variety of applications."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Fairness in AI",
          "justification": "The paper specifically focuses on improving the fairness of language models by modulating attention weights.",
          "quote": "we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Text Classification",
          "justification": "The paper evaluates their proposed method on different text classification datasets.",
          "quote": "Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Text Generation",
          "justification": "The paper also evaluates the proposed method in the context of text generation using language models.",
          "quote": "Finally, we show the potential of our method to extend beyond text classification and to address social biases in the context of text generation using GPT-Neo."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is used for evaluating the proposed fairness improvement method.",
          "quote": "Recent advancements in transformer-based pre-trained language models, such as BERT (Devlin et al., 2019)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "BERT is an existing model used for evaluation, not a contribution of the paper.",
          "quote": "Recent advancements in transformer-based pre-trained language models, such as BERT (Devlin et al., 2019)"
        },
        "is_executed": {
          "value": 1,
          "justification": "It was explicitly mentioned that BERT was used in their experiments.",
          "quote": "BERT and RoBERTa base models for 15 epochs."
        },
        "is_compared": {
          "value": 1,
          "justification": "BERT is compared with other language models to show the efficacy of the proposed method.",
          "quote": "We study the effect of attention distribution entropy on both fairness and performance and find that the relationship between the modelâ€™s bias and the entropy of its attention distribution is both dataset and architecture-dependent."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "This is the referenced paper title for BERT used in their research.",
          "quote": "BERT (Devlin et al., 2019)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Twitter dataset",
          "justification": "The Twitter dataset was used in the experimental evaluation for text classification.",
          "quote": "We used the following datasets: Twitter dataset (Waseem and Hovy, 2016) with 16, 000 tweets binarized into sexist and non-sexist."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Hateful symbols or hateful people? Predictive features for hate speech detection on Twitter",
          "justification": "This is the referenced paper title for the Twitter dataset used in their research.",
          "quote": "Twitter dataset (Waseem and Hovy, 2016)"
        }
      },
      {
        "name": {
          "value": "Wikipedia dataset",
          "justification": "The Wikipedia dataset was used for toxicity detection in the experimental evaluation.",
          "quote": "Wikipedia dataset (Dixon et al., 2018) with around 160, 000 comments categorized as toxic or non-toxic."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Measuring and mitigating unintended bias in text classification",
          "justification": "This is the referenced paper title for the Wikipedia dataset used in their research.",
          "quote": "Wikipedia dataset (Dixon et al., 2018)"
        }
      },
      {
        "name": {
          "value": "Jigsaw dataset",
          "justification": "The Jigsaw dataset was used for toxicity detection in the experimental evaluation.",
          "quote": "the Jigsaw dataset with around 1.8 million examples binarized into toxic and non-toxic."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Jigsaw Unintended Bias in Toxicity Classification",
          "justification": "This is the referenced paper title for the Jigsaw dataset used in their research.",
          "quote": "the Jigsaw dataset with around 1.8 million examples binarized into toxic and non-toxic."
        }
      },
      {
        "name": {
          "value": "BOLD (Bias in Open-Ended Language Generation)",
          "justification": "The BOLD dataset was used to evaluate social biases in text generation.",
          "quote": "bias in open-ended language generation dataset (BOLD) (Dhamala et al., 2021) with 23, 679 prompts referring to professions, genders, races, as well as religious and political groups."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "BOLD: Dataset and metrics for measuring biases in open-ended language generation",
          "justification": "This is the referenced paper title for the BOLD dataset used to evaluate social biases.",
          "quote": "bias in open-ended language generation dataset (BOLD) (Dhamala et al., 2021)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The code and experiments were run using the PyTorch library.",
          "quote": "Our code will be made public for reproducibility."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "PyTorch was used for implementing and running the experiments.",
          "quote": "Our code will be made public for reproducibility."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1358,
    "prompt_tokens": 15054,
    "total_tokens": 16412
  }
}