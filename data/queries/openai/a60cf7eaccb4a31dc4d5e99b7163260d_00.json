{
  "paper": "a60cf7eaccb4a31dc4d5e99b7163260d.txt",
  "words": 13054,
  "extractions": {
    "title": {
      "value": "C HALLENGING C OMMON A SSUMPTIONS ABOUT C ATASTROPHIC F ORGETTING AND K NOWLEDGE ACCUMULATION",
      "justification": "It's the title at the beginning of the provided research paper copy.",
      "quote": "C HALLENGING C OMMON A SSUMPTIONS ABOUT C ATASTROPHIC F ORGETTING AND K NOWLEDGE ACCUMULATION"
    },
    "description": "This paper investigates the phenomenon of knowledge accumulation in deep neural networks (DNNs) in continual learning (CL) scenarios. It proposes a new framework, SCoLe (Scaling Continual Learning), to study how DNNs can accumulate knowledge over long sequences of tasks with data recurrence. The research shows that DNNs trained with stochastic gradient descent (SGD) can accumulate knowledge significantly, despite catastrophic forgetting when tasks reoccur. The findings challenge current understandings of catastrophic forgetting and suggest new strategies to enhance knowledge accumulation in continual learning.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experiments and empirical data to support its findings on knowledge accumulation and catastrophic forgetting.",
      "quote": "We empirically investigate KA in DNNs under various data occurrence frequencies."
    },
    "primary_research_field": {
      "name": {
        "value": "Continual Learning",
        "justification": "The paper mainly focuses on continual learning and investigates phenomena related to it, such as knowledge accumulation and catastrophic forgetting.",
        "quote": "Building learning agents that can progressively learn and accumulate knowledge is the core goal of the continual learning (CL) research field."
      },
      "aliases": [
        "CL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Catastrophic Forgetting",
          "justification": "Catastrophic forgetting is a key topic addressed in the paper.",
          "quote": "In the CL literature, this effect is referred to as catastrophic forgetting (CF)."
        },
        "aliases": [
          "CF"
        ]
      },
      {
        "name": {
          "value": "Knowledge Accumulation",
          "justification": "Knowledge accumulation is a major phenomenon investigated in the paper.",
          "quote": "Nevertheless, despite CF, recent work showed that SGD training on linear models accumulates knowledge in a CL regression setup."
        },
        "aliases": [
          "KA"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet",
          "justification": "The paper mentions using a ResNet model architecture in their experiments.",
          "quote": "We train a model with ResNet architecture, consequently since CNNs usually rely on features related to neighbouring pixels to learn, the pixel permutation of the distractor tasks should be especially perturbing and should lead to forgetting."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The ResNet model is not contributed by the paper and has been referenced as part of their experiments.",
          "quote": "We train a model with ResNet architecture"
        },
        "is_executed": {
          "value": 1,
          "justification": "The ResNet model was executed during the experiments performed in the paper.",
          "quote": "We train a model with ResNet architecture"
        },
        "is_compared": {
          "value": 1,
          "justification": "The ResNet model's performance was likely compared as part of their empirical analysis.",
          "quote": "We measure both the test and meta-test performance of the incrementally trained model."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "Referenced for the ResNet model architecture.",
          "quote": "ResNet architecture"
        }
      },
      {
        "name": {
          "value": "Adam",
          "justification": "The paper mentions using the Adam optimizer in their experiments.",
          "quote": "Adam"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Adam optimizer is not contributed by the paper.",
          "quote": "We see in Figure 5a that during the first tasks, no baseline seems to accumulate knowledge, which is in accordance with the common understanding of the CF phenomenon."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Adam optimizer was executed during the experiments in the paper.",
          "quote": "We use the two most popular optimizers in CL and ML in general: SGD with momentum (Qian, 1999), and Adam (Kingma & Ba, 2014)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Adam optimizer's performance was compared as part of their empirical analysis.",
          "quote": "We use the two most popular optimizers in CL and ML in general: SGD with momentum (Qian, 1999), and Adam (Kingma & Ba, 2014)."
        },
        "referenced_paper_title": {
          "value": "Adam: A Method for Stochastic Optimization",
          "justification": "The Adam optimizer method referenced in the paper.",
          "quote": "Adam (Kingma & Ba, 2014)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is used in the paper's experiments.",
          "quote": "We propose an experimentation framework “SCoLe” (Scaling Continual Learning) to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Gradient-Based Learning Applied to Document Recognition",
          "justification": "The MNIST dataset is used for comparing model performance.",
          "quote": "MNIST"
        }
      },
      {
        "name": {
          "value": "Fashion-MNIST",
          "justification": "The Fashion-MNIST dataset is used in the paper's experiments.",
          "quote": "We propose an experimentation framework “SCoLe” (Scaling Continual Learning) to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
          "justification": "The Fashion-MNIST dataset is used for comparing model performance.",
          "quote": "Fashion-MNIST"
        }
      },
      {
        "name": {
          "value": "KMNIST",
          "justification": "The KMNIST dataset is used in the paper's experiments.",
          "quote": "We propose an experimentation framework “SCoLe” (Scaling Continual Learning) to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "KMNIST: A Novel Kuzushiji Dataset",
          "justification": "The KMNIST dataset is used for comparing model performance.",
          "quote": "KMNIST"
        }
      },
      {
        "name": {
          "value": "CIFAR10",
          "justification": "The CIFAR10 dataset is used in the paper's experiments.",
          "quote": "We propose an experimentation framework “SCoLe” (Scaling Continual Learning) to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The CIFAR10 dataset is used for comparing model performance.",
          "quote": "CIFAR10"
        }
      },
      {
        "name": {
          "value": "CIFAR100",
          "justification": "The CIFAR100 dataset is used in the paper's experiments.",
          "quote": "We propose an experimentation framework “SCoLe” (Scaling Continual Learning) to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The CIFAR100 dataset is used for comparing model performance.",
          "quote": "CIFAR100"
        }
      },
      {
        "name": {
          "value": "Tiny-ImageNet200",
          "justification": "The Tiny-ImageNet200 dataset is used in the paper's experiments.",
          "quote": "We propose an experimentation framework “SCoLe” (Scaling Continual Learning) to study and assess knowledge accumulation through a long sequence of (reoccurring) tasks on a variety of datasets (MNIST, Fashion-MNIST, KMNIST, CIFAR10, CIFAR100, Tiny-ImageNet200)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Tiny ImageNet Visual Recognition Challenge",
          "justification": "The Tiny-ImageNet200 dataset is used for comparing model performance.",
          "quote": "Tiny-ImageNet200"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2239,
    "prompt_tokens": 22454,
    "total_tokens": 24693
  }
}