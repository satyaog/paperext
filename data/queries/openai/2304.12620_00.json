{
  "paper": "2304.12620.txt",
  "words": 6374,
  "extractions": {
    "title": {
      "value": "Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation",
      "justification": "This allows easy identification and reference to the paper being discussed.",
      "quote": "Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation"
    },
    "description": "The paper proposes Medical SAM Adapter (Med-SA) to enhance the Segment Anything Model (SAM) for medical image segmentation. Med-SA uses domain-specific knowledge and efficient parameter tuning techniques, achieving state-of-the-art performance across various medical image modalities.",
    "type": {
      "value": "Empirical",
      "justification": "The study involves extensive experimentation and evaluation on 17 medical image segmentation tasks, demonstrating the effectiveness of the proposed method.",
      "quote": "We conduct comprehensive evaluation experiments on 17 medical image segmentation tasks across various image modalities."
    },
    "primary_research_field": {
      "name": {
        "value": "Medical Image Segmentation",
        "justification": "The paper focuses on improving the segmentation of medical images using deep learning techniques.",
        "quote": "Our objective is to enhance the medical capability of the SAM architecture for medical image segmentation tasks."
      },
      "aliases": [
        "Medical Imaging"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "Medical image segmentation is a specific application within the broader field of computer vision.",
          "quote": "The Segment Anything Model (SAM) has recently gained popularity in the field of image segmentation due to its impressive capabilities in various segmentation tasks."
        },
        "aliases": [
          "CV",
          "Vision"
        ]
      },
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The paper discusses and applies deep learning techniques, particularly in adapting the SAM model for medical image segmentation.",
          "quote": "We believe Adaption is the most fitting technique for carrying SAM to the medical domain."
        },
        "aliases": [
          "DL",
          "Neural Networks"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Segment Anything Model (SAM)",
          "justification": "SAM is a foundational model discussed throughout the paper for segmentation tasks.",
          "quote": "Very recently, the Segmentation Anything Model (SAM) (Kirillov et al. 2023) has gained significant attention as a powerful and versatile vision segmentation model."
        },
        "aliases": [
          "SAM"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SAM is the base model upon which the adaptations are built; it is not a contribution of this paper.",
          "quote": "Very recently, the Segmentation Anything Model (SAM) (Kirillov et al. 2023) has gained significant attention as a powerful and versatile vision segmentation model."
        },
        "is_executed": {
          "value": 1,
          "justification": "SAM is compared and extended within the experiments conducted in the paper.",
          "quote": "In this paper, instead of fine-tuning the SAM model, we propose the Medical SAM Adapter (Med-SA), which incorporates domain-specific medical knowledge into the segmentation model using a light yet effective adaptation technique."
        },
        "is_compared": {
          "value": 1,
          "justification": "SAM's performance in medical image segmentation tasks is compared against Med-SA and other state-of-the-art methods.",
          "quote": "Med-SA outperforms both SAM and fully finetuned SAM (MedSAM)(Ma and Wang 2023) with a significant performance gap."
        },
        "referenced_paper_title": {
          "value": "Segment Anything",
          "justification": "The Segment Anything Model is based on this reference.",
          "quote": "Very recently, the Segmentation Anything Model (SAM) (Kirillov et al. 2023) has gained significant attention as a powerful and versatile vision segmentation model."
        }
      },
      {
        "name": {
          "value": "Medical SAM Adapter (Med-SA)",
          "justification": "Med-SA is the primary model proposed and validated in this paper.",
          "quote": "In this paper, we propose the Medical SAM Adapter (Med-SA), which incorporates domain-specific medical knowledge into the segmentation model using a light yet effective adaptation technique."
        },
        "aliases": [
          "Med-SA"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "Med-SA is the main model contribution of this paper.",
          "quote": "In this paper, we propose the Medical SAM Adapter (Med-SA)."
        },
        "is_executed": {
          "value": 1,
          "justification": "Med-SA is thoroughly evaluated in various experiments conducted in the paper.",
          "quote": "We conduct comprehensive evaluation experiments on 17 medical image segmentation tasks across various image modalities. Med-SA outperforms several state-of-the-art (SOTA) medical image segmentation methods."
        },
        "is_compared": {
          "value": 1,
          "justification": "Med-SA is compared to SAM, MedSAM, and other state-of-the-art methods in the experiments.",
          "quote": "Med-SA outperforms several state-of-the-art (SOTA) medical image segmentation methods, while updating only 2% of the parameters."
        },
        "referenced_paper_title": {
          "value": "Segment Anything",
          "justification": "Med-SA builds on the Segment Anything Model.",
          "quote": "Very recently, the Segmentation Anything Model (SAM) (Kirillov et al. 2023) has gained significant attention as a powerful and versatile vision segmentation model."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "BTCV dataset",
          "justification": "The BTCV dataset is used as a benchmark in the paper for abdominal multi-organ segmentation.",
          "quote": "We utilized the BTCV dataset (Fang and Yan 2020), a widely-used and publicly available benchmark with twelve anatomies as the benchmark."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Multi-organ segmentation over partially labeled datasets with multi-scale feature abstraction",
          "justification": "This reference describes the BTCV dataset used in this study.",
          "quote": "We utilized the BTCV dataset (Fang and Yan 2020)"
        }
      },
      {
        "name": {
          "value": "REFUGE2 dataset",
          "justification": "The REFUGE2 dataset is used for optic disc and cup segmentation over fundus images.",
          "quote": "For the fundus image segmentation, we conducted experiments on REFUGE2(Fang et al. 2022) dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "REFUGE2 Challenge: Treasure for Multi-Domain Learning in Glaucoma Assessment",
          "justification": "This reference describes the REFUGE2 dataset used in this study.",
          "quote": "For the fundus image segmentation, we conducted experiments on REFUGE2(Fang et al. 2022) dataset."
        }
      },
      {
        "name": {
          "value": "BraTs 2021 dataset",
          "justification": "The BraTs 2021 dataset is used for brain tumor segmentation.",
          "quote": "For brain tumor segmentation, we conducted experiments on the BraTs 2021 dataset(Baid et al. 2021)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification",
          "justification": "This reference describes the BraTs 2021 dataset used in this study.",
          "quote": "For brain tumor segmentation, we conducted experiments on the BraTs 2021 dataset(Baid et al. 2021)."
        }
      },
      {
        "name": {
          "value": "TNMIX benchmark",
          "justification": "The TNMIX benchmark is used for thyroid nodule segmentation.",
          "quote": "For thyroid nodule segmentation, we used the TNMIX benchmark, a mixed dataset containing 4554 images from TNSCUI and 637 images from DDTI."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Ultrasound image-based thyroid nodule automatic segmentation using convolutional neural networks",
          "justification": "This reference corresponds to the TNSCUI part of the TNMIX benchmark.",
          "quote": "For thyroid nodule segmentation, we used the TNMIX benchmark, a mixed dataset containing 4554 images from TNSCUI"
        }
      },
      {
        "name": {
          "value": "ISIC 2019 dataset",
          "justification": "The ISIC 2019 dataset is used for melanoma or nevus segmentation.",
          "quote": "For melanoma or nevus segmentation, we conducted experiments on the ISIC 2019 dataset(Milton 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automated skin lesion classification using ensemble of deep neural networks in isic 2018: Skin lesion analysis towards melanoma detection challenge",
          "justification": "This reference describes the ISIC 2019 dataset used in this study.",
          "quote": "For melanoma or nevus segmentation, we conducted experiments on the ISIC 2019 dataset(Milton 2019)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions implementation with the PyTorch platform.",
          "quote": "All the experiments are implemented with the PyTorch platform and trained/tested on 4 NVIDIA A100 GPUs."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2202,
    "prompt_tokens": 13124,
    "total_tokens": 15326
  }
}