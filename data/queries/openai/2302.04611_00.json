{
  "paper": "2302.04611.txt",
  "words": 15853,
  "extractions": {
    "title": {
      "value": "A Text-guided Protein Design Framework",
      "justification": "This is the title of the research paper",
      "quote": "A Text-guided Protein Design Framework"
    },
    "description": "This research paper proposes ProteinDT, a multi-modal framework leveraging textual descriptions for protein design. The model aligns text and protein sequence representations to facilitate tasks like text-guided protein generation, editing, and property prediction. The authors also introduce a new dataset, SwissProtCLAP, with 441K text-protein pairs for training the model.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves constructing a dataset, developing a multi-modal framework, and empirically verifying its effectiveness through various tasks and experiments.",
      "quote": "To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks."
    },
    "primary_research_field": {
      "name": {
        "value": "Bioinformatics",
        "justification": "The paper focuses on protein design leveraging deep learning and text data, which is a core area of bioinformatics.",
        "quote": "Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteinsâ€™ high-level functionalities."
      },
      "aliases": [
        "Computational Biology",
        "Biomedical Text Mining"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing (NLP)",
          "justification": "This subfield is relevant because the framework leverages textual data and involves NLP techniques to align text and protein sequence representations.",
          "quote": "Motivated by recent breakthroughs of foundation models, approaches in the computational chemistry domain that utilize textual descriptions of drugs in addition to their intrinsic chemical and structural information have proved to be effective in small-molecule drug discovery."
        },
        "aliases": [
          "NLP",
          "Text Mining"
        ]
      },
      {
        "name": {
          "value": "Computational Chemistry",
          "justification": "The paper adapts techniques from computational chemistry, using textual descriptions alongside molecular data to inform protein design.",
          "quote": "Motivated by recent breakthroughs of foundation models, approaches in the computational chemistry domain that utilize textual descriptions of drugs in addition to their intrinsic chemical and structural information have proved to be effective in small-molecule drug discovery."
        },
        "aliases": [
          "Comp Chem"
        ]
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The study incorporates machine learning models like autoregressive models and diffusion models for protein generation and editing.",
          "quote": "Concretely, we present a robust solution to the demanding task by introducing the most powerful generative models for sequence data: one autoregressive (AR) and two diffusion models (ProteinDiff)."
        },
        "aliases": [
          "ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ProteinDT",
          "justification": "ProteinDT is the main multi-modal framework proposed in the paper for protein design using text data.",
          "quote": "To attain the aforementioned goal, we propose Protein Design with Text (ProteinDT), a multi-modal framework for protein design."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The ProteinDT framework is the primary contribution of the research paper.",
          "quote": "To attain the aforementioned goal, we propose Protein Design with Text (ProteinDT), a multi-modal framework for protein design."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experiments in the paper.",
          "quote": "To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs."
        },
        "is_compared": {
          "value": 1,
          "justification": "ProteinDT is compared to state-of-the-art models on various tasks such as protein generation, editing, and property prediction.",
          "quote": "We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ProteinCLAP",
          "justification": "ProteinCLAP is a component of ProteinDT that aligns text and protein sequence representations using contrastive learning.",
          "quote": "A Contrastive LAnguage and Protein pretraining (ProteinCLAP) step to align the representations between text sequences and protein sequences."
        },
        "aliases": [
          "Contrastive Language and Protein Pretraining"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "ProteinCLAP is introduced as a novel component within the ProteinDT framework in this paper.",
          "quote": "A Contrastive LAnguage and Protein pretraining (ProteinCLAP) step to align the representations between text sequences and protein sequences."
        },
        "is_executed": {
          "value": 1,
          "justification": "ProteinCLAP is executed as part of the pretraining process for ProteinDT.",
          "quote": "ProteinCLAP adopts the contrastive learning paradigm to align the representation space of the two modalities."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not compare ProteinCLAP directly but evaluates the whole ProteinDT framework including ProteinCLAP.",
          "quote": "We consider three downstream tasks to verify the versatile functionalities of ProteinDT."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ProteinFacilitator",
          "justification": "ProteinFacilitator is another component in ProteinDT that generates protein sequence representations from text.",
          "quote": "The ProteinFacilitator model in ProteinDT aims at mapping a piece of text (a text prompt describing the properties of the target protein) to a protein representation that captures both the semantic information of the text and essential protein sequence patterns."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "ProteinFacilitator is introduced as a novel component within ProteinDT in this research paper.",
          "quote": "The ProteinFacilitator model in ProteinDT aims at mapping a piece of text (a text prompt describing the properties of the target protein) to a protein representation that captures both the semantic information of the text and essential protein sequence patterns."
        },
        "is_executed": {
          "value": 1,
          "justification": "ProteinFacilitator is executed as part of the ProteinDT framework to convert text to protein sequence representations.",
          "quote": "The ProteinFacilitator model in ProteinDT aims at mapping a piece of text (a text prompt describing the properties of the target protein) to a protein representation that captures both the semantic information of the text and essential protein sequence patterns."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not compare ProteinFacilitator directly but evaluates the effectiveness of ProteinDT as a whole.",
          "quote": "We consider three downstream tasks to verify the versatile functionalities of ProteinDT."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ProteinDiff",
          "justification": "ProteinDiff includes two diffusion models used for protein generation in ProteinDT.",
          "quote": "Concretely, we present a robust solution to the demanding task by introducing the most powerful generative models for sequence data: one autoregressive (AR) and two diffusion models (ProteinDiff)."
        },
        "aliases": [
          "Diffusion Models for Protein Generation"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "ProteinDiff is introduced in the paper as part of the generative model suite within ProteinDT.",
          "quote": "Concretely, we present a robust solution to the demanding task by introducing the most powerful generative models for sequence data: one autoregressive (AR) and two diffusion models (ProteinDiff)."
        },
        "is_executed": {
          "value": 1,
          "justification": "ProteinDiff is executed as part of the protein generation task within ProteinDT.",
          "quote": "Concretely, we present a robust solution to the demanding task by introducing the most powerful generative models for sequence data: one autoregressive (AR) and two diffusion models (ProteinDiff)."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not separately compare ProteinDiff to other models but evaluates the entire ProteinDT framework.",
          "quote": "We consider three downstream tasks to verify the versatile functionalities of ProteinDT."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SwissProtCLAP",
          "justification": "SwissProtCLAP is a large dataset constructed by the authors for training ProteinDT, containing 441K text and protein pairs.",
          "quote": "To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs."
        },
        "aliases": [],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "UniProt: the universal protein resource (Nucleic Acids Research 36, D190-D195, 2007)",
          "justification": "The dataset is derived from UniProt, a comprehensive protein sequence resource.",
          "quote": "We construct SwissProtCLAP, a text-protein pair dataset, from UniProt."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is used as the encoder for both text and protein sequences in ProteinDT.",
          "quote": "We use BERT as the encoder. It is one of the most advanced Transformer models for modeling sequential data."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (arXiv preprint arXiv:1810.04805, 2018)",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ProtBERT",
          "justification": "ProtBERT is a pretrained BERT model on a large protein sequence corpus, used as the protein sequence encoder in ProteinDT.",
          "quote": "Additionally, we take the BERT model with ProtBERT pretraining: it was pretrained on a large protein sequence corpus, and the pretraining task was to reconstruct the masked tokens."
        },
        "aliases": [
          "Protein BERT"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ProtTrans: Towards Cracking the Language of Lifeâ€™s Code Through Self-Supervised Deep Learning and High Performance Computing (arXiv preprint arXiv:2007.06225, 2020)",
          "justification": "This referenced paper is the source of ProtBERT.",
          "quote": "Additionally, we take the BERT model with ProtBERT pretraining: it was pretrained on a large protein sequence corpus, and the pretraining task was to reconstruct the masked tokens."
        }
      },
      {
        "name": {
          "value": "SciBERT",
          "justification": "SciBERT is a pretrained BERT model on scientific literature, used as the text sequence encoder in ProteinDT.",
          "quote": "We further adapt the pretrained SciBERT, which was pretrained on the full-text computer science and biomedical papers from Semantic Scholar."
        },
        "aliases": [
          "Scientific BERT"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "SciBERT: A Pretrained Language Model for Scientific Text (arXiv preprint arXiv:1903.10676, 2019)",
          "justification": "This referenced paper is the source of SciBERT.",
          "quote": "We further adapt the pretrained SciBERT, which was pretrained on the full-text computer science and biomedical papers from Semantic Scholar."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2316,
    "prompt_tokens": 29931,
    "total_tokens": 32247
  }
}