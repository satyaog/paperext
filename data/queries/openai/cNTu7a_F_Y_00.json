{
  "paper": "cNTu7a_F_Y.txt",
  "words": 10785,
  "extractions": {
    "title": {
      "value": "Replay Buffer with Local Forgetting for Adaptive Deep Model-Based Reinforcement Learning",
      "justification": "The title is prominently displayed on the first page of the paper.",
      "quote": "REPLAY BUFFER WITH LOCAL FORGETTING FOR ADAPTIVE DEEP MODEL-BASED REINFORCEMENT LEARNING"
    },
    "description": "This paper presents a variation of the traditional replay buffer, termed LoFo (Local Forgetting), designed for deep model-based reinforcement learning (MBRL) methods. The proposed buffer aims to overcome the limitations of traditional FIFO replay buffers in terms of adaptivity to local environmental changes and catastrophic forgetting. The empirical results showed that the LoFo replay buffer improves performance and adaptability in various environments.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental results to demonstrate the effectiveness of the proposed LoFo replay buffer in improving the adaptability of deep MBRL methods.",
      "quote": "We demonstrate the effectiveness of the LoFo replay buffer by combining it with a deep version of the classical Dyna method and measuring its adaptivity on a variation of the MountainCar task as well as a mini-grid domain, using the same Local Change Adaptation (LoCA) setup as used by Wan et al. (2022)."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on adaptive deep model-based reinforcement learning methods and introduces a new variant of replay buffer aimed at improving adaptability.",
        "quote": "In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement-learning (MBRL) methods adapt poorly to such changes."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Catastrophic Forgetting",
          "justification": "The paper addresses the issue of catastrophic forgetting within deep model-based reinforcement learning by proposing the LoFo replay buffer.",
          "quote": "This is challenging for deep-learning-based world models due to catastrophic forgetting."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Replay Buffer",
          "justification": "The primary contribution of this paper is a variation of the replay buffer, named LoFo, which is designed to improve the adaptivity and address the limitations of traditional FIFO buffers.",
          "quote": "While a replay buffer can mitigate the effects of catastrophic forgetting, the traditional first-in-first-out replay buffer precludes effective adaptation due to maintaining stale data."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "DreamerV2",
          "justification": "DreamerV2 is one of the recent methods used to demonstrate the effectiveness of the proposed LoFo replay buffer in the paper.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet (Hafner et al., 2019b) and DreamerV2 (Hafner et al., 2020), which use world models based on recurrent networks and are intended for continuous-action domains."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "DreamerV2 is a pre-existing model used in the experiments for comparative evaluation.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet (Hafner et al., 2019b) and DreamerV2 (Hafner et al., 2020), which use world models based on recurrent networks and are intended for continuous-action domains."
        },
        "is_executed": {
          "value": 1,
          "justification": "The DreamerV2 model was implemented and tested in the scope of this paper to evaluate the proposed LoFo replay buffer.",
          "quote": "we tested the limits of our approach by applying the same idea to both PlaNet (Hafner et al., 2019b) and DreamerV2 (Hafner et al., 2020)."
        },
        "is_compared": {
          "value": 1,
          "justification": "DreamerV2 was compared with other models to demonstrate the effectiveness of the LoFo replay buffer.",
          "quote": "Experiments with these modified methods on variations of the MuJoCo Reacher domain demonstrate that a LoFo replay buffer can substantially improve adaptivity of more advanced deep MBRL methods as well."
        },
        "referenced_paper_title": {
          "value": "Mastering Atari with Discrete World Models",
          "justification": "This is the reference paper for DreamerV2.",
          "quote": "(Hafner et al., 2020)"
        }
      },
      {
        "name": {
          "value": "PlaNet",
          "justification": "PlaNet is another recent method used to showcase the improvements brought by the LoFo replay buffer in the context of deep MBRL.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet (Hafner et al., 2019b) and DreamerV2 (Hafner et al., 2020), which use world models based on recurrent networks and are intended for continuous-action domains."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "PlaNet is a pre-existing model used for the experiments and was not introduced as a new model in this paper.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet (Hafner et al., 2019b) and DreamerV2 (Hafner et al., 2020), which use world models based on recurrent networks and are intended for continuous-action domains."
        },
        "is_executed": {
          "value": 1,
          "justification": "PlaNet was implemented and tested to evaluate the proposed LoFo replay buffer.",
          "quote": "We tested the limits of our approach by applying the same idea to both PlaNet (Hafner et al., 2019b) and DreamerV2 (Hafner et al., 2020)."
        },
        "is_compared": {
          "value": 1,
          "justification": "PlaNet was compared with other models to demonstrate the effectiveness of the LoFo replay buffer.",
          "quote": "Experiments with these modified methods on variations of the MuJoCo Reacher domain demonstrate that a LoFo replay buffer can substantially improve adaptivity of more advanced deep MBRL methods as well."
        },
        "referenced_paper_title": {
          "value": "Learning Latent Dynamics for Planning from Pixels",
          "justification": "This is the reference paper for PlaNet.",
          "quote": "(Hafner et al., 2019b)"
        }
      },
      {
        "name": {
          "value": "Dyna-Q",
          "justification": "Dyna-Q was used as a basic model where the LoFo replay buffer was applied to showcase its improvement in handling local changes and catastrophic forgetting.",
          "quote": "We demonstrate the effectiveness of the LoFo replay buffer by combining it with a deep version of the classical Dyna method and measuring its adaptivity on a variation of the MountainCar task as well as a mini-grid domain, using the same Local Change Adaptation (LoCA) setup as used by Wan et al. (2022)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Dyna-Q is a classical method that was not introduced in this paper but used for demonstrating the effectiveness of the LoFo replay buffer.",
          "quote": "We demonstrate the effectiveness of the LoFo replay buffer by combining it with a deep version of the classical Dyna method."
        },
        "is_executed": {
          "value": 1,
          "justification": "The deep version of the Dyna-Q model was executed in experiments for evaluating the proposed LoFo replay buffer.",
          "quote": "We demonstrate the effectiveness of the LoFo replay buffer by combining it with a deep version of the classical Dyna method."
        },
        "is_compared": {
          "value": 1,
          "justification": "Dyna-Q was compared in experimental results to show the adaptivity improvements brought by LoFo.",
          "quote": "We demonstrate the effectiveness of the LoFo replay buffer by combining it with a deep version of the classical Dyna method and measuring its adaptivity on a variation of the MountainCar task as well as a mini-grid domain, using the same Local Change Adaptation (LoCA) setup as used by Wan et al. (2022)."
        },
        "referenced_paper_title": {
          "value": "The Dyna Architecture for Creating Planning and Learning Agents",
          "justification": "This is the reference paper for Dyna-Q.",
          "quote": "Dyna-Q: The Dyna Architecture for Creating Planning and Learning Agents."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MountainCarLoCA",
          "justification": "MountainCarLoCA is one of the datasets used to evaluate the adaptivity and performance of the proposed LoFo replay buffer.",
          "quote": "We demonstrate the effectiveness of the LoFo replay buffer by combining it with a deep version of the classical Dyna method and measuring its adaptivity on a variation of the MountainCar task as well as a mini-grid domain, using the same Local Change Adaptation (LoCA) setup as used by Wan et al. (2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning",
          "justification": "MountainCarLoCA is introduced in this reference paper, as mentioned in the main text.",
          "quote": "as used by Wan et al. (2022)."
        }
      },
      {
        "name": {
          "value": "MiniGridLoCA",
          "justification": "The MiniGridLoCA dataset was used to measure the adaptivity of the LoFo replay buffer in the experiments conducted in the paper.",
          "quote": "We demonstrate the effectiveness of the LoFo replay buffer by combining it with a deep version of the classical Dyna method and measuring its adaptivity on a variation of the MountainCar task as well as a mini-grid domain, using the same Local Change Adaptation (LoCA) setup as used by Wan et al. (2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning",
          "justification": "MiniGridLoCA is introduced in this reference paper, as mentioned in the main text.",
          "quote": "as used by Wan et al. (2022)."
        }
      },
      {
        "name": {
          "value": "MuJoCo Reacher",
          "justification": "The MuJoCo Reacher dataset was used in experiments to evaluate the effectiveness of the LoFo replay buffer with advanced deep MBRL methods such as PlaNet and DreamerV2.",
          "quote": "Experiments with these modified methods on variations of the MuJoCo Reacher domain demonstrate that a LoFo replay buffer can substantially improve adaptivity of more advanced deep MBRL methods as well."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepMind Control Suite",
          "justification": "The MuJoCo Reacher is part of the DeepMind Control Suite.",
          "quote": "Tassa et al., 2018"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2587,
    "prompt_tokens": 17693,
    "total_tokens": 20280
  }
}