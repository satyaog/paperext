{
  "paper": "2306.07397.txt",
  "words": 14058,
  "extractions": {
    "title": {
      "value": "Adversarial Attacks on the Interpretation of Neuron Activation Maximization",
      "justification": "This is the exact title given at the beginning of the paper.",
      "quote": "Adversarial Attacks on the Interpretation of Neuron Activation Maximization"
    },
    "description": "This paper explores adversarial methods to manipulate the interpretability of convolutional neural networks (CNNs) using neuron activation maximization techniques. The authors propose several attacks to change the top-k images that activate particular neurons, aiming to deceive these interpretability methods without changing the overall network accuracy.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves proposing different attack methods and demonstrating their effectiveness through empirical experimentation on models like AlexNet and EfficientNet using datasets like ImageNet.",
      "quote": "We introduce our notation, attacks, threat models, and attack success characterization methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Adversarial Machine Learning",
        "justification": "The study focuses on adversarial attacks aimed at manipulating the interpretability of CNNs, which falls under the domain of adversarial machine learning.",
        "quote": "In this work, we consider the concept of an adversary manipulating a model for the purpose of deceiving the interpretation."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Interpretability and Explainability",
          "justification": "The primary focus of the paper is on attacking interpretability methods, specifically neuron activation maximization techniques used for model interpretability.",
          "quote": "However, interpretability methods may be subject to being deceived. In this work, we consider the concept of an adversary manipulating a model for the purpose of deceiving the interpretation."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Convolutional Neural Networks (CNN)",
          "justification": "The attacks studied in the paper primarily focus on convolutional neural network architectures, such as AlexNet and EfficientNet.",
          "quote": "We concentrate on convnet architectures for which interpretation by activation maximization or feature visualization methods has been popular."
        },
        "aliases": [
          "CNN"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "AlexNet",
          "justification": "AlexNet is used as one of the primary models for demonstrating the effectiveness of the proposed adversarial attacks.",
          "quote": "For all of our attacks, we use the ImageNet training set as D. We use the PyTorch pretrained AlexNet for our analysis."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "AlexNet is not a contribution of this paper but a pre-existing model used for the experiments.",
          "quote": "We use the PyTorch pretrained AlexNet for our analysis."
        },
        "is_executed": {
          "value": 1,
          "justification": "It is specifically stated and demonstrated in the experimental setups.",
          "quote": "We use the PyTorch pretrained AlexNet for our analysis."
        },
        "is_compared": {
          "value": 0,
          "justification": "No direct comparison with other models in terms of performance is made; the focus is on the interpretability of the same model pre- and post-attack.",
          "quote": "A successful modification of the interpretation results while keeping outputs constant is evidence for the manipulability of the interpretation approach."
        },
        "referenced_paper_title": {
          "value": "ImageNet Classification with Deep Convolutional Neural Networks",
          "justification": "The referenced paper relates to the original introduction of AlexNet.",
          "quote": "We use the PyTorch pretrained AlexNet for our analysis."
        }
      },
      {
        "name": {
          "value": "EfficientNet",
          "justification": "EfficientNet is also used to generalize the proposed attacks to more recent CNN architectures.",
          "quote": "We have also run an ablation study on EfficientNet."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "EfficientNet is not a contribution of this paper but a pre-existing model used for the experiments.",
          "quote": "We have also run an ablation study on EfficientNet."
        },
        "is_executed": {
          "value": 1,
          "justification": "It is clearly executed as part of the experiments.",
          "quote": "We have also run an ablation study on EfficientNet."
        },
        "is_compared": {
          "value": 0,
          "justification": "The focus is on the interpretability post-attack, with no performance comparison with other models.",
          "quote": "We have also run an ablation study on EfficientNet."
        },
        "referenced_paper_title": {
          "value": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
          "justification": "The referenced paper relates to the original introduction of EfficientNet.",
          "quote": "We have also run an ablation study on EfficientNet."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is primarily used to train and evaluate the proposed adversarial attacks on the CNN models.",
          "quote": "For all of our attacks, we use the ImageNet training set as D."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "The referenced paper relates to the original introduction of ImageNet used for training and evaluation.",
          "quote": "For all of our attacks, we use the ImageNet training set as D."
        }
      },
      {
        "name": {
          "value": "ImageNet People Subtree",
          "justification": "This subset of ImageNet with annotated demography (e.g., gender, race) is used to assess fairness and apply fairwashing attacks.",
          "quote": "In order to run and evaluate the fairwashing attack, we need a dataset with a labeled protected attribute. For this purpose, we use the ImageNet People Subtree dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the ImageNet hierarchy",
          "justification": "The referenced paper relates to the ImageNet People Subtree dataset used for evaluating fairwashing attacks.",
          "quote": "For this purpose, we use the ImageNet People Subtree dataset."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is the deep learning library used for implementing and running the experiments.",
          "quote": "We use the PyTorch pretrained AlexNet for our analysis."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The referenced paper relates to the PyTorch deep learning library used for the experiments.",
          "quote": "We use the PyTorch pretrained AlexNet for our analysis."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1620,
    "prompt_tokens": 25618,
    "total_tokens": 27238
  }
}