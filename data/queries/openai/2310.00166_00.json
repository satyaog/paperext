{
  "paper": "2310.00166.txt",
  "words": 17913,
  "extractions": {
    "title": {
      "value": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback",
      "justification": "Title of the paper",
      "quote": "Motif: Intrinsic Motivation from Artificial Intelligence Feedback"
    },
    "description": "This paper proposes a method named Motif that leverages preferences from a Large Language Model (LLM) to generate an intrinsic reward used to train agents via reinforcement learning. This intrinsic reward is evaluated on the NetHack game, showcasing superior performance compared to traditional methods.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experimental evaluation on the NetHack game to demonstrate the efficacy of the proposed method, Motif.",
      "quote": "We evaluate Motif’s performance and behavior on the challenging, open-ended and procedurally-generated NetHack game."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary focus is on using reward signals to optimize agent behavior, a core aspect of Reinforcement Learning.",
        "quote": "Motif uses an LLM to express preferences over pairs of event captions extracted from a dataset of observations and then distills them into an intrinsic reward. The resulting reward is then maximized directly or in combination with an extrinsic reward coming from the environment."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Intrinsic Motivation",
          "justification": "Central to the paper is the concept of intrinsic motivation, derived from preferences indicated by an LLM.",
          "quote": "We evaluate Motif’s performance and behavior on the challenging, open-ended and procedurally-generated NetHack game. Surprisingly, by only learning to maximize its intrinsic reward, Motif achieves a higher game score than an algorithm directly trained to maximize the score itself."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Language Models",
          "justification": "The paper leverages Large Language Models (LLMs) to generate preferences that guide the intrinsic motivation mechanism.",
          "quote": "In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Motif",
          "justification": "The model proposed in the paper.",
          "quote": "Our method, named Motif, uses an LLM to express preferences over pairs of event captions extracted from a dataset of observations and then distills them into an intrinsic reward."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The paper introduces and evaluates the Motif method.",
          "quote": "In this paper, we propose Motif, a general method to interface such prior knowledge from a Large Language Model (LLM) with an agent."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed in experiments as part of the research.",
          "quote": "We evaluate Motif’s performance and behavior on the challenging, open-ended and procedurally-generated NetHack game."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance was compared to other approaches in the NetHack game.",
          "quote": "When combining Motif’s intrinsic reward with the environment reward, our method significantly outperforms existing approaches..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The model is a novel contribution in this paper, and not referenced from another paper.",
          "quote": "Our method, named Motif, uses an LLM to express preferences over pairs of event captions extracted from a dataset of observations and then distills them into an intrinsic reward."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "NetHack Learning Environment (NLE)",
          "justification": "The dataset used for evaluations and experiments in the paper.",
          "quote": "We apply Motif to the challenging NetHack Learning Environment (NLE) (Küttler et al., 2020), and learn intrinsic rewards from Llama 2’s preferences (Touvron et al., 2023) on a dataset of gameplays."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The NetHack Learning Environment",
          "justification": "The dataset referenced in the experiments section of the paper.",
          "quote": "We apply Motif to the challenging NetHack Learning Environment (NLE) (Küttler et al., 2020), and learn intrinsic rewards from Llama 2’s preferences (Touvron et al., 2023) on a dataset of gameplays."
        }
      },
      {
        "name": {
          "value": "Llama 2 gameplay preferences dataset",
          "justification": "The dataset created and used in the paper for training the intrinsic reward model.",
          "quote": "We apply Motif to the challenging NetHack Learning Environment (NLE) (Küttler et al., 2020), and learn intrinsic rewards from Llama 2’s preferences (Touvron et al., 2023) on a dataset of gameplays."
        },
        "aliases": [],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
          "justification": "Llama 2 is used to generate the preferences for this dataset.",
          "quote": "We apply Motif to the challenging NetHack Learning Environment (NLE) (Küttler et al., 2020), and learn intrinsic rewards from Llama 2’s preferences (Touvron et al., 2023) on a dataset of gameplays."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Llama 2",
          "justification": "This library was used to generate preferences for constructing the intrinsic reward in the Motif method.",
          "quote": "We apply Motif to the challenging NetHack Learning Environment (NLE) (Küttler et al., 2020), and learn intrinsic rewards from Llama 2’s preferences (Touvron et al., 2023) on a dataset of gameplays."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
          "justification": "Llama 2 is referenced in connection with training the intrinsic reward model.",
          "quote": "We apply Motif to the challenging NetHack Learning Environment (NLE) (Küttler et al., 2020), and learn intrinsic rewards from Llama 2’s preferences (Touvron et al., 2023) on a dataset of gameplays."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1275,
    "prompt_tokens": 30200,
    "total_tokens": 31475
  }
}