{
  "paper": "1sxiBaGEtg.txt",
  "words": 15115,
  "extractions": {
    "title": {
      "value": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
      "justification": "This name matches the title of the paper provided.",
      "quote": "Hyena Hierarchy: Towards Larger Convolutional Language Models"
    },
    "description": "This paper proposes the Hyena operator, a subquadratic replacement for attention in Transformers. It explores the computational and empirical advantages of Hyena, highlighting its performance in tasks requiring long context, including language modeling and classification.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes benchmark results, empirical evaluations, and performance comparisons for the proposed Hyena operator.",
      "quote": "In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy... We set a new state-of-the-art for dense-attention-free architectures on language modeling..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary experiments and applications of the proposed Hyena operator are focused on language modeling and tasks related to natural language understanding.",
        "quote": "On the T HE P ILE at the 335M parameter scale, we match Transformer perplexity with a 20% reduction in the total count of floating point operations (FLOPs)."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper also explores the applications of the Hyena operator in image classification tasks using the Vision Transformer, indicating its relevance to the field of computer vision.",
          "quote": "As an extension, we investigate the generality of Hyena... On CIFAR-2D... on ImageNet-1k from scratch."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Hyena",
          "justification": "The Hyena model is explicitly described as the main model introduced and evaluated in the paper.",
          "quote": "In this work, we propose Hyena, a subquadratic drop-in replacement for attention..."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The Hyena model is the primary contribution of the paper.",
          "quote": "In this work, we propose Hyena, a subquadratic drop-in replacement for attention..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper presents empirical benchmarks and performance evaluations, indicating that the model was executed.",
          "quote": "In recall and reasoning tasks... Hyena improves accuracy by more than 50 points..."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Hyena model is compared with other models based on Transformers and other architectures in terms of performance and computational efficiency.",
          "quote": "Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods..."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "The Hyena model was introduced and defined in this paper itself.",
          "quote": "In this work, we propose Hyena, a subquadratic drop-in replacement for attention..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "WikiText-103",
          "justification": "The WikiText-103 dataset is mentioned as one of the standard datasets used for benchmarking and evaluating the Hyena model's performance.",
          "quote": "We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (W IKI T EXT 103 and T HE P ILE)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "None",
          "justification": "This dataset is a standard benchmark in the field of language modeling and does not correspond to a specific referenced paper title within this work.",
          "quote": "We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (W IKI T EXT 103 and T HE P ILE)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions the use of PyTorch for implementing standard attention mechanisms.",
          "quote": "standard attention implementation in PyTorch runs out of memory."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "None",
          "justification": "The reference to PyTorch is in the context of its general use and not a specific referenced paper.",
          "quote": "standard attention implementation in PyTorch runs out of memory."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 858,
    "prompt_tokens": 25644,
    "total_tokens": 26502
  }
}