{
  "paper": "1910.02344.txt",
  "words": 9506,
  "extractions": {
    "title": {
      "value": "Neural Multisensory Scene Inference",
      "justification": "The title of the paper as given by the authors is 'Neural Multisensory Scene Inference', which succinctly encapsulates the paper's focus on inferring 3D scenes using multiple sensory modalities.",
      "quote": "Neural Multisensory Scene Inference"
    },
    "description": "The paper proposes the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities. It also introduces the Amortized Product-of-Experts (APoE), a novel method to improve computational efficiency and robustness to unseen combinations of modalities at test time. The paper highlights the ability of the model to infer robust and modality-invariant 3D-scene representations from various combinations of sensory modalities and perform accurate cross-modal generation.",
    "type": {
      "value": "empirical",
      "justification": "The paper primarily focuses on the design, implementation, and evaluation of new deep learning models (GMN and APoE). It includes extensive experimental results demonstrating the effectiveness of these models.",
      "quote": "Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations from arbitrary combinations of modalities and perform accurate cross-modal generation."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper falls under the field of Deep Learning as it involves designing neural network architectures for learning representations from multisensory data.",
        "quote": "In this paper, we propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The paper specifically addresses the problem of learning representations from multiple sensory modalities, which is the focus of the sub-field of Multimodal Learning within Deep Learning.",
          "quote": "Despite its importance, multisensory 3D scene representation learning has received less attention compared to the unimodal setting."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Generative Multisensory Network",
          "justification": "GMN is the primary model introduced in the paper for learning 3D scene representations from multisensory data.",
          "quote": "we propose the Generative Multisensory Network (GMN) for learning latent representations of 3D scenes which are partially observable through multiple sensory modalities."
        },
        "aliases": [
          "GMN"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Amortized Product-of-Experts",
          "justification": "APoE is introduced in the paper to improve the computational efficiency and robustness of GMN.",
          "quote": "We also introduce a novel method, called the Amortized Product-of-Experts, to improve the computational efficiency and the robustness to unseen combinations of modalities at test time."
        },
        "aliases": [
          "APoE"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Consistent Generative Query Network",
          "justification": "C-GQN is referenced in this paper as the basis upon which extensions were made to develop GMN.",
          "quote": "To this end, we formalize the problem as a probabilistic latent variable model based on the Generative Query Network (Eslami et al., 2018) framework and introduce the Amortized Product-of-Experts (APoE)."
        },
        "aliases": [
          "C-GQN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "None"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MultiSensory Embodied 3D-Scene Environment",
          "justification": "MESE is a dataset created and used in this paper to test the performance of GMN and APoE models.",
          "quote": "To perform this exploration, we also develop the Multisensory Embodied 3D-Scene Environment (MESE)."
        },
        "aliases": [
          "MESE"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Shepard-Metzler Dataset",
          "justification": "The Shepard-Metzler Dataset is used in the environment for training and evaluating the proposed models.",
          "quote": "Our main task is similar to the Shepard-Metzler object experiments used in Eslami et al. (2018) but extends it with the MPL hand."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is one of the primary deep learning libraries used for implementing and training the models in this paper.",
          "quote": "Code is available at: https://github.com/lim0606/pytorch-generative-multisensory-network"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CUDA",
          "justification": "CUDA is used for parallel computation in training the models.",
          "quote": "PyTorch (Paszke et al., 2017), CUDA-9.0 (Nickolls et al., 2008), and cuDNN7 (Chetlur et al., 2014) are used for the implementations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "cuDNN",
          "justification": "cuDNN is used alongside CUDA for optimized deep learning computations.",
          "quote": "PyTorch (Paszke et al., 2017), CUDA-9.0 (Nickolls et al., 2008), and cuDNN7 (Chetlur et al., 2014) are used for the implementations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1240,
    "prompt_tokens": 15616,
    "total_tokens": 16856
  }
}