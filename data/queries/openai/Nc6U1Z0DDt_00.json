{
  "paper": "Nc6U1Z0DDt.txt",
  "words": 9358,
  "extractions": {
    "title": {
      "value": "BALAUR: Language Model Pretraining with Lexical Semantic Relations",
      "justification": "This is the title of the paper as it appears at the beginning.",
      "quote": "BALAUR: Language Model Pretraining with Lexical Semantic Relations"
    },
    "description": "This paper introduces BALAUR, a model that improves the performance of large transformer-based language models on tasks informed by lexical semantic relations (LSRs) such as hypernymy, synonymy, and antonymy. The model achieves this by integrating LSRs into the hidden states of language models throughout pretraining. The authors validate their approach with various evaluations, showing superior performance on hypernymy-informed tasks and the original language modeling objective. Additionally, the paper contributes a new dataset, HYPCC, for evaluating hypernymy-based cloze prompts.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a new model (BALAUR) and demonstrates its effectiveness through various experiments and evaluations on different tasks.",
      "quote": "We validate our hypothesis and demonstrate that BALAUR generally improves the performance of large transformer-based LMs on a comprehensive set of hypernymy-informed tasks, as well as on the original LM objective."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on language models, which are a core topic in Natural Language Processing (NLP).",
        "quote": "Pretrained language models (LMs) trained on ever increasing compute and data have achieved state- of-the-art performance on a wide variety of NLP benchmarks."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Language Models",
          "justification": "The paper is centered around pretrained language models and their improvement.",
          "quote": "Pretrained language models (LMs) trained on ever increasing compute and data have achieved state- of-the-art performance on a wide variety of NLP benchmarks."
        },
        "aliases": [
          "LMs"
        ]
      },
      {
        "name": {
          "value": "Lexical Semantics",
          "justification": "The focus of the paper is to integrate lexical semantic relations (LSRs) into language models, making this a relevant subfield.",
          "quote": "We propose BALAUR (Figure 1), an approach to Transformer LM pretraining which directly models LSRs in the latent representations of the LM."
        },
        "aliases": [
          "LSRs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BALAUR",
          "justification": "The main contribution of the paper is BALAUR, a model designed to integrate LSRs into language models.",
          "quote": "In this paper, we propose BALAUR, a model that addresses this challenge by modeling LSRs directly in the LM’s hidden states throughout pretraining."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "BALAUR is the main contribution of the paper.",
          "quote": "In this paper, we propose BALAUR, a model that addresses this challenge by modeling LSRs directly in the LM’s hidden states throughout pretraining."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes various experiments where BALAUR is executed and evaluated.",
          "quote": "We validate our hypothesis and demonstrate that BALAUR generally improves the performance of large transformer-based LMs on a comprehensive set of hypernymy-informed tasks, as well as on the original LM objective."
        },
        "is_compared": {
          "value": true,
          "justification": "BALAUR is compared against existing models on several tasks to show its effectiveness.",
          "quote": "We validate our hypothesis and demonstrate that BALAUR generally improves the performance of large transformer-based LMs on a comprehensive set of hypernymy-informed tasks, as well as on the original LM objective."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "BALAUR is introduced in this paper, so there is no reference to a previous work specifically for this model.",
          "quote": "In this paper, we propose BALAUR, a model that addresses this challenge by modeling LSRs directly in the LM’s hidden states throughout pretraining."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "HYPCC",
          "justification": "HYPCC is introduced and used in the paper for evaluating hypernymy-informed cloze prompts.",
          "quote": "Finally, as part of this evaluation, we create and HYPCC, a dataset of hypernymy-informed cloze completion prompts improving on previous datasets with a better coverage of hypernymy and hyponymy."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "HYPCC is introduced in this paper, so there is no reference to a previous work specifically for this dataset.",
          "quote": "Finally, as part of this evaluation, we create and HYPCC, a dataset of hypernymy-informed cloze completion prompts improving on previous datasets with a better coverage of hypernymy and hyponymy."
        }
      },
      {
        "name": {
          "value": "WordNet",
          "justification": "WordNet is used as a source for extracting lexical semantic relations.",
          "quote": "As a first step, we extract related token-concept pairs for hypernymy, hyponymy, antonymy and synonymy from WordNet’s noun hierarchy (Miller, 1995)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "WordNet: a lexical database for English",
          "justification": "This is the standard reference for WordNet.",
          "quote": "We extract related token-concept pairs for hypernymy, hyponymy, antonymy and synonymy from WordNet's noun hierarchy (Miller, 1995)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "transformers",
          "justification": "The evaluation includes comparisons with models from the transformers library.",
          "quote": "We extract related token-concept pairs for hypernymy, hyponymy, antonymy and synonymy from WordNet’s noun hierarchy (Miller, 1995)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The library is used within this work.",
          "quote": "We note that RoBERTa was trained on an order of magnitude more data than the models used in our experiments (16GB versus 161GB), which has a significant impact on downstream performance (Liu et al., 2019) and helps explain the discrepancy in performance."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1270,
    "prompt_tokens": 17493,
    "total_tokens": 18763
  }
}