{
  "paper": "2312.14279.txt",
  "words": 16269,
  "extractions": {
    "title": {
      "value": "Characterizing and Classifying Developer Forum Posts with their Intentions",
      "justification": "The title is taken directly from the research paper to ensure accuracy.",
      "quote": "Characterizing and Classifying Developer Forum Posts with their Intentions"
    },
    "description": "The paper focuses on the characterization and classification of developer forum posts based on their intentions. The authors propose a refined taxonomy for the intentions of technical forum posts and design a pre-trained transformer-based model to predict these intentions automatically. Their best model variant achieves competitive performance.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper involves empirical methods such as qualitative studies and experimental evaluations to analyze the data and validate the proposed model.",
      "quote": "In this paper, we refer to these reasons as intentions. To exemplify the distinctions between technique-oriented tag taxonomies and an intention-based taxonomy for technical posts, we present a concrete example."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The study utilizes deep learning models to predict the intentions of forum posts, making it a part of the Deep Learning research field.",
        "quote": "Through manual labeling and analysis on a sampled post dataset extracted from online forums, we understand the relevance between the constitution of posts (code, error messages) and their intentions."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper uses transformer-based models, which are a key technology in Natural Language Processing, to analyze and predict the intentions of forum posts.",
          "quote": "In the framework, we employ transformer-based pre-trained language models to generate embeddings for both title and description of posts."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including BERT."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including RoBERTa."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ALBERT",
          "justification": "ALBERT is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including ALBERT."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "DistilBERT is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including DistilBERT."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BERTOverflow",
          "justification": "BERTOverflow is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including BERTOverflow."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CodeBERT",
          "justification": "CodeBERT is explicitly mentioned as one of the models tested in the study.",
          "quote": "We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including CodeBERT."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Stack Overflow",
          "justification": "Stack Overflow posts are one of the main sources of data used in the study.",
          "quote": "The data dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities), mainly from three different platforms: Stack Exchange3, Lithium4 forums and Discourse5 forums."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Discourse forums",
          "justification": "Discourse forums are one of the main sources of data used in the study.",
          "quote": "The data dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities), mainly from three different platforms: Stack Exchange3, Lithium4 forums and Discourse5 forums."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Lithium forums",
          "justification": "Lithium forums are one of the main sources of data used in the study.",
          "quote": "The data dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities), mainly from three different platforms: Stack Exchange3, Lithium4 forums and Discourse5 forums."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face",
          "justification": "Hugging Face's implementation of transformer models is used in the study.",
          "quote": "We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1269,
    "prompt_tokens": 23932,
    "total_tokens": 25201
  }
}