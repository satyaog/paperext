{
  "paper": "877f0955a0d5398077ce96a9540e1fab.txt",
  "words": 14741,
  "extractions": {
    "title": {
      "value": "Robust and Versatile Bipedal Jumping Control through Reinforcement Learning",
      "justification": "This is the title of the paper as provided by the user.",
      "quote": "Robust and Versatile Bipedal Jumping Control\nthrough Reinforcement Learning"
    },
    "description": "This paper presents a reinforcement learning framework for training a bipedal robot, specifically a torque-controlled bipedal robot named Cassie, to perform dynamic jumping maneuvers. The research focuses on developing a versatile and robust control policy that can be trained in simulation and transferred directly to real-world hardware without further tuning.",
    "type": {
      "value": "Empirical Study",
      "justification": "The research includes experimental validation both in simulation and on hardware, presenting empirical results.",
      "quote": "We now deploy the goal-conditioned policies obtained in\nsimulation, the flat-ground policy that is trained on different\ngoals to jump to various locations and turning directions, and\nthe discrete-terrain policy that is specialized in jumping to\nvariable locations and elevations, on the hardware of Cassie."
    },
    "primary_research_field": {
      "name": {
        "value": "Robotics",
        "justification": "The paper primarily focuses on control policies for bipedal robots using reinforcement learning.",
        "quote": "This work aims to push the limits of agility for\nbipedal robots by enabling a torque-controlled bipedal robot to\nperform robust and versatile dynamic jumps in the real world."
      },
      "aliases": [
        "Robotic Locomotion",
        "Bipedal Robots",
        "Legged Robots"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The control policies are developed using reinforcement learning methods.",
          "quote": "We present a reinforcement learning framework for training\na robot to accomplish a large variety of jumping tasks, such\nas jumping to different locations and directions."
        },
        "aliases": [
          "RL"
        ]
      },
      {
        "name": {
          "value": "Sim-to-Real Transfer",
          "justification": "The paper emphasizes training in simulation and then transferring the policy to real hardware without further tuning.",
          "quote": "The policies are trained in simulation and deployed on the hardware without further tuning."
        },
        "aliases": [
          "Simulation to Real World"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Cassie",
          "justification": "Cassie is the specific bipedal robot used in the experiments for dynamic jumping maneuvers.",
          "quote": "Cassie (see Fig. 1) is a life-sized bipedal robot and is around\n1.1 meter tall, with a weight of 31 Kg."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Cassie is an existing robotic platform and not introduced in this paper.",
          "quote": "Cassie (see Fig. 1) is a life-sized bipedal robot and is around\n1.1 meter tall, with a weight of 31 Kg."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed on real hardware to validate the results.",
          "quote": "the flat-ground policy that is trained on different\ngoals to jump to various locations and turning directions, and\nthe discrete-terrain policy that is specialized in jumping to\nvariable locations and elevations, on the hardware of Cassie."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the policy on Cassie is compared with previous methods.",
          "quote": "We next validate the proposed method in simulation (MuJoCo). In this section, we\naim to address two questions: (1) what are the advantages of\nthe proposed policy architecture compared to models used in\nprior work, (2) whether training with multiple tasks can further\nimprove the robustness of the policy over single-goal training,\nby allowing the robot to utilize more diverse maneuvers to\nrecover from unstable states or unknown perturbations."
        },
        "referenced_paper_title": {
          "value": "Feedback Control For Cassie With Deep Reinforcement Learning",
          "justification": "This is the reference model mentioned in the paper.",
          "quote": "Feedback Control For Cassie With Deep Reinforcement Learning"
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "MuJoCo",
          "justification": "MuJoCo is used for the simulation environment where the reinforcement learning policies were trained.",
          "quote": "The training\nenvironment is developed in a simulation of Cassie using\nMuJoCo"
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "MuJoCo: A physics engine for model-based control",
          "justification": "This paper is referenced in the context of MuJoCo being used for simulation.",
          "quote": "MuJoCo: A physics engine for model-based control"
        }
      },
      {
        "name": {
          "value": "Proximal Policy Optimization (PPO)",
          "justification": "PPO is used as the reinforcement learning algorithm to train the policies.",
          "quote": "Proximal Policy Optimization (PPO) [56] is used to train\nall policies πθ in simulation, with a value function represented\nby a 2-layered MLP, which has an access to the ground truth\nobservations."
        },
        "aliases": [
          "PPO"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "This paper is referenced in the context of Proximal Policy Optimization being the algorithm used for training.",
          "quote": "Proximal Policy Optimization Algorithms"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1327,
    "prompt_tokens": 24884,
    "total_tokens": 26211
  }
}