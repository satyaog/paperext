{
  "paper": "2310.07064.txt",
  "words": 26859,
  "extractions": {
    "title": {
      "value": "Large Language Models can Learn Rules",
      "justification": "This is the title of the paper.",
      "quote": "Large Language Models can Learn Rules"
    },
    "description": "This paper introduces Hypotheses-to-Theories (HtT), a framework that enhances large language models (LLMs) with rule learning capabilities for solving reasoning tasks. HtT includes an induction stage and a deduction stage. The induction stage involves generating and verifying rules, while the deduction stage augments the LLM with the learned rule library for improved reasoning on test questions. Experiments show significant improvements in accuracy across various tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experiments and evaluation on several datasets to demonstrate the effectiveness of the proposed Hypotheses-to-Theories (HtT) framework.",
      "quote": "Experiments on relational reasoning, numerical reasoning and concept learning problems show that HtT improves existing prompting methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus of the paper is on enhancing the capabilities of large language models in reasoning tasks, which falls under the domain of Natural Language Processing.",
        "quote": "When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reasoning",
          "justification": "The paper's major contribution is in the area of reasoning with large language models, specifically through learning and applying rules.",
          "quote": "To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The techniques employed such as rule extraction and application are core aspects of machine learning, especially in the context of model inference and learning.",
          "quote": "The induction stage aims to learn rules from training examples without rule annotation."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Hypotheses-to-Theories (HtT)",
          "justification": "HtT is introduced as a novel framework in the scope of this paper for enhancing reasoning capabilities of LLMs.",
          "quote": "we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "HtT is a novel contribution presented by the authors in this paper.",
          "quote": "we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments involving HtT were conducted to demonstrate its effectiveness.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "is_compared": {
          "value": 1,
          "justification": "HtT was compared with baseline prompting methods in terms of performance.",
          "quote": "Experiments show that HtT consistently improves over baseline prompting methods across the models and datasets considered."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no single reference paper as HtT is an original contribution by the paper's authors.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-3.5",
          "justification": "GPT-3.5 was used in the experiments to verify the effectiveness of HtT on reasoning tasks.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-3.5 was used but not introduced as a new model in this paper.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments using GPT-3.5 were conducted as part of the study.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "is_compared": {
          "value": 1,
          "justification": "GPT-3.5's performance with HtT was compared to baseline methods.",
          "quote": "Experiments show that HtT consistently improves over baseline prompting methods across the models and datasets considered."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The referenced paper introduces GPT-3, which GPT-3.5 is based on.",
          "quote": "Coinciding with their tremendous growth in scale, large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023; Anil et al., 2023; Gemini Team et al., 2023, inter alia) have demonstrated emergent capabilities across a wide range of reasoning tasks."
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 was used in the experiments to verify the effectiveness of HtT on reasoning tasks.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-4 was used but not introduced as a new model in this paper.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments using GPT-4 were conducted as part of the study.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "is_compared": {
          "value": 1,
          "justification": "GPT-4's performance with HtT was compared to baseline methods.",
          "quote": "Experiments show that HtT consistently improves over baseline prompting methods across the models and datasets considered."
        },
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "The referenced paper provides a detailed description of GPT-4.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CLUTRR",
          "justification": "CLUTRR is explicitly used in the experiments to evaluate the HtT framework on relational reasoning.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "CLUTRR: A diagnostic benchmark for inductive reasoning from text",
          "justification": "The referenced paper introduces the CLUTRR dataset used in the experiments.",
          "quote": "We evaluate HtT on CLUTRR (Sinha et al., 2019), a relational reasoning dataset that queries the relationship between two family members in a family tree."
        }
      },
      {
        "name": {
          "value": "Arithmetic",
          "justification": "Arithmetic is explicitly used in the experiments to evaluate the HtT framework on numerical reasoning.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks",
          "justification": "The referenced paper introduces the Arithmetic dataset used in the experiments.",
          "quote": "We use the Arithmetic dataset (Wu et al., 2023) to evaluate the LLMs on numerical reasoning in non-decimal systems."
        }
      },
      {
        "name": {
          "value": "List Functions",
          "justification": "List Functions dataset is explicitly used in the experiments to evaluate the HtT framework on concept learning.",
          "quote": "Empirically, we verify the effectiveness of HtT with GPT-3.5 and GPT-4 (OpenAI, 2023) on the CLUTRR (Sinha et al., 2019), Arithmetic Wu et al. (2023) and List Functions (Rule et al., 2020) datasets."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The child as hacker",
          "justification": "The referenced paper introduces the List Functions dataset used in the experiments.",
          "quote": "To assess the potential of HtT in learning complex rules, we further evaluate HtT on the concept learning problem using List Functions (Rule et al., 2020)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is mentioned in the paper for running the experiments.",
          "quote": "For all experiments, we use the PyTorch framework."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The referenced paper provides a detailed description of PyTorch.",
          "quote": "For all experiments, we use the PyTorch framework (Paszke et al., 2019)."
        }
      },
      {
        "name": {
          "value": "Transformers",
          "justification": "The Transformers library is mentioned in the paper for running the large language models.",
          "quote": "We use the Huggingface Transformers library to run the models."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-art natural language processing",
          "justification": "The referenced paper provides a detailed description of the Transformers library.",
          "quote": "We use the Huggingface Transformers library to run the models (Wolf et al., 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2839,
    "prompt_tokens": 51062,
    "total_tokens": 53901
  }
}