{
  "paper": "2306.12509.txt",
  "words": 13225,
  "extractions": {
    "title": {
      "value": "Joint Prompt Optimization of Stacked LLMs using Variational Inference",
      "justification": "The specified title is the exact title provided at the beginning of the paper.",
      "quote": "Joint Prompt Optimization of Stacked LLMs using Variational Inference"
    },
    "description": "This paper presents methodologies for prompt optimization in Large Language Models (LLMs) when stacked in multiple layers forming Deep Language Networks (DLNs). The paper discusses the performance of these models, especially focusing on 1-layer and 2-layer DLNs, and compares them across various natural language processing tasks using models like GPT-3 and GPT-4.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper presents multiple experiments where the proposed methods (1-layer and 2-layer DLNs) are tested on various tasks from NLU to reasoning tasks. The empirical results are compared against baselines using LLMs like GPT-3 and GPT-4.",
      "quote": "Our first set of experiments evaluates the 1-layer language network (DLN-1) described in Section 2. Table 1 presents results on the full suite of test tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus of the paper is on optimizing prompts for Large Language Models (LLMs) such as GPT-3 and GPT-4 in natural language understanding tasks.",
        "quote": "We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks."
      },
      "aliases": [
        "NLP",
        "Natural Language Understanding",
        "NLU"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reasoning",
          "justification": "The paper discusses reasoning tasks and how DLNs can optimize prompts for improved performance on such tasks.",
          "quote": "On Hyper., Trec, and Disaster, it even surpasses GPT-4 baselines, unsurprisingly underperforming GPT-4 on all other tasks. DLN-1’s excellent performance on Hyper., a BBH task about ordering adjectives according to linguistic convention, is a surprise."
        },
        "aliases": [
          "Logical Reasoning",
          "Deductive Reasoning"
        ]
      },
      {
        "name": {
          "value": "Prompt Engineering",
          "justification": "The research focuses on prompt optimization for LLMs, making Prompt Engineering a key subfield of the study.",
          "quote": "We expand our work to learn parts of such templates: we expect this to make the variational bound tighter and thus easing DLN’s optimization."
        },
        "aliases": [
          "Prompt Optimization"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DLN-1",
          "justification": "The paper discusses and evaluates DLN-1, which is a 1-layer Deep Language Network designed for prompt optimization in LLMs.",
          "quote": "We first test the effectiveness of DLN-1 in multiple reasoning and natural language understanding tasks."
        },
        "aliases": [
          "1-layer DLN"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "DLN-1 is proposed as a new model for optimizing prompts in single-layer language networks.",
          "quote": "Our first set of experiments evaluates the 1-layer language network (DLN-1) described in Section 2."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper contains experiment results for DLN-1, indicating that it was executed to produce numerical results.",
          "quote": "Our prompt optimization techniques extend the Automatic Prompt Engineer (APE) procedure from Zhou et al."
        },
        "is_compared": {
          "value": 1,
          "justification": "DLN-1 is compared numerically with various methods, including GPT-3 and GPT-4, in multiple tables and sections.",
          "quote": "Table 1 presents results on the full suite of test tasks."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "DLN-1 is introduced in this paper, and there's no specific referenced paper for it.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "DLN-2",
          "justification": "The paper proposes and evaluates DLN-2, a 2-layer Deep Language Network designed for optimized prompt learning in LLMs.",
          "quote": "Then, we show how to train a 2-layer DLN (DLN-2), which parametrizes a probability distribution"
        },
        "aliases": [
          "2-layer DLN"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "DLN-2 is introduced in this paper as an extension of DLN-1 for multi-layer language networks.",
          "quote": "The natural extension of DLN-1 is DLN-2, in which language layers are stacked, i.e. the output of the first language layer is the input to the second one."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper contains experimental results for DLN-2, indicating that it was executed as part of the study.",
          "quote": "Results for DLN-2 can be found in Table 2."
        },
        "is_compared": {
          "value": 1,
          "justification": "DLN-2 is compared numerically with DLN-1, GPT-3, and GPT-4 in various sections.",
          "quote": "Compared to DLN-1, DLN-2 provides an average boost of 7.2% absolute score."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "DLN-2 is introduced in this paper and not referenced from an external source.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "GPT-3",
          "justification": "GPT-3 is frequently used as the backbone model for the proposed DLNs in the paper.",
          "quote": "Throughout this paper, we use OpenAI’s models, specifically GPT-3 (text-davinci-003) and GPT-4, as the backbone to our proposed systems unless otherwise specified."
        },
        "aliases": [
          "Generative Pre-trained Transformer 3"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-3 is an existing LLM used as a baseline in this paper, but it is not contributed by the paper itself.",
          "quote": "We show that DLN-2 can reach higher performance than a single layer, showing promise that we might reach comparable performance to GPT-4, even when each LLM in the network is smaller and less powerful."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper uses GPT-3 to generate prompts and as a baseline for comparison, indicating it was executed.",
          "quote": "Then, we show how to train a 2-layer DLN (DLN-2), which parametrizes a probability distribution"
        },
        "is_compared": {
          "value": 1,
          "justification": "GPT-3 is utilized as a comparison model to evaluate the effectiveness of DLNs.",
          "quote": "Our prompt optimization techniques extend the Automatic Prompt Engineer (APE) procedure from Zhou et al."
        },
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "GPT-3 is first introduced in the paper titled 'Language Models are Few-Shot Learners'.",
          "quote": "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901."
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is used as one of the LLMs for benchmarking the results of the DLNs.",
          "quote": "We even surpass GPT-4 baselines, unsurprisingly underperforming GPT-4 on all other tasks."
        },
        "aliases": [
          "Generative Pre-trained Transformer 4"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Although extensively used, GPT-4 is not introduced by this paper.",
          "quote": "On Hyper., Trec, and Disaster, it even surpasses GPT-4 baselines, unsurprisingly underperforming GPT-4 on all other tasks."
        },
        "is_executed": {
          "value": 1,
          "justification": "GPT-4 was used in experimental comparisons, implying it was executed.",
          "quote": "Compared to 0-shot GPT-4 results in Table 1, on Subj and Disaster, DLN-2 on average provides more than 20% in absolute improvement."
        },
        "is_compared": {
          "value": 1,
          "justification": "The effectiveness of the proposed DLNs is frequently compared with GPT-4's performance.",
          "quote": "shows GPT-4 baselines, unsurprisingly underperforming GPT-4 on all other tasks."
        },
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "GPT-4 is the successor to GPT-3, thus referenced similarly.",
          "quote": "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "BigBench Hard",
          "justification": "BigBench Hard (BBH) tasks are used to test the effectiveness of DLNs.",
          "quote": "For tasks adopted from BigBench-Hard (BBH) [42] (Hyper., Nav., Date. and Logic.7)"
        },
        "aliases": [
          "BBH",
          "BigBench Hard"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
          "justification": "The BigBench dataset is part of the Beyond the Imitation Game paper.",
          "quote": "Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al. (2022). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615."
        }
      },
      {
        "name": {
          "value": "Leopard",
          "justification": "Leopard dataset is employed in the experiments to list tasks and their statistics.",
          "quote": "For tasks adopted from Leopard [1] (Disaster and Airline)"
        },
        "aliases": [
          "Leopard"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks",
          "justification": "The Leopard dataset is part of the Learning to Few-Shot Learn Across Diverse Natural Language Classification Tasks paper.",
          "quote": "Bansal, T., Jha, R., and McCallum, A. (2020). Learning to few-shot learn across diverse natural language classification tasks. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5108–5123."
        }
      },
      {
        "name": {
          "value": "Automatic Prompt Engineer (APE)",
          "justification": "Automatic Prompt Engineer (APE) is referenced in the paper as a benchmark for comparison with DLNs.",
          "quote": "Our prompt optimization techniques extend the Automatic Prompt Engineer (APE) procedure from Zhou et al."
        },
        "aliases": [
          "APE"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Large language models are human-level prompt engineers",
          "justification": "The paper titled 'Large language models are human-level prompt engineers' discusses APE in detail.",
          "quote": "Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. (2023). Large language models are human-level prompt engineers. International Conference on Learning Representations."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "GPT-3 API",
          "justification": "The authors used OpenAI’s GPT-3 API for accessing the GPT-3 model in all their experiments.",
          "quote": "Throughout this paper, we use OpenAI’s models, specifically GPT-3 (text-davinci-003) and GPT-4, as the backbone to our proposed systems unless otherwise specified."
        },
        "aliases": [
          "OpenAI GPT-3 API",
          "text-davinci-003"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The usage of the GPT-3 API for accessing GPT-3 model does not warrant an individual referenced paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "GPT-4 API",
          "justification": "The authors used OpenAI’s GPT-4 API for accessing the GPT-4 model in their experiments.",
          "quote": "Throughout this paper, we use OpenAI’s models, specifically GPT-3 (text-davinci-003) and GPT-4, as the backbone to our proposed systems unless otherwise specified."
        },
        "aliases": [
          "OpenAI GPT-4 API"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The usage of the GPT-4 API to access the GPT-4 model does not warrant an individual referenced paper.",
          "quote": "N/A"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2663,
    "prompt_tokens": 23743,
    "total_tokens": 26406
  }
}