{
  "paper": "2311.01990.txt",
  "words": 11535,
  "extractions": {
    "title": {
      "value": "Conditions on Preference Relations that Guarantee the Existence of Optimal Policies",
      "justification": "This is the title provided at the top of the paper.",
      "quote": "Conditions on Preference Relations that Guarantee\nthe Existence of Optimal Policies"
    },
    "description": "This paper introduces the Direct Preference Process (DPP) framework to analyze Learning from Preferential Feedback (LfPF) problems in partially-observable, non-Markovian environments. It establishes conditions under which optimal policies exist in the absence of a reward function, emphasizing the need for preference-based learning strategies without assumed reward generation.",
    "type": {
      "value": "Theoretical",
      "justification": "The paper primarily focuses on defining a new framework and establishing theoretical conditions for the existence of optimal policies in preference-based learning without relying on reward functions.",
      "quote": "We define the Direct Preference Process, a model of preference-based learning in partiallyobservable, non-Markovian environments (Section 4). We provide necessary and sufficient conditions that determine when a Direct Preference Process can be cast as an\ninstance of reinforcement learning (RL)."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper discusses the Direct Preference Process within the context of Reinforcement Learning (RL) and addresses conditions for optimal policies in preference-based RL.",
        "quote": "A key feature of the Direct Preference\nProcess is that abstracts away the details of how feedback is given to a learning agent, instead working “directly” with the ordinal structure inferred from the preferences. This abstraction is\nparticularly well suited for LfPF problems, where a variety of feedback mechanisms are used\nduring training, including offline reward modelling (Ziegler et al., 2020; Bai et al., 2022), once-per-episode trajectory feedback (Chatterji et al., 2021) and online feedback between trajectory segments (Christiano et al., 2017).\n"
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Learning from Preferential Feedback",
          "justification": "Learning from Preferential Feedback (LfPF) is described as both an application area and key context in which the Direct Preference Process operates.",
          "quote": "Learning from Preferential Feedback (LfPF) plays an essential role in training Large\nLanguage Models, as well as certain types of interactive learning agents."
        },
        "aliases": [
          "LfPF"
        ]
      },
      {
        "name": {
          "value": "Partially Observable Environments",
          "justification": "The paper introduces a framework (DPP) specifically for partially-observable, non-Markovian environments, making it a significant subfield of focus.",
          "quote": "We introduce the Direct Preference Process, a new framework for analyzing LfPF problems in\npartially-observable, non-Markovian environments."
        },
        "aliases": [
          "POE"
        ]
      }
    ],
    "models": [],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 566,
    "prompt_tokens": 20220,
    "total_tokens": 20786
  }
}