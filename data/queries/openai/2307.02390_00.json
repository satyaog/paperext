{
  "paper": "2307.02390.txt",
  "words": 5250,
  "extractions": {
    "title": {
      "value": "Causal Discovery with Language Models as Imperfect Experts",
      "justification": "",
      "quote": ""
    },
    "description": "This research explores how expert knowledge, specifically from large language models (LLMs), can improve the identification of causal graphs beyond Markov equivalence classes. The study addresses the scenario where experts might provide erroneous information, proposes strategies to amend such knowledge based on consistency properties, and reports empirical results using LLMs as imperfect experts.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves an empirical evaluation of the proposed strategies using real-world causal Bayesian networks and large language models as experts.",
      "quote": "We now evaluate the ability of our approach to leverage imperfect expert knowledge using real-world causal Bayesian networks from the bnlearn repository (Scutari, 2010)."
    },
    "primary_research_field": {
      "name": {
        "value": "Causal Inference",
        "justification": "The paper focuses on improving causal discovery methods, which are essential for determining cause-and-effect relationships within data.",
        "quote": "Understanding the cause-and-effect relationships that underlie a complex system is critical to accurate decision-making."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The study uses large language models, a core component of NLP, as imperfect experts for causal discovery.",
          "quote": "We then empirically assess if the approach holds when taking a large language model as the expert – with mitigated results."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-3.5",
          "justification": "The paper utilizes GPT-3.5 models (text-davinci-002 and text-davinci-003) as large language model experts for causal discovery.",
          "quote": "For the LLM-based experts, we consider the text-davinci-{002, 003} versions of GPT-3.5."
        },
        "aliases": [
          "text-davinci-002",
          "text-davinci-003"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The GPT-3.5 models are used as existing tools rather than new contributions.",
          "quote": "For the LLM-based experts, we consider the text-davinci-{002, 003} versions of GPT-3.5."
        },
        "is_executed": {
          "value": 1,
          "justification": "The GPT-3.5 models are executed to provide expert knowledge during the experiments.",
          "quote": "The expert/strategy combinations were evaluated based on: (i) the resulting size of their equivalence class, |ME,S |, (ii) the structural Hamming distance (SHD) between the completed partially DAG (CP-DAG; see Glymour et al. (2019)) of ME,S and the true graph G⋆ , (iii) an empirical estimate of p(G⋆ ∈ ME,S ), taken over repetitions of the experiment."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the GPT-3.5 models is empirically assessed and compared against benchmarks like ε-experts.",
          "quote": "On all datasets, the LLM-based experts achieve SHDs that are on par or better than those of their naive counterparts (Long et al., 2023) for η = 1, while additionally enabling the control of the probability of excluding G⋆."
        },
        "referenced_paper_title": {
          "value": "Training language models to follow instructions with human feedback",
          "justification": "This referenced paper provides the foundational understanding and development of the GPT-3.5 models used in this study.",
          "quote": "For the LLM-based experts, we also considered a naive strategy that consists of simply orienting all edges according to the expert, as in Long et al. (2023)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "bnlearn Repository",
          "justification": "The real-world causal Bayesian networks used for evaluation are sourced from the bnlearn repository.",
          "quote": "We now evaluate the ability of our approach to leverage imperfect expert knowledge using real-world causal Bayesian networks from the bnlearn repository (Scutari, 2010)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Bayesian Networks with the bnlearn R Package",
          "justification": "This referenced paper describes the bnlearn repository from which the evaluation datasets are sourced.",
          "quote": "We now evaluate the ability of our approach to leverage imperfect expert knowledge using real-world causal Bayesian networks from the bnlearn repository (Scutari, 2010)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "bnlearn",
          "justification": "The bnlearn library is used to source the real-world causal Bayesian networks for empirical evaluation.",
          "quote": "We now evaluate the ability of our approach to leverage imperfect expert knowledge using real-world causal Bayesian networks from the bnlearn repository (Scutari, 2010)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Bayesian Networks with the bnlearn R Package",
          "justification": "This referenced paper describes the bnlearn library which is leveraged for sourcing the evaluation datasets.",
          "quote": "We now evaluate the ability of our approach to leverage imperfect expert knowledge using real-world causal Bayesian networks from the bnlearn repository (Scutari, 2010)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1040,
    "prompt_tokens": 10669,
    "total_tokens": 11709
  }
}