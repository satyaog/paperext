{
  "paper": "cmJiEqniEc.txt",
  "words": 7680,
  "extractions": {
    "title": {
      "value": "Detecting Backdoors with Meta-Models",
      "justification": "This is the title of the paper.",
      "quote": "Detecting Backdoors with Meta-Models"
    },
    "description": "The paper proposes using meta-models, neural networks that take another network's parameters as input, to detect backdoors in neural networks. The approach is validated using CNNs trained on CIFAR-10 and shows high accuracy in backdoor detection tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents empirical results from experiments using a proposed meta-model to detect backdoors in neural networks.",
      "quote": "In this section we present empirical results on three main meta-modeling tasks: predicting data properties, mapping transformer parameters to equivalent programs written in human-readable code, and detecting and removing backdoors."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary dataset used is CIFAR-10, which is a popular dataset in the field of Computer Vision.",
        "quote": "We train base models on CIFAR-10 (Krizhevsky, Hinton, et al. 2009), using a simple CNN architecture with 70,000 parameters."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Interpretability",
          "justification": "The research involves understanding the internal mechanisms of neural networks, which is a key aspect of model interpretability.",
          "quote": "A line of work often referred to as mechanistic interpretability studies the internal workings of trained neural networks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Adversarial Machine Learning",
          "justification": "The paper deals with detecting backdoors, a form of adversarial attack, making it relevant to Adversarial Machine Learning.",
          "quote": "Data poisoning and backdoors. Data poisoning is the act of tampering with the training data to be fed to a model, in such a way that a model trained on this data exhibits undesired or malicious behavior."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Meta-Model",
          "justification": "The meta-model is the primary model proposed and discussed throughout the paper.",
          "quote": "We propose to use meta-models, neural networks that take another network’s parameters as input, to detect backdoors directly from model weights."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The meta-model architecture is proposed by the authors as a new contribution.",
          "quote": "We propose a meta-model architecture that can operate on arbitrary base model architectures."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments and results related to the meta-model's execution are presented in the paper.",
          "quote": "In this section we present empirical results on three main meta-modeling tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "The meta-model's performance is compared against previous methods.",
          "quote": "We compare against previous work on meta-models and find that our approach outperforms a previous method on predicting base model hyperparameters from weights."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper provided for the meta-model as it is a new contribution.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "CNN",
          "justification": "The base models trained on CIFAR-10 are simple CNN architectures.",
          "quote": "We train base models on CIFAR-10 (Krizhevsky, Hinton, et al. 2009), using a simple CNN architecture with 70,000 parameters."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The CNN is not a new contribution but used for the experiments.",
          "quote": "We train base models on CIFAR-10 (Krizhevsky, Hinton, et al. 2009), using a simple CNN architecture with 70,000 parameters."
        },
        "is_executed": {
          "value": 1,
          "justification": "The CNN models were executed as part of the experiments.",
          "quote": "We train a set of clean models and a set of poisoned models for every poison type."
        },
        "is_compared": {
          "value": 0,
          "justification": "The CNNs are used as base models and are not subjects of comparison themselves.",
          "quote": "N/A"
        },
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is the reference paper for CIFAR-10, the dataset on which the CNNs are trained.",
          "quote": "Krizhevsky, Hinton, et al. “Learning multiple layers of features from tiny images”."
        }
      },
      {
        "name": {
          "value": "Transformer Decoder",
          "justification": "The transformer decoder is used within the meta-model architecture to process model weights.",
          "quote": "Each chunk is passed through a linear embedding layer and then a transformer decoder."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The transformer decoder is not a new contribution but part of the meta-model architecture.",
          "quote": "The inputs are the weights of a base model (in our experiments either a CNN or transformer)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The transformer decoder is executed as part of the meta-model.",
          "quote": "Each chunk is passed through a linear embedding layer and then a transformer decoder."
        },
        "is_compared": {
          "value": 0,
          "justification": "The transformer decoder itself is not compared to other models; it is a component of the meta-model.",
          "quote": "N/A"
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No specific referenced paper for the transformer decoder as it's a common architecture.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is the primary dataset used to train the base models in the experiments.",
          "quote": "We train base models on CIFAR-10 (Krizhevsky, Hinton, et al. 2009)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is the reference for CIFAR-10 dataset.",
          "quote": "Krizhevsky, Hinton, et al. “Learning multiple layers of features from tiny images”."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1593,
    "prompt_tokens": 14100,
    "total_tokens": 15693
  }
}