{
  "paper": "2206.00529.txt",
  "words": 23422,
  "extractions": {
    "title": {
      "value": "Variance Reduction is an Antidote to Byzantine Workers: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top",
      "justification": "The title is extracted from the header of the paper.",
      "quote": "VARIANCE R EDUCTION IS AN A NTIDOTE TO B YZANTINE W ORKERS : B ETTER R ATES , W EAKER\nA SSUMPTIONS AND C OMMUNICATION C OMPRESSION AS A C HERRY ON THE T OP"
    },
    "description": "The paper focuses on the concept of Byzantine-robustness, which is significant in collaborative and federated learning, particularly in the context of distributed optimization algorithms. It discusses the challenges of incorporating variance reduction and communication compression to improve robustness against Byzantine workers. The authors introduce Byz-VR-MARINA, a novel method that combines Byzantine-tolerance with variance reduction and compression, and provide theoretical convergence guarantees for this algorithm. The empirical results are compared against current state-of-the-art methods.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents a novel method and supports it with theoretical guarantees and empirical experiments.",
      "quote": "We derive theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-Łojasiewicz loss functions. ... Numerical experiments corroborate our theoretical findings."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The paper aims to improve collaborative and federated learning systems by making them more robust against Byzantine workers.",
        "quote": "Byzantine-robustness has been gaining a lot of attention due to the growth of the interest in collaborative and federated learning."
      },
      "aliases": [
        "Collaborative Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Distributed Optimization",
          "justification": "The paper leverages distributed optimization to enhance the performance of federated learning.",
          "quote": "Distributed optimization algorithms play a vital role in the training of the modern machine learning models."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Byzantine Robustness",
          "justification": "The paper's primary focus is on improving robustness against Byzantine workers.",
          "quote": "This work addresses this gap and proposes Byz-VR-MARINA—a new Byzantine-tolerant method with variance reduction and compression."
        },
        "aliases": [
          "Byzantine-resilience"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Byz-VR-MARINA",
          "justification": "The paper introduces Byz-VR-MARINA as a method that combines variance reduction and communication compression to handle Byzantine workers.",
          "quote": "This work addresses this gap and proposes Byz-VR-MARINA—a new Byzantine-tolerant method with variance reduction and compression."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "Byz-VR-MARINA is introduced in this paper.",
          "quote": "This work addresses this gap and proposes Byz-VR-MARINA—a new Byzantine-tolerant method with variance reduction and compression."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experimental evaluation presented in the paper.",
          "quote": "Numerical experiments corroborate our theoretical findings."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares Byz-VR-MARINA numerically to other models such as Byrd-SAGA, BR-SGDm, and BR-DIANA.",
          "quote": "Our theory works for a slightly more general setting than the one we discussed in the main part of the paper. In particular, instead of Assumption 2.2 we consider a more general assumption on the heterogeneity."
        },
        "referenced_paper_title": {
          "value": "MARINA: Faster Non-Convex Distributed Learning with Compression",
          "justification": "The referenced paper is part of the theoretical foundation for Byz-VR-MARINA.",
          "quote": "Our algorithm is based on the recently proposed variance-reduced method with compression (VR-MARINA) from (Gorbunov et al., 2021b)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "a9a",
          "justification": "The a9a dataset was used in the numerical experiments of the paper.",
          "quote": "In particular, we consider the standard logistic regression model with `2 -regularization fi,j (x) = −yi,j log(h(x, ai,j ))−(1−yi,j ) log(1−h(x, ai,j ))+λkxk2 , where we used the a9a LIBSVM dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "LIBSVM: a library for support vector machines",
          "justification": "The source of the a9a dataset is from LIBSVM, as referenced in the paper.",
          "quote": "In particular, we consider the standard logistic regression model with `2 -regularization fi,j (x) = −yi,j log(h(x, ai,j ))−(1−yi,j ) log(1−h(x, ai,j ))+λkxk2 , where we used the a9a LIBSVM dataset (Chang & Lin, 2011)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch was used for implementing the numerical experiments in the paper.",
          "quote": "Our implementation is based on PyTorch (Paszke et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The referenced paper provides documentation of the PyTorch library used in the experiments.",
          "quote": "Our implementation is based on PyTorch (Paszke et al., 2019)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1164,
    "prompt_tokens": 46434,
    "total_tokens": 47598
  }
}