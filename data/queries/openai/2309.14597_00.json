{
  "paper": "2309.14597.txt",
  "words": 11624,
  "extractions": {
    "title": {
      "value": "Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control",
      "justification": "This is the title of the paper.",
      "quote": "Policy Optimization in a Noisy Neighborhood: On Return Landscapes in Continuous Control"
    },
    "description": "This paper studies the performance instability of deep reinforcement learning agents for continuous control by analyzing the return landscape, which is the mapping from policy parameters to returns. The authors investigate how a single update to policy parameters can lead to a wide range of returns and propose a distribution-aware procedure to improve policy robustness by navigating away from noisy neighborhoods.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental studies and results to support its analysis and conclusions, making it empirical.",
      "quote": "Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time.\nIn this work, we provide a fresh perspective on these behaviors by studying the return landscape: the mapping\nbetween a policy and a return."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper is focused on studying the return landscape and performance stability of deep reinforcement learning agents.",
        "quote": "Deep reinforcement learning agents for continuous control are known to exhibit significant instability in their performance over time."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Continuous Control",
          "justification": "The paper explicitly discusses and experiments with continuous control environments.",
          "quote": "The problem is particularly acute in continuous control, where these variations make it difficult to compare the end product of different algorithms or implementations of the same algorithm."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Soft Actor-Critic",
          "justification": "The paper investigates the return landscapes for policies discovered by the Soft Actor-Critic algorithm.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "aliases": [
          "SAC"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is not introduced for the first time in this paper; it is used for investigation.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses the SAC model to study the return landscape in continuous control environments.",
          "quote": "We will use the policies discovered by popular algorithms to characterize the topology\nof the return landscape. We focus on policy-based deep reinforcement learning algorithms for\ncontinuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and\nPPO [43] which use neural network function approximators to represent the policy."
        },
        "is_compared": {
          "value": true,
          "justification": "The SAC model is compared to other models like TD3 and PPO in terms of the return landscape and policy stability.",
          "quote": "We will use the policies discovered by popular algorithms to characterize the topology\nof the return landscape. We focus on policy-based deep reinforcement learning algorithms for\ncontinuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and\nPPO [43] which use neural network function approximators to represent the policy."
        },
        "referenced_paper_title": {
          "value": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
          "justification": "This is the reference paper for the SAC model, as cited in the current study.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        }
      },
      {
        "name": {
          "value": "Twin-Delayed DDPG",
          "justification": "The paper investigates the return landscapes for policies discovered by the Twin-Delayed DDPG algorithm.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "aliases": [
          "TD3"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is not introduced for the first time in this paper; it is used for investigation.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses the TD3 model to study the return landscape in continuous control environments.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "is_compared": {
          "value": true,
          "justification": "The TD3 model is compared to other models like SAC and PPO in terms of the return landscape and policy stability.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "referenced_paper_title": {
          "value": "Addressing Function Approximation Error in Actor-Critic Methods",
          "justification": "This is the reference paper for the TD3 model, as cited in the current study.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        }
      },
      {
        "name": {
          "value": "Proximal Policy Optimization",
          "justification": "The paper investigates the return landscapes for policies discovered by the Proximal Policy Optimization algorithm.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "aliases": [
          "PPO"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is not introduced for the first time in this paper; it is used for investigation.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses the PPO model to study the return landscape in continuous control environments.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "is_compared": {
          "value": true,
          "justification": "The PPO model is compared to other models like SAC and TD3 in terms of the return landscape and policy stability.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        },
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "This is the reference paper for the PPO model, as cited in the current study.",
          "quote": "We focus on policy-based deep reinforcement learning algorithms for continuous control, such as Soft Actor-Critic (SAC) [19], Twin-Delayed DDPG (TD3), and PPO [43] which use neural network function approximators to represent the policy."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "The JAX library is mentioned in the context of building the reinforcement learning models for the study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "JAX: composable transformations of Python+NumPy programs",
          "justification": "This is the reference paper for the JAX library, as cited in the current study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        }
      },
      {
        "name": {
          "value": "Jupyter",
          "justification": "The Jupyter library is mentioned as part of the tools used for developing and running the experiments in the study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Jupyter Notebooks - a publishing format for reproducible computational workflows",
          "justification": "This is the reference paper for the Jupyter library, as cited in the current study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        }
      },
      {
        "name": {
          "value": "Matplotlib",
          "justification": "The Matplotlib library is mentioned as part of the tools used for visualizing the data and results in the study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Matplotlib: A 2D Graphics Environment",
          "justification": "This is the reference paper for the Matplotlib library, as cited in the current study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        }
      },
      {
        "name": {
          "value": "numpy",
          "justification": "The numpy library is mentioned as part of the tools used for numerical computations in the study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The NumPy Array: A Structure for Efficient Numerical Computation",
          "justification": "This is the reference paper for the numpy library, as cited in the current study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        }
      },
      {
        "name": {
          "value": "pandas",
          "justification": "The pandas library is mentioned as part of the tools used for data manipulation in the study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Data Structures for Statistical Computing in Python",
          "justification": "This is the reference paper for the pandas library, as cited in the current study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        }
      },
      {
        "name": {
          "value": "SciPy",
          "justification": "The SciPy library is mentioned as part of the tools used for scientific computing in the study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SciPy: Open source scientific tools for Python",
          "justification": "This is the reference paper for the SciPy library, as cited in the current study.",
          "quote": "We acknowledge the Python community [34, 47] for developing the core set of tools that enabled this work, including JAX [4, 8], Jupyter [26], Matplotlib [22], numpy [33, 46], pandas [36, 48], and\nSciPy [24]. Our implementations of TD3 and SAC are based on jaxrl [27] and our implementation\nof PPO is based on CleanRL [21]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3356,
    "prompt_tokens": 20480,
    "total_tokens": 23836
  }
}