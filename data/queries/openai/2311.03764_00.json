{
  "paper": "2311.03764.txt",
  "words": 3616,
  "extractions": {
    "title": {
      "value": "NEURO-GPT: TOWARDS A FOUNDATION MODEL FOR EEG",
      "justification": "The paper clearly states this title in the header.",
      "quote": "NEURO-GPT: TOWARDS A FOUNDATION MODEL FOR EEG"
    },
    "description": "This paper introduces Neuro-GPT, a foundation model combining an EEG encoder with a GPT model, aimed at improving classification performance in Brain-Computer Interface (BCI) tasks by leveraging large-scale pre-training and fine-tuning strategies.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experimental validation of the proposed Neuro-GPT model using pre-training and fine-tuning on datasets, and evaluates its performance through empirical results.",
      "quote": "Our experiments demonstrate that applying a foundation model can significantly improve classification performance compared to a model trained from scratch"
    },
    "primary_research_field": {
      "name": {
        "value": "Brain-Computer Interfaces",
        "justification": "The paper's main focus is on improving EEG-based Brain-Computer Interface tasks using the Neuro-GPT model.",
        "quote": "To handle the scarcity and heterogeneity of electroencephalography (EEG) data for Brain-Computer Interface (BCI) tasks, and to harness the power of large publicly available data sets, we propose Neuro-GPT"
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The foundation model Neuro-GPT is pre-trained using a self-supervised task that reconstructs masked EEG segments.",
          "quote": "The foundation model is pre-trained on a large-scale data set using a self-supervised task that learns how to reconstruct masked EEG segments"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Transformers",
          "justification": "The paper employs a Generative Pre-trained Transformer (GPT) model as part of its Neuro-GPT architecture.",
          "quote": "We employ a Generative Pre-trained Transformer (GPT) model [10], which uses a decoder-only transformer architecture"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Neuro-GPT",
          "justification": "The proposed foundation model described in the paper is named Neuro-GPT and combines an EEG encoder with a GPT model.",
          "quote": "we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model"
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The paper's primary contribution is the Neuro-GPT model.",
          "quote": "we propose Neuro-GPT, a foundation model consisting of an EEG encoder and a GPT model"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper describes experiments where the Neuro-GPT model is executed to validate its performance.",
          "quote": "Our experiments demonstrate that applying a foundation model can significantly improve classification performance compared to a model trained from scratch"
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of Neuro-GPT with other models, including BENDR.",
          "quote": "we compare the proposed Neuro-GPT with BENDR [8], a BERT-inspired transformer model trained on TUH EEG data using contrastive self-supervised learning"
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper for Neuro-GPT as it is the primary contribution of this paper.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BENDR",
          "justification": "BENDR is used as a baseline model for comparison in the Neuro-GPT paper.",
          "quote": "we compare the proposed Neuro-GPT with BENDR [8], a BERT-inspired transformer model trained on TUH EEG data using contrastive self-supervised learning"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "BENDR is not a contribution of this paper but is used for comparison.",
          "quote": "BENDR [8], a BERT-inspired transformer model trained on TUH EEG data using contrastive self-supervised learning"
        },
        "is_executed": {
          "value": 0,
          "justification": "The Neuro-GPT paper does not execute BENDR but references its performance for comparison.",
          "quote": "As shown in Table 1, Neuro-GPT significantly improved the classification performance compared with the best performance of BENDR"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Neuro-GPT is compared with that of BENDR in the paper.",
          "quote": "we compare the proposed Neuro-GPT with BENDR [8], a BERT-inspired transformer model trained on TUH EEG data using contrastive self-supervised learning"
        },
        "referenced_paper_title": {
          "value": "BENDR: using transformers and a contrastive self-supervised learning task to learn from massive amounts of EEG data",
          "justification": "The referenced paper for the BENDR model is titled 'BENDR: using transformers and a contrastive self-supervised learning task to learn from massive amounts of EEG data'.",
          "quote": "BENDR: using transformers and a contrastive self-supervised learning task to learn from massive amounts of EEG data"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Temple University Hospital (TUH) EEG Corpus",
          "justification": "The TUH EEG corpus is used as the pre-training dataset for the foundation model Neuro-GPT.",
          "quote": "The foundation model is pre-trained on the TUH EEG dataset [11]"
        },
        "aliases": [
          "TUH EEG dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The temple university hospital eeg data corpus",
          "justification": "The referenced paper for the TUH EEG corpus is titled 'The temple university hospital eeg data corpus'.",
          "quote": "The temple university hospital eeg data corpus"
        }
      },
      {
        "name": {
          "value": "BCI Competition IV Dataset 2a",
          "justification": "The BCI Competition IV Dataset 2a is used for the downstream motor imagery classification task to fine-tune the Neuro-GPT model.",
          "quote": "using the BCI Competition IV Dataset 2a provided by Graz University of Technology [17]"
        },
        "aliases": [
          "BCI 2a dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Bci competition 2008–graz data set a",
          "justification": "The referenced paper for the BCI Competition IV Dataset 2a is titled 'Bci competition 2008–graz data set a'.",
          "quote": "Bci competition 2008–graz data set a"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face Transformers",
          "justification": "The Neuro-GPT implementation uses the open-source GPT-2 model provided by Hugging Face.",
          "quote": "We employ the open-source GPT-2 [10] model provided by Hugging Face [16]"
        },
        "aliases": [
          "Hugging Face",
          "Hugging Face GPT-2"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Huggingface’s transformers: State-of-the-art natural language processing",
          "justification": "The referenced paper for the Hugging Face Transformers library is titled 'Huggingface’s transformers: State-of-the-art natural language processing'.",
          "quote": "Huggingface’s transformers: State-of-the-art natural language processing"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1428,
    "prompt_tokens": 6899,
    "total_tokens": 8327
  }
}