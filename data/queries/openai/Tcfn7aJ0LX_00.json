{
  "paper": "Tcfn7aJ0LX.txt",
  "words": 11495,
  "extractions": {
    "title": {
      "value": "Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks",
      "justification": "Title of the paper is explicitly mentioned at the top of the document.",
      "quote": "Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks"
    },
    "description": "This paper explores the effectiveness of auxiliary tasks in improving the representations learned by deep reinforcement learning agents. The authors introduce a new family of auxiliary tasks based on the successor measure and develop a representation learning algorithm called Proto-Value Networks (PVNs). Through experiments on the Arcade Learning Environment, they demonstrate that PVNs facilitate linear value approximations comparable to established algorithms with fewer interactions with the environment's reward function.",
    "type": {
      "value": "Empirical study",
      "justification": "The study conducts various experiments in the Arcade Learning Environment to empirically demonstrate the effectiveness of Proto-Value Networks in learning representations using auxiliary tasks.",
      "quote": "Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms, using only linear approximation and a small number (~4M) of interactions with the environment’s reward function."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Reinforcement Learning",
        "justification": "The primary focus of the paper is on improving state representations in deep reinforcement learning using auxiliary tasks.",
        "quote": "In deep reinforcement learning (RL), an agent maps observations to a policy or return prediction by means of a neural network."
      },
      "aliases": [
        "Deep RL",
        "DRL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper focuses on improving representation learning with auxiliary tasks in deep reinforcement learning settings.",
          "quote": "Auxiliary tasks improve the representations learned by deep reinforcement learning agents."
        },
        "aliases": [
          "Representation Learning in DL"
        ]
      },
      {
        "name": {
          "value": "Successor Representation",
          "justification": "The auxiliary tasks introduced are based on the successor measure, extending the concept of the successor representation.",
          "quote": "Our approach is to construct a family of auxiliary rewards that can be sampled and subsequently. Specifically, we implement the successor measure (Blier et al., 2021; Touati & Ollivier, 2021), which extends the successor representation (Dayan, 1993) by replacing state-equality with set-inclusion."
        },
        "aliases": [
          "SR"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Proto-Value Networks (PVNs)",
          "justification": "Proto-Value Networks (PVNs) is the primary model proposed in the paper for improving representation learning in deep reinforcement learning.",
          "quote": "Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extending Mahadevan & Maggioni (2007)’s proto-value functions to deep reinforcement learning – accordingly, we call the resulting object proto-value networks."
        },
        "aliases": [
          "PVNs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "PVNs are introduced and proposed as a novel contribution in this paper.",
          "quote": "we call the resulting object proto-value networks."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper states that the experiments, including the implementation of PVNs, are conducted in the Arcade Learning Environment, which typically requires GPU for deep learning models.",
          "quote": "Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features."
        },
        "is_compared": {
          "value": 1,
          "justification": "PVNs are compared with various baseline methods like DQN, Behavior Cloning, and others in the experiments.",
          "quote": "We compare against the following pre-training baselines:..."
        },
        "referenced_paper_title": {
          "value": "Proto-Value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes",
          "justification": "The foundational work that PVNs extend is 'Proto-Value Functions' by Mahadevan & Maggioni (2007).",
          "quote": "extending Mahadevan & Maggioni (2007)’s proto-value functions to deep reinforcement learning"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Arcade Learning Environment (ALE)",
          "justification": "The dataset used for evaluating the effectiveness of PVNs in the experiments is the Arcade Learning Environment (ALE).",
          "quote": "We study the effectiveness of this method on the Arcade Learning Environment (ALE) (Bellemare et al., 2013)."
        },
        "aliases": [
          "ALE"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
          "justification": "The reference for ALE is the paper by Bellemare et al. (2013).",
          "quote": "We study the effectiveness of this method on the Arcade Learning Environment (ALE) (Bellemare et al., 2013)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Acme Library",
          "justification": "The Acme library is explicitly mentioned as being used for implementing the RL agents in the paper.",
          "quote": "Our agents are implemented using the Acme library (Hoffman et al., 2020)."
        },
        "aliases": [
          "Acme"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Acme: A Research Framework for Distributed Reinforcement Learning",
          "justification": "The referential paper for the Acme library is cited as Hoffman et al. (2020).",
          "quote": "Our agents are implemented using the Acme library (Hoffman et al., 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1085,
    "prompt_tokens": 20578,
    "total_tokens": 21663
  }
}