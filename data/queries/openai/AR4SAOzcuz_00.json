{
  "paper": "AR4SAOzcuz.txt",
  "words": 3882,
  "extractions": {
    "title": {
      "value": "Semi-Supervised Object Detection for Agriculture",
      "justification": "The title of the paper as provided by the user.",
      "quote": "Semi-Supervised Object Detection for Agriculture"
    },
    "description": "The paper addresses the challenges of applying semi-supervised object detection (SSOD) methods in the agricultural domain. It introduces a new dataset called smallSSD, proposes the Calibrated Teacher-Student Learning method, and provides Python packages to facilitate further research and application in SSOD for agriculture.",
    "type": {
      "value": "Empirical",
      "justification": "The paper introduces new datasets, methods, and experimental results, which implies it focuses on practical implementations and empirical evaluations.",
      "quote": "We introduce the smallSSD dataset – a semi-supervised object detection dataset for agriculture... We present a Python package to more easily test SSOD methods... Finally, we introduce two components to the standard SSOD pipeline which demonstrably improve performance on our dataset."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on object detection, which is a subfield of Computer Vision.",
        "quote": "Semi-Supervised Object-Detection consists of learning to identify objects from a training set consisting of labeled and unlabeled images."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Semi-Supervised Learning",
          "justification": "The paper leverages semi-supervised learning techniques in the context of object detection for agriculture.",
          "quote": "Semi-Supervised Object-Detection consists of learning to identify objects from a training set consisting of labelled and unlabelled images."
        },
        "aliases": [
          "SSL",
          "Semi-Supervised"
        ]
      },
      {
        "name": {
          "value": "Agricultural Data Analysis",
          "justification": "The methods and datasets used are specific to agricultural contexts.",
          "quote": "Current SSOD methods have been designed specifically for Flickr-based datasets and may not be appropriate for the unique challenges encountered in agricultural contexts, limiting their usefulness in practice."
        },
        "aliases": [
          "Agriculture",
          "Agri"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Faster R-CNN",
          "justification": "The model is tested as a baseline and within the proposed methodology.",
          "quote": "We evaluate our proposed algorithm by testing whether it improves the best-performing baseline model – Faster RCNN."
        },
        "aliases": [
          "FasterRCNN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Faster R-CNN is not introduced in this paper; it is used as a baseline model for comparison.",
          "quote": "For all models except YOLOv4 we fine-tune models pre-trained on ImageNet."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experimental evaluation.",
          "quote": "We evaluate our proposed algorithm by testing whether it improves the best-performing baseline model – Faster RCNN."
        },
        "is_compared": {
          "value": 1,
          "justification": "Faster R-CNN is compared numerically to other models in the experimental section.",
          "quote": "The results of the fully-supervised models are shown in Table 1."
        },
        "referenced_paper_title": {
          "value": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
          "justification": "This is the original paper introducing Faster R-CNN.",
          "quote": "For all models except YOLOv4 we fine-tune models pre-trained on ImageNet."
        }
      },
      {
        "name": {
          "value": "YOLOv4",
          "justification": "This model is used and compared in the experimental results.",
          "quote": "For YOLOv4 we used a CSPDarknet53 backbone pre-trained on COCO."
        },
        "aliases": [
          "YOLO"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "YOLOv4 is not introduced in this paper; it is used as a baseline model for comparison.",
          "quote": "For YOLOv4 we used a CSPDarknet53 backbone pre-trained on COCO."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experimental evaluation.",
          "quote": "For YOLOv4 we used a CSPDarknet53 backbone pre-trained on COCO."
        },
        "is_compared": {
          "value": 1,
          "justification": "YOLOv4 is compared numerically to other models in the paper.",
          "quote": "The results of the fully-supervised models are shown in Table 1."
        },
        "referenced_paper_title": {
          "value": "YOLOv4: Optimal Speed and Accuracy of Object Detection",
          "justification": "This is the original paper introducing YOLOv4.",
          "quote": "For YOLOv4 we used a CSPDarknet53 backbone pre-trained on COCO."
        }
      },
      {
        "name": {
          "value": "SSD",
          "justification": "SSD is one of the baseline models used in the experiments.",
          "quote": "We run the algorithm without the calibrated threshold (“T”) and without an ensembled teacher (“E”), reporting the % change between the fully and semi-supervised methods without these components. When we do not calibrate the threshold, we use a fixed threshold of 0.7 as in Liu et al. (2021). We find that while each component in isolation yields an improvement over the baseline it is the combination of both components which leads to the overall improvement."
        },
        "aliases": [
          "SSD"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SSD is not introduced in this paper; it is used as a baseline model for comparison.",
          "quote": "We evaluate a variety of model architectures, encompassing both single-stage and multi-stage object detectors."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experimental evaluation.",
          "quote": "We evaluate a variety of model architectures, encompassing both single-stage and multi-stage object detectors."
        },
        "is_compared": {
          "value": 1,
          "justification": "SSD is compared numerically to other models in the paper.",
          "quote": "We evaluate a variety of model architectures, encompassing both single-stage and multi-stage object detectors."
        },
        "referenced_paper_title": {
          "value": "SSD: Single Shot MultiBox Detector",
          "justification": "This is the original paper introducing SSD.",
          "quote": "We evaluate a variety of model architectures, encompassing both single-stage and multi-stage object detectors."
        }
      },
      {
        "name": {
          "value": "RetinaNet",
          "justification": "RetinaNet is one of the models used in the experiments.",
          "quote": "As baselines, we evaluate a variety of model architectures, encompassing both single-stage and multi-stage object detectors (described in Table 1). For all models except YOLOv4 (Bochkovskiy, Wang, and Liao 2020) we fine-tune models pre-trained on ImageNet (Deng et al. 2009a) and implemented in the torchvision framework."
        },
        "aliases": [
          "RetinaNet"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "RetinaNet is not introduced in this paper; it is used as a baseline model for comparison.",
          "quote": "As baselines, we evaluate a variety of model architectures, encompassing both single-stage and multi-stage object detectors."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed as part of the experimental evaluation.",
          "quote": "We evaluate a variety of model architectures, encompassing both single-stage and multi-stage object detectors."
        },
        "is_compared": {
          "value": 1,
          "justification": "RetinaNet is compared numerically to other models in the paper.",
          "quote": "As baselines, we evaluate a variety of model architectures, encompassing both single-stage and multi-stage object detectors."
        },
        "referenced_paper_title": {
          "value": "Focal Loss for Dense Object Detection",
          "justification": "This is the original paper introducing RetinaNet.",
          "quote": "For all models except YOLOv4 (Bochkovskiy, Wang, and Liao 2020) we fine-tune models pre-trained on ImageNet (Deng et al. 2009a) and implemented in the torchvision framework."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "smallSSD",
          "justification": "smallSSD is the primary dataset introduced and used in this paper.",
          "quote": "The small size of labelled object detection datasets in agricultural contexts make SSOD methods tailored for agricultural contexts especially important to investigate. The main contributions of this paper are: • The introduction of the smallSSD dataset, a semi-supervised object detection dataset for agriculture consisting of over 100,000 images."
        },
        "aliases": [
          "smallSSD"
        ],
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The smallSSD dataset is introduced in this paper and does not refer to an existing study for its introduction.",
          "quote": "We introduce the smallSSD dataset1 – a semi-supervised object detection dataset for agriculture."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The PyTorch library is used for the implementation of the dataset and models.",
          "quote": "Returning datasets in a torchvision compatible format means all torchvision tools (e.g. for visualization) can be leveraged."
        },
        "aliases": [
          "torch",
          "torchvision"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the standard reference for the PyTorch library, which is used in the paper.",
          "quote": "Returning datasets in a torchvision compatible format means all torchvision tools (e.g. for visualization) can be leveraged."
        }
      },
      {
        "name": {
          "value": "PyTorch-Lightning",
          "justification": "PyTorch-Lightning is used for ease of model training and reproducibility.",
          "quote": "We additionally introduce smallteacher, a pip installable python package built on top of PyTorch-Lightning to encourage easily-adoptable research in real-world semi-supervised object detection."
        },
        "aliases": [
          "PyTorch Lightning"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The paper itself introduces the use of PyTorch-Lightning in conjunction with the other tools and datasets but doesn't refer to another paper for it.",
          "quote": "We additionally introduce smallteacher, a pip installable python package built on top of PyTorch-Lightning to encourage easily-adoptable research in real-world semi-supervised object detection."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "Models pre-trained on ImageNet are fine-tuned for the study.",
          "quote": "For all models except YOLOv4 we fine-tune models pre-trained on ImageNet."
        },
        "aliases": [
          "ImageNet"
        ],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "This is the standard reference for the ImageNet dataset used for model pre-training.",
          "quote": "Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei, L. 2009a. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition (CVPR)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2184,
    "prompt_tokens": 7947,
    "total_tokens": 10131
  }
}