{
  "paper": "ZOOwHgxfR4.txt",
  "words": 12560,
  "extractions": {
    "title": {
      "value": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts",
      "justification": "The title of the paper is mentioned at the top of the document.",
      "quote": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts"
    },
    "description": "The paper presents ProtST, a framework aimed at enhancing Protein Sequence pre-training and understanding by incorporating biomedical texts. It introduces the ProtDescribe dataset, which pairs protein sequences with textual property descriptions, and proposes three types of tasks for pre-training: unimodal mask prediction, multimodal representation alignment, and multimodal mask prediction.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper involves experiments and evaluations of the proposed ProtST framework using various protein-related tasks and benchmarks.",
      "quote": "We investigate the PLMs trained under ProtST by representation learning and zero-shot prediction."
    },
    "primary_research_field": {
      "name": {
        "value": "Bioinformatics",
        "justification": "The paper focuses on the application of deep learning to protein sequences and biomedical texts, which falls under bioinformatics.",
        "quote": "Proteins serve as the mainstay governing diverse biological processes and life itself, inducing important applications in drug discovery and healthcare."
      },
      "aliases": [
        "Bioinformatics"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The incorporation of biomedical texts into protein sequence representation learning involves Natural Language Processing techniques.",
          "quote": "Compared to the texts from general domains like newswire and Web, biomedical texts differ a lot in terms of vocabulary and expressions."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Computational Biology",
          "justification": "The work involves computational methods to understand and predict biological properties and functions of proteins.",
          "quote": "Based on this dataset, we propose the ProtST framework to enhance protein sequence pre-training and understanding by biomedical texts."
        },
        "aliases": [
          "CompBio"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ProtST",
          "justification": "ProtST is the main model proposed in the paper for the purpose of enhancing protein sequence pre-training and understanding through the use of biomedical texts.",
          "quote": "Based on this dataset, we propose the ProtST framework to enhance protein sequence pre-training and understanding by biomedical texts."
        },
        "aliases": [
          "ProtST"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "ProtST is introduced as a new model in this paper.",
          "quote": "we propose the ProtST framework to enhance protein sequence pre-training and understanding by biomedical texts."
        },
        "is_executed": {
          "value": 1,
          "justification": "The ProtST model was executed using computational resources, specifically GPUs, for training and evaluation.",
          "quote": "Training Configurations: An Adam optimizer (Kingma & Ba, 2014) (learning rate: 1.0 × 10−5 , weight decay: 0) is used to train the whole model for 20 epochs on 4 Tesla V100 GPUs."
        },
        "is_compared": {
          "value": 1,
          "justification": "ProtST is compared to other existing models including OntoProtein, ProtBert, ESM-1b, and ESM-2 across various benchmarks.",
          "quote": "We compare the proposed ProtST-ESM-1b with four performant PLMs, i.e., ESM-1b (Rives et al., 2021), ESM-1v (Meier et al., 2021), Tranception L (w/o retrieval) (Notin et al., 2022) and Progen2 XL (Nijkamp et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "ProtST is the primary contribution of this paper, hence it is not referenced from another paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "ProtBert",
          "justification": "ProtBert is used as one of the baseline models for comparison and as part of the larger ProtST framework.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines and seek to enhance their representation power by modeling biomedical texts at the same time as protein sequence modeling."
        },
        "aliases": [
          "ProtBert"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ProtBert is an existing model referenced in the paper.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines."
        },
        "is_executed": {
          "value": 1,
          "justification": "ProtBert was executed as part of the experiments conducted in the paper.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines and seek to enhance their representation power by modeling biomedical texts at the same time as protein sequence modeling."
        },
        "is_compared": {
          "value": 1,
          "justification": "ProtBert serves as a baseline model against which the performance of the ProtST-enhanced models is compared.",
          "quote": "We compare the proposed ProtST-ESM-1b with four performant PLMs, i.e., ESM-1b (Rives et al., 2021), ESM-1v (Meier et al., 2021), Tranception L (w/o retrieval) (Notin et al., 2022), and Progen2 XL (Nijkamp et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "ProtTrans: towards cracking the language of life’s code through self-supervised deep learning and high performance computing",
          "justification": "The referenced paper title is provided in the context of introducing ProtBert.",
          "quote": "ProtBert (Elnaggar et al., 2020)"
        }
      },
      {
        "name": {
          "value": "ESM-1b",
          "justification": "ESM-1b is used as one of the baseline models for comparison and as part of the ProtST-enhanced models.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines and seek to enhance their representation power by modeling biomedical texts at the same time as protein sequence modeling."
        },
        "aliases": [
          "ESM-1b"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ESM-1b is an existing model referenced in the paper.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines."
        },
        "is_executed": {
          "value": 1,
          "justification": "ESM-1b was executed as part of the experiments conducted in the paper.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines and seek to enhance their representation power by modeling biomedical texts at the same time as protein sequence modeling."
        },
        "is_compared": {
          "value": 1,
          "justification": "ESM-1b serves as a baseline model against which the performance of the ProtST-enhanced models is compared.",
          "quote": "We compare the proposed ProtST-ESM-1b with four performant PLMs, i.e., ESM-1b (Rives et al., 2021), ESM-1v (Meier et al., 2021), Tranception L (w/o retrieval) (Notin et al., 2022), and Progen2 XL (Nijkamp et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences",
          "justification": "The referenced paper title is provided in the context of introducing ESM-1b.",
          "quote": "ESM-1b (Rives et al., 2021)"
        }
      },
      {
        "name": {
          "value": "ESM-2",
          "justification": "ESM-2 is used as one of the baseline models for comparison and as part of the ProtST-enhanced models.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines and seek to enhance their representation power by modeling biomedical texts at the same time as protein sequence modeling."
        },
        "aliases": [
          "ESM-2"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ESM-2 is an existing model referenced in the paper.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines."
        },
        "is_executed": {
          "value": 1,
          "justification": "ESM-2 was executed as part of the experiments conducted in the paper.",
          "quote": "In this work, we select three state-of-the-art PLMs, ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), as baselines and seek to enhance their representation power by modeling biomedical texts at the same time as protein sequence modeling."
        },
        "is_compared": {
          "value": 1,
          "justification": "ESM-2 serves as a baseline model against which the performance of the ProtST-enhanced models is compared.",
          "quote": "We compare the proposed ProtST-ESM-1b with four performant PLMs, i.e., ESM-1b (Rives et al., 2021), ESM-1v (Meier et al., 2021), Tranception L (w/o retrieval) (Notin et al., 2022), and Progen2 XL (Nijkamp et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "Language models of protein sequences at the scale of evolution enable accurate structure prediction",
          "justification": "The referenced paper title is provided in the context of introducing ESM-2.",
          "quote": "ESM-2 (Lin et al., 2022)"
        }
      },
      {
        "name": {
          "value": "PubMedBERT",
          "justification": "PubMedBERT is a biomedical language model employed by the ProtST framework to represent the biomedical text descriptions of proteins.",
          "quote": "In this work, we employ a performant biomedical language model, PubMedBERT (Gu et al., 2021), to represent the biomedical text descriptions of proteins."
        },
        "aliases": [
          "PubMedBERT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "PubMedBERT is an existing model referenced in the paper.",
          "quote": "In this work, we employ a performant biomedical language model, PubMedBERT (Gu et al., 2021), to represent the biomedical text descriptions of proteins."
        },
        "is_executed": {
          "value": 1,
          "justification": "PubMedBERT was executed as part of the ProtST framework to process the biomedical text descriptions.",
          "quote": "To start this process with decent representations of protein sequences and biomedical texts, we use pre-trained PLM (i.e., ProtBert (Elnaggar et al., 2020), ESM-1b (Rives et al., 2021) or ESM-2 (Lin et al., 2022)) and pre-trained BLM (i.e., PubMedBERT (Gu et al., 2021)) for initialization."
        },
        "is_compared": {
          "value": 0,
          "justification": "PubMedBERT is not directly compared to other models in terms of performance; it is used within the ProtST framework.",
          "quote": "N/A"
        },
        "referenced_paper_title": {
          "value": "Domain-specific language model pretraining for biomedical natural language processing",
          "justification": "The referenced paper title is provided in the context of introducing PubMedBERT.",
          "quote": "PubMedBERT (Gu et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ProtDescribe",
          "justification": "The ProtDescribe dataset is introduced in this paper as a paired dataset of protein sequences and textual property descriptions.",
          "quote": "we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties."
        },
        "aliases": [
          "ProtDescribe"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "ProtDescribe is contributed in this paper and not referenced from another paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Swiss-Prot",
          "justification": "Swiss-Prot is referenced as the source of high-quality protein annotations used to construct the ProtDescribe dataset.",
          "quote": "We resort to the Swiss-Prot database (Bairoch & Apweiler, 2000) for high-quality protein annotations."
        },
        "aliases": [
          "Swiss-Prot"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The Swiss-Prot protein sequence database and its supplement TrEMBL in 2000",
          "justification": "The referenced paper title is provided in the context of mentioning the source of protein annotations.",
          "quote": "the Swiss-Prot (Bairoch & Apweiler, 2000) database"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3439,
    "prompt_tokens": 24314,
    "total_tokens": 27753
  }
}