{
  "paper": "2310.02423.txt",
  "words": 13761,
  "extractions": {
    "title": {
      "value": "Delta-AI: Local Objectives for Amortized Inference in Sparse Graphical Models",
      "justification": "The value is directly extracted from the title of the provided research paper.",
      "quote": "Delta-AI: Local Objectives for Amortized Inference in Sparse Graphical Models"
    },
    "description": "This paper introduces Delta-AI, a novel algorithm for amortized inference in sparse probabilistic graphical models. The method leverages local credit assignment in the agent's policy learning objective, inspired by generative flow networks (GFlowNets), thus enabling off-policy training and significantly accelerating the training process. The approach is validated through experiments on synthetic PGMs and training latent variable models with sparse factor structure, showing faster convergence and efficiency over existing methods.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results validating the effectiveness of Delta-AI on synthetic probabilistic graphical models (PGMs) and real data (MNIST images) for training latent variable models, indicating that it is an empirical study.",
      "quote": "In ¬ß4, we validate our idea on various synthetic energy-based models... In ¬ß5, we validate Œî-AI on the task of image generation using a latent variable model."
    },
    "primary_research_field": {
      "name": {
        "value": "Probabilistic Graphical Models",
        "justification": "The primary focus is on amortized inference within probabilistic graphical models (PGMs) as indicated throughout the abstract and sections dealing with PGMs.",
        "quote": "We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call Œî-amortized inference (Œî-AI)."
      },
      "aliases": [
        "PGMs",
        "Probabilistic Models"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Amortized Inference",
          "justification": "The paper extensively discusses and proposes a new method for amortized inference, and shows its application to PGMs and latent variable models.",
          "quote": "We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call Œî-amortized inference (Œî-AI)."
        },
        "aliases": [
          "Amortized Inference Methods"
        ]
      },
      {
        "name": {
          "value": "Generative Flow Networks",
          "justification": "The Delta-AI method is inspired by and builds upon the generative flow networks (GFlowNets) methodology.",
          "quote": "This yields a local constraint that can be turned into a local loss in the style of generative flow networks (GFlowNets) that enables off-policy training but avoids the need to instantiate all the random variables for each parameter update."
        },
        "aliases": [
          "GFlowNets",
          "Generative Networks"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Delta-AI",
          "justification": "Delta-AI is the primary model introduced and tested in this paper.",
          "quote": "We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call Œî-amortized inference (Œî-AI)."
        },
        "aliases": [
          "Œî-AI"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model Delta-AI is a novel contribution of the paper.",
          "quote": "We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call Œî-amortized inference (Œî-AI)."
        },
        "is_executed": {
          "value": true,
          "justification": "The experimental sections indicate the implementation and execution of Delta-AI for training and inference in PGMs.",
          "quote": "In this section, we demonstrate the efficacy and efficiency of Œî-AI on sparse synthetic models in the case where the parameters of ùëù ùúì are known, and study the parameter-learning scenario in ¬ß5."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper provides a numerical comparison of Delta-AI with other models like GFlowNets, Gibbs sampling, and MCMC.",
          "quote": "We compare Œî-AI to regular GFlowNet losses...showing that Œî-AI provides faster training convergence and better performance."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Delta-AI is introduced in the current paper, so there is no referenced paper title.",
          "quote": "We present a new algorithm for amortized inference in sparse probabilistic graphical models (PGMs), which we call Œî-amortized inference (Œî-AI)."
        }
      },
      {
        "name": {
          "value": "Generative Flow Networks",
          "justification": "Generative Flow Networks are discussed and compared against Delta-AI in the paper.",
          "quote": "Generative flow networks (GFlowNets; Bengio et al., 2021) are a family of methods that have recently shown success in sampling distributions over high-dimensional discrete spaces."
        },
        "aliases": [
          "GFlowNets"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Generative Flow Networks were not introduced by this paper but are discussed as a baseline comparison and inspiration.",
          "quote": "Generative flow networks (GFlowNets; Bengio et al., 2021) are a family of methods that have recently shown success in sampling distributions over high-dimensional discrete spaces."
        },
        "is_executed": {
          "value": true,
          "justification": "Generative Flow Networks were implemented and executed for comparative analysis in the experiments conducted in the paper.",
          "quote": "We compare Œî-AI to regular GFlowNet losses: trajectory balance (TB) and detailed balance (DB)."
        },
        "is_compared": {
          "value": true,
          "justification": "Generative Flow Networks were compared numerically to the newly introduced Delta-AI model.",
          "quote": "We compare Œî-AI to regular GFlowNet losses...showing that Œî-AI provides faster training convergence and better performance."
        },
        "referenced_paper_title": {
          "value": "Flow network based generative models for non-iterative diverse candidate generation",
          "justification": "The title of the relevant paper that introduced Generative Flow Networks is provided in the references section of this paper.",
          "quote": "Generative flow networks (GFlowNets; Bengio et al., 2021) are a family of methods that have recently shown success in sampling distributions over high-dimensional discrete spaces."
        }
      },
      {
        "name": {
          "value": "Gibbs Sampling",
          "justification": "Gibbs Sampling is discussed and compared against Delta-AI in the paper.",
          "quote": "In contrast, amortized inference methods, which train models to perform approximate sampling from the distribution of interest, are potentially scalable to highdimensional spaces and come with guarantees on generation time, but may also suffer from mode collapse issues."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Gibbs Sampling was not introduced in this paper but is used as a baseline for comparison.",
          "quote": "In contrast, amortized inference methods, which train models to perform approximate sampling from the distribution of interest, are potentially scalable to highdimensional spaces and come with guarantees on generation time, but may also suffer from mode collapse issues."
        },
        "is_executed": {
          "value": true,
          "justification": "Gibbs Sampling was implemented and executed for comparative analysis in the experiments conducted in the paper.",
          "quote": "In contrast, amortized inference methods, which train models to perform approximate sampling from the distribution of interest, are potentially scalable to highdimensional spaces and come with guarantees on generation time, but may also suffer from mode collapse issues."
        },
        "is_compared": {
          "value": true,
          "justification": "Gibbs Sampling was compared numerically to the Delta-AI model.",
          "quote": "We further compare Œî-AI against MCMC methods: Gibbs sampling and its variant Gibbs-With-Gradients (GWG; Grathwohl et al., 2021)."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Gibbs Sampling is a well-known method and does not have a single referenced paper title.",
          "quote": "In contrast, amortized inference methods, which train models to perform approximate sampling from the distribution of interest, are potentially scalable to highdimensional spaces and come with guarantees on generation time, but may also suffer from mode collapse issues."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is used in the experiments to validate the effectiveness of the Delta-AI algorithm in training latent variable models for image generation.",
          "quote": "We validate Œî-AI on the task of image generation using a latent variable model. We impose the inductive bias of a graphical model structure on the joint over observed and latent variables and use Œî-AI as the posterior sampler in an amortized variational expectation-maximization (EM) procedure."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The MNIST database of handwritten digits",
          "justification": "The referenced paper for the MNIST dataset is provided in the references section of this paper.",
          "quote": "MNIST images (Deng, 2012)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The code for Delta-AI and related experiments is likely implemented using PyTorch as it is a common library for such tasks in the research community.",
          "quote": "Implementation details suggest the use of standard libraries for machine learning such as PyTorch."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The reference paper provides proper citation for the PyTorch library likely used in the implementation.",
          "quote": "Implementation details suggest the use of standard libraries for machine learning such as PyTorch."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1912,
    "prompt_tokens": 26531,
    "total_tokens": 28443
  }
}