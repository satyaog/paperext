{
  "paper": "2305.15249.txt",
  "words": 28158,
  "extractions": {
    "title": {
      "value": "Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees",
      "justification": "The title of the paper is clearly stated at the beginning.",
      "quote": "Decision-Aware Actor-Critic with Function Approximation and Theoretical Guarantees"
    },
    "description": "This paper presents a framework for actor-critic methods in reinforcement learning that co-trains the actor and critic using a joint decision-aware objective. The framework supports any policy and value function parametrization, and guarantees monotonic policy improvement under specific conditions. The framework is validated theoretically and empirically in simple RL environments.",
    "type": {
      "value": "theoretical study",
      "justification": "The focus of the paper is on developing and characterizing the framework theoretically, with some empirical validation as well.",
      "quote": "We explicitly characterize the conditions under which the resulting algorithm guarantees monotonic policy improvement, regardless of the choice of the policy and critic parameterization."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper is focused on actor-critic methods, which are a subset of reinforcement learning techniques.",
        "quote": "Actor-critic (AC) methods are widely used in reinforcement learning (RL)"
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Policy Gradient Methods",
          "justification": "The paper mentions policy gradient methods as a subclass of the techniques it builds upon.",
          "quote": "Policy gradient (PG) methods [59, 54, 29, 25, 47] are an important class of algorithms that can easily handle function approximation and structured state-action spaces"
        },
        "aliases": [
          "PG"
        ]
      },
      {
        "name": {
          "value": "Value-Based Reinforcement Learning",
          "justification": "The critic part of the actor-critic framework uses value-based reinforcement learning.",
          "quote": "Actor-critic (AC) methods [29, 43, 5] alleviate this issue by using value-based approaches [52, 58] in conjunction with PG methods"
        },
        "aliases": [
          "Value-Based RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Actor-Critic (AC)",
          "justification": "The paper proposes an actor-critic framework that it refers to as AC multiple times.",
          "quote": "we study a decision-aware AC method with function approximation and equipped with theoretical guarantees on its performance"
        },
        "aliases": [
          "AC"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper presents a new decision-aware actor-critic framework.",
          "quote": "we empirically demonstrate the benefit of our decision-aware actor-critic framework on simple RL problems"
        },
        "is_executed": {
          "value": 1,
          "justification": "The empirical results involve executing the model in simple RL environments.",
          "quote": "Finally, we empirically demonstrate the benefit of our decision-aware actor-critic framework on simple RL problems."
        },
        "is_compared": {
          "value": 1,
          "justification": "The framework is compared to both the standard actor-critic methods and advantage-weighted actor-critic methods empirically.",
          "quote": "comparison of decision-aware loss to MSE and advantage-weighted MSE"
        },
        "referenced_paper_title": {
          "value": "A general class of surrogate functions for stable and efficient reinforcement learning.",
          "justification": "The referenced paper is mentioned when discussing the actor update part of the new framework.",
          "quote": "The actor update involves optimizing a surrogate function that depends on the current policy, and consequently supports off-policy updates, i.e. similar to common PG methods such as TRPO [46], PPO [48], the algorithm can update the policy without requiring additional interactions with the environment. This property coupled with the use of a critic makes the resulting algorithm sample-efficient in practice. In contrast with TRPO/PPO, both the off-policy actor updates and critic updates in Algorithm 1 are designed to maximize the same lower bound on the policy return."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Cliff World",
          "justification": "The Cliff World environment is used for empirical evaluation of the framework.",
          "quote": "We demonstrate the benefit of the decision-aware framework over the standard AC algorithm where the critic is trained by minimizing the squared error. We instantiate Algorithm 1 for the direct and softmax representations, and evaluate the performance on two grid-world environments, namely Cliff World [53] and Frozen Lake [6] "
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Reinforcement Learning: An Introduction",
          "justification": "The referenced book by Sutton & Barto is commonly known to describe the Cliff World environment in RL literature.",
          "quote": "Cliff World [53]"
        }
      },
      {
        "name": {
          "value": "Frozen Lake",
          "justification": "The Frozen Lake environment is used for empirical evaluation of the framework.",
          "quote": "We demonstrate the benefit of the decision-aware framework over the standard AC algorithm where the critic is trained by minimizing the squared error. We instantiate Algorithm 1 for the direct and softmax representations, and evaluate the performance on two grid-world environments, namely Cliff World [53] and Frozen Lake [6]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "OpenAI Gym",
          "justification": "The reference for Frozen Lake often refers to the OpenAI Gym environments.",
          "quote": "Frozen Lake [6]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Gym",
          "justification": "The Gym library is used for the Frozen Lake environment.",
          "quote": "Frozen Lake [6] (see App. F for details)."
        },
        "aliases": [
          "OpenAI Gym"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "OpenAI Gym",
          "justification": "The reference for Frozen Lake often refers to the OpenAI Gym environments.",
          "quote": "Frozen Lake [6]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1125,
    "prompt_tokens": 55082,
    "total_tokens": 56207
  }
}