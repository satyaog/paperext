{
  "paper": "2311.11108.txt",
  "words": 12417,
  "extractions": {
    "title": {
      "value": "Auxiliary Losses for Learning Generalizable Concept-based Models",
      "justification": "This is the title of the paper found at the beginning.",
      "quote": "Auxiliary Losses for Learning Generalizable Concept-based Models"
    },
    "description": "This paper introduces the cooperative-Concept Bottleneck Model (coop-CBM) aimed at improving the performance of Concept Bottleneck Models (CBMs) without sacrificing interpretability. The authors propose a new auxiliary loss (concept orthogonal loss, COL) to enhance concept separation and reduce intra-concept distance. Extensive experiments are conducted on image classification tasks across different datasets and distributional shift settings.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves extensive experiments on real-world datasets to evaluate the effectiveness of the proposed models and methods.",
      "quote": "This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL."
    },
    "primary_research_field": {
      "name": {
        "value": "Explainable AI (XAI)",
        "justification": "The paper aims to enhance the interpretability of neural network models using Concept Bottleneck Models (CBMs) and auxiliary losses.",
        "quote": "As a solution to enhance model transparency, Concept Bottleneck Models (CBMs) have gained popularity since their introduction."
      },
      "aliases": [
        "XAI",
        "Explainable Artificial Intelligence"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Classification",
          "justification": "One of the primary tasks and experimental setups described in the paper involves image classification.",
          "quote": "This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Neural Network Interpretability",
          "justification": "The focus on Concept Bottleneck Models (CBMs) and concept orthogonal loss (COL) directly pertains to making neural networks more interpretable.",
          "quote": "CBMs essentially limit the latent space of a model to human-understandable high-level concepts."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "coop-CBM",
          "justification": "The cooperative-Concept Bottleneck Model (coop-CBM) is introduced as a new concept-based architecture and the primary contribution of the paper.",
          "quote": "To overcome the performance trade-off, we propose cooperative-Concept Bottleneck Model (coop-CBM)."
        },
        "aliases": [
          "cooperative-Concept Bottleneck Model"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The coop-CBM is explicitly introduced and tested in this paper as a novel contribution.",
          "quote": "In this work, we propose cooperative-CBM (coop-CBM) model."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments involving coop-CBM were performed, as described in the various results sections.",
          "quote": "Our proposed model coop-CBM improves the downstream task accuracy over black box standard models."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of coop-CBM is compared against other CBM variants and standard models.",
          "quote": "We compare the performance of coop-CBM against other baseline models."
        },
        "referenced_paper_title": {
          "value": "Not Applicable",
          "justification": "The coop-CBM is a new model introduced in this paper, hence there are no prior papers specifically for referencing it.",
          "quote": "We propose cooperative-CBM (coop-CBM) model."
        }
      },
      {
        "name": {
          "value": "Joint-CBM",
          "justification": "Joint-CBM is a baseline model described and compared against the newly proposed coop-CBM.",
          "quote": "Koh et al. [27] reports that joint CBMs have the highest task accuracy among the different CBM training procedures."
        },
        "aliases": [
          "Joint Concept Bottleneck Model"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Joint-CBM is previously established in the literature and used as a baseline in this paper.",
          "quote": "Koh et al. [27] reports that joint CBMs have the highest task accuracy among the different CBM training procedures."
        },
        "is_executed": {
          "value": 1,
          "justification": "Experiments involving Joint-CBM were executed and results are detailed in the paper.",
          "quote": "Our results show the robustness of coop-CBM and COL to background spurious correlations achieving state-of-art results among the concept-based models."
        },
        "is_compared": {
          "value": 1,
          "justification": "Joint-CBM is compared numerically to other models, including the newly proposed coop-CBM.",
          "quote": "Our results show the robustness of coop-CBM and COL to background spurious correlations achieving state-of-art results among the concept-based models."
        },
        "referenced_paper_title": {
          "value": "Concept bottleneck models",
          "justification": "The original paper where Joint-CBM was introduced is cited as Koh et al. [27].",
          "quote": "Koh et al. [27] reports that joint CBMs have the highest task accuracy among the different CBM training procedures."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CUB",
          "justification": "CUB is one of the primary datasets used in the experiments for image classification.",
          "quote": "This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL."
        },
        "aliases": [
          "Caltech-UCSD Birds-200-2011"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Caltech-ucsd birds 200",
          "justification": "The dataset is referred to with its official title in the experiments.",
          "quote": "We use Caltech-UCSD Birds-200-2011 (CUB) [55] dataset for the task of bird identification."
        }
      },
      {
        "name": {
          "value": "AwA2",
          "justification": "AwA2 is one of the primary datasets used in the experiments for image classification.",
          "quote": "This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL."
        },
        "aliases": [
          "Animals with Attributes 2"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Zero-shot learningâ€”a comprehensive evaluation of the good, the bad and the ugly",
          "justification": "The dataset is referred to with its official title in the experiments.",
          "quote": "Additionally, we compare our performance with recent concept-based models that are built on CBMs to either improve the downstream task accuracy or mitigate the concept leakage. We additionally use Animals with Attributes 2 (AwA2) dataset for the task of animal classification."
        }
      },
      {
        "name": {
          "value": "CelebA",
          "justification": "CelebA is one of the primary datasets used in the experiments for image classification.",
          "quote": "This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL."
        },
        "aliases": [
          "CelebFaces Attributes"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Large-scale celebfaces attributes (celeba) dataset",
          "justification": "The dataset is referred to with its official title in the experiments.",
          "quote": "Additionally, we compare our performance with recent concept-based models that are built on CBMs to either improve the downstream task accuracy or mitigate the concept leakage. We also utilize the Large-scale CelebFaces Attributes (CelebA) dataset for gender classification."
        }
      },
      {
        "name": {
          "value": "TIL",
          "justification": "TIL is one of the primary datasets used in the experiments for image classification.",
          "quote": "This paper presents extensive experiments on real-world datasets for image classification tasks, namely CUB, AwA2, CelebA and TIL."
        },
        "aliases": [
          "Tumor-Infiltrating Lymphocytes"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Spatial organization and molecular correlation of tumor-infiltrating lymphocytes using deep learning on pathology images",
          "justification": "The dataset is referred to with its official title in the experiments.",
          "quote": "Tumor-Infiltrating Lymphocytes Maps from TCGA HE Whole Slide Pathology Images. The dataset contains tumor maps from the most common cancer tumor types."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "InceptionV3",
          "justification": "InceptionV3 is one of the feature extractors used in the experiments for the concept encoder model.",
          "quote": "The feature extractor was InceptionV3 as a concept encoder model."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Rethinking the inception architecture for computer vision",
          "justification": "The library is referred to with its official title in the experiments.",
          "quote": "The feature extractor was InceptionV3 as a concept encoder model."
        }
      },
      {
        "name": {
          "value": "VIT",
          "justification": "VIT is one of the feature extractors used in the experiments for the concept encoder model.",
          "quote": "The feature extractor was VIT as a concept encoder model."
        },
        "aliases": [
          "Vision Transformer"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "An image is worth 16x16 words: Transformers for image recognition at scale",
          "justification": "The library is referred to with its official title in the experiments.",
          "quote": "The feature extractor was VIT as a concept encoder model."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1839,
    "prompt_tokens": 23142,
    "total_tokens": 24981
  }
}