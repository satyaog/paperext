{
  "paper": "2009.13716.txt",
  "words": 11081,
  "extractions": {
    "title": {
      "value": "Grow-Push-Prune: aligning deep discriminants for effective structural network compression",
      "justification": "This is the primary title of the paper as provided in the beginning of the document.",
      "quote": "Grow-Push-Prune: aligning deep discriminants for effective structural network compression"
    },
    "description": "The paper proposes a method for deriving task-dependent compact models from deep neural networks through an iterative approach involving pushing deep discriminants into alignment with neurons and pruning less useful ones. The method utilizes deconvolution to reverse effects of unimportant filters and includes a simple network growing strategy based on the Inception module. The efficacy of the proposed approach is demonstrated on datasets like MNIST, CIFAR10, and ImageNet.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes experiments and results on datasets such as MNIST, CIFAR10, and ImageNet to demonstrate the efficacy of the proposed approach.",
      "quote": "Experiments on the MNIST, CIFAR10, and ImageNet datasets demonstrate our approach’s efficacy."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary application domain addressed in the paper involves optimizing neural network architectures for image datasets, which falls under the category of Computer Vision.",
        "quote": "Experiments on the MNIST, CIFAR10, and ImageNet datasets demonstrate our approach’s efficacy."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Network Compression",
          "justification": "The primary focus of the paper is on deriving compact models from deep neural networks through pruning and other techniques.",
          "quote": "In this paper, we attempt to derive task-dependent compact models from a deep discriminant analysis perspective."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Inception-88",
          "justification": "This model is introduced as one of the grown deep Inception nets achieving high accuracy on ImageNet.",
          "quote": "On ImageNet, by pushing and pruning our grown Inception-88 model, we achieve more accurate models than Inception nets generated during growing, residual nets, and popular compact nets at similar sizes."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The model Inception-88 was proposed and developed as part of this paper's contributions.",
          "quote": "One of our grown deep Inception net, Inception-88, beats ResNet-50 (slightly larger) after training with the conventional cross-entropy and L2 losses."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed on ImageNet, a large-scale benchmark dataset that typically requires GPU support.",
          "quote": "Experiments on the MNIST, CIFAR10, and ImageNet datasets demonstrate our approach’s efficacy."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model was compared numerically to other models including ResNet-50 and other popular compact nets.",
          "quote": "On ImageNet, by pushing and pruning our grown Inception-88 model, we achieve more accurate models than Inception nets generated during growing, residual nets, and popular compact nets at similar sizes."
        },
        "referenced_paper_title": {
          "value": "Going deeper with convolutions",
          "justification": "The referenced paper for the base Inception model is 'Going deeper with convolutions' by Szegedy et al., 2015.",
          "quote": "We grow deep Inception nets by greedily and iteratively adding more modules according to Algorithm 2, which can be viewed as a trial-and-error evolutionary process."
        }
      },
      {
        "name": {
          "value": "ResNet-50",
          "justification": "ResNet-50 is used as a baseline for performance comparison with the proposed grown Inception-88 model.",
          "quote": "On ImageNet, by pushing and pruning our grown Inception-88 model, we achieve more accurate models than Inception nets generated during growing, residual nets, and popular compact nets at similar sizes."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet-50 is a widely recognized and pre-existing model used for benchmark comparisons.",
          "quote": "We grow deep Inception nets by greedily and iteratively adding more modules according to Algorithm 2, which can be viewed as a trial-and-error evolutionary process."
        },
        "is_executed": {
          "value": 1,
          "justification": "ResNet-50 was executed on the ImageNet dataset for benchmarking purposes, which involves using GPUs.",
          "quote": "On ImageNet, by pushing and pruning our grown Inception-88 model, we achieve more accurate models than Inception nets generated during growing, residual nets, and popular compact nets at similar sizes."
        },
        "is_compared": {
          "value": 1,
          "justification": "ResNet-50 is numerically compared with the model proposed in the paper, Inception-88.",
          "quote": "On ImageNet, by pushing and pruning our grown Inception-88 model, we achieve more accurate models than Inception nets generated during growing, residual nets, and popular compact nets at similar sizes."
        },
        "referenced_paper_title": {
          "value": "Deep residual learning for image recognition",
          "justification": "The reference paper for ResNet-50 is 'Deep residual learning for image recognition' by He et al., 2015.",
          "quote": "Compared to ResNets with up to hundreds of layers, current Inception models are relatively shallow and they only have a dozen or so modules."
        }
      },
      {
        "name": {
          "value": "VGG-16",
          "justification": "VGG-16 is used as a baseline for performance comparisons in experiments on the CIFAR10 dataset.",
          "quote": "We start with a VGG-16 model pre-trained on ImageNet. Cross-entropy loss with L2 regularization leads to a validation accuracy of 95.19% on CIFAR10."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "VGG-16 is a pre-existing model used for performance benchmarking.",
          "quote": "We start with a VGG-16 model pre-trained on ImageNet. Cross-entropy loss with L2 regularization leads to a validation accuracy of 95.19% on CIFAR10."
        },
        "is_executed": {
          "value": 1,
          "justification": "The VGG-16 model was executed on the CIFAR10 dataset as part of the experiments conducted in the paper.",
          "quote": "We start with a VGG-16 model pre-trained on ImageNet. Cross-entropy loss with L2 regularization leads to a validation accuracy of 95.19% on CIFAR10."
        },
        "is_compared": {
          "value": 1,
          "justification": "VGG-16 is numerically compared with the models developed in the paper.",
          "quote": "We start with a VGG-16 model pre-trained on ImageNet. Cross-entropy loss with L2 regularization leads to a validation accuracy of 95.19% on CIFAR10."
        },
        "referenced_paper_title": {
          "value": "Very deep convolutional networks for large-scale image recognition",
          "justification": "The referenced paper for VGG-16 is 'Very deep convolutional networks for large-scale image recognition' by Simonyan and Zisserman, 2015.",
          "quote": "We start with a VGG-16 model pre-trained on ImageNet. Cross-entropy loss with L2 regularization leads to a validation accuracy of 95.19% on CIFAR10."
        }
      },
      {
        "name": {
          "value": "MobileNet",
          "justification": "MobileNet is used as a benchmark for comparing compact neural networks in the experiments.",
          "quote": "We add after-the-fact deep LDA pruning (Tian et al., 2021) and activation-based filter pruning (as mentioned in Molchanov et al. (2016)), MobileNet (Howard et al., 2017), SqueezeNet (Iandola et al., 2016), and tiny ResNets."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "MobileNet is a pre-existing model used for performance benchmarking in the paper.",
          "quote": "We add after-the-fact deep LDA pruning (Tian et al., 2021) and activation-based filter pruning (as mentioned in Molchanov et al. (2016)), MobileNet (Howard et al., 2017), SqueezeNet (Iandola et al., 2016), and tiny ResNets."
        },
        "is_executed": {
          "value": 0,
          "justification": "MobileNet is referenced for comparison and not executed within the scope of this paper.",
          "quote": "We add after-the-fact deep LDA pruning (Tian et al., 2021) and activation-based filter pruning (as mentioned in Molchanov et al. (2016)), MobileNet (Howard et al., 2017), SqueezeNet (Iandola et al., 2016), and tiny ResNets."
        },
        "is_compared": {
          "value": 1,
          "justification": "MobileNet is numerically compared as a compact neural network benchmark.",
          "quote": "We add after-the-fact deep LDA pruning (Tian et al., 2021) and activation-based filter pruning (as mentioned in Molchanov et al. (2016)), MobileNet (Howard et al., 2017), SqueezeNet (Iandola et al., 2016), and tiny ResNets."
        },
        "referenced_paper_title": {
          "value": "MobileNets: Efficient convolutional neural networks for mobile vision applications",
          "justification": "The referenced paper for MobileNet is 'MobileNets: Efficient convolutional neural networks for mobile vision applications' by Howard et al., 2017.",
          "quote": "We add after-the-fact deep LDA pruning (Tian et al., 2021) and activation-based filter pruning (as mentioned in Molchanov et al. (2016)), MobileNet (Howard et al., 2017), SqueezeNet (Iandola et al., 2016), and tiny ResNets."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "One of the primary datasets used to validate the proposed method in the experiments.",
          "quote": "Experiments on the MNIST, CIFAR10, and ImageNet datasets demonstrate our approach’s efficacy."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-based learning applied to document recognition",
          "justification": "The reference for the MNIST dataset is 'Gradient-based learning applied to document recognition' by LeCun et al., 1998.",
          "quote": "We use the MNIST dataset to illustrate deep LDA pushing’s influence on the latent space. MNIST details can be found in LeCun et al. (1998)."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "Used in experiments to demonstrate the efficacy of the proposed pruning method.",
          "quote": "Experiments on the MNIST, CIFAR10, and ImageNet datasets demonstrate our approach’s efficacy."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The reference for the CIFAR-10 dataset is 'Learning multiple layers of features from tiny images' by Krizhevsky and Hinton, 2009.",
          "quote": "Please refer to Krizhevsky and Hinton (2009) for dataset details."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "A major dataset used in the experiments to validate the proposed method and demonstrate its superiority over other models.",
          "quote": "Experiments on the MNIST, CIFAR10, and ImageNet datasets demonstrate our approach’s efficacy."
        },
        "aliases": [
          "ILSVRC2012"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "The reference for the ImageNet dataset is 'ImageNet Large Scale Visual Recognition Challenge' by Russakovsky et al., 2015.",
          "quote": "In this subsection, we demonstrate our ‘grow-push-prune’ pipeline’s efficacy on the ImageNet (ILSVRC12) dataset Russakovsky et al. (2015)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "TensorFlow",
          "justification": "The TensorFlow library is implied through references to models and methods that are commonly implemented using TensorFlow, although it is not explicitly mentioned.",
          "quote": "Unlike the ResNet-50 achieving 76% in Tensorflow, no bounding box info is used in any of our models. Only 1-center crop is used for validation."
        },
        "aliases": [
          "TF"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "TensorFlow: Large-scale machine learning on heterogeneous distributed systems",
          "justification": "The reference for TensorFlow is 'TensorFlow: Large-scale machine learning on heterogeneous distributed systems' by Abadi et al., 2016.",
          "quote": "Unlike the ResNet-50 achieving 76% in Tensorflow, no bounding box info is used in any of our models. Only 1-center crop is used for validation."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2614,
    "prompt_tokens": 20537,
    "total_tokens": 23151
  }
}