{
  "paper": "2307.16877.txt",
  "words": 10539,
  "extractions": {
    "title": {
      "value": "Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering",
      "justification": "The extracted title represents the full and complete title of the research paper.",
      "quote": "Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering"
    },
    "description": "This paper evaluates instruction-following models for question answering (QA) on correctness (how well the models satisfy user's information needs) and faithfulness (whether the information disseminated is supported by the provided knowledge). It highlights the shortcomings of traditional metrics and proposes alternative token-overlap metrics. Additionally, it evaluates several instruction-following models and discusses their performance in terms of correctness and faithfulness.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper conducts experiments and provides empirical evidence on the performance of various instruction-following models for QA, focusing on correctness and faithfulness.",
      "quote": "In this work, we evaluate instruction-following models along two fronts: 1) how well they satisfy user’s information need (correctness), and 2) whether they disseminate information supported by the provided knowledge (faithfulness)."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary research field is Natural Language Processing, as the paper focuses on evaluating and improving models for question answering, a common NLP task.",
        "quote": "Instruction-following models are attractive alternatives to fine-tuned approaches for question answering (QA)."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Question Answering",
          "justification": "The paper specifically deals with the evaluation of models' performance in question answering tasks.",
          "quote": "In this work, we evaluate instruction-following models along two fronts: 1) how well they satisfy user’s information need (correctness), and 2) whether they disseminate information supported by the provided knowledge (faithfulness)."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Model Evaluation",
          "justification": "The paper focuses on evaluating the correctness and faithfulness of different instruction-following models, proposing new metrics for evaluation.",
          "quote": "We annotate responses from instruction-following models for QA along the dimensions of correctness and faithfulness, and evaluate several evaluation metrics."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Flan-T5",
          "justification": "Flan-T5 is explicitly mentioned as one of the instruction-following models evaluated in the study.",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b)"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper does not contribute Flan-T5 but uses it for evaluation purposes.",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b)"
        },
        "is_executed": {
          "value": 1,
          "justification": "Flan-T5 is executed as a part of the study to evaluate its performance.",
          "quote": "We evaluate four instruction-following models across three diverse QA tasks."
        },
        "is_compared": {
          "value": 1,
          "justification": "Flan-T5 is compared numerically to other models in terms of correctness and faithfulness.",
          "quote": "We evaluate four instruction-following models across three diverse QA tasks."
        },
        "referenced_paper_title": {
          "value": "Scaling Instruction-Finetuned Language Models",
          "justification": "The referenced paper for Flan-T5, as cited in the study, is by Chung et al. (2022).",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Natural Questions",
          "justification": "Natural Questions is explicitly mentioned as one of the datasets used for evaluation in the study.",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b) – on three popular factual information-seeking QA datasets – Natural Questions (NQ; Kwiatkowski et al. 2019), HotpotQA (Yang et al., 2018), and TopiOCQA (Adlakha et al., 2022)."
        },
        "aliases": [
          "NQ"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Natural Questions: A Benchmark for Question Answering Research",
          "justification": "The referenced paper for Natural Questions is the one by Kwiatkowski et al. (2019), as cited in the study.",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b) – on three popular factual information-seeking QA datasets – Natural Questions (NQ; Kwiatkowski et al. 2019), HotpotQA (Yang et al., 2018), and TopiOCQA (Adlakha et al., 2022)."
        }
      },
      {
        "name": {
          "value": "HotpotQA",
          "justification": "HotpotQA is explicitly mentioned as one of the datasets used for evaluation in the study.",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b) – on three popular factual information-seeking QA datasets – Natural Questions (NQ; Kwiatkowski et al. 2019), HotpotQA (Yang et al., 2018), and TopiOCQA (Adlakha et al., 2022)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
          "justification": "The referenced paper for HotpotQA is the one by Yang et al. (2018), as cited in the study.",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b) – on three popular factual information-seeking QA datasets – Natural Questions (NQ; Kwiatkowski et al. 2019), HotpotQA (Yang et al., 2018), and TopiOCQA (Adlakha et al., 2022)."
        }
      },
      {
        "name": {
          "value": "TopiOCQA",
          "justification": "TopiOCQA is explicitly mentioned as one of the datasets used for evaluation in the study.",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b) – on three popular factual information-seeking QA datasets – Natural Questions (NQ; Kwiatkowski et al. 2019), HotpotQA (Yang et al., 2018), and TopiOCQA (Adlakha et al., 2022)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "TopiOCQA: Open-domain Conversational Question Answering with Topic Switching",
          "justification": "The referenced paper for TopiOCQA is the one by Adlakha et al. (2022), as cited in the study.",
          "quote": "We evaluate several recent instruction-following models – Flan-T5 (Chung et al., 2022), Alpaca (Taori et al., 2023), GPT-3.5 (sibling model of Ouyang et al. 2022), and Llama-2 (Touvron et al., 2023b) – on three popular factual information-seeking QA datasets – Natural Questions (NQ; Kwiatkowski et al. 2019), HotpotQA (Yang et al., 2018), and TopiOCQA (Adlakha et al., 2022)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "DPR",
          "justification": "DPR (Dense Passage Retrieval) is explicitly mentioned as one of the retrievers used in the study for providing relevant passages to instruction-following models.",
          "quote": "For each task, we employ a task-specific variant of DPR (Dense Passage Retrieval; Karpukhin et al. 2020)."
        },
        "aliases": [
          "Dense Passage Retrieval"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Dense Passage Retrieval for Open-Domain Question Answering",
          "justification": "The referenced paper for DPR is the one by Karpukhin et al. (2020), as cited in the study.",
          "quote": "For each task, we employ a task-specific variant of DPR (Dense Passage Retrieval; Karpukhin et al. 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2099,
    "prompt_tokens": 19961,
    "total_tokens": 22060
  }
}