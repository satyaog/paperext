{
  "paper": "2312.00886.txt",
  "words": 18246,
  "extractions": {
    "title": {
      "value": "Nash Learning from Human Feedback",
      "justification": "This is the title of the paper.",
      "quote": "Nash Learning from Human Feedback"
    },
    "description": "The paper introduces Nash Learning from Human Feedback (NLHF), an alternative approach to fine-tuning large language models (LLMs) using human preferences. The approach revolves around learning a preference model instead of a reward model and finding the Nash equilibrium of this preference model. The paper also presents new algorithms, particularly Nash-MD, inspired by mirror descent, and conducts empirical experiments on a text summarization task.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper involves experimentation and numerical evaluation on a text summarization task to demonstrate the effectiveness of the proposed methods.",
      "quote": "We illustrate the effectiveness of our approach by presenting experimental results on a text summarization task."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus of the paper is on enhancing the performance and alignment of large language models, which falls under the field of Natural Language Processing.",
        "quote": "Large language models (LLMs) (Glaese et al., 2022; Anil et al., 2023; OpenAI, 2023; Ouyang et al., 2022) have made remarkable strides in enhancing natural language understanding and generation."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper introduces new reinforcement learning algorithms and compares them with traditional reinforcement learning from human feedback (RLHF) techniques.",
          "quote": "Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences."
        },
        "aliases": [
          "RLHF"
        ]
      },
      {
        "name": {
          "value": "Machine Learning Theory",
          "justification": "The paper presents new theoretical concepts such as the Nash equilibrium of a preference model and the Nash-MD algorithm founded on the principles of mirror descent.",
          "quote": "Our contributions in this work can be summarized as follows. First, we introduce the concept of Nash learning from human feedback (NLHF), framing it as the task of computing the Nash equilibrium for a general preference model."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Nash-MD",
          "justification": "Nash-MD is a novel algorithmic solution presented in the paper, founded on the principles of mirror descent.",
          "quote": "In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The Nash-MD algorithm is a novel contribution introduced in this paper.",
          "quote": "In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments conducted in the paper utilize the Nash-MD algorithm.",
          "quote": "In these experiments, we employ the NLHF approach to train several models."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper presents a numerical performance comparison of the Nash-MD algorithm against other models.",
          "quote": "We compare several algorithms for NLHF (Self-Play, Best-Response against µ, Nash-MD-PG and Nash-EMA-PG) as well as a RLHF baseline."
        },
        "referenced_paper_title": {
          "value": "Nash Learning from Human Feedback",
          "justification": "The Nash-MD algorithm is introduced and detailed in this paper.",
          "quote": "In the context of a tabular policy representation, we present a novel algorithmic solution, Nash-MD, founded on the principles of mirror descent."
        }
      },
      {
        "name": {
          "value": "Nash-MD-PG",
          "justification": "Nash-MD-PG is introduced as a gradient-based deep learning adaptation of the Nash-MD algorithm.",
          "quote": "We introduce policy-gradient algorithms for deep learning architectures, Nash-MD-PG and Nash-EMA-PG, inspired by the tabular algorithms."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "Nash-MD-PG is a new contribution proposed in this paper.",
          "quote": "We introduce policy-gradient algorithms for deep learning architectures, Nash-MD-PG and Nash-EMA-PG, inspired by the tabular algorithms."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Nash-MD-PG algorithm is applied in the experiments conducted in this paper.",
          "quote": "Nash-MD-PG as well as a RLHF baseline were trained for 10000 steps. The Nash-MD models (as well as SP and BR) and Nash-EMA are trained with a regularization coefficient of τ = 0.008."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Nash-MD-PG is compared against other models in the experiments.",
          "quote": "Nash-MD-PG with β ∈ {0.125, 0.25, 0.375} emerged as the best-performing method, surpassing the other models in this pairwise comparison."
        },
        "referenced_paper_title": {
          "value": "Nash Learning from Human Feedback",
          "justification": "The Nash-MD-PG algorithm is introduced and detailed in this paper.",
          "quote": "We introduce policy-gradient algorithms for deep learning architectures, Nash-MD-PG and Nash-EMA-PG, inspired by the tabular algorithms."
        }
      },
      {
        "name": {
          "value": "Nash-EMA-PG",
          "justification": "Nash-EMA-PG is introduced as a gradient-based deep learning adaptation inspired by variants of Nash-MD.",
          "quote": "We introduce policy-gradient algorithms for deep learning architectures, Nash-MD-PG and Nash-EMA-PG, inspired by the tabular algorithms."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "Nash-EMA-PG is a new model proposed in this paper.",
          "quote": "We introduce policy-gradient algorithms for deep learning architectures, Nash-MD-PG and Nash-EMA-PG, inspired by the tabular algorithms."
        },
        "is_executed": {
          "value": 1,
          "justification": "The Nash-EMA-PG algorithm is applied in the experiments conducted in the paper.",
          "quote": "The Nash-EMA models, specially for β ∈ [0.125, 0.375] emerge as the best-performing method, surpassing the other models in the pairwise comparison."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Nash-EMA-PG is compared against other models in the experiments.",
          "quote": "We compare several algorithms for NLHF (Self-Play, Best-Response against µ, Nash-MD-PG and Nash-EMA-PG) as well as a RLHF baseline."
        },
        "referenced_paper_title": {
          "value": "Nash Learning from Human Feedback",
          "justification": "The Nash-EMA-PG algorithm is introduced and detailed in this paper.",
          "quote": "We introduce policy-gradient algorithms for deep learning architectures, Nash-MD-PG and Nash-EMA-PG, inspired by the tabular algorithms."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "TL;DR dataset",
          "justification": "The TL;DR dataset is mentioned as the dataset used for text summarization experiments.",
          "quote": "We present the results of numerical experiments conducted on a text summarizing task utilizing the TL;DR dataset (Völske et al., 2017)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TL;DR: Mining Reddit to Learn Automatic Summarization",
          "justification": "The dataset is described in a paper by Völske et al., 2017.",
          "quote": "We present the results of numerical experiments conducted on a text summarizing task utilizing the TL;DR dataset (Völske et al., 2017)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1950,
    "prompt_tokens": 33610,
    "total_tokens": 35560
  }
}