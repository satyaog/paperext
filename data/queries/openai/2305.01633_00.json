{
  "paper": "2305.01633.txt",
  "words": 5816,
  "extractions": {
    "title": {
      "value": "Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP",
      "justification": "This is the title of the paper as cited by the user.",
      "quote": "Missing Information, Unresponsive Authors, Experimental Flaws: The Impossibility of Assessing the Reproducibility of Previous Human Evaluations in NLP"
    },
    "description": "This paper reports efforts to identify a set of previous human evaluations in NLP suitable for a coordinated study to examine what makes human evaluations in NLP more/less reproducible. Despite the attempts, the paper highlights significant barriers to reproduction due to missing information, unresponsive authors, and experimental flaws, concluding that the majority of human evaluations in NLP are not reproducible.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents an analysis and reports findings based on empirical efforts to reproduce human evaluations in NLP.",
      "quote": "We report our efforts in identifying a set of previous human evaluations in NLP that would be suitable for a coordinated study examining what makes human evaluations in NLP more/less reproducible. We present our results and findings..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus of the paper is on the reproducibility of human evaluations within the NLP field.",
        "quote": "There is increasing awareness in Natural Language Processing (NLP) that reproducibility of results, most particularly of results from system evaluations, matters greatly..."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Evaluation Methods",
          "justification": "The paper is particularly centered on the human evaluation methods within NLP and the issues associated with their reproducibility.",
          "quote": "The work reported in this paper forms part of the ReproHum project... to investigate systematically what factors make a human evaluation more—or less—reproducible."
        },
        "aliases": []
      }
    ],
    "models": [],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "HEDS (Human Evaluation Datasheet)",
          "justification": "HEDS is mentioned as an important tool for recording details of human evaluation experiments in NLP, which the paper advocates for to improve reproducibility.",
          "quote": "The way forward would appear to be to accept the overhead of detailed recording of experimental details, e.g. with HEDS (Shimorina and Belz, 2022)..."
        },
        "aliases": [
          "HEDS"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The human evaluation datasheet: A template for recording details of human evaluation experiments in NLP",
          "justification": "HEDS is referenced in the context of recording experimental details to improve reproducibility.",
          "quote": "The way forward would appear to be to accept the overhead of detailed recording of experimental details, e.g. with HEDS (Shimorina and Belz, 2022)..."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 575,
    "prompt_tokens": 9861,
    "total_tokens": 10436
  }
}