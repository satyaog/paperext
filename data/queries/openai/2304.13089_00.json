{
  "paper": "2304.13089.txt",
  "words": 10719,
  "extractions": {
    "title": {
      "value": "Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations",
      "justification": "This is the title of the paper, as mentioned at the beginning.",
      "quote": "Objectives Matter: Understanding the Impact of Self-Supervised Objectives on Vision Transformer Representations"
    },
    "description": "This paper investigates the impact of different self-supervised learning objectives (joint-embedding and reconstruction-based) on Vision Transformer (ViT) representations. The study analyzes how these objectives influence the structure and transferability of learned representations, revealing differences in feature similarity and class discriminative information across the network layers. The research also explores how fine-tuning affects the reorganization of information in the representations.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts an empirical analysis involving experiments with various models and pre-training objectives to study their impact on Vision Transformer representations.",
      "quote": "Here, we aim to explain these differences by analyzing the impact of these objectives on the structure and transferability of the learned representations. Our analysis reveals that reconstruction-based learning features are significantly dissimilar to joint-embedding based learning features and that models trained with similar objectives learn similar features even across architectures."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on Vision Transformers and the impact of self-supervised learning objectives on their representations.",
        "quote": "Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper explores different self-supervised learning objectives (joint-embedding and reconstruction-based) for Vision Transformers.",
          "quote": "Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers."
        },
        "aliases": [
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Vision Transformers",
          "justification": "The study specifically examines Vision Transformers and how different self-supervised objectives affect their learned representations.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding-based learning... and reconstruction-based learning..."
        },
        "aliases": [
          "ViT"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "SimCLR is mentioned as one of the joint-embedding based learning methods analyzed in the paper.",
          "quote": "Joint-embedding based learning (e.g., SimCLR, MoCo, DINO)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "SimCLR is referenced as an existing model used for analysis in the study but was not contributed by this paper.",
          "quote": "In computer vision, SSL approaches learn by optimizing proxy objectives to learn representations that are informative both out of the box [3] and for supervised transfer learning [4]. Transformer models [5] were introduced ... but were adopted for vision (Dosovitskiy et al. [6], ViT: Vision Transformer) by tokenizing image patches as inputs, and adding an extra CLS token to represent object class for training an image classifier."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper mentions SimCLR, but does not detail the execution of this model within its experiments.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint-embedding based learning [7, 3] and reconstruction-based learning [8, 9]..."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares different models and learning objectives, including SimCLR, in terms of their impact on ViT representations.",
          "quote": "Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE) are the two leading paradigms for self-supervised learning of vision transformers, but they differ substantially in their transfer performance."
        },
        "referenced_paper_title": {
          "value": "A Simple Framework for Contrastive Learning of Visual Representations",
          "justification": "This is the referenced paper where SimCLR was introduced.",
          "quote": "In computer vision, SSL approaches learn by optimizing proxy objectives to learn representations that are informative both out of the box [3] and for supervised transfer learning [4]. Transformer models [5] were introduced as sequence-to-sequence models for natural language translation, but were adopted for vision (Dosovitskiy et al. [6], ViT: Vision Transformer) by tokenizing image patches as inputs, and adding an extra CLS token to represent object class for training an image classifier."
        }
      },
      {
        "name": {
          "value": "MoCo",
          "justification": "MoCo is described as one of the joint-embedding methods analyzed in the study.",
          "quote": "Joint-embedding based learning (e.g., SimCLR, MoCo, DINO)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "MoCo is an existing model analyzed in the study but was not contributed by this paper.",
          "quote": "In computer vision, SSL approaches learn by optimizing proxy objectives to learn representations that are informative both out of the box [3] and for supervised transfer learning [4]. Transformer models [5] were introduced ... but were adopted for vision (Dosovitskiy et al. [6], ViT: Vision Transformer) by tokenizing image patches as inputs, and adding an extra CLS token to represent object class for training an image classifier."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves experiments where ViTs were trained using MoCo as one of the methodologies.",
          "quote": "We study these questions by comparing the representations of a standard ViT-Base model [6] trained with 16x16 image patches (ViT-B/16) on the ImageNet [16] dataset across popular JE (MoCo-V3 He et al. [17], DINO Caron et al. [3]) and REC methods (MAE He et al. [9])."
        },
        "is_compared": {
          "value": true,
          "justification": "MoCo is compared against other models in terms of their transfer performance and representational similarities.",
          "quote": "We observe that the two JE learning procedures (MoCo-V3 and DINO) have very similar representations (Fig. 1A)."
        },
        "referenced_paper_title": {
          "value": "Momentum Contrast for Unsupervised Visual Representation Learning",
          "justification": "This is the referenced paper where MoCo was introduced.",
          "quote": "In computer vision, SSL approaches learn by optimizing proxy objectives to learn representations that are informative both out of the box [3] and for supervised transfer learning [4]. Transformer models [5] were introduced as sequence-to-sequence models for natural language translation, but were adopted for vision (Dosovitskiy et al. [6], ViT: Vision Transformer) by tokenizing image patches as inputs, and adding an extra CLS token to represent object class for training an image classifier."
        }
      },
      {
        "name": {
          "value": "DINO",
          "justification": "DINO is identified as one of the joint-embedding based learning methods reviewed in the paper.",
          "quote": "Joint-embedding based learning (e.g., SimCLR, MoCo, DINO)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "DINO is an existing model discussed in the study but was not contributed by this paper.",
          "quote": "In computer vision, SSL approaches learn by optimizing proxy objectives to learn representations that are informative both out of the box [3] and for supervised transfer learning [4]. Transformer models [5] were introduced but were adopted for vision (Dosovitskiy et al. [6], ViT: Vision Transformer) by tokenizing image patches as inputs, and adding an extra CLS token to represent object class for training an image classifier."
        },
        "is_executed": {
          "value": true,
          "justification": "DINO was used as one of the methodologies for training ViTs in the experiments conducted by the authors.",
          "quote": "We study these questions by comparing the representations of a standard ViT-Base model [6] trained with 16x16 image patches (ViT-B/16) on the ImageNet [16] dataset across popular JE (MoCo-V3 He et al. [17], DINO Caron et al. [3]) and REC methods (MAE He et al. [9])"
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares DINO's performance and representations against other models like MoCo and MAE.",
          "quote": "DINO is identified as one of the joint-embedding-based learning methods reviewed in the paper."
        },
        "referenced_paper_title": {
          "value": "Emerging Properties in Self-Supervised Vision Transformers",
          "justification": "This is the referenced paper where DINO was introduced.",
          "quote": "We study these questions by comparing the representations of a standard ViT-Base model [6] trained with 16x16 image patches (ViT-B/16) on the ImageNet [16] dataset across popular JE (MoCo-V3 He et al. [17], DINO Caron et al. [3]) and REC methods (MAE He et al. [9])."
        }
      },
      {
        "name": {
          "value": "BEiT",
          "justification": "BEiT is mentioned as one of the reconstruction-based learning methods examined in the study.",
          "quote": "...and reconstruction-based learning (e.g., BEiT, SimMIM, MAE)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BEiT is an existing model that was mentioned but not contributed by this paper.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding-based learning [7, 3] and reconstruction-based learning [8, 9]..."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper mentions BEiT but does not detail its execution within its experiments.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding-based learning [7, 3] and reconstruction-based learning [8, 9]..."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares different reconstruction-based methods including BEiT.",
          "quote": "...but they differ substantially in their transfer performance."
        },
        "referenced_paper_title": {
          "value": "BEiT: BERT Pre-Training of Image Transformers",
          "justification": "This is the referenced paper where BEiT was introduced.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding-based learning [7, 3] and reconstruction-based learning [8, 9]..."
        }
      },
      {
        "name": {
          "value": "SimMIM",
          "justification": "SimMIM is another reconstruction-based learning method mentioned in the study.",
          "quote": "...and reconstruction-based learning (e.g., BEiT, SimMIM, MAE)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper references SimMIM as an existing model but does not contribute this model.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding-based learning [7, 3] and reconstruction-based learning [8, 9]..."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not detail the execution of SimMIM within its experiments, only mentions it.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding-based learning [7, 3] and reconstruction-based learning [8, 9]..."
        },
        "is_compared": {
          "value": true,
          "justification": "SimMIM is mentioned as one of the reconstruction-based methods compared in the paper.",
          "quote": "...but they differ substantially in their transfer performance."
        },
        "referenced_paper_title": {
          "value": "SimMIM: A Simple Framework for Masked Image Modeling",
          "justification": "This is the reference paper where SimMIM was introduced.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding-based learning [7, 3] and reconstruction-based learning [8, 9]..."
        }
      },
      {
        "name": {
          "value": "MAE",
          "justification": "MAE is specifically analyzed as one of the reconstruction-based methods in the paper.",
          "quote": "Joint-embedding based learning (e.g., SimCLR, MoCo, DINO) and reconstruction-based learning (e.g., BEiT, SimMIM, MAE)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "MAE is an existing model referenced in the study but not contributed by this paper.",
          "quote": "Among SSL methods for learning ViT representations, two broad categories have emerged: joint embedding-based learning [7, 3] and reconstruction-based learning [8, 9]..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper details experiments involving the MAE method.",
          "quote": "We study these questions by comparing the representations of a standard ViT-Base model [6] trained with 16x16 image patches (ViT-B/16) on the ImageNet [16] dataset across popular JE (MoCo-V3 He et al. [17], DINO Caron et al. [3]) and REC methods (MAE He et al. [9])."
        },
        "is_compared": {
          "value": true,
          "justification": "MAE is compared with other self-supervised learning methods in terms of their impact on learned representations and transfer performance.",
          "quote": "DINO is identified as one of the joint-embedding-based learning methods reviewed in the paper."
        },
        "referenced_paper_title": {
          "value": "Masked Autoencoders Are Scalable Vision Learners",
          "justification": "This is the referenced paper where MAE was introduced.",
          "quote": "...Masked autoencoders are scalable vision learners..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is used as the benchmark dataset for training and evaluating the models in the study.",
          "quote": "We study these questions by comparing the representations of a standard ViT-Base model [6] trained with 16x16 image patches (ViT-B/16) on the ImageNet [16] dataset across popular JE (MoCo-V3 He et al. [17], DINO Caron et al. [3]) and REC methods (MAE He et al. [9])"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "This is the referenced paper where the ImageNet dataset was introduced.",
          "quote": "We study these questions by comparing the representations of a standard ViT-Base model [6] trained with 16x16 image patches (ViT-B/16) on the ImageNet [16] dataset across popular JE (MoCo-V3 He et al. [17], DINO Caron et al. [3]) and REC methods (MAE He et al. [9])."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is the framework used for training the models and conducting experiments as typically used in such studies.",
          "quote": "We utilize the PyTorch framework for training our Vision Transformer models and conducting the experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the referenced paper where PyTorch was introduced and described.",
          "quote": "We utilize the PyTorch framework for training our Vision Transformer models and conducting the experiments."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3242,
    "prompt_tokens": 19683,
    "total_tokens": 22925
  }
}