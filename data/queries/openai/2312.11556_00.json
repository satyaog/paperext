{
  "paper": "2312.11556.txt",
  "words": 11594,
  "extractions": {
    "title": {
      "value": "StarVector: Generating Scalable Vector Graphics Code from Images",
      "justification": "The title directly represents the main contribution and scope of the paper.",
      "quote": "StarVector: Generating Scalable Vector Graphics Code from Images"
    },
    "description": "This paper introduces StarVector, a multimodal SVG generation model that utilizes Code Generation Large Language Models (CodeLLMs) and vision models to convert images into SVG code. The model employs a CLIP image encoder and StarCoder model to generate complex SVGs from pixel images. The paper also presents SVG-Bench, a comprehensive benchmark for evaluating SVG methods, and introduces datasets such as SVG-Emoji and SVG-Stack for pre-training and evaluation.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves the practical implementation and evaluation of the StarVector model, including experimental results comparing it to other methods.",
      "quote": "We evaluate StarVector and prior baselines on SVG-Bench which focuses on the image-to-SVG generation task."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary task involves image processing and generation of SVGs from images, which falls under Computer Vision.",
        "quote": "This paper studies the task of image-to-SVG generation (Figure 1), which has been traditionally approached as a form of image vectorization [42, 85]"
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generative Models",
          "justification": "The study focuses on generating SVG code from images using models that involve multimodal learning.",
          "quote": "In this work, we propose a novel paradigm, where a multimodal LLM learns SVG synthesis as an image-to-code generation task."
        },
        "aliases": [
          "Multimodal Models"
        ]
      },
      {
        "name": {
          "value": "Image-to-Code Generation",
          "justification": "The main problem tackled by the paper is generating SVG code from input images.",
          "quote": "We study the task of image-to-SVG generation by learning a CLIP [57] image encoder coupled with an adapter model to project images into visual token embeddings and use them to condition a StarCoder [40] model to generate an SVG code associated with the input image."
        },
        "aliases": [
          "Image Code Generation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "StarVector",
          "justification": "The model proposed by this paper for generating SVGs from images is called StarVector.",
          "quote": "This paper introduces StarVector, a multimodal SVG generation model that effectively integrates Code Generation Large Language Models (CodeLLMs) and vision models."
        },
        "aliases": [
          "Star Vector",
          "Star-Vector"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "StarVector is introduced and developed as the main contribution of the paper.",
          "quote": "We propose StarVector, a Large Multimodal Model for code generation, which leverages image and language modalities for generating executable SVG code from images."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper discusses the execution and training of the model using GPUs.",
          "quote": "All experiments were done using 4 A100 80GB GPUs."
        },
        "is_compared": {
          "value": 1,
          "justification": "StarVector's performance was compared with several existing methods like DeepSVG, Im2Vec, and VTracer in the paper.",
          "quote": "We evaluate StarVector and prior baselines on SVG-Bench which focuses on the image-to-SVG generation task."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "StarVector is a novel model introduced by the authors and not referenced from another paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SVG-Stack",
          "justification": "SVG-Stack is one of the main datasets introduced in the paper for pre-training and evaluating SVG generation models.",
          "quote": "we present SVG-Bench, a unified evaluation benchmark for SVG generation methods, which facilitates access to popular SVG datasets and metrics. Within this benchmark, we introduce two new datasets namely SVG-Emoji (composed of 10k complex emoji SVGs) and SVG-Stack (a large-scale dataset with over 2M real-world SVGs)."
        },
        "aliases": [
          "SVG Stack",
          "SVGStack"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "SVG-Stack is a new dataset introduced by the authors.",
          "quote": "Within this benchmark, we introduce two new datasets namely SVG-Emoji (composed of 10k complex emoji SVGs) and SVG-Stack (a large-scale dataset with over 2M real-world SVGs)."
        }
      },
      {
        "name": {
          "value": "SVG-Emoji",
          "justification": "SVG-Emoji is another main dataset introduced in the paper for evaluating the model on complex emoji SVGs.",
          "quote": "SVG-Emoji: a dataset of 10k image-SVG pairs created by collating multiple smaller emoji datasets from different sources into a unified dataset."
        },
        "aliases": [
          "SVG Emoji",
          "SVGEmoji"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "SVG-Emoji is a new dataset introduced by the authors.",
          "quote": "SVG-Emoji: a dataset of 10k image-SVG pairs created by collating multiple smaller emoji datasets from different sources into a unified dataset."
        }
      },
      {
        "name": {
          "value": "SVG-Fonts",
          "justification": "SVG-Fonts is used in the evaluation benchmarks of the model.",
          "quote": "To encompass varying degrees of visual complexity across different colors, shapes, and text, we select datasets comprising examples of fonts, emojis, icons, and real-world examples e.g., the ones seen on websites. The datasets included in SVG-Bench are visually diverse and frequently used by digital artists in real-life scenarios. We use SVG-Fonts introduced as Glypazzn [45] and SVG-Icons in DeepSVG [13]."
        },
        "aliases": [
          "SVG Fonts",
          "SVGFonts"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "SVG-Fonts: Glypazzn",
          "justification": "The referenced paper for SVG-Fonts is Glypazzn.",
          "quote": "We use SVG-Fonts introduced as Glypazzn [45]"
        }
      },
      {
        "name": {
          "value": "SVG-Icons",
          "justification": "SVG-Icons is used in the evaluation benchmarks of the model.",
          "quote": "The datasets included in SVG-Bench are visually diverse and frequently used by digital artists in real-life scenarios. We use SVG-Fonts introduced as Glypazzn [45] and SVG-Icons in DeepSVG [13]."
        },
        "aliases": [
          "SVG Icons",
          "SVGIcons"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "SVG-Icons: DeepSVG",
          "justification": "The referenced paper for SVG-Icons is DeepSVG.",
          "quote": "We use SVG-Fonts introduced as Glypazzn [45] and SVG-Icons in DeepSVG [13]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used for the implementation of the StarVector model.",
          "quote": "We use HuggingFace Transformers [82] and PyTorch [53] for the implementation."
        },
        "aliases": [
          "Py Torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "The referenced paper for PyTorch is 'Automatic differentiation in PyTorch'.",
          "quote": "We use HuggingFace Transformers [82] and PyTorch [53] for the implementation."
        }
      },
      {
        "name": {
          "value": "HuggingFace Transformers",
          "justification": "HuggingFace Transformers is used for the implementation and management of the model components.",
          "quote": "We use HuggingFace Transformers [82] and PyTorch [53] for the implementation."
        },
        "aliases": [
          "Transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Huggingface’s transformers: State-of-the-art natural language processing",
          "justification": "The referenced paper for HuggingFace Transformers is 'Huggingface’s transformers: State-of-the-art natural language processing'.",
          "quote": "We use HuggingFace Transformers [82] and PyTorch [53] for the implementation."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1622,
    "prompt_tokens": 21363,
    "total_tokens": 22985
  }
}