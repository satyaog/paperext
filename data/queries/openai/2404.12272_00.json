{
  "paper": "2404.12272.txt",
  "words": 14420,
  "extractions": {
    "title": {
      "value": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
      "justification": "This is the main title of the paper.",
      "quote": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences"
    },
    "description": "The paper presents EvalGen, a mixed-initiative approach to aligning LLM-generated evaluation functions with human preferences. EvalGen assists users in generating evaluation criteria and implementing assertions, aiming for the evaluation functions to reflect user preferences. A qualitative study with industry practitioners explored the tool's effectiveness, and the paper draws implications for future design of LLM evaluation assistants.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents a qualitative study involving industry practitioners to evaluate the effectiveness of EvalGen.",
      "quote": "A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment."
    },
    "primary_research_field": {
      "name": {
        "value": "Human-Computer Interaction",
        "justification": "The paper centers around designing an interface (EvalGen) for human users to interact with automated evaluation generators, making it primarily focused on Human-Computer Interaction.",
        "quote": "Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper focuses on using Large Language Models (LLMs) for evaluating LLM outputs, which is a key area within Natural Language Processing.",
          "quote": "Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-4",
          "justification": "The paper mentions using GPT-4 for generating criteria and candidate assertions.",
          "quote": "We use an GPT-4 to propose various binary evaluation criteria in natural language, such as response length or tone."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-4 was used in the study and not developed as part of the research.",
          "quote": "We use an GPT-4 to propose various binary evaluation criteria in natural language, such as response length or tone."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed in the context of the study for generating evaluations.",
          "quote": "We use an GPT-4 to propose various binary evaluation criteria in natural language, such as response length or tone."
        },
        "is_compared": {
          "value": 1,
          "justification": "The outputs of GPT-4 were compared to user-generated or manually graded outputs for alignment.",
          "quote": "For code-based assertions, EvalGen’s interpretation of the criterion (i.e., GPT-4’s interpretation) did not match what the participants expected."
        },
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "The referenced paper title for GPT-4 usage in general context.",
          "quote": "Language Models are Few-Shot Learners"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ACI-Bench",
          "justification": "Used to provide medical conversation data for evaluation tasks.",
          "quote": "The medical pipeline operates on a dataset of 84 unstructured text transcripts from doctor-patient calls [54], aiming to extract specific information (e.g., symptoms, medication) without revealing any personally identifiable information (PII)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ACI-Bench: A Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation",
          "justification": "The dataset has been mentioned as ACI-Bench in the linked reference.",
          "quote": "The medical pipeline operates on a dataset of 84 unstructured text transcripts from doctor-patient calls [54], aiming to extract specific information (e.g., symptoms, medication) without revealing any personally identifiable information (PII)."
        }
      },
      {
        "name": {
          "value": "Amazon Product Reviews",
          "justification": "Used to provide product review data for SEO optimization evaluation tasks.",
          "quote": "The product pipeline involved crafting SEO-friendly descriptions for 100 Amazon products and their reviews [21]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Bridging Language and Items for Retrieval and Recommendation",
          "justification": "The referenced paper title for the dataset used to provide Amazon review data.",
          "quote": "The product pipeline involved crafting SEO-friendly descriptions for 100 Amazon products and their reviews [21]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "ChainForge",
          "justification": "ChainForge was the main system in which EvalGen was embedded for running LLM pipelines and evaluations.",
          "quote": "EvalGen is embedded inside an existing open-source interface for prompt engineering and auditing, ChainForge [1]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing",
          "justification": "This is the reference paper linked to the library ChainForge.",
          "quote": "EvalGen is embedded inside an existing open-source interface for prompt engineering and auditing, ChainForge [1]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1032,
    "prompt_tokens": 23395,
    "total_tokens": 24427
  }
}