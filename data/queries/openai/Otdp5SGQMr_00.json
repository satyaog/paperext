{
  "paper": "Otdp5SGQMr.txt",
  "words": 10586,
  "extractions": {
    "title": {
      "value": "Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "Privacy-Aware Compression for Federated Learning Through Numerical Mechanism Design"
    },
    "description": "This paper introduces the Interpolated Minimum Variance Unbiased (I-MVU) mechanism to enhance communication efficiency and privacy in Federated Learning (FL). The I-MVU mechanism extends the existing MVU mechanism by interpolating in the natural exponential family parameters, allowing for superior privacy-utility trade-offs and scalability. The paper evaluates I-MVU against several benchmarks, demonstrating state-of-the-art results on communication-efficient private FL.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts experimental evaluations on various datasets to demonstrate the efficacy of the proposed I-MVU mechanism, indicating it as an empirical study.",
      "quote": "Experimentally, we find that under both client-level and sample-level DP settings and across various benchmark datasets, the I-MVU mechanism provides a better privacy-utility trade-off."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The primary focus of the paper is on federated learning, specifically the communication efficiency and privacy aspects within this field.",
        "quote": "In private federated learning (FL), a server aggregates differentially private updates from a large number of clients in order to train a machine learning model."
      },
      "aliases": [
        "FL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Differential Privacy",
          "justification": "The paper extensively discusses differential privacy mechanisms and their applications in the context of federated learning.",
          "quote": "In private federated learning (FL), a server aggregates differentially private updates from a large number of clients in order to train a machine learning model."
        },
        "aliases": [
          "DP"
        ]
      },
      {
        "name": {
          "value": "Communication Efficiency",
          "justification": "A significant portion of the paper is dedicated to discussing and improving the communication efficiency of federated learning models.",
          "quote": "The result is the new Interpolated MVU mechanism that is more scalable, has a better privacy-utility trade-off, and provides SOTA results on communication-efficient private FL on a variety of datasets."
        },
        "aliases": []
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is mentioned in the experiments section.",
          "quote": "We first evaluate under the client-level DP setting on MNIST."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "No specific reference title is explicitly mentioned for MNIST.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is mentioned in the experiments section.",
          "quote": "We first evaluate under the client-level DP setting on CIFAR-10."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "No specific reference title is explicitly mentioned for CIFAR-10.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "FEMNIST",
          "justification": "FEMNIST is mentioned in the sample-level DP experiments.",
          "quote": "Next, we evaluate under the sample-level DP setting on the FEMNIST dataset."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Leaf: A benchmark for federated settings",
          "justification": "The reference title for FEMNIST is mentioned in the paper.",
          "quote": "The dataset has a pre-defined train split with 3, 500 clients, from which we randomly select 3, 150 clients for training and the remaining 350 clients for testing. A set of 5 clients is selected in each training round."
        }
      },
      {
        "name": {
          "value": "CIFAR-10 WideResNet-28-10",
          "justification": "CIFAR-10 with an ImageNet pre-trained WideResNet-28-10 model is mentioned in the experiments section.",
          "quote": "In Figure 3 we repeat the experiment with an ImageNet pre-trained WideResNet-28-10 model."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Wide residual networks",
          "justification": "The reference title for WideResNet-28-10 model is mentioned.",
          "quote": "In Figure 3 we repeat the experiment with an ImageNet pre-trained WideResNet-28-10 model."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2122,
    "prompt_tokens": 40410,
    "total_tokens": 42532
  }
}