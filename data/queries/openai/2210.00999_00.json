{
  "paper": "2210.00999.txt",
  "words": 13246,
  "extractions": {
    "title": {
      "value": "Latent State Marginalization as a Low-Cost Approach for Improving Exploration",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "L ATENT S TATE M ARGINALIZATION AS A L OW- COST A PPROACH FOR I MPROVING E XPLORATION"
    },
    "description": "This paper proposes the use of latent variable policies within the maximum entropy (MaxEnt) reinforcement learning framework to improve exploration and robustness. The authors introduce a method called Stochastic Marginal Actor-Critic (SMAC), which involves marginalizing both the actor and critic to make full use of the latent state at minimal additional cost. They validate their method through continuous control tasks and show improvements in exploration and training robustness.",
    "type": {
      "value": "empirical study",
      "justification": "The paper includes experimental validation of the proposed method on continuous control tasks, demonstrating its effectiveness through empirical results.",
      "quote": "We experimentally validate our method on continuous control tasks, showing that effective marginalization can lead to better exploration and more robust training."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on improving exploration in reinforcement learning by using latent variable policies within the MaxEnt framework.",
        "quote": "A fundamental goal of machine learning is to develop methods capable of sequential decision making, where reinforcement learning (RL) has achieved great success in recent decades."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Exploration in Reinforcement Learning",
          "justification": "The primary focus of the paper is on improving exploration within the reinforcement learning framework.",
          "quote": "One of the core problems in RL is exploration, the process by which an agent learns to interact with its environment."
        },
        "aliases": [
          "Exploration in RL"
        ]
      },
      {
        "name": {
          "value": "Maximum Entropy Reinforcement Learning",
          "justification": "The paper builds on the MaxEnt RL framework to propose its improvements.",
          "quote": "While the maximum entropy (MaxEnt) RL framework (Todorov, 2006; Rawlik et al., 2012) is often motivated for learning complex multi-modal1 behaviors through a stochastic agent, algorithms that are most often used in practice rely on simple agents that only make local perturbations around a single action."
        },
        "aliases": [
          "MaxEnt RL"
        ]
      },
      {
        "name": {
          "value": "Actor-Critic Methods",
          "justification": "The proposed method, SMAC, is implemented within the actor-critic framework.",
          "quote": "We instantiate our method under the actor-critic framework, marginalizing both the actor and critic."
        },
        "aliases": [
          "Actor-Critic"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Stochastic Marginal Actor-Critic",
          "justification": "The paper proposes SMAC as the new model for improving exploration and robustness in reinforcement learning.",
          "quote": "The resulting algorithm, referred to as Stochastic Marginal Actor-Critic (SMAC), is simple yet effective."
        },
        "aliases": [
          "SMAC"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "SMAC is the primary contribution of the paper.",
          "quote": "The resulting algorithm, referred to as Stochastic Marginal Actor-Critic (SMAC), is simple yet effective."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper presents experimental results demonstrating the execution of SMAC on various tasks.",
          "quote": "We experimentally validate our method on continuous control tasks, showing that effective marginalization can lead to better exploration and more robust training."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares SMAC with other models like SAC and TD3.",
          "quote": "We first compare SMAC with SAC and TD3 (Fujimoto et al., 2018) baselines on a variety of state-based environments to demonstrate the advantage of latent variable policies."
        },
        "referenced_paper_title": {
          "value": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
          "justification": "The referenced paper title for the model SAC, which the proposed method builds upon, is clearly mentioned.",
          "quote": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "DeepMind Control Suite",
          "justification": "The paper uses this dataset for evaluating their proposed method on various continuous control tasks.",
          "quote": "We evaluate SMAC on a series of diverse continuous control tasks from DeepMind Control Suite (DMC; Tassa et al. (2018))."
        },
        "aliases": [
          "DMC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "reacher_easy",
          "justification": "The reacher_easy task from the DeepMind Control Suite is explicitly mentioned as one of the tasks used for evaluation.",
          "quote": "We first compare SMAC with SAC and TD3 (Fujimoto et al., 2018) baselines on a variety of state-based environments to demonstrate the advantage of latent variable policies. We show eight environments in Figure 4, and leave more results in Appendix due to space limitations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "hopper_stand",
          "justification": "The hopper_stand task from the DeepMind Control Suite is explicitly mentioned as one of the tasks used for evaluation.",
          "quote": "We evaluate SMAC on a series of diverse continuous control tasks from DeepMind Control Suite (DMC; Tassa et al. (2018)). These tasks include challenging cases in the sense of having sparse rewards, high dimensional action space, or pixel observations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The implementation of SMAC leverages PyTorch, which is a widely-used deep learning library.",
          "quote": "We follow the PyTorch model-free SAC implementation of Tandon (2020) for this part."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "TensorFlow",
          "justification": "The original implementation of Soft Actor-Critic (SAC), which SMAC builds upon, is associated with TensorFlow.",
          "quote": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor",
          "justification": "The referenced paper title for the model SAC, which the proposed method builds upon, is clearly mentioned.",
          "quote": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1376,
    "prompt_tokens": 25626,
    "total_tokens": 27002
  }
}