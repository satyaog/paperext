{
  "paper": "dc7fee36b3f97b94c12fef07111ec19f.txt",
  "words": 9232,
  "extractions": {
    "title": {
      "value": "Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding"
    },
    "description": "The paper presents a method to improve compositional reasoning in Vision-Language Models (VLMs) like CLIP by refining and expanding the image-text contrastive learning framework. The proposed approach focuses on generating and adequately using hard negatives to enhance alignment between images and captions. This method yields notable improvements over state-of-the-art baselines across five vision-language compositional benchmarks without requiring additional annotations or extra parameters.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experimental validation of the proposed methods using quantitative metrics on various benchmarks.",
      "quote": "To validate the effectiveness, we conduct experiments on two models: the versatile CLIP and the strong X-VLM [71]. Our evaluation across various compositional datasets..."
    },
    "primary_research_field": {
      "name": {
        "value": "Vision-Language Compositionality",
        "justification": "The primary focus of the paper is on enhancing the compositional understanding capabilities of Vision-Language Models by improving image-text alignment and contrastive learning objectives.",
        "quote": "Our method better leverages available datasets by refining and expanding the standard image-text contrastive learning framework."
      },
      "aliases": [
        "VLC"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Contrastive Learning",
          "justification": "The paper introduces refinements in contrastive learning methods specifically tailored for Vision-Language Models.",
          "quote": "We focus on two dimensions. First, we advocate for a clearer distinction in the representations of positive and hard-negative captions, aiming to boost the model’s ability to recognize nuanced semantic variations."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Vision-Language Alignment",
          "justification": "The study deals with improving the alignment between images and text in Vision-Language Models.",
          "quote": "The root of this limitation lies in the inadequate alignment between the images and captions in the pretraining datasets."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Hard Negative Mining",
          "justification": "The generation and usage of hard negatives is a core component of the proposed method.",
          "quote": "We introduce a simple and effective method to improve compositional reasoning in VLMs. Our method better leverages available datasets by refining and expanding the standard image-text contrastive learning framework."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP (Contrastive Language–Image Pre-training)",
          "justification": "CLIP is the baseline Vision-Language Model evaluated and improved upon in this study.",
          "quote": "To validate the effectiveness, we conduct experiments on two models: the versatile CLIP and the strong X-VLM [71]."
        },
        "aliases": [
          "CLIP"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "CLIP is used as a baseline model in the study, not introduced by it.",
          "quote": "To validate the effectiveness, we conduct experiments on two models: the versatile CLIP and the strong X-VLM [71]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The study conducts experiments involving fine-tuning and evaluating CLIP.",
          "quote": "We employ the CLIP-VIT/32-B from the Open-CLIP implementation and the X-VLM-16M from its primary code repository for evaluation purposes."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the improved CLIP model is compared to its baseline and other models on various benchmarks.",
          "quote": "Our evaluation across various compositional datasets consistently reveals performance enhancements, establishing our method as a new state-of-the-art across all assessed benchmarks."
        },
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models From Natural Language Supervision",
          "justification": "This is the original paper where CLIP was introduced.",
          "quote": "Strong Vision-Language Models (VLMs), such as CLIP [52], are even pushing the boundaries in text-to-image generation..."
        }
      },
      {
        "name": {
          "value": "X-VLM",
          "justification": "X-VLM is another Vision-Language Model used for comparative analysis in the paper.",
          "quote": "To validate the effectiveness, we conduct experiments on two models: the versatile CLIP and the strong X-VLM [71]."
        },
        "aliases": [
          "X-VLM"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "X-VLM is used for comparison and as one of the evaluation models in the paper.",
          "quote": "To validate the effectiveness, we conduct experiments on two models: the versatile CLIP and the strong X-VLM [71]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experiments involving X-VLM to demonstrate the effectiveness of the proposed approach.",
          "quote": "We employ the CLIP-VIT/32-B from the Open-CLIP implementation and the X-VLM-16M from its primary code repository for evaluation purposes."
        },
        "is_compared": {
          "value": 1,
          "justification": "X-VLM's performance is compared against CLIP and other models on multiple benchmarks.",
          "quote": "Specific improvements of 0.5%, 2.5% respectively on the ARO Relation and Attribution splits, 1.3% on VALSE and 2.1% on VL-CheckList on top of the already strong X-VLM model upon application of our method."
        },
        "referenced_paper_title": {
          "value": "Multi-grained Vision Language Pre-training: Aligning Texts with Visual Concepts",
          "justification": "This is the reference paper for X-VLM.",
          "quote": "To validate the effectiveness, we conduct experiments on two models: the versatile CLIP and the strong X-VLM [71]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "COCO",
          "justification": "COCO is one of the datasets on which the fine-tuning of CLIP and evaluation of the proposed method were conducted.",
          "quote": "Specifically, training CLIP with our method on the COCO dataset leads to an improvement of 23.7% and 13.5% respectively on the Relation and Attribution splits of the ARO benchmark [70], 7.2% on the VALSE benchmark [49], 5.9% on the VL-CheckList benchmark [74], and a significant improvement of 12.1% on the recently developed SugarCrepe benchmark [24]."
        },
        "aliases": [
          "Common Objects in Context"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Microsoft COCO: Common Objects in Context",
          "justification": "This is the reference paper for the COCO dataset.",
          "quote": "Specifically, training CLIP with our method on the COCO dataset leads to an improvement of 23.7% and 13.5% respectively on the Relation and Attribution splits of the ARO benchmark [70], 7.2% on the VALSE benchmark [49], 5.9% on the VL-CheckList benchmark [74], and a significant improvement of 12.1% on the recently developed SugarCrepe benchmark [24]."
        }
      },
      {
        "name": {
          "value": "CC3M",
          "justification": "CC3M is another dataset that was leveraged to train the enhanced version of CLIP (CE-CLIP+).",
          "quote": "Evaluation across various compositional datasets consistently reveals performance enhancements, establishing our method as a new state-of-the-art across all assessed benchmarks. Specifically, training CLIP with our method on the COCO dataset leads to an improvement of 23.7% and 13.5% respectively on the Relation and Attribution splits of the ARO benchmark [70], 7.2% on the VALSE benchmark [49], 5.9% on the VL-CheckList benchmark [74], and a significant improvement of 12.1% on the recently developed SugarCrepe benchmark [24]."
        },
        "aliases": [
          "Conceptual Captions"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-Text Dataset For Automatic Image Captioning",
          "justification": "This is the reference paper for the CC3M dataset.",
          "quote": "Evaluation across various compositional datasets consistently reveals performance enhancements, establishing our method as a new state-of-the-art across all assessed benchmarks. Specifically, training CLIP with our method on the COCO dataset leads to an improvement of 23.7% and 13.5% respectively on the Relation and Attribution splits of the ARO benchmark [70], 7.2% on the VALSE benchmark [49], 5.9% on the VL-CheckList benchmark [74], and a significant improvement of 12.1% on the recently developed SugarCrepe benchmark [24]."
        }
      },
      {
        "name": {
          "value": "Visual Genome",
          "justification": "Visual Genome is another dataset that was leveraged to train the enhanced version of CLIP (CE-CLIP+).",
          "quote": "Evaluation across various compositional datasets consistently reveals performance enhancements, establishing our method as a new state-of-the-art across all assessed benchmarks. Specifically, training CLIP with our method on the COCO dataset leads to an improvement of 23.7% and 13.5% respectively on the Relation and Attribution splits of the ARO benchmark [70], 7.2% on the VALSE benchmark [49], 5.9% on the VL-CheckList benchmark [74], and a significant improvement of 12.1% on the recently developed SugarCrepe benchmark [24]."
        },
        "aliases": [
          "VG"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Visual Genome: Connecting Language and Vision using Crowdsourced Dense Image Annotations",
          "justification": "This is the reference paper for the Visual Genome dataset.",
          "quote": "Evaluation across various compositional datasets consistently reveals performance enhancements, establishing our method as a new state-of-the-art across all assessed benchmarks. Specifically, training CLIP with our method on the COCO dataset leads to an improvement of 23.7% and 13.5% respectively on the Relation and Attribution splits of the ARO benchmark [70], 7.2% on the VALSE benchmark [49], 5.9% on the VL-CheckList benchmark [74], and a significant improvement of 12.1% on the recently developed SugarCrepe benchmark [24]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Spacy",
          "justification": "Spacy was used for Part-Of-Speech (POS) parsing in the process of generating hard negatives.",
          "quote": "To generate these hard negatives, we employ Part-Of-Speech (POS) parsing and Language Models. Utilizing Spacy [23], we parse the captions and assign POS tags to each word."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "spaCy 2: Natural Language Understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
          "justification": "This is the reference paper for Spacy.",
          "quote": "To generate these hard negatives, we employ Part-Of-Speech (POS) parsing and Language Models. Utilizing Spacy [23], we parse the captions and assign POS tags to each word."
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa was used in the process of generating hard negatives, specifically for filling in masked areas in the captions.",
          "quote": "For each caption, we generate all four types of hard negatives, replacing any examples in which the requisite words or two objects are absent from the caption with a placeholder string. This approach ensures comprehensive and robust training dataset for enhancing our model’s performance."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "justification": "This is the reference paper for RoBERTa.",
          "quote": "To generate these hard negatives, we employ Part-Of-Speech (POS) parsing and Language Models. Utilizing Spacy [23], we parse the captions and assign POS tags to each word."
        }
      },
      {
        "name": {
          "value": "Open-CLIP",
          "justification": "Open-CLIP implementation was used for evaluating the CLIP model.",
          "quote": "We employ the CLIP-VIT/32-B from the Open-CLIP implementation and the X-VLM-16M from its primary code repository for evaluation purposes."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models From Natural Language Supervision",
          "justification": "This library is related to the original implementation and evaluation of CLIP.",
          "quote": "We employ the CLIP-VIT/32-B from the Open-CLIP implementation and the X-VLM-16M from its primary code repository for evaluation purposes."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2590,
    "prompt_tokens": 19517,
    "total_tokens": 22107
  }
}