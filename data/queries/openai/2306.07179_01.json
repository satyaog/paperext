{
  "paper": "2306.07179.txt",
  "words": 45669,
  "extractions": {
    "title": {
      "value": "Benchmarking Neural Network Training Algorithms",
      "justification": "It is the official title of the paper as mentioned at the beginning.",
      "quote": "Benchmarking Neural Network Training Algorithms"
    },
    "description": "The paper provides a systematic procedure for benchmarking neural network training algorithms, called AlgoPerf, to reliably identify improvements in training algorithms. It introduces a time-to-result benchmark using multiple workloads across various datasets and models, specifying fixed hardware for experiments. The paper outlines the challenges associated with empirical comparisons of training algorithms and provides baseline results demonstrating the feasibility of their benchmark. Additionally, it includes randomized workloads to ensure the robustness of the training algorithms.",
    "type": {
      "value": "empirical study",
      "justification": "The paper is based on conducting numerous experiments comparing different training algorithms on predefined tasks and hardware setups.",
      "quote": "In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks..."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on benchmarking training algorithms specifically within the context of deep learning models and tasks.",
        "quote": "Training algorithms improvements that speed up training across a wide variety of workloads... could save time, save computational resources..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Network Optimization",
          "justification": "The main focus of the paper is on optimizing the training algorithms used for neural networks.",
          "quote": "...our benchmark is feasible and that there is a non-trivial gap between different methods."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet-50",
          "justification": "ResNet-50 is explicitly mentioned as a model used in the experiments within the paper.",
          "quote": "For example, training ResNet-50 on ImageNet"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ViT",
          "justification": "ViT is explicitly mentioned as a model used in experiments within the paper.",
          "quote": "ViT, Vision Transformer model."
        },
        "aliases": [
          "Vision Transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "U-Net",
          "justification": "U-Net is used in MRI reconstruction tasks as specified in the experimental workloads.",
          "quote": "We train a U-Net model similar to the one described in Ronneberger et al. (2015)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DLRM",
          "justification": "DLRM is mentioned as the model used for click-through rate prediction in the Criteo 1TB dataset.",
          "quote": "The single embedding table is of size 4M entries with an embedding dimension of 128. The dense features are fed into a three-layer fully-connected network with 512, 256, 128 units per layer. The outputs of this layer are then concatenated to the embedding lookups of the categorical features, and fed into the cross-interaction layer. Finally, the cross-interaction output is passed into a five-layer fully-connected network with 1024, 1024, 512, 256, 1 units per layer."
        },
        "aliases": [
          "Deep Learning Recommendation Model"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Conformer",
          "justification": "Conformer is explicitly mentioned as a model used in experiments within the paper for speech recognition.",
          "quote": "Conformer (Gulati et al., 2020) is an architecture combining attention and convolution layers to capture both global and local relationships in input audio."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DeepSpeech",
          "justification": "DeepSpeech is included as a model used in the paper for the LibriSpeech dataset.",
          "quote": "We use a variant of the DeepSpeech (Amodei et al., 2016) model with residual connections, dropout (Srivastava et al., 2014), layer normalization, and SpecAugment (Park et al., 2019) to improve performance"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Transformer",
          "justification": "A large Transformer model is used for the WMT translation tasks as described in the paper.",
          "quote": "We use the Transformer-big architecture from Vaswani et al. (2017) with some modifications."
        },
        "aliases": [
          "Big"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "inference"
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet dataset is explicitly mentioned in the context of training ResNet-50 and ViT models.",
          "quote": "For example, training ResNet-50 on ImageNet"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "fastMRI",
          "justification": "The fastMRI dataset is used for MRI reconstruction tasks with the U-Net model.",
          "quote": "We train a U-Net model similar to the one described in Ronneberger et al. (2015)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Criteo 1TB",
          "justification": "Criteo 1TB dataset is referenced for click-through rate prediction using the DLRM model.",
          "quote": "We train on the Criteo 1TB Click Logs dataset"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "LibriSpeech",
          "justification": "The LibriSpeech dataset is used for speech recognition tasks with Conformer and DeepSpeech models.",
          "quote": "We use the LibriSpeech dataset (Panayotov et al., 2015)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "OGBG-MOLPCBA",
          "justification": "OGBG-MOLPCBA is mentioned for training the GNN model for molecular property prediction.",
          "quote": "We use the OGBG-MOLPCBA dataset (Hu et al., 2020) containing molecular graphs and 128 molecular properties."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "WMT 2017",
          "justification": "WMT 2017 is mentioned as the dataset for machine translation tasks using the Transformer model.",
          "quote": "The models are trained on the WMT 2017 GermanÑEnglish (DeÑEn) training dataset"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "The paper mentions that all workloads have open-source implementations in JAX, and it is one of the frameworks supported by the benchmark.",
          "quote": "We provide open-source JAX"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions that all workloads have open-source implementations in PyTorch, and it is one of the frameworks supported by the benchmark.",
          "quote": "We provide open-source...PyTorch implementations of all workloads"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1669,
    "prompt_tokens": 70512,
    "total_tokens": 72181
  }
}