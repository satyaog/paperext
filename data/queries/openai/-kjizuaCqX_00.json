{
  "paper": "-kjizuaCqX.txt",
  "words": 7594,
  "extractions": {
    "title": {
      "value": "Interventional Causal Representation Learning",
      "justification": "This is the title of the paper",
      "quote": "Interventional Causal Representation Learning"
    },
    "description": "This paper explores the role of interventional data in identifiable representation learning, studying the identifiability of latent causal factors with and without interventional data under minimal distributional assumptions on latents. It investigates how high-level latents can be identified using both observational and interventional data, aiming to develop methods for extracting reusable causal knowledge from data.",
    "type": {
      "value": "Theoretical Study",
      "justification": "The paper focuses on the theoretical aspects of leveraging interventional data for identifiable representation learning, proving various conditions under which latent causal factors can be identified.",
      "quote": "We study the identifiability of latent causal factors with and without interventional data, under minimal distributional assumptions on latents. We prove that, if the true latent maps to the observed high-dimensional data via a polynomial function, then representation learning via minimizing standard reconstruction loss (used in autoencoders) can identify the true latents up to affine transformation."
    },
    "primary_research_field": {
      "name": {
        "value": "Causal Representation Learning",
        "justification": "The primary focus of the paper is on causal representation learning, aiming to extract high-level causal factors from data using interventional information.",
        "quote": "This question is central to the emerging field of causal representation learning (Schölkopf et al., 2021)."
      },
      "aliases": [
        "CRL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Identifiable Representation Learning",
          "justification": "This subfield is relevant because the paper is concerned with conditions under which representation learning can provably identify latent objects and their causal relationships.",
          "quote": "A core task in causal representation learning is provable representation identification, i.e., developing conditions under which representation learning algorithms can provably identify latent objects (or factors) and their causal relationships."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Interventional Data Analysis",
          "justification": "The paper specifically explores the role of interventional data, which is a key subfield covered in this research.",
          "quote": "In this work, we seek to understand how we can leverage such interventional data to identify high level (causal) factors from low level data."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-3",
          "justification": "GPT-3 is mentioned as an example of a modern deep learning model that is a remarkable representation learner but still far from human ability to adapt to new tasks.",
          "quote": "Modern deep learning models like GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) are remarkable representation learners."
        },
        "aliases": [
          "Generative Pre-trained Transformer 3"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not a contribution of this paper but is instead referenced as an example.",
          "quote": "Modern deep learning models like GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) are remarkable representation learners."
        },
        "is_executed": {
          "value": 0,
          "justification": "The model was not executed as part of this study.",
          "quote": "Modern deep learning models like GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) are remarkable representation learners."
        },
        "is_compared": {
          "value": 0,
          "justification": "The model was not compared to other models in this study.",
          "quote": "Modern deep learning models like GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) are remarkable representation learners."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The referenced paper is cited in the context of mentioning GPT-3.",
          "quote": "Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901."
        }
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is mentioned as another example of a modern deep learning model that excels at representation learning.",
          "quote": "Modern deep learning models like GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) are remarkable representation learners."
        },
        "aliases": [
          "Contrastive Language–Image Pre-training"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not a contribution of this paper but is instead referenced as an example.",
          "quote": "Modern deep learning models like GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) are remarkable representation learners."
        },
        "is_executed": {
          "value": 0,
          "justification": "The model was not executed as part of this study.",
          "quote": "Modern deep learning models like GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) are remarkable representation learners."
        },
        "is_compared": {
          "value": 0,
          "justification": "The model was not compared to other models in this study.",
          "quote": "Modern deep learning models like GPT-3 (Brown et al., 2020) and CLIP (Radford et al., 2021) are remarkable representation learners."
        },
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models From Natural Language Supervision",
          "justification": "The referenced paper is cited in the context of mentioning CLIP.",
          "quote": "Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748–8763. PMLR."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "This common dataset is used in reference to related works on identifiable representation learning.",
          "quote": "We refer to commonly used datasets such as CIFAR-10 and MNIST in our related works."
        },
        "aliases": [
          "CIFAR-10"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the reference for the CIFAR-10 dataset.",
          "quote": "Krizhevsky, A., and Hinton, G. (2009). Learning Multiple Layers of Features from Tiny Images. Technical report, University of Toronto."
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "This common dataset is used in reference to related works on identifiable representation learning.",
          "quote": "We refer to commonly used datasets such as CIFAR-10 and MNIST in our related works."
        },
        "aliases": [
          "MNIST"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. 2010. MNIST handwritten digit database. 2010. Freely available at http://yann.lecun.com/exdb/mnist",
          "justification": "This is the reference for the MNIST dataset.",
          "quote": "Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. 2010. MNIST handwritten digit database. 2010. Freely available at http://yann.lecun.com/exdb/mnist"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3141,
    "prompt_tokens": 29633,
    "total_tokens": 32774
  }
}