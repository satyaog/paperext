{
  "paper": "2311.04293.txt",
  "words": 7522,
  "extractions": {
    "title": {
      "value": "Lie Point Symmetry and Physics Informed Networks",
      "justification": "This is the title mentioned at the beginning of the paper.",
      "quote": "Lie Point Symmetry and Physics Informed Networks"
    },
    "description": "This paper explores the integration of Lie point symmetries into Physics-Informed Neural Networks (PINNs) to improve the efficiency and generalization capabilities of neural solvers for partial differential equations (PDEs). The authors propose a novel loss function that includes symmetry information and demonstrate, through empirical evaluations, that this approach significantly enhances the sample efficiency of PINNs.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper includes empirical evaluations to demonstrate the effectiveness of integrating Lie point symmetries into PINNs.",
      "quote": "Empirical evaluations indicate that the inductive bias introduced by the Lie point symmetries of the PDEs greatly boosts the sample efficiency of PINNs."
    },
    "primary_research_field": {
      "name": {
        "value": "Mathematics & Theory",
        "justification": "The paper is primarily focused on mathematical concepts like Lie point symmetries and their application in neural networks for solving PDEs.",
        "quote": "Symmetries have been leveraged to improve the generalization of neural networks through different mechanisms from data augmentation to equivariant architectures. However, despite their potential, their integration into neural solvers for partial differential equations (PDEs) remains largely unexplored."
      },
      "aliases": [
        "Mathematics",
        "Theory"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Physics-Informed Neural Networks",
          "justification": "The main application discussed in the paper is enhancing PINNs by incorporating Lie point symmetries.",
          "quote": "In this work, for the first time we integrate symmetries into PINNs and demonstrate the effectiveness of this symmetry regularization on generalization capabilities."
        },
        "aliases": [
          "PINNs"
        ]
      },
      {
        "name": {
          "value": "Partial Differential Equations",
          "justification": "The paper deals with solving PDEs using neural networks enhanced by symmetry information.",
          "quote": "A prominent example is the role of deep learning in solving partial differential equations (PDEs), which are ubiquitous in many scientific disciplines."
        },
        "aliases": [
          "PDEs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Physics-Informed Neural Networks",
          "justification": "PINNs are used as the primary model in the study.",
          "quote": "In PINNs, the PDE solution u(t, x) of Eq. (1) is a neural network uθ (t, x) with parameters θ."
        },
        "aliases": [
          "PINNs",
          "Physics-Informed NN",
          "PINN"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "PINNs are not introduced in this paper but are used as a foundation for the proposed integration of symmetries.",
          "quote": "Lie point symmetries have been introduced to deep learning by Brandstetter et al. [2022a] for Lie point symmetry data augmentation which places a firm mathematical footing on data augmentation pipelines for neural PDE solvers."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical evaluations, indicating that PINNs were executed during the experiments.",
          "quote": "Empirical evaluations indicate that the inductive bias introduced by the Lie point symmetries of the PDEs greatly boosts the sample efficiency of PINNs."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of PINNs with and without the symmetry loss is compared.",
          "quote": "The effect of training the PDE solver for the heat equation with and without the symmetry loss for one of the PDEs in the test dataset."
        },
        "referenced_paper_title": {
          "value": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations",
          "justification": "This is the seminal paper introducing Physics-Informed Neural Networks (PINNs).",
          "quote": "As such, PINNs can be seen as a data-free learning approach, utilizing available physics information to guide neural network training.\nLie point symmetries have been introduced to deep learning by Brandstetter et al. [2022a] for Lie point symmetry data augmentation which places a firm mathematical footing on data augmentation pipelines for neural PDE solvers."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Heat Equation",
          "justification": "The dataset related to the Heat Equation is used for empirical validation in the paper.",
          "quote": "Example 1 (Heat Equation). The one-dimensional heat equation describes the heat conduction in a one-dimensional rod, with viscosity coefficient ν."
        },
        "aliases": [
          "Heat Equation Dataset"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Learning data-driven discretizations for partial differential equations",
          "justification": "This referenced paper is cited in the context of using Heat Equation as a benchmark.",
          "quote": "Similar to Brandstetter et al. [2022b,a] and Bar-Sinai et al. [2019], we represent the initial condition functions by truncated Fourier series with coefficients Ak , lk , ϕk sampled randomly, and K = 10"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "ADAM",
          "justification": "ADAM optimizer is explicitly mentioned as used in the training of models.",
          "quote": "We used ADAM optimizer with learning rate of 0.001 for the training and performed early stopping using the validation dataset."
        },
        "aliases": [
          "Adaptive Moment Estimation"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ADAM: A Method for Stochastic Optimization",
          "justification": "This is the original paper where the ADAM optimizer was introduced.",
          "quote": "We used ADAM optimizer with learning rate of 0.001 for the training and performed early stopping using the validation dataset."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1124,
    "prompt_tokens": 12964,
    "total_tokens": 14088
  }
}