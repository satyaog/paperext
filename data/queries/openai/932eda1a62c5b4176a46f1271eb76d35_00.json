{
  "paper": "932eda1a62c5b4176a46f1271eb76d35.txt",
  "words": 7156,
  "extractions": {
    "title": {
      "value": "AsmDocGen: Generating Functional Natural Language Descriptions for Assembly Code",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "AsmDocGen: Generating Functional Natural Language Descriptions for Assembly Code"
    },
    "description": "This research explores the use of NLP models for summarizing assembly code, specifically highlighting the utilization of CodeBERT model for generating human-readable comments for assembly functions, thereby aiding reverse engineering tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves the creation of a dataset and experiments with different NLP models to generate and evaluate summaries for assembly code.",
      "quote": "The results of our experiments show a notable advantage of CodeBERT: despite its initial training on high-level programming languages alone, it excels in learning assembly language, outperforming other pre-trained NLP models."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on using pre-trained NLP models to summarize assembly code.",
        "quote": "We then focused on automatic assembly code summarization using transfer learning with pre-trained natural language processing (NLP) models, including BERT, DistilBERT, RoBERTa, and CodeBERT."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Code Summarization",
          "justification": "The study specifically addresses the summarization of code, particularly assembly code, using NLP models.",
          "quote": "This study explores the field of software reverse engineering through the lens of code summarization, which involves generating informative and concise summaries of code functionality."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The research employs transfer learning techniques using pre-trained models to enhance assembly code summarization.",
          "quote": "We provide evidence that NLP models pre-trained on other corpora can be successfully retrained and tuned to be applied to assembly code."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CodeBERT",
          "justification": "CodeBERT is specifically highlighted as the model used in the proposed AsmDocGen solution.",
          "quote": "Our experiments show a notable advantage of CodeBERT: despite its initial training on high-level programming languages alone, it excels in learning assembly language, outperforming other pre-trained NLP models."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "While CodeBERT is utilized as part of the research, it is not a new contribution made by the authors of this paper.",
          "quote": "We chose CodeBERT (Feng et al., 2020) as our foundation to build AsmDocGen."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves experiments conducted using CodeBERT for generating comments on assembly code.",
          "quote": "Our experiments show a notable advantage of CodeBERT."
        },
        "is_compared": {
          "value": true,
          "justification": "CodeBERT's performance is compared with other models like BERT, DistilBERT, and RoBERTa in the experiments.",
          "quote": "We provide evidence that NLP models pre-trained on other corpora can be successfully retrained and tuned to be applied to assembly code."
        },
        "referenced_paper_title": {
          "value": "Code-bert: A pre-trained model for programming and natural languages",
          "justification": "The referenced paper is clearly stated where CodeBERT is introduced and utilized in the research paper.",
          "quote": "Feng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang, D., and Zhou, M. (2020). Code-bert: A pre-trained model for programming and natural languages."
        }
      },
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is listed among the NLP models that were used to compare against the proposed AsmDocGen approach in the paper.",
          "quote": "We then focused on automatic assembly code summarization using transfer learning with pre-trained natural language processing (NLP) models, including BERT, DistilBERT, RoBERTa, and CodeBERT."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BERT is not a new contribution by the authors. It is a pre-existent model used for comparison.",
          "quote": "BERT, introduced by Devlin et al. (Devlin et al., 2019) in 2019, is an NLP model developed by Google in 2018."
        },
        "is_executed": {
          "value": true,
          "justification": "BERT was used in the experiments to compare its performance on summarizing assembly code against CodeBERT.",
          "quote": "The outcome of the comparison between AsmDocGen, BERT, RoBERTa, and DistilBERT shows the high capability of CodeBERT to learn and understand the assembly language."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares BERT's performance on assembly code summarization with that of other models.",
          "quote": "The outcome of the comparison between AsmDocGen, BERT, RoBERTa, and DistilBERT shows the high capability of CodeBERT to learn and understand the assembly language."
        },
        "referenced_paper_title": {
          "value": "Bert: Pre-training of deep bidirectional transformers for language understanding",
          "justification": "The BERT model is referenced from the paper where it was first introduced by Devlin et al.",
          "quote": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding."
        }
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "The DistilBERT model was used as one of the baseline models in the experimental comparison.",
          "quote": "We then focused on automatic assembly code summarization using transfer learning with pre-trained natural language processing (NLP) models, including BERT, DistilBERT, RoBERTa, and CodeBERT."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "DistilBERT is not contributed by this paper, it's used as a benchmark model for comparison.",
          "quote": "DistilBERT, introduced by Sanh et al. (Sanh et al., 2019), is a compact and efficient version of the BERT model."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses DistilBERT in its experimental comparison of models for assembly code summarization.",
          "quote": "We evaluated the performance of our model by comparing it against a convenience sample of four leading NLP models: RoBERTa, BERT, DistilBERT and Transformer."
        },
        "is_compared": {
          "value": true,
          "justification": "DistilBERT is among the models compared in the paper's experiments.",
          "quote": "The outcome of the comparison between AsmDocGen, BERT, RoBERTa, and DistilBERT shows the high capability of CodeBERT to learn and understand the assembly language."
        },
        "referenced_paper_title": {
          "value": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
          "justification": "The referenced paper provides the origin of DistilBERT as mentioned in the paper.",
          "quote": "Sanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter."
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is used as one of the baseline models in the experiments comparing different NLP models for the task at hand.",
          "quote": "We then focused on automatic assembly code summarization using transfer learning with pre-trained natural language processing (NLP) models, including BERT, DistilBERT, RoBERTa, and CodeBERT."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "RoBERTa is an existing model not developed specifically for this research.",
          "quote": "RoBERTa, introduced by Liu et al. (Liu and Lapata, 2019) in 2019, is an NLP model that is based on the same architecture as BERT but with several modifications to its training process and hyperparameters."
        },
        "is_executed": {
          "value": true,
          "justification": "RoBERTa was executed in the experiments to compare its performance on the assembly code summarization task.",
          "quote": "We evaluated the performance of our model by comparing it against a convenience sample of four leading NLP models: RoBERTa, BERT, DistilBERT and Transformer."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper includes comparisons of RoBERTa with other NLP models used in the experiments.",
          "quote": "The outcome of the comparison between AsmDocGen, BERT, RoBERTa, and DistilBERT shows the high capability of CodeBERT to learn and understand the assembly language."
        },
        "referenced_paper_title": {
          "value": "Text summarization with pretrained encoders",
          "justification": "This quote explains what makes RoBERTa unique among NLP models and is part of the referenced literature.",
          "quote": "Liu, Y. and Lapata, M. (2019). Text summarization with pretrained encoders."
        }
      },
      {
        "name": {
          "value": "AsmDocGen",
          "justification": "AsmDocGen is specifically proposed and introduced as a result of this research for generating comments on assembly code.",
          "quote": "We propose AsmDocGen, a CodeBERT-based solution that generates human-readable comments for assembly functions."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "AsmDocGen is newly proposed in this paper as a novel approach for summarizing assembly code.",
          "quote": "This innovative approach sets a new standard in the area."
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments are conducted using the AsmDocGen model.",
          "quote": "AsmDocGen represents a significant advance in the field of automatic code commenting for low-level programming languages."
        },
        "is_compared": {
          "value": true,
          "justification": "AsmDocGen's performance is compared to other NLP models like BERT, RoBERTa, and DistilBERT.",
          "quote": "Table 2 shows the BLEU score for Transformer, RoBERTa, DistilBERT, BERT and AsmDocGen."
        },
        "referenced_paper_title": {
          "value": "AsmDocGen: Generating Functional Natural Language Descriptions for Assembly Code",
          "justification": "AsmDocGen is introduced and described in this paper hence this is its originating work.",
          "quote": "We propose AsmDocGen, a CodeBERT-based solution that generates human-readable comments for assembly functions."
        }
      },
      {
        "name": {
          "value": "Transformer",
          "justification": "The Transformer model is used as a baseline model to compare with the proposed AsmDocGen model and other models used in the paper's experiments.",
          "quote": "To test our transfer learning hypothesis and evaluate AsmDocGen’s performance, we train a sample of NLP models to compare their results with AsmDocGen. The CPU architecture of the code is x86/x64."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Transformer is used in the paper's experiments but was not contributed by this research.",
          "quote": "Transformer, introduced by Vaswani et al. (Vaswani et al., 2017), is a neural network architecture based on the concept of self-attention, which allows the model to weigh the importance of different parts of the input sequence when generating output."
        },
        "is_executed": {
          "value": true,
          "justification": "The Transformer was used in the paper's experiments to compare performance.",
          "quote": "We evaluated the performance of our model by comparing it against a convenience sample of four leading NLP models: RoBERTa, BERT, DistilBERT, and Transformer."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of the Transformer model is compared with various other models in the experiments.",
          "quote": "The comparison between the Transformer and the pre-trained models shows the effectiveness of transfer learning in training a model for code summarization using a small dataset."
        },
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "Transformer, as referenced in the paper, is based on the architecture conceptualized in the referenced work.",
          "quote": "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AsmDocGen Dataset",
          "justification": "The paper explicitly mentions a dataset of 5,084 assembly function comment pairs curated for training and evaluating summarization solutions.",
          "quote": "We provide a well-curated dataset of 5,084 assembly function comment pairs for training and validating assembly code summarization solutions."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "AsmDocGen: Generating Functional Natural Language Descriptions for Assembly Code",
          "justification": "The dataset is contributed as part of the AsmDocGen research and outlined within this paper.",
          "quote": "We provide a well-curated dataset of 5,084 assembly function comment pairs for training and validating assembly code summarization solutions."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2725,
    "prompt_tokens": 12642,
    "total_tokens": 15367,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}