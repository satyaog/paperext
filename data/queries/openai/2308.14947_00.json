{
  "paper": "2308.14947.txt",
  "words": 6193,
  "extractions": {
    "title": {
      "value": "Improving Generalization in Reinforcement Learning Training Regimes for Social Robot Navigation",
      "justification": "The title is directly taken from the provided paper text.",
      "quote": "Improving Generalization in Reinforcement Learning Training Regimes for Social Robot Navigation"
    },
    "description": "This paper explores ways to improve the generalization performance of RL social navigation methods using curriculum learning. Different pedestrian dynamics models and varied environment types are used to train models, aiming for better generalization to more challenging scenarios.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves conducting experiments with various training regimes and compares their performance in different testing environments.",
      "quote": "This paper explores ways to improve the quantitative and qualitative performance of three state-of-the-art RL social navigation models (CADRL [5], LSTM-RL [6], and SARL [4]) through training."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary focus of the paper is on improving training regimes for reinforcement learning in social robot navigation contexts.",
        "quote": "Reinforcement learning (RL) has emerged as an effective method to train sequential decision-making policies that are able to respect these norms."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Social Robot Navigation",
          "justification": "The context of the study is specifically focused on social robot navigation, aiming to improve how robots interact with human crowds.",
          "quote": "We hypothesize that conducting training with a wider distribution of navigation scenarios and pedestrian behavior will increase robustness of RL models in novel and unseen situations."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CADRL",
          "justification": "CADRL is specifically mentioned as one of the models being evaluated and trained under different conditions.",
          "quote": "We explore how to improve the quantitative and qualitative performance of three state-of-the-art RL social navigation models (CADRL [5], LSTM-RL [6], and SARL [4]) through training."
        },
        "aliases": [
          "BL-CADRL",
          "D-CADRL",
          "C-CADRL",
          "CD-CADRL"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses CADRL as part of its experiments but does not contribute this model to the field.",
          "quote": "One of the first prominent examples of this is CADRL [5], which was conceived for two-agent pairwise collision avoidance."
        },
        "is_executed": {
          "value": 1,
          "justification": "All experiments were conducted using a computer with a GPU, where models including CADRL were executed.",
          "quote": "All experiments were conducted on a computer with an AMD Ryzen 9 5950X 16-Core Processor CPU, and an Nvidia GeForce RTX 3080 GPU."
        },
        "is_compared": {
          "value": 1,
          "justification": "CADRL was compared numerically against other models like LSTM-RL and SARL in various settings.",
          "quote": "In this study, we empirically illustrate the limitations of training and testing social navigation RL models in overly homogeneous environments."
        },
        "referenced_paper_title": {
          "value": "Decentralized non-communicating multiagent collision avoidance with deep reinforcement learning",
          "justification": "This is the reference paper that introduced CADRL, mentioned as reference [5] in the current paper.",
          "quote": "One of the first prominent examples of this is CADRL [5], which was conceived for two-agent pairwise collision avoidance."
        }
      },
      {
        "name": {
          "value": "LSTM-RL",
          "justification": "LSTM-RL is one of the models evaluated and trained in the study.",
          "quote": "We explore how to improve the quantitative and qualitative performance of three state-of-the-art RL social navigation models (CADRL [5], LSTM-RL [6], and SARL [4]) through training."
        },
        "aliases": [
          "BL-LSTM-RL",
          "D-LSTM-RL",
          "C-LSTM-RL",
          "CD-LSTM-RL"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The paper employs LSTM-RL for experimentation but does not introduce it as a new model.",
          "quote": "Another notable approach is LSTM-RL [6], which addresses this issue by considering all pedestrians together, ordered sequentially based on their proximity."
        },
        "is_executed": {
          "value": 1,
          "justification": "Model training and evaluations were conducted on a computer with an Nvidia GPU, as specified.",
          "quote": "All experiments were conducted on a computer with an AMD Ryzen 9 5950X 16-Core Processor CPU, and an Nvidia GeForce RTX 3080 GPU."
        },
        "is_compared": {
          "value": 1,
          "justification": "LSTM-RL is compared with other models such as CADRL and SARL in the study.",
          "quote": "We also find that the models that were best able to succeed in these novel settings were those that had the capacity to take advantage of the diversity of experience afforded by the new training methods."
        },
        "referenced_paper_title": {
          "value": "Motion planning among dynamic, decision-making agents with deep reinforcement learning",
          "justification": "This is the reference paper where LSTM-RL was originally introduced, identified as reference [6] in the current paper.",
          "quote": "Another notable approach is LSTM-RL [6], which addresses this issue by considering all pedestrians together, ordered sequentially based on their proximity."
        }
      },
      {
        "name": {
          "value": "SARL",
          "justification": "SARL is one of the main models evaluated and discussed in the paper.",
          "quote": "We explore how to improve the quantitative and qualitative performance of three state-of-the-art RL social navigation models (CADRL [5], LSTM-RL [6], and SARL [4]) through training."
        },
        "aliases": [
          "BL-SARL",
          "D-SARL",
          "C-SARL",
          "CD-SARL"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "SARL is utilized for experimentation, not introduced as a new model in this paper.",
          "quote": "More recently, SARL [4] proposed to model the importance of pedestrians in a more sophisticated manner, assigning each an attention score using a self-attention mechanism."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was trained and tested on a computer equipped with an Nvidia GPU.",
          "quote": "All experiments were conducted on a computer with an AMD Ryzen 9 5950X 16-Core Processor CPU, and an Nvidia GeForce RTX 3080 GPU."
        },
        "is_compared": {
          "value": 1,
          "justification": "The evaluation involves numerical comparison with other models like CADRL and LSTM-RL.",
          "quote": "Our best method, CD-SARL, generalizes better to the Diverse-4 environments, with a 0.95 success rate (see Table 2)."
        },
        "referenced_paper_title": {
          "value": "Crowd-robot interaction: Crowd-aware robot navigation with attention-based deep reinforcement learning",
          "justification": "The originating paper for SARL is cited as reference [4] in the present study.",
          "quote": "More recently, SARL [4] proposed to model the importance of pedestrians in a more sophisticated manner, assigning each an attention score using a self-attention mechanism."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "OpenAI Gym",
          "justification": "The simulator for the experiments is built on OpenAI Gym.",
          "quote": "A simple top-down view simulator from [4] built on OpenAI Gym is used in this study."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "OpenAI Gym: A toolkit for developing and comparing reinforcement learning algorithms",
          "justification": "This is the reference paper for OpenAI Gym, mentioned as the foundational toolkit used for building the simulator.",
          "quote": "A simple top-down view simulator from [4] built on OpenAI Gym is used in this study."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1562,
    "prompt_tokens": 10771,
    "total_tokens": 12333
  }
}