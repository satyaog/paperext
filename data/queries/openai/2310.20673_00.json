{
  "paper": "2310.20673.txt",
  "words": 18905,
  "extractions": {
    "title": {
      "value": "Balancing Act: Constraining Disparate Impact in Sparse Models",
      "justification": "This is the title of the paper as provided in the document.",
      "quote": "BALANCING ACT: CONSTRAINING DISPARATE IMPACT IN SPARSE MODELS"
    },
    "description": "The paper addresses the problem of model pruning in large deep learning models and its disparate impact on the performance of different data sub-groups. It proposes a constrained optimization approach that uses group-level accuracy gaps to directly address this issue. Experimental results show the approach's effectiveness across multiple models, datasets, and sparsity levels.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes extensive experimental results to demonstrate the effectiveness of the proposed approach across multiple architectures, datasets, and sparsity levels.",
      "quote": "Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups."
    },
    "primary_research_field": {
      "name": {
        "value": "Fairness in Machine Learning",
        "justification": "The paper primarily deals with mitigating the disparate impact of model pruning on different data sub-groups, which falls under the research field of fairness in machine learning.",
        "quote": "We hope our empirical observations will motivate further research on improving the generalization properties of methods for mitigating the disparate impact of pruning."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Pruning",
          "justification": "The main method studied in the paper is model pruning and its effect on fairness.",
          "quote": "Model pruning is a popular approach to enable the deployment of large deep learning models on edge devices with restricted computational or storage capacities."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Constrained Optimization",
          "justification": "The proposed approach uses constrained optimization to address disparate impact.",
          "quote": "We propose a constrained optimization approach..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Intersectional Fairness",
          "justification": "The paper considers intersectional groups in its evaluation, which relates to intersectional fairness.",
          "quote": "Our experiments demonstrate that we can reliably mitigate the disparate impact of pruning across multiple architectures, datasets, and sparsity levels (§5). These results carry over to tasks with intersectional groups, and up to hundreds of constraints."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CEAG (Constrained Excess Accuracy Gaps)",
          "justification": "The paper introduces CEAG as the main model for mitigating the disparate impact of pruning.",
          "quote": "Finally, our approach introduces a negligible computational overhead (Appendix E.1) compared to (disparity-agnostic) naive fine-tuning of the sparse model, making it applicable to problems with large numbers of groups, such as intersectional fairness tasks."
        },
        "aliases": [
          "CEAG"
        ],
        "is_contributed": {
          "value": true,
          "justification": "This model is proposed and evaluated within this paper.",
          "quote": "We propose a constrained optimization approach that uses group-level accuracy gaps to directly address this issue."
        },
        "is_executed": {
          "value": true,
          "justification": "Experimental results demonstrate the execution of the model.",
          "quote": "Experimental results demonstrate that our technique scales reliably to problems involving large models and hundreds of protected sub-groups."
        },
        "is_compared": {
          "value": true,
          "justification": "The CEAG model is compared with NFT, NFT+ES, and EL+RB.",
          "quote": "Table 1 includes results for FairFace classification at 99% sparsity. We compare the behavior of NFT, NFT+ES, EL+RB, and CEAG."
        },
        "referenced_paper_title": {
          "value": "Controlled Sparsity via Constrained Optimization or: How I Learned to Stop Tuning Penalties and Love Constraints",
          "justification": "This paper forms the basis for the constrained optimization approach used in CEAG.",
          "quote": "Gallego-Posada et al., 2022"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "UTKFace",
          "justification": "The paper uses UTKFace for evaluating the proposed methods.",
          "quote": "Lin et al. (2022) apply their method on UTKFace, but remove race group Others from the dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Age Progression/Regression by Conditional Adversarial Autoencoder",
          "justification": "This is the paper where UTKFace is introduced.",
          "quote": "Zhang et al., 2017"
        }
      },
      {
        "name": {
          "value": "FairFace",
          "justification": "The paper uses the FairFace dataset for its experiments.",
          "quote": "We use the UTKFace (Zhang et al., 2017) and FairFace (Kärkkäinen & Joo, 2021) datasets in this work."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age",
          "justification": "This is the paper where FairFace is introduced.",
          "quote": "Kärkkäinen & Joo, 2021"
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "The paper uses CIFAR-100 for a task with a large number of sub-groups.",
          "quote": "Additionally, we perform experiments on CIFAR-100 (Krizhevsky, 2009), a task with a large number of sub-groups."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the paper where CIFAR-100 is introduced.",
          "quote": "Krizhevsky, 2009"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments in the paper are implemented using PyTorch.",
          "quote": "Our implementations use PyTorch 1.13.0 (Paszke et al., 2019) and the Cooper library for constrained optimization (Gallego-Posada & Ramirez, 2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the reference paper for PyTorch.",
          "quote": "Paszke et al., 2019"
        }
      },
      {
        "name": {
          "value": "Cooper",
          "justification": "The paper mentions using the Cooper library for constrained optimization.",
          "quote": "Our implementations use PyTorch 1.13.0 (Paszke et al., 2019) and the Cooper library for constrained optimization (Gallego-Posada & Ramirez, 2022)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Cooper: a toolkit for Lagrangian-based constrained optimization",
          "justification": "This is the reference paper for Cooper library.",
          "quote": "Gallego-Posada & Ramirez, 2022"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1357,
    "prompt_tokens": 37484,
    "total_tokens": 38841
  }
}