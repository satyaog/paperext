{
  "paper": "2304.13850.txt",
  "words": 11265,
  "extractions": {
    "title": {
      "value": "Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning",
      "justification": "Title of the research paper.",
      "quote": "Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning"
    },
    "description": "The paper performs a systematic study of the unintended memorization of image-specific information in self-supervised learning (SSL) models. The study defines this phenomenon as 'déjà vu memorization' and shows that given the trained model and a crop of a training image containing only the background, it is possible to infer the foreground object with high accuracy or even visually reconstruct it.",
    "type": {
      "value": "Empirical Study",
      "justification": "The study involves systematic experiments and observations to evaluate the extent of unintended memorization in SSL models.",
      "quote": "In this work, we perform a systematic study of the unintended memorization of image-specific information in SSL models."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The study focuses on the behavior of self-supervised learning models applied to image data.",
        "quote": "Self-supervised learning (SSL) algorithms can produce useful image representations by learning to associate different parts of natural images with one another."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Privacy in Machine Learning",
          "justification": "The paper addresses privacy risks, specifically unintended memorization by models, raising concerns about information leakage.",
          "quote": "Our study of déjà vu memorization reveals previously unknown privacy risks in SSL models, as well as suggests potential practical mitigation strategies."
        },
        "aliases": [
          "Privacy in ML",
          "Machine Learning Privacy"
        ]
      },
      {
        "name": {
          "value": "Self-supervised Learning",
          "justification": "The paper evaluates the phenomenon within the context of self-supervised learning models.",
          "quote": "Self-supervised learning (SSL) is a machine learning paradigm that leverages unlabeled data to learn representations."
        },
        "aliases": [
          "SSL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "The paper tests the degree of memorization for SimCLR among other SSL models.",
          "quote": "SimCLR is trained with the 2-layer fully connected projector used in the original paper with layer dimensions 2048-256."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "SimCLR was not introduced by this paper, but it was used for experimentation.",
          "quote": "SimCLR is trained with the 2-layer fully connected projector used in the original paper with layer dimensions 2048-256."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed and its embeddings were used for experiments.",
          "quote": "We embed crop(Ai) using SSLA , and find its K = 100 L2 nearest neighbors within the SSLA embeddings of X."
        },
        "is_compared": {
          "value": 1,
          "justification": "SimCLR was compared to other models to evaluate memorization effects.",
          "quote": "Figure 4a shows how déjà vu memorization changes with number of training epochs for VICReg. From 250 to 1000 epochs, the déjà vu score grows threefold. Same trends for other models SimCLR and Byol."
        },
        "referenced_paper_title": {
          "value": "A Simple Framework for Contrastive Learning of Visual Representations",
          "justification": "SimCLR was detailed in this referenced paper.",
          "quote": "Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A Simple Framework for Contrastive Learning of Visual Representations. In ICML, 2020."
        }
      },
      {
        "name": {
          "value": "VICReg",
          "justification": "The VICReg model was a specific focus of the paper.",
          "quote": "VICReg: Variance-invariance-covariance regularization for self-supervised learning."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "VICReg was not introduced by this paper, but it was used for experimentation.",
          "quote": "We test déjà vu memorization on a variety of popular SSL algorithms, with a focus on VICReg."
        },
        "is_executed": {
          "value": 1,
          "justification": "VICReg was executed and its embeddings were used for experiments.",
          "quote": "The training loss hyperparameters λ, μ, ν were set to 25, 25, 1, respectively, in training VICReg."
        },
        "is_compared": {
          "value": 1,
          "justification": "VICReg was compared to other models to evaluate memorization effects.",
          "quote": "We test déjà vu memorization on a variety of popular SSL algorithms, with a focus on VICReg."
        },
        "referenced_paper_title": {
          "value": "VICReg: Variance-invariance-covariance regularization for self-supervised learning",
          "justification": "The authors reference the original VICReg paper.",
          "quote": "Bardes, A., Ponce, J., and LeCun, Y. VICReg: Variance-invariance-covariance regularization for self-supervised learning."
        }
      },
      {
        "name": {
          "value": "DINO",
          "justification": "The paper evaluates DINO among other SSL models.",
          "quote": "The paper evaluates multiple SSL models like SimCLR, BYOL, and DINO for déjà vu memorization."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "DINO was not introduced by this paper, but it was used for experimentation.",
          "quote": "The paper evaluates multiple SSL models like SimCLR, BYOL, and DINO for déjà vu memorization."
        },
        "is_executed": {
          "value": 1,
          "justification": "DINO was executed and tested for memorization.",
          "quote": "The paper evaluates multiple SSL models like SimCLR, BYOL, and DINO for déjà vu memorization."
        },
        "is_compared": {
          "value": 1,
          "justification": "DINO was compared to other models to evaluate memorization effects.",
          "quote": "The paper evaluates multiple SSL models like SimCLR, BYOL, and DINO for déjà vu memorization."
        },
        "referenced_paper_title": {
          "value": "Emerging Properties in Self-Supervised Vision Transformers",
          "justification": "DINO was detailed in this referenced paper.",
          "quote": "Caron, M., Touvron, H., Misra, I., Jegou, H., and Joulin, J. , M. P. B. A. Emerging properties in self-supervised vision transformers. In ICCV, 2021."
        }
      },
      {
        "name": {
          "value": "BYOL",
          "justification": "BYOL was tested for degree of memorization among SSL models.",
          "quote": "We also test the degree of memorization for BYOL among other SSL models."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "BYOL was not introduced by this paper, but it was used for experimentation.",
          "quote": "BYOL was tested for degree of memorization among SSL models."
        },
        "is_executed": {
          "value": 1,
          "justification": "BYOL was executed and tested for memorization.",
          "quote": "We also test the degree of memorization for BYOL among other SSL models."
        },
        "is_compared": {
          "value": 1,
          "justification": "BYOL was compared to other models to evaluate memorization effects.",
          "quote": "Figure 4a shows how déjà vu memorization changes with number of training epochs for BYOL. Same trends for other models SimCLR."
        },
        "referenced_paper_title": {
          "value": "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning",
          "justification": "The referenced paper presents details of BYOL.",
          "quote": "Grill, J.-B., Strub, F., Altche, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doersch, C., Pires, B. A., Guo, Z. D., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos, R., and Valko, M. Bootstrap your own latent: A new approach to self-supervised learning."
        }
      },
      {
        "name": {
          "value": "Barlow Twins",
          "justification": "Barlow Twins was tested for degree of memorization among SSL models.",
          "quote": "Barlow Twins criterion was implemented in the self-supervised learning methodology."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Barlow Twins was not introduced by this paper, but it was used for experimentation.",
          "quote": "Barlow Twins criterion was implemented in the self-supervised learning methodology."
        },
        "is_executed": {
          "value": 1,
          "justification": "Barlow Twins was executed and tested for memorization.",
          "quote": "Barlow Twins criterion was implemented in the self-supervised learning methodology."
        },
        "is_compared": {
          "value": 1,
          "justification": "Barlow Twins was compared to other models to evaluate memorization effects.",
          "quote": "Barlow Twins criterion was implemented in the self-supervised learning methodology."
        },
        "referenced_paper_title": {
          "value": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "justification": "The referenced paper presents details of Barlow Twins.",
          "quote": "Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. Barlow Twins: Self-Supervised Learning via Redundancy Reduction."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "The paper uses ImageNet as the primary dataset for evaluating memorization.",
          "quote": "We test déjà vu memorization for SSL models trained on the ImageNet dataset."
        },
        "aliases": [
          "ImageNet-1K"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "The referenced paper introduces the ImageNet dataset.",
          "quote": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. ImageNet: A large-scale hierarchical image database."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch was used for implementing SSL models.",
          "quote": "We use PyTorch with FFCV-SSL."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The referenced paper introduces PyTorch.",
          "quote": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., and Antiga, L. PyTorch: An imperative style, high-performance deep learning library."
        }
      },
      {
        "name": {
          "value": "FFCV-SSL",
          "justification": "FFCV-SSL was used for implementing SSL models.",
          "quote": "We use PyTorch with FFCV-SSL."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Towards democratizing joint-embedding self-supervised learning",
          "justification": "The referenced paper introduces FFCV-SSL.",
          "quote": "Bordes, F., Balestriero, R., and Vincent, P. Towards democratizing joint-embedding self-supervised learning."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2813,
    "prompt_tokens": 17847,
    "total_tokens": 20660
  }
}