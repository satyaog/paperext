{
  "paper": "2306.11128.txt",
  "words": 8248,
  "extractions": {
    "title": {
      "value": "CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning",
      "justification": "This is the title as provided in the research paper.",
      "quote": "CAMMARL : Conformal Action Modeling in Multi Agent Reinforcement Learning"
    },
    "description": "This paper introduces CAMMARL, a novel multi-agent reinforcement learning (MARL) algorithm that leverages conformal predictions to enhance an agent's decision-making process. The method involves modeling the actions of other agents as conformal prediction sets, which provide statistically accurate uncertainty estimates. The study shows the algorithm's efficacy in improving cooperative behaviors in environments requiring multi-agent coordination.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper primarily presents experimental results to demonstrate the efficacy of the CAMMARL algorithm in various multi-agent environments.",
      "quote": "Through several experiments in two fully cooperative multi-agent tasks, we show that CAMMARL elevates the capabilities of an autonomous agent in MARL by modeling conformal prediction sets over the behavior of other agents in the environment and utilizing such estimates to enhance its policy learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on multi-agent reinforcement learning, which is a sub-field of deep learning.",
        "quote": "We are interested in the particular aspect of an interactive, autonomous agent that involves learning an additional, independent model to make predictions about the actions of the other agents in the environment, supplemental to its reinforcement learning-based policy to make decisions related to its downstream task."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Agent Reinforcement Learning",
          "justification": "The core contribution of the paper is an algorithm designed for multi-agent reinforcement learning scenarios.",
          "quote": "In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL , which involves modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their true actions with a high probability."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CAMMARL",
          "justification": "This is the main model proposed by the authors.",
          "quote": "In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL , which involves modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their true actions with a high probability."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Cooperative Navigation",
          "justification": "This dataset/environment is used in the experiments to test CAMMARL.",
          "quote": "We focus on four cooperative multi-agent environments: Cooperative Navigation."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Level-based Foraging",
          "justification": "This dataset/environment is used in the experiments to test CAMMARL.",
          "quote": "We focus on four cooperative multi-agent environments: Level-based Foraging."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Pressure Plate",
          "justification": "This dataset/environment is used in the experiments to test CAMMARL.",
          "quote": "Finally, we also run CAMMARL in Google Football, where 3 agents try to score a goal against a defender and a keeper in a game of football."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Google Football",
          "justification": "This dataset/environment is used in the experiments to test CAMMARL.",
          "quote": "Finally, we also run CAMMARL in Google Football, where 3 agents try to score a goal against a defender and a keeper in a game of football."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Proximal Policy Optimization",
          "justification": "The PPO library is used for training the reinforcement learning agents.",
          "quote": "We use proximal policy optimization (PPO) (Schulman et al., 2017) to update the decision-making policy for both the RL agents, however, any other RL algorithm could be used alternatively."
        },
        "aliases": [
          "PPO"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 863,
    "prompt_tokens": 13108,
    "total_tokens": 13971
  }
}