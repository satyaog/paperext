{
  "paper": "gZXFNUcnHd.txt",
  "words": 10258,
  "extractions": {
    "title": {
      "value": "Towards Reliable Neural Specifications",
      "justification": "This is the title of the paper.",
      "quote": "Towards Reliable Neural Specifications"
    },
    "description": "This paper discusses the development of new specifications for neural networks based on neural activation patterns (NAPs). These specifications aim to improve the reliability and verifiability of neural network predictions by addressing the limitations of the traditional 'data as specification' paradigm. The paper introduces methods to mine NAPs and demonstrates their effectiveness through experiments on MNIST and CIFAR10 datasets.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves empirical evaluations on MNIST and CIFAR10 datasets to demonstrate the effectiveness of neural activation patterns as specifications for neural network verification.",
      "quote": "We show that by using NAP, we can verify a significant region of the input space, while still recalling 84% of the data on MNIST. Moreover, we can push the verifiable bound to 10 times larger on the CIFAR10 benchmark."
    },
    "primary_research_field": {
      "name": {
        "value": "Neural Network Verification",
        "justification": "The primary focus of the paper is on verifying the correctness and robustness of neural network predictions using new specifications based on neural activation patterns.",
        "quote": "this new family of specifications called neural representation as specification. This form of specifications uses the intrinsic information of neural networks, specifically neural activation patterns (NAPs), rather than input data to specify the correctness and/or robustness of neural network predictions."
      },
      "aliases": [
        "Neural Network Validation"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Adversarial Robustness",
          "justification": "The paper discusses verifying adversarial robustness of neural networks and shows that NAPs can help in robust verification against adversarial examples.",
          "quote": "Most works (Katz et al., 2017; 2019; Huang et al., 2017; 2020; Wang et al., 2021) use the specification of adversarial robustness for classification tasks that states that the NN correctly classifies an image as a given adversarial label under perturbations with a specific norm (usually L8 )."
        },
        "aliases": [
          "Adversarial Attacks and Defenses"
        ]
      },
      {
        "name": {
          "value": "Formal Methods for AI",
          "justification": "The paper builds on formal methods to verify properties of neural networks using mathematical specifications and proofs, positioning it within the research field of formal methods for AI.",
          "quote": "Existing works approach this challenge by building on formal methods – a field of computer science and engineering that involves verifying properties of systems using rigorous mathematical specifications and proofs."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "mnistfc 256x4",
          "justification": "This is one of the models evaluated in the paper for verifying the effectiveness of NAPs.",
          "quote": "For MNIST, we use the two largest models mnistfc 256x4 and mnistfc 256x6, a 4- and 6-layers fully connected network with 256 neurons for each layer, respectively."
        },
        "aliases": [
          "mnistfc 256x4"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used to demonstrate the effectiveness of NAPs, but it is not a contribution of the paper.",
          "quote": "For MNIST, we use the two largest models mnistfc 256x4 and mnistfc 256x6, a 4- and 6-layers fully connected network with 256 neurons for each layer, respectively."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed to demonstrate its verifiability using NAPs.",
          "quote": "For MNIST, we use the two largest models mnistfc 256x4 and mnistfc 256x6, a 4- and 6-layers fully connected network with 256 neurons for each layer, respectively."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's verification performance is compared to other methods and models.",
          "quote": "By slightly relaxing the NAP (δ = 0.99), all of the chosen inputs can be proven to be robust. Furthermore, with δ = 0.99, we can verify the robustness for 6 of the 7 inputs (Table 4) with ϵ = 0.3."
        },
        "referenced_paper_title": {
          "value": "VNNCOMP 2021",
          "justification": "The MNIST models used in the paper reference the VNNCOMP 2021 benchmark.",
          "quote": "Our experiments are based on benchmarks from VNNCOMP (2021) – the annual neural network verification competition."
        }
      },
      {
        "name": {
          "value": "mnistfc 256x6",
          "justification": "This is one of the models evaluated in the paper for verifying the effectiveness of NAPs.",
          "quote": "For MNIST, we use the two largest models mnistfc 256x4 and mnistfc 256x6, a 4- and 6-layers fully connected network with 256 neurons for each layer, respectively."
        },
        "aliases": [
          "mnistfc 256x6"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used to demonstrate the effectiveness of NAPs, but it is not a contribution of the paper.",
          "quote": "For MNIST, we use the two largest models mnistfc 256x4 and mnistfc 256x6, a 4- and 6-layers fully connected network with 256 neurons for each layer, respectively."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed to demonstrate its verifiability using NAPs.",
          "quote": "For MNIST, we use the two largest models mnistfc 256x4 and mnistfc 256x6, a 4- and 6-layers fully connected network with 256 neurons for each layer, respectively."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's verification performance is compared to other methods and models.",
          "quote": "By slightly relaxing the NAP (δ = 0.99), all of the chosen inputs can be proven to be robust. Furthermore, with δ = 0.99, we can verify the robustness for 6 of the 7 inputs (Table 4) with ϵ = 0.3."
        },
        "referenced_paper_title": {
          "value": "VNNCOMP 2021",
          "justification": "The MNIST models used in the paper reference the VNNCOMP 2021 benchmark.",
          "quote": "Our experiments are based on benchmarks from VNNCOMP (2021) – the annual neural network verification competition."
        }
      },
      {
        "name": {
          "value": "cifar10 small.onnx",
          "justification": "This is the model evaluated in the paper for verifying the effectiveness of NAPs on CIFAR10 dataset.",
          "quote": "For CIFAR10, we use the convolutional neural net cifar10 small.onnx with 2568 ReLUs."
        },
        "aliases": [
          "cifar10 small.onnx"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is used to demonstrate the effectiveness of NAPs, but it is not a contribution of the paper.",
          "quote": "For CIFAR10, we use the convolutional neural net cifar10 small.onnx with 2568 ReLUs."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed to demonstrate its verifiability using NAPs.",
          "quote": "For CIFAR10, we use the convolutional neural net cifar10 small.onnx with 2568 ReLUs."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's verification performance is compared to other methods and models.",
          "quote": "As with MNIST, we observe that by relaxing δ, we were able to verify more examples at every ϵ."
        },
        "referenced_paper_title": {
          "value": "VNNCOMP 2021",
          "justification": "The CIFAR10 model used in the paper references the VNNCOMP 2021 benchmark.",
          "quote": "Our experiments are based on benchmarks from VNNCOMP (2021) – the annual neural network verification competition."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is used to demonstrate the effectiveness of NAPs for neural network verification.",
          "quote": "For the MNIST dataset, a verifiable NAP mined from the training images could cover up to 84% testing images."
        },
        "aliases": [
          "MNIST"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "VNNCOMP 2021",
          "justification": "The MNIST dataset references the VNNCOMP 2021 benchmark for neural network verification.",
          "quote": "Our experiments are based on benchmarks from VNNCOMP (2021) – the annual neural network verification competition."
        }
      },
      {
        "name": {
          "value": "CIFAR10",
          "justification": "The CIFAR10 dataset is used to demonstrate the effectiveness of NAPs for neural network verification.",
          "quote": "Moreover, we can push the verifiable bound to 10 times larger on the CIFAR10 benchmark."
        },
        "aliases": [
          "CIFAR-10"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "VNNCOMP 2021",
          "justification": "The CIFAR10 dataset references the VNNCOMP 2021 benchmark for neural network verification.",
          "quote": "Our experiments are based on benchmarks from VNNCOMP (2021) – the annual neural network verification competition."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Marabou",
          "justification": "Marabou is used as the neural network verification tool in the paper to check the properties of NAPs.",
          "quote": "We show that NAPs can be easily checked by out-of-the-box neural network verification tools used in VNNCOMP – the annual neural network verification competition, such as Marabou."
        },
        "aliases": [
          "Marabou"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "Reluplex: An efficient SMT solver for verifying deep neural networks",
          "justification": "Marabou is based on the Reluplex algorithm as indicated in the reference title.",
          "quote": "Marabou (Katz et al., 2019), a dedicated state-of-the-art NN verifier. Marabou extends the Simplex (Nelder & Mead, 1965) algorithm for solving linear programming with special mechanisms to handle non-linear activation functions."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2106,
    "prompt_tokens": 19038,
    "total_tokens": 21144
  }
}