{
  "paper": "paUJOst3OE.txt",
  "words": 18467,
  "extractions": {
    "title": {
      "value": "MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization",
      "justification": "The provided title is taken directly from the paper.",
      "quote": "MAPO: Boosting Large Language Model Performance with Model-Adaptive Prompt Optimization"
    },
    "description": "This paper proposes a model-adaptive prompt optimizer (MAPO) approach to refine prompts for specific large language models (LLMs) to enhance their performance across various natural language processing (NLP) downstream tasks. The paper quantitatively demonstrates the necessity of adapting prompts to different LLMs and utilizes supervised fine-tuning and reinforcement learning to establish optimized prompts. Extensive experiments show significant improvements in LLM performance via MAPO.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes extensive experiments to demonstrate the performance improvements of the proposed MAPO method, hence it is empirical.",
      "quote": "Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper deals with the enhancement of Large Language Models (LLMs) across various downstream NLP tasks.",
        "quote": "Extensive experiments indicate that the proposed method can effectively refine prompts for an LLM, leading to significant improvements over various downstream tasks in NLP."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Prompt Engineering",
          "justification": "The primary focus of the paper is on prompt optimization for large language models.",
          "quote": "Then we novelly propose a model-adaptive prompt optimizer (MAPO) method that optimizes the original prompts for each specific LLM in downstream tasks."
        },
        "aliases": [
          "Prompt Optimization"
        ]
      },
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "Reinforcement learning is used as a key component in the MAPO method for prompt optimization.",
          "quote": "We combine Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to optimize original prompts for each specific LLM in various downstream tasks."
        },
        "aliases": [
          "RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BLOOM-7B",
          "justification": "BLOOM-7B was explicitly mentioned as one of the models used in the empirical study.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance on question-answering (QA), classification, and generation tasks with same task-specific prompts."
        },
        "aliases": [
          "BLOOM",
          "BLOOM-7B"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is used in the paper but is not the focus of the proposed research.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance"
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed in the empirical studies.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance"
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance was compared with other models in the paper.",
          "quote": "The results reveal significant variations across different LLMs’ performance."
        },
        "referenced_paper_title": {
          "value": "BLOOM: A 176B-parameter Open-Access Multilingual Language Model",
          "justification": "This is the referenced paper title for the BLOOM model used in the study.",
          "quote": "BLOOM-7B (Scao et al., 2022)"
        }
      },
      {
        "name": {
          "value": "GPT-J-6B",
          "justification": "GPT-J-6B was explicitly mentioned as one of the models used in the empirical study.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance on question-answering (QA), classification, and generation tasks with same task-specific prompts."
        },
        "aliases": [
          "GPT-J",
          "GPT-J-6B"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is used in the paper but is not the focus of the proposed research.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance"
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed in the empirical studies.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance on question-answering (QA), classification, and generation tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance was compared with other models in the paper.",
          "quote": "The results reveal significant variations across different LLMs’ performance."
        },
        "referenced_paper_title": {
          "value": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",
          "justification": "This is the referenced paper title for the GPT-J model used in the study.",
          "quote": "GPT-J-6B (Wang and Komatsuzaki, 2021)"
        }
      },
      {
        "name": {
          "value": "LLaMA-7B",
          "justification": "LLaMA-7B was explicitly mentioned as one of the models used in the empirical study.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance on question-answering (QA), classification, and generation tasks with same task-specific prompts."
        },
        "aliases": [
          "LLaMA",
          "LLaMA-7B"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is used in the paper but is not the focus of the proposed research.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance"
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed in the empirical studies.",
          "quote": "We conduct extensive experiments with three LLMs (BLOOM-7B, GPT-J-6B, and LLaMA-7B) to evaluate their separate performance on question-answering (QA), classification, and generation tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance was compared with other models in the paper.",
          "quote": "The results reveal significant variations across different LLMs’ performance."
        },
        "referenced_paper_title": {
          "value": "LLaMA: Open and Efficient Foundation Language Models",
          "justification": "This is the referenced paper title for the LLaMA model used in the study.",
          "quote": "LLaMA-7B (Touvron et al., 2023)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AdverQA",
          "justification": "AdverQA was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "AdversarialQA"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "AdversarialQA: Robust Question Answering Benchmark",
          "justification": "This is the referenced paper title for the AdverQA dataset used in the study.",
          "quote": "AdverQA (https://huggingface.co/datasets/bigscience/P3/tree/main/data/adversarial_qa_dbidaf_question_context_answer)"
        }
      },
      {
        "name": {
          "value": "OpenQA",
          "justification": "OpenQA was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "OpenBookQA"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "OpenBookQA: Open Book Question Answering",
          "justification": "This is the referenced paper title for the OpenQA dataset used in the study.",
          "quote": "OpenQA (https://huggingface.co/datasets/bigscience/P3/tree/main/data/openbookqa_main_which_correct)"
        }
      },
      {
        "name": {
          "value": "CloseQA",
          "justification": "CloseQA was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "SciQ"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "SciQ: Science Questions Dataset",
          "justification": "This is the referenced paper title for the CloseQA dataset used in the study.",
          "quote": "CloseQA (https://huggingface.co/datasets/bigscience/P3/tree/main/data/sciq_Direct_Question_Closed_Book_)"
        }
      },
      {
        "name": {
          "value": "News",
          "justification": "News was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "AG-News"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "AG-News: Large-scale News Classification Dataset",
          "justification": "This is the referenced paper title for the News dataset used in the study.",
          "quote": "News (https://huggingface.co/datasets/bigscience/P3/tree/main/data/ag_news_classify)"
        }
      },
      {
        "name": {
          "value": "Movie",
          "justification": "Movie was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "Rotten Tomatoes"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Rotten Tomatoes Dataset",
          "justification": "This is the referenced paper title for the Movie dataset used in the study.",
          "quote": "Movie (https://huggingface.co/datasets/bigscience/P3/tree/main/data/rotten_tomatoes_Movie_Expressed_Sentiment)"
        }
      },
      {
        "name": {
          "value": "QASC",
          "justification": "QASC was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "QASC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "QASC: Question Answering via Sentence Composition",
          "justification": "This is the referenced paper title for the QASC dataset used in the study.",
          "quote": "QASC (https://huggingface.co/datasets/bigscience/P3/tree/main/data/qasc_is_correct_1)"
        }
      },
      {
        "name": {
          "value": "Topics",
          "justification": "Topics was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "CommonGen"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "CommonGen: A Constrained Text Generation Challenge",
          "justification": "This is the referenced paper title for the Topics dataset used in the study.",
          "quote": "Topics (https://huggingface.co/datasets/bigscience/P3/tree/main/data/common_gen_topics_from_the_sentence)"
        }
      },
      {
        "name": {
          "value": "Summary",
          "justification": "Summary was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "Samsum"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Samsum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization",
          "justification": "This is the referenced paper title for the Summary dataset used in the study.",
          "quote": "Summary (https://huggingface.co/datasets/bigscience/P3/tree/main/data/samsum_Sum_up_the_following_dialogue)"
        }
      },
      {
        "name": {
          "value": "Explan",
          "justification": "Explan was explicitly mentioned as one of the datasets used in the empirical study.",
          "quote": "We use nine dataset from P3 (Sanh et al., 2021) covering three downstream tasks with details in Appendix E. P3 is a widely-used prompt benchmark which contains original prompts and the corresponding ground-truth answers."
        },
        "aliases": [
          "CosMOS"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "CosMOS: Commonsense Reasoning via Understanding Language Explanation",
          "justification": "This is the referenced paper title for the Explan dataset used in the study.",
          "quote": "Explan (https://huggingface.co/datasets/bigscience/P3/tree/main/data/cos_e_v1.11_generate_explanation_given_text)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The PyTorch library was explicitly mentioned as being used in the training process.",
          "quote": "The experiments are executed on 4 Nvidia A100 GPUs with 80GB each, using PyTorch in Python."
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Open Source Machine Learning Framework",
          "justification": "This is the referenced paper title for the PyTorch framework used in this study.",
          "quote": "using PyTorch in Python"
        }
      },
      {
        "name": {
          "value": "DeepSpeed",
          "justification": "The DeepSpeed library was explicitly mentioned as being used in the training process.",
          "quote": "DeepSpeed 2 is utilized in the training process."
        },
        "aliases": [
          "DeepSpeed"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepSpeed: Accelerating Large-scale Model Training and Inference",
          "justification": "This is the referenced paper title for the DeepSpeed library used in this study.",
          "quote": "DeepSpeed 2 is utilized in the training process."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3072,
    "prompt_tokens": 39433,
    "total_tokens": 42505
  }
}