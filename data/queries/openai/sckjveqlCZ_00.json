{
  "paper": "sckjveqlCZ.txt",
  "words": 12653,
  "extractions": {
    "title": {
      "value": "Broken Neural Scaling Laws",
      "justification": "The title is clearly stated at the beginning and in the header of each page, indicating it as the work being discussed.",
      "quote": "Published as a conference paper at ICLR 2023\n\nBROKEN NEURAL SCALING LAWS"
    },
    "description": "The paper introduces 'broken neural scaling laws' (BNSL), a functional form that better predicts and extrapolates the scaling behavior of deep neural networks across a variety of architectures and tasks. The authors assert that BNSL can model non-monotonic behaviors and inflection points, providing more accurate scaling predictions compared to previous models.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper focuses on empirical evaluation of the predictive power of the proposed broken neural scaling laws across different tasks and architectures.",
      "quote": "An extensive empirical evaluation demonstrates that BNSL accurately model and extrapolate the scaling behaviors for various architectures and for each of various tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper discusses scaling laws, a concept that is fundamental to the field of deep learning, and applies it across various deep learning models and tasks.",
        "quote": "We present a smoothly broken power law functional form (referred to by us as a broken neural scaling law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Scaling Laws",
          "justification": "The entire focus of the paper is on improving the functional forms used to predict the scaling behavior of neural networks, specifically addressing the limitations of existing scaling laws.",
          "quote": "Our use of BNSLs is inspired by the observation that scaling is not always well predicted by a simple power law; nor are many of the modifications which have been applied in previous works sufficient to capture the qualitative properties of empirical scaling curves."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Neural Network Training",
          "justification": "The paper discusses the impact of scaling laws on various aspects critical to training neural networks, such as compute resources, model parameters, and dataset sizes.",
          "quote": "The amount of compute used for training, number of model parameters, and training dataset size of the most capable artificial neural networks keeps increasing and will probably keep rapidly increasing for the foreseeable future."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Vision Transformer (ViT)",
          "justification": "ViT is mentioned as one of the models used for empirical evaluation in vision tasks.",
          "quote": "The following architectures of various sizes are pretrained on subsets of JFT-300M (Sun et al., 2017): big-transfer residual neural networks (BiT) (Kolesnikov et al., 2020), MLP mixers (MiX) (Tolstikhin et al., 2021), and vision transformers (ViT) (Dosovitskiy et al., 2020)."
        },
        "aliases": [
          "ViT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ViT is an existing model used in the experiments; it is not introduced in this paper.",
          "quote": "Vision transformers (ViT) (Dosovitskiy et al., 2020)."
        },
        "is_executed": {
          "value": 1,
          "justification": "ViT was run on subsets of the JFT-300M dataset, implying execution in the experiments.",
          "quote": "The following architectures of various sizes are pretrained on subsets of JFT-300M."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of ViT is evaluated against other architectures within the scope of the paper.",
          "quote": "BNSL yields extrapolations with the lowest RMSLE (Root Mean Squared Logarithmic Error) for 69.44% of tasks."
        },
        "referenced_paper_title": {
          "value": "An image is worth 16x16 words: Transformers for image recognition at scale",
          "justification": "This is the reference paper for Vision Transformer (ViT).",
          "quote": "Vision transformers (ViT) (Dosovitskiy et al., 2020)."
        }
      },
      {
        "name": {
          "value": "MLP-Mixer",
          "justification": "MLP-Mixer is one of the architectures evaluated for extrapolating scaling behavior in vision tasks.",
          "quote": "The following architectures of various sizes are pretrained on subsets of JFT-300M (Sun et al., 2017): big-transfer residual neural networks (BiT) (Kolesnikov et al., 2020), MLP mixers (MiX) (Tolstikhin et al., 2021), and vision transformers (ViT) (Dosovitskiy et al., 2020)."
        },
        "aliases": [
          "MiX"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "MLP-Mixer is an existing model used in the paper's experiments.",
          "quote": "MLP mixers (MiX) (Tolstikhin et al., 2021)."
        },
        "is_executed": {
          "value": 1,
          "justification": "MLP-Mixers were run on subsets of the JFT-300M dataset, implying execution in the experiments.",
          "quote": "The following architectures of various sizes are pretrained on subsets of JFT-300M."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of MLP-Mixer is evaluated against other architectures within the scope of the paper.",
          "quote": "BNSL yields extrapolations with the lowest RMSLE (Root Mean Squared Logarithmic Error) for 69.44% of tasks."
        },
        "referenced_paper_title": {
          "value": "MLP-Mixer: An all-MLP Architecture for Vision",
          "justification": "This is the reference paper for MLP-Mixer.",
          "quote": "MLP mixers (MiX) (Tolstikhin et al., 2021)."
        }
      },
      {
        "name": {
          "value": "Big-Transfer (BiT)",
          "justification": "Big-Transfer is one of the evaluated architectures in the vision tasks of the study.",
          "quote": "The following architectures of various sizes are pretrained on subsets of JFT-300M (Sun et al., 2017): big-transfer residual neural networks (BiT) (Kolesnikov et al., 2020), MLP mixers (MiX) (Tolstikhin et al., 2021), and vision transformers (ViT) (Dosovitskiy et al., 2020)."
        },
        "aliases": [
          "BiT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Big-Transfer is an existing model used in the experiments.",
          "quote": "big-transfer residual neural networks (BiT) (Kolesnikov et al., 2020)."
        },
        "is_executed": {
          "value": 1,
          "justification": "Big-Transfer architectures were run on subsets of the JFT-300M dataset, implying execution in the experiments.",
          "quote": "The following architectures of various sizes are pretrained on subsets of JFT-300M."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Big-Transfer is evaluated against other architectures within the scope of the paper.",
          "quote": "BNSL yields extrapolations with the lowest RMSLE (Root Mean Squared Logarithmic Error) for 69.44% of tasks."
        },
        "referenced_paper_title": {
          "value": "Big Transfer (BiT): General Visual Representation Learning",
          "justification": "This is the reference paper for Big-Transfer (BiT).",
          "quote": "big-transfer residual neural networks (BiT) (Kolesnikov et al., 2020)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is used as one of the downstream tasks for evaluating the models.",
          "quote": "The downstream tasks are: Birds 200 (Welinder et al., 2010), Caltech101 (Fei-Fei et al., 2004), CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "This is the reference paper for ImageNet.",
          "quote": "ImageNet (Deng et al., 2009)."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is used as one of the downstream tasks for evaluating the models.",
          "quote": "The downstream tasks are: Birds 200 (Welinder et al., 2010), Caltech101 (Fei-Fei et al., 2004), CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the reference paper for CIFAR-100.",
          "quote": "CIFAR-100 (Krizhevsky et al., 2009)."
        }
      },
      {
        "name": {
          "value": "Caltech101",
          "justification": "Caltech101 is used as one of the downstream tasks for evaluating the models.",
          "quote": "The downstream tasks are: Birds 200 (Welinder et al., 2010), Caltech101 (Fei-Fei et al., 2004), CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories",
          "justification": "This is the reference paper for Caltech101.",
          "quote": "Caltech101 (Fei-Fei et al., 2004)."
        }
      },
      {
        "name": {
          "value": "Birds 200",
          "justification": "Birds 200 is used as one of the downstream tasks for evaluating the models.",
          "quote": "The downstream tasks are: Birds 200 (Welinder et al., 2010), Caltech101 (Fei-Fei et al., 2004), CIFAR-100 (Krizhevsky et al., 2009), and ImageNet (Deng et al., 2009)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Caltech-UCSD Birds 200",
          "justification": "This is the reference paper for Birds 200.",
          "quote": "Birds 200 (Welinder et al., 2010)."
        }
      },
      {
        "name": {
          "value": "JFT-300M",
          "justification": "JFT-300M is used as the pretraining dataset for various architectures evaluated in the paper.",
          "quote": "The following architectures of various sizes are pretrained on subsets of JFT-300M (Sun et al., 2017): big-transfer residual neural networks (BiT) (Kolesnikov et al., 2020), MLP mixers (MiX) (Tolstikhin et al., 2021), and vision transformers (ViT) (Dosovitskiy et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Revisiting Unreasonable Effectiveness of Data in Deep Learning Era",
          "justification": "This is the reference paper for JFT-300M.",
          "quote": "JFT-300M (Sun et al., 2017)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SciPy",
          "justification": "SciPy is explicitly mentioned as being used for curve fitting in the experiments.",
          "quote": "(In our experiments, SciPy curve-fitting library was used.)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python",
          "justification": "This is the reference paper for SciPy.",
          "quote": "SciPy (Virtanen et al., 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 4896,
    "prompt_tokens": 55891,
    "total_tokens": 60787
  }
}