{
  "paper": "2305.19394.txt",
  "words": 15010,
  "extractions": {
    "title": {
      "value": "Synaptic weight distributions depend on the geometry of plasticity",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "Synaptic weight distributions depend on the geometry of plasticity"
    },
    "description": "This paper explores how the distribution of synaptic weights in the brain is influenced by the choice of geometry used for synaptic plasticity. Using theoretical tools provided by mirror descent, the authors demonstrate that synaptic weight distributions will vary depending on whether Euclidean or non-Euclidean distances are used for synaptic changes. The paper shows that log-normal weight distributions observed experimentally in the brain are inconsistent with standard gradient descent (Euclidean geometry) but align with non-Euclidean geometries. The work provides a framework for experimentally determining the true geometry of synaptic plasticity by comparing synaptic weight distributions before and after learning.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper uses theoretical models and analyses to understand synaptic weight distributions and synaptic plasticity, rather than conducting empirical experiments.",
      "quote": "Using theoretical tools provided by mirror descent, we show that the distribution of synaptic weights will depend on the geometry of synaptic plasticity."
    },
    "primary_research_field": {
      "name": {
        "value": "Computational Neuroscience",
        "justification": "The paper primarily deals with computational models and theories related to synaptic plasticity in the brain.",
        "quote": "A growing literature in computational neuroscience leverages gradient descent and learning algorithms that approximate it to study synaptic plasticity in the brain."
      },
      "aliases": [
        "Computational Neural Science"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization Algorithms",
          "justification": "The paper discusses and uses optimization algorithms such as gradient descent and mirror descent extensively.",
          "quote": "More broadly, the theory of mirror descent provides tools for analyzing and building algorithms that use different distances in parameter space."
        },
        "aliases": [
          "Optimization Techniques",
          "Optimization Methods"
        ]
      },
      {
        "name": {
          "value": "Deep Learning Theory",
          "justification": "The paper explores theoretical aspects of deep learning, particularly how different geometries affect weight distributions.",
          "quote": "A large and growing literature explores how synaptic plasticity approximates gradient descent."
        },
        "aliases": [
          "DL Theory",
          "Deep Learning Foundations"
        ]
      }
    ],
    "models": [],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 561,
    "prompt_tokens": 26767,
    "total_tokens": 27328
  }
}