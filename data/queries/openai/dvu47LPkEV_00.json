{
  "paper": "dvu47LPkEV.txt",
  "words": 18802,
  "extractions": {
    "title": {
      "value": "Convergence of Proximal Point and Extragradient-Based Methods Beyond Monotonicity: the Case of Negative Comonotonicity",
      "justification": "The title is explicitly stated at the beginning of the paper.",
      "quote": "Convergence of Proximal Point and Extragradient-Based Methods Beyond Monotonicity: the Case of Negative Comonotonicity"
    },
    "description": "This paper studies the convergence of Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods under the more general assumption of negative comonotonicity, rather than the traditional monotonicity. It provides theoretical guarantees and constructs worst-case examples to demonstrate the efficiency of these methods.",
    "type": {
      "value": "theoretical study",
      "justification": "The paper primarily focuses on providing theoretical convergence guarantees and analyses of optimization methods rather than empirical evaluations or experiments.",
      "quote": "In this work, we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization",
        "justification": "The paper revolves around optimization techniques for variational inequality problems and related optimization methods like Proximal Point, Extragradient, and Optimistic Gradient.",
        "quote": "The study of efficient first-order methods for solving variational inequality problems (VIP) have known a surge of interest due to the development of recent machine learning (ML) formulations involving multiple objectives."
      },
      "aliases": [
        "VIP",
        "Variational Inequality Problems"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Mathematical Optimization",
          "justification": "The paper involves mathematical formulations and proofs related to optimization methods and variational inequalities.",
          "quote": "As we show in Appendix B, (4) can be formulated as a semidefinite program (SDP)."
        },
        "aliases": [
          "Convex Optimization",
          "Non-Convex Optimization"
        ]
      },
      {
        "name": {
          "value": "First-Order Methods",
          "justification": "The paper specifically focuses on first-order methods for optimization, such as Proximal Point, Extragradient, and Optimistic Gradient methods.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place."
        },
        "aliases": [
          "PP",
          "EG",
          "OG"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Proximal Point Method (PP)",
          "justification": "This model is one of the main subjects of the paper, which discusses its convergence properties under negative comonotonicity.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place. These two algorithms have been traditionally analyzed under the assumption that the considered operator is monotone and Lipschitz (Korpelevich, 1976; Popov, 1980) and are often interpreted as an approximation to the proximal point (PP) method (Nemirovski, 2004; Mokhtari et al., 2019)."
        },
        "aliases": [
          "PP"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model itself is not a new contribution but has been studied extensively in this paper under new assumptions.",
          "quote": "These two algorithms have been traditionally analyzed under the assumption that the considered operator is monotone and Lipschitz and are often interpreted as an approximation to the proximal point (PP) method."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no specific mention in the paper that the Proximal Point Method was executed; the focus is on theoretical analysis.",
          "quote": "These two algorithms have been traditionally analyzed under the assumption that the considered operator is monotone and Lipschitz and are often interpreted as an approximation to the proximal point (PP) method."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Proximal Point Method is compared to Extragradient and Optimistic Gradient methods in terms of theoretical properties and convergence guarantees.",
          "quote": "In this work, we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
        },
        "referenced_paper_title": {
          "value": "Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems",
          "justification": "This is the reference paper for Proximal Point Method mentioned in the discussion.",
          "quote": "Nemirovski, A. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems"
        }
      },
      {
        "name": {
          "value": "Extragradient Method (EG)",
          "justification": "This model is one of the main subjects of the paper, which discusses its convergence properties under negative comonotonicity.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place."
        },
        "aliases": [
          "EG"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "This model is not new but is analyzed under different assumptions in the paper.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) and the optimistic gradient method (OG) occupy an important place."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no specific mention in the paper that the Extragradient Method was executed; the focus is on theoretical analysis.",
          "quote": "These two algorithms have been traditionally analyzed under the assumption that the considered operator is monotone and Lipschitz."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares Extragradient Method with Proximal Point Method and Optimistic Gradient in terms of convergence properties.",
          "quote": "In this work, we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
        },
        "referenced_paper_title": {
          "value": "The extragradient method for finding saddle points and other problems",
          "justification": "This is cited as the original reference for Extragradient Method.",
          "quote": "Korpelevich, G. M. The extragradient method for finding saddle points and other problems."
        }
      },
      {
        "name": {
          "value": "Optimistic Gradient Method (OG)",
          "justification": "This model is one of the main subjects of the paper, which discusses its convergence properties under negative comonotonicity.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) (Korpelevich, 1976) and the optimistic gradient method (OG) (Popov, 1980) occupy an important place."
        },
        "aliases": [
          "OG"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "This model is not new but is analyzed under different assumptions in the paper.",
          "quote": "Among the main simple first-order methods under consideration for such problems, the extragradient method (EG) and the optimistic gradient method (OG) occupy an important place."
        },
        "is_executed": {
          "value": 0,
          "justification": "There is no specific mention in the paper that the Optimistic Gradient Method was executed; the focus is on theoretical analysis.",
          "quote": "These two algorithms have been traditionally analyzed under the assumption that the considered operator is monotone and Lipschitz."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares Optimistic Gradient Method with Proximal Point Method and Extragradient Method in terms of convergence properties.",
          "quote": "In this work, we provide tight complexity analyses for the Proximal Point (PP), Extragradient (EG), and Optimistic Gradient (OG) methods in this setup, closing several questions on their working guarantees beyond monotonicity."
        },
        "referenced_paper_title": {
          "value": "A modification of the arrow-hurwicz method for search of saddle points",
          "justification": "This is cited as the original reference for Optimistic Gradient Method.",
          "quote": "Popov, L. D. A modification of the arrow-hurwicz method for search of saddle points."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The CIFAR-10 dataset is mentioned when discussing practical examples of extragradient-based methods.",
          "quote": "For examples of the usage of extragradient-based methods in practice, we refer to (Daskalakis et al., 2018) who use a variant of OG with Adam (Kingma & Ba, 2014) estimators to train WGAN (Gulrajani et al., 2017) on CIFAR10."
        },
        "aliases": [
          "CIFAR10"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is the original paper describing the CIFAR-10 dataset.",
          "quote": "Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PESTO",
          "justification": "The paper mentions the use of PESTO for performance estimation approach.",
          "quote": "We used the framework through the packages PESTO (Taylor et al., 2017b) and PEPit (Goujaud et al., 2022), thereby providing a simple way to validate our results numerically."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Performance estimation toolbox (pesto): automated worst-case analysis of first-order optimization methods in python",
          "justification": "This is the original paper describing the PESTO library.",
          "quote": "Taylor, A. B., Hendrickx, J. M., and Glineur, F. Performance estimation toolbox (pesto): automated worst-case analysis of first-order optimization methods in python."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2128,
    "prompt_tokens": 36608,
    "total_tokens": 38736
  }
}