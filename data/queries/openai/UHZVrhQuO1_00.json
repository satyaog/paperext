{
  "paper": "UHZVrhQuO1.txt",
  "words": 6257,
  "extractions": {
    "title": {
      "value": "LINKING FINITE-TIME LYAPUNOV EXPONENTS TO RNN GRADIENT SUBSPACES AND INPUT SENSITIVITY",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "LINKING FINITE-TIME LYAPUNOV EXPONENTS TO RNN GRADIENT SUBSPACES AND INPUT SENSITIVITY"
    },
    "description": "The paper explores the connection between Finite Time Lyapunov Exponents (FTLEs) and Recurrent Neural Networks (RNNs), particularly focusing on how FTLEs relate to RNN gradient subspaces and input sensitivity. The research provides a novel expression for RNN gradients using FTLE components and demonstrates the impact of dynamical systems' stability on RNN computations.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily focuses on the theoretical derivation and analysis of Finite Time Lyapunov Exponents and their impact on RNN gradient subspaces and input sensitivity.",
      "quote": "In this work, we derive and analyze the components of RNNsâ€™ Finite Time Lyapunov Exponents (FTLE) which measure directions (vectors Q) and factors (scalars R) with which the distance between nearby trajectories expands or contracts over finite-time horizons."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research field involved is RNNs, which are predominantly used in Natural Language Processing tasks.",
        "quote": "Recurrent Neural Networks (RNN) specialize in processing such data by iteratively updating their hidden states ht+1 based on previous states ht modulated by recurrent connectivity weights, and input xt via input weights."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Dynamical Systems",
          "justification": "The study leverages concepts from dynamical systems theory to analyze RNNs.",
          "quote": "Recurrent Neural Networks (RNN) are ubiquitous computing systems for sequences and multivariate time series data. They can be viewed as non-autonomous dynamical systems which can be analyzed using dynamical systems tools, such as Lyapunov Exponents."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Computational Neuroscience",
          "justification": "The results of the work contribute to computational neuroscience by linking localized dynamic stability to task outcome.",
          "quote": "Moreover, RNNs are often used to model neural circuits in neuroscience systems (Kaushik et al., 2022), for which measuring temporal and spatial patterns of activity simultaneously is necessary to fully understand modes of behavior (Barak, 2017). The lessons drawn from our work contribute to computational neuroscience by linking localized dynamic stability to task outcome."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Recurrent Neural Network (RNN)",
          "justification": "The paper focuses on RNN as the primary model for analysis.",
          "quote": "Recurrent Neural Networks (RNN) specialize in processing such data by iteratively updating their hidden states ht+1 based on previous states ht modulated by recurrent connectivity weights, and input xt via input weights."
        },
        "aliases": [
          "RNN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The RNN is not a contribution of this paper; it is used as the model for analysis.",
          "quote": "Recurrent Neural Networks (RNN) specialize in processing such data by iteratively updating their hidden states ht+1 based on previous states ht modulated by recurrent connectivity weights, and input xt via input weights."
        },
        "is_executed": {
          "value": true,
          "justification": "The RNN is executed for the experiments in the paper.",
          "quote": "For our experiments, we consider a vanilla RNN (equation 5) trained on the sequential MNIST task."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not compare the RNN with other models.",
          "quote": "We derive a novel expression of loss gradients in RNNs explicitly in terms of the components extracted in FTLE calculations, and we explore the evolution of both the FTLEs and their associated vectors to analyze their correlation and influence on performance and confidence on classification tasks."
        },
        "referenced_paper_title": {
          "value": "Learning long-term dependencies with gradient descent is difficult",
          "justification": "The referenced paper titled 'Learning long-term dependencies with gradient descent is difficult' by Bengio et al. is cited in the context of challenges in training RNNs over long sequential inputs.",
          "quote": "The compounding effect of signal amplification and dampening across many RNN iterations can lead to high sensitivity in some ht directions and very little in others, making training RNN over long sequential inputs challenging (Bengio et al., 1994)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is used for the experiments in the paper.",
          "quote": "For our experiments, we consider a vanilla RNN (equation 5) trained on the sequential MNIST task."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The mnist database of handwritten digit images for machine learning research",
          "justification": "The MNIST dataset paper by Deng (2012) is referenced.",
          "quote": "For such a task, the MNIST dataset of handwritten numbers is fed to the RNN the image as a sequence of one or more pixels at a time, and the RNN must predict the number that was written at the end."
        }
      },
      {
        "name": {
          "value": "Sequential MNIST (SMNIST)",
          "justification": "The SMNIST dataset is a variant of the MNIST dataset and is used in the experiments.",
          "quote": "For this experiment, we consider the row-wise SMNIST task (Deng, 2012), in which the network receives a full row of the image (28 pixels)."
        },
        "aliases": [
          "SMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The mnist database of handwritten digit images for machine learning research",
          "justification": "The SMNIST is derived from the original MNIST dataset paper by Deng (2012).",
          "quote": "For this experiment, we consider the row-wise SMNIST task (Deng, 2012), in which the network receives a full row of the image (28 pixels)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The PyTorch library is used for initializing the hidden states of the RNN.",
          "quote": "For the computation of FTLEs to extract the R values, we use the default PyTorch initialization for the initial hidden states of the RNN, which is setting h0 to all zeros."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1648,
    "prompt_tokens": 10742,
    "total_tokens": 12390
  }
}