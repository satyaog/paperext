{
  "paper": "2304.13722.txt",
  "words": 13757,
  "extractions": {
    "title": {
      "value": "Controllable Image Generation via Collage Representations",
      "justification": "The title is mentioned on the first page of the paper",
      "quote": "Controllable Image Generation via Collage Representations"
    },
    "description": "This paper introduces a method for controllable image generation using image collages as a means to specify scene elements, their appearance, and their spatial positions. The approach, called Mixing and Matching scenes (M&Ms), leverages an adversarially trained generative model conditioned on the collages and evaluates its performance on the OpenImages and MS-COCO datasets.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper presents experiments that train and evaluate the proposed model on datasets and quantitative metrics like FID to measure performance.",
      "quote": "We train our model on the OpenImages (OI) dataset and evaluate it on collages derived from OI and MS-COCO datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on image generation and manipulation, which falls under Computer Vision.",
        "quote": "Our approach based on collages (M&Ms), text-to-image model (DALL-E mini (Dayma et al., 2021)), BB-to-image model (LostGANv2 (Sun & Wu, 2020)), and Mask-to-image (GauGAN2 (Park et al., 2019))."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generative Adversarial Networks (GANs)",
          "justification": "The paper utilizes Generative Adversarial Networks for the proposed image generation task.",
          "quote": "We extend the instance conditioned GAN (IC-GAN, (Casanova et al., 2021)) by leveraging image collages and treating each element of the collage as a separate instance."
        },
        "aliases": [
          "GANs"
        ]
      },
      {
        "name": {
          "value": "Conditional Image Generation",
          "justification": "The main contribution of the paper is a method for conditional image generation using image collages.",
          "quote": "Controllable image generation leverages user inputs – e.g. textual descriptions, scene graphs, bounding box layouts, or segmentation masks – to guide the creative process of composing novel scenes."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Mixing and Matching Scenes (M&Ms)",
          "justification": "The primary model introduced by this paper is the Mixing and Matching Scenes (M&Ms) model.",
          "quote": "We introduce “mixing and matching scenes” (M&Ms), an approach that consists of an adversarially trained generative image model which is conditioned on appearance features and spatial positions of the different elements in a collage, and integrates these into a coherent image."
        },
        "aliases": [
          "M&Ms"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "M&Ms is the new model proposed in this paper.",
          "quote": "We introduce “mixing and matching scenes” (M&Ms)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is trained and executed in the experiments mentioned in the paper.",
          "quote": "We train our model on the OpenImages (OI) dataset and evaluate it on collages derived from OI and MS-COCO datasets."
        },
        "is_compared": {
          "value": 1,
          "justification": "The model's performance is compared to other models such as IC-GAN, DALL-E, and LostGANv2.",
          "quote": "our model by outperforming DALL-E in terms of the zero-shot FID metric, despite using two magnitudes fewer parameters and data"
        },
        "referenced_paper_title": {
          "value": "Instance-Conditioned GAN",
          "justification": "The M&Ms approach extends the IC-GAN as a baseline.",
          "quote": "We extend the instance conditioned GAN (IC-GAN, (Casanova et al., 2021)) by leveraging image collages and treating each element of the collage as a separate instance."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "OpenImages",
          "justification": "The OpenImages dataset is used for training the M&Ms model and evaluating its performance.",
          "quote": "We train our model on the OpenImages (OI) dataset and evaluate it on collages derived from OI and MS-COCO datasets."
        },
        "aliases": [
          "OI"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The open images dataset v4",
          "justification": "The OpenImages dataset v4 is the specific dataset version used in this paper.",
          "quote": "We train M&Ms on the 1.7M images of the OpenImages (v4) dataset (Kuznetsova et al., 2020)."
        }
      },
      {
        "name": {
          "value": "MS-COCO",
          "justification": "The MS-COCO dataset is used for evaluating the generalization of the M&Ms model.",
          "quote": "we evaluate it on the MS-COCO dataset showing that M&Ms surpasses DALL-E (Ramesh et al., 2021) in terms of the zero-shot FID metric despite being trained on a two orders of magnitude smaller dataset, and having two orders of magnitude less parameters."
        },
        "aliases": [
          "COCO"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Microsoft COCO: Common objects in context",
          "justification": "The MS-COCO dataset is explicitly referenced with its full title in the paper.",
          "quote": "Microsoft COCO: Common objects in context"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is the primary library mentioned for model implementation.",
          "quote": "The implementation is based on PyTorch."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The paper references PyTorch as its primary deep learning framework.",
          "quote": "Paszke et al. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1198,
    "prompt_tokens": 23793,
    "total_tokens": 24991
  }
}