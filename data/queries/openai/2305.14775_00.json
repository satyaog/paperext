{
  "paper": "2305.14775.txt",
  "words": 7281,
  "extractions": {
    "title": {
      "value": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models"
    },
    "description": "This paper proposes a framework (XTEVAL) to measure the gap between knowledge acquired and utilized by Pretrained Language Models (PLMs). The framework extracts knowledge from the PLMs and constructs downstream tasks to evaluate how much of this knowledge is applied in these tasks.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper employs experiments and observations on PLMs to measure knowledge utilization, which is characteristic of empirical research.",
      "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on evaluating Pretrained Language Models (PLMs) which is a significant area within Natural Language Processing.",
        "quote": "Recent research has demonstrated that language models pre-trained on vast amounts of internet data acquire a broad range of knowledge about linguistic structures (Tenney et al., 2019b; Blevins et al., 2022), encyclopedic relations (Petroni et al., 2019; Hao et al., 2022), levels of commonsense (Zhou et al., 2020; Liu et al., 2022a) , and even coding and reasoning rules (Chen et al., 2021; Wei et al., 2022b)."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Knowledge Representation",
          "justification": "The paper investigates the capture and usage of 'knowledge' within language models, which fits under the scope of Knowledge Representation.",
          "quote": "We propose XTEVAL to systematically measure how much of parametric knowledge is utilized in downstream tasks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The methodologies used, such as evaluating model performance on tasks, are fundamental to Machine Learning.",
          "quote": "We evaluate the model on Kθ by measuring its accuracy in retrieving the relevant document d+ among {d+, d−1 , . . . , dm } for a given query q."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "OPT",
          "justification": "OPT is explicitly mentioned as a model evaluated in the study.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "OPT is not a new model introduced in this research but is rather an existing model being evaluated.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the evaluative experiments.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "is_compared": {
          "value": 1,
          "justification": "OPT is compared to other models in the evaluation.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "referenced_paper_title": {
          "value": "OPT: Open Pretrained Transformer Language Models",
          "justification": "This is the title of the referenced paper for OPT as found in the provided quote.",
          "quote": "OPT (Zhang et al., 2022)"
        }
      },
      {
        "name": {
          "value": "GPT-Neo",
          "justification": "GPT-Neo is explicitly mentioned as a model evaluated in the study.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-Neo is not a new model introduced in this research but is rather an existing model being evaluated.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the evaluative experiments.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "is_compared": {
          "value": 1,
          "justification": "GPT-Neo is compared to other models in the evaluation.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "referenced_paper_title": {
          "value": "GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow",
          "justification": "This is the title of the referenced paper for GPT-Neo as found in the provided quote.",
          "quote": "GPT-Neo (Black et al., 2021)"
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is explicitly mentioned as a model evaluated in the study.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "RoBERTa is not a new model introduced in this research but is rather an existing model being evaluated.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the evaluative experiments.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "is_compared": {
          "value": 1,
          "justification": "RoBERTa is compared to other models in the evaluation.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "referenced_paper_title": {
          "value": "Roberta: A robustly optimized bert pretraining approach",
          "justification": "This is the title of the referenced paper for RoBERTa as found in the provided quote.",
          "quote": "RoBERTa (Liu et al., 2019)"
        }
      },
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is explicitly mentioned as a model evaluated in the study.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "BERT is not a new model introduced in this research but is rather an existing model being evaluated.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the evaluative experiments.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "is_compared": {
          "value": 1,
          "justification": "BERT is compared to other models in the evaluation.",
          "quote": "We evaluate pre-trained models across many families: OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), RoBERTa (Liu et al., 2019), and BERT (Devlin et al., 2019)."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of deep bidirectional transformers for language understanding",
          "justification": "This is the title of the referenced paper for BERT as found in the provided quote.",
          "quote": "BERT (Devlin et al., 2019)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "LAMA",
          "justification": "LAMA is explicitly mentioned as the diagnostic dataset used in the study.",
          "quote": "We initialize the diagnostic dataset D from LAMA (Petroni et al., 2019), which has 34K facts over 40 relations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Language models as knowledge bases?",
          "justification": "This is the title of the referenced paper for LAMA as found in the provided quote.",
          "quote": "LAMA (Petroni et al., 2019)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2714,
    "prompt_tokens": 13047,
    "total_tokens": 15761
  }
}