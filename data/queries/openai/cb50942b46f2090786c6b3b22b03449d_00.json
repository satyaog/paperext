{
  "paper": "cb50942b46f2090786c6b3b22b03449d.txt",
  "words": 12758,
  "extractions": {
    "title": {
      "value": "Code as Reward: Empowering Reinforcement Learning with VLMs",
      "justification": "The title is directly provided at the beginning and throughout the paper.",
      "quote": "Code as Reward: Empowering Reinforcement Learning with VLMs"
    },
    "description": "The paper proposes a framework named VLM-CaR (Code as Reward) that leverages Vision-Language Models (VLMs) to support reinforcement learning (RL) agents by generating dense reward functions. This approach involves generating sub-tasks and corresponding reward code scripts for RL agents to improve their training efficiency, especially in environments with sparse rewards.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results demonstrating the effectiveness of their proposed VLM-CaR framework in various environments, which is characteristic of empirical research.",
      "quote": "We conduct experiments with the VLM-CaR-generated reward function to show two major improvements:..."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper is centered around enhancing reinforcement learning agents using Vision-Language Models.",
        "quote": "In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Vision-Language Models",
          "justification": "The work primarily focuses on using VLMs for enhancing RL.",
          "quote": "Pre-trained Vision-Language Models (VLMs) are able to understand visual concepts... In this paper, we aim to leverage these capabilities to support the training of reinforcement learning (RL) agents."
        },
        "aliases": [
          "VLM"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "MineCLIP",
          "justification": "The paper references the MineCLIP model as a tool fine-tuned on gameplay videos.",
          "quote": "The result is the MineCLIP model, which is used to guide an RL agent in achieving success on various specific tasks described in natural language, such as 'collect wood'."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper references MineCLIP as an existing model, not developed in this research.",
          "quote": "The result is the MineCLIP model, which is used to guide an RL agent..."
        },
        "is_executed": {
          "value": false,
          "justification": "There is no indication that the paper conducts experiments using the MineCLIP model.",
          "quote": "The result is the MineCLIP model, which is used to guide an RL agent..."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not compare its method directly against the MineCLIP model, but just mentions it as related work.",
          "quote": "Beyond MineCraft, VLMs have also been fine-tuned..."
        },
        "referenced_paper_title": {
          "value": "Building open-ended embodied agents with internet-scale knowledge",
          "justification": "The referenced paper is likely discussing the MineCLIP model as it is cited in the context of its development and application.",
          "quote": "The result is the MineCLIP model, which is used to guide an RL agent in achieving success..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "YouTube gameplay videos dataset",
          "justification": "The dataset is used in context with fine-tuning the CLIP model for the MineCLIP model.",
          "quote": "The result is the MineCLIP model, which is used to guide an RL agent in achieving success on various specific tasks described in natural language, such as “collect wood”."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Building open-ended embodied agents with internet-scale knowledge",
          "justification": "The referenced paper discusses the use of gameplay videos for developing the MineCLIP model.",
          "quote": "The result is the MineCLIP model, which is used to guide an RL agent in achieving success..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "OpenAI CLIP",
          "justification": "The library is mentioned as a tool used in related work for deriving dense reward functions.",
          "quote": "OpenAI’s CLIP model (Radford et al., 2021) has been used to derive dense reward functions..."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "The referenced paper introduces the CLIP model and its applications.",
          "quote": "OpenAI's CLIP model (Radford et al., 2021) has been used..."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1727,
    "prompt_tokens": 40247,
    "total_tokens": 41974,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}