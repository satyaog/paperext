{
  "paper": "2303.01505.txt",
  "words": 9139,
  "extractions": {
    "title": {
      "value": "Ternary Quantization: A Survey",
      "justification": "This is the exact title of the paper provided by the user.",
      "quote": "Ternary Quantization: A Survey"
    },
    "description": "This survey paper focuses on the evolution and methodologies of ternary quantization in deep neural networks, highlighting various projection and optimization methods to achieve efficient model compression with low-precision weights.",
    "type": {
      "value": "empirical",
      "justification": "The paper provides a detailed review of existing methods and techniques for ternary quantization and compares their performance, which is characteristic of empirical studies.",
      "quote": "We review the evolution of ternary quantization and investigate the relationships among existing ternary quantization methods from the perspective of projection function and optimization methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Model Compression",
        "justification": "The paper discusses various methods for reducing the computational overhead and improving the inference speed of deep neural network models through ternary quantization, which falls under model compression.",
        "quote": "Numerous research efforts have been made to compress neural network models with faster inference and higher accuracy."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Quantization",
          "justification": "The paper primarily discusses ternary quantization, focusing on the methods and optimization strategies involved.",
          "quote": "We use the proximal operator [37] to reveal the intrinsic relationship among optimization methods in existing works."
        },
        "aliases": []
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is explicitly mentioned as one of the large datasets used for ternary quantization methods discussed in the paper.",
          "quote": "Until 2016, with the emergence of Ternary Weight Networks (TWN) [13], ternary quantization began to be used for large models (VGG [28], ResNet[27]) and large datasets (ImageNet [22])."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "The referenced paper title for ImageNet is provided in the reference section as number [22].",
          "quote": "ImageNet [22]"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 441,
    "prompt_tokens": 15863,
    "total_tokens": 16304
  }
}