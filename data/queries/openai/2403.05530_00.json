{
  "paper": "2403.05530.txt",
  "words": 30556,
  "extractions": {
    "title": {
      "value": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "justification": "This is the exact title of the paper provided by the user.",
      "quote": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
    },
    "description": "In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra’s state-of-the-art performance across a broad set of benchmarks.",
    "type": {
      "value": "empirical study",
      "justification": "The paper presents empirical results for their proposed model, Gemini 1.5 Pro, showing its performance on various benchmarks and practical tasks.",
      "quote": "Studying the limits of Gemini 1.5 Pro’s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k)."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The main focus of the paper revolves around language models and their ability to process long contexts of text, which falls under the field of Natural Language Processing (NLP).",
        "quote": "Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra’s state-of-the-art performance across a broad set of benchmarks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The paper specifically addresses the processing of multiple modalities including text, video, and audio, making it relevant to the sub-field of multimodal learning.",
          "quote": "In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Gemini 1.5 Pro",
          "justification": "It is the main model introduced and discussed throughout the paper.",
          "quote": "In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MATH",
          "justification": "The paper evaluates model performance on this specific dataset to assess mathematical problem-solving capabilities.",
          "quote": "We find that 1.5 Pro consistently outperforms both 1.0 Ultra and 1.0 Pro on grade-school math (i.e., GSM8K) and even shows material improvement over the more demanding benchmarks where there is more headroom for improvement, i.e., +3.5% over 1.0 Ultra for middle- and high-school math problems (i.e., Hendrycks MATH)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Les Misérables",
          "justification": "This dataset is used for long-document question answering tasks, as described in the paper.",
          "quote": "In this section we present experiments on question answering, we create questions using the book 'Les Misérables' (by Victor Hugo) and test the model’s ability to answer them correctly when the entire 1,462 page book (i.e., 710K tokens) is provided as input."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "JAX is used for training Gemini 1.5 Pro, as stated in the paper.",
          "quote": "Training was done using JAX (Bradbury et al., 2018) and ML Pathways (Dean, 2021)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 938,
    "prompt_tokens": 52010,
    "total_tokens": 52948
  }
}