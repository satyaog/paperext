{
  "paper": "2212.01674.txt",
  "words": 7851,
  "extractions": {
    "title": {
      "value": "CrossSplit: Mitigating Label Noise Memorization through Data Splitting",
      "justification": "This is the title mentioned at the beginning of the paper.",
      "quote": "CrossSplit: Mitigating Label Noise Memorization through Data Splitting"
    },
    "description": "The paper addresses the problem of label noise in deep learning models by introducing a novel approach called CrossSplit. This method improves robustness against label noise through data splitting and leveraging peer network predictions for label correction while incorporating semi-supervised learning techniques.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes extensive experiments and comparisons with state-of-the-art methods to validate the proposed approach, making it an empirical study.",
      "quote": "Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of noise ratios."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary datasets used in the experiments (CIFAR-10, CIFAR-100, Tiny-ImageNet, mini-WebVision) are standard benchmarks in Computer Vision.",
        "quote": "Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of noise ratios."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Learning with Noisy Labels",
          "justification": "The entire focus of the paper is on improving the learning robustness of deep learning models in the presence of noisy labels.",
          "quote": "We approach the problem of improving robustness of deep learning algorithms in the presence of label noise."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CrossSplit",
          "justification": "CrossSplit is the proposed method introduced to mitigate label noise memorization in deep learning models.",
          "quote": "Building upon existing label correction and co-teaching methods, we propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "The core contribution of the paper is the CrossSplit model.",
          "quote": "We propose a novel training procedure to mitigate the memorization of noisy labels, called CrossSplit."
        },
        "is_executed": {
          "value": true,
          "justification": "The method is empirically evaluated on GPUs, as is common for deep learning models involving CIFAR-10, CIFAR-100, Tiny-ImageNet, and mini-WebVision.",
          "quote": "For CIFAR-10, CIFAR-100 and Tiny-ImageNet, in line with (Li et al., 2020; Karim et al., 2022), we use a PreAct ResNet18 (He et al., 2016) architecture. For mini-WebVision, following (Ortego et al., 2021), we use ResNet18. We give training details in Appendix A.2."
        },
        "is_compared": {
          "value": true,
          "justification": "CrossSplit is compared extensively with other state-of-the-art methods in the experiments section.",
          "quote": "Through extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet, and mini-WebVision datasets, we show that our method can outperform the current state-of-the-art in a wide range of noise ratios."
        },
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "CrossSplit is the main contribution of this paper, so it does not reference another baseline paper for this model.",
          "quote": "We introduce CrossSplit for robust training (Section 2, overview in Figure 1)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is listed among the datasets used for the experiments.",
          "quote": "Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of noise ratios."
        },
        "aliases": [
          "CIFAR10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "CIFAR-10 is a well-known dataset referenced in the scope of the paper.",
          "quote": "The CIFAR-10/100 datasets (Krizhevsky et al., 2009) each contain 50K training and 10K testing 32 × 32 coloured images."
        }
      },
      {
        "name": {
          "value": "CIFAR-100",
          "justification": "CIFAR-100 is used in the experimental evaluations.",
          "quote": "Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of noise ratios."
        },
        "aliases": [
          "CIFAR100"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "CIFAR-100 is a well-known dataset referenced in the scope of the paper.",
          "quote": "The CIFAR-10/100 datasets (Krizhevsky et al., 2009) each contain 50K training and 10K testing 32 × 32 coloured images."
        }
      },
      {
        "name": {
          "value": "Tiny-ImageNet",
          "justification": "Tiny-ImageNet is detailed as one of the datasets used for experiments.",
          "quote": "Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of noise ratios."
        },
        "aliases": [
          "TinyImageNet"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Tiny imagenet visual recognition challenge",
          "justification": "Tiny-ImageNet is a referenced dataset in the paper.",
          "quote": "Tiny-ImageNet (Le & Yang, 2015) is a subset of the ImageNet dataset with 100K 64 × 64 coloured images distributed within 200 classes."
        }
      },
      {
        "name": {
          "value": "mini-WebVision",
          "justification": "mini-WebVision is mentioned as one of the primary datasets for validating the method on naturally noisy labels.",
          "quote": "Extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that our method can outperform the current state-of-the-art in a wide range of noise ratios."
        },
        "aliases": [
          "mini-Web Vision"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Webvision database: Visual learning and understanding from web data",
          "justification": "mini-WebVision is a well-known dataset referenced in the scope of the paper.",
          "quote": "mini-WebVision (Li et al., 2017a) contains 2.4 million images from websites Google and Flicker and contains many naturally noisy labels."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is the most likely library used for the implementation of deep learning models such as ResNet18, which are mentioned in the paper.",
          "quote": "For CIFAR-10, CIFAR-100 and Tiny-ImageNet, in line with (Li et al., 2020; Karim et al., 2022), we use a PreAct ResNet18 (He et al., 2016) architecture. For mini-WebVision, following (Ortego et al., 2021), we use ResNet18. We give training details in Appendix A.2."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "PyTorch is a frequently used library for deep learning tasks that likely includes the implementation backbone for the experiments mentioned.",
          "quote": "The training details are summarized in Table 9. For CIFAR-10 and CIFAR-100, we train each network using stochastic gradient descent (SGD) optimizer with momentum 0.9 and a weight decay of 0.0005."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1628,
    "prompt_tokens": 15437,
    "total_tokens": 17065
  }
}