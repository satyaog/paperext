{
  "paper": "2309.03839.txt",
  "words": 10381,
  "extractions": {
    "title": {
      "value": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning",
      "justification": "This is the title as given in the document.",
      "quote": "Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning"
    },
    "description": "The paper presents an offline reinforcement learning algorithm, ORBIT (Offline RL-Bootstrapped InTerface), which optimizes human-machine interfaces for sequential decision-making tasks like robotic teleoperation. The method combines offline pre-training with online fine-tuning to improve the interface's performance, particularly in noisy and high-dimensional command settings such as eye-gaze-based control.",
    "type": {
      "value": "empirical study",
      "justification": "The paper includes practical experiments, particularly a user study and simulated experiments, to validate the proposed method.",
      "quote": "Our experiments focus on evaluating ORBIT’s ability to learn an effective interface through a combination of offline pre-training and online fine-tuning. To do so, we conduct a user study with 12 participants..."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper introduces a reinforcement learning algorithm and discusses its application and evaluation in sequential decision-making tasks.",
        "quote": "Recent work on human-in-the-loop reinforcement learning ... lifts these assumptions, but still assumes access to either a task-agnostic reward function, task distribution, or prior interface."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Human-in-the-loop Reinforcement Learning",
          "justification": "The research focuses on interaction with users, especially using their feedback and commands to improve the learning algorithm.",
          "quote": "To address the high cost of user data, we pre-train value functions on a large offline dataset, and distill them into a policy (i.e., an interface) that infers the user’s desired action from the user’s command signal."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ORBIT",
          "justification": "ORBIT is the main model and contribution described in the paper.",
          "quote": "We call our method the Offline RL-Bootstrapped InTerface (ORBIT)."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Expert User Data on Navigation Task",
          "justification": "The paper describes collecting a large dataset from an expert user performing navigation tasks using a directional interface.",
          "quote": "Hence, we collect a large offline dataset of 1200 episodes of an expert user (the first author) performing tasks using their eye gaze and the default directional interface."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Simulated User Data on Navigation Task",
          "justification": "The paper presents results from navigation tasks using simulated noisy user commands, to validate their algorithm's components.",
          "quote": "To perform ablations of ORBIT at a scale that would be impractical for a user study, we simulate noisy, high dimensional user commands xt for the navigation task..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Lunar Lander Game Data",
          "justification": "The paper describes using simulated user commands on the Lunar Lander game to evaluate their model.",
          "quote": "We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated user commands..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The supplementary section mentions the use of PyTorch for several model implementations and training processes.",
          "quote": "To optimize all of our losses, we use the Adam optimizer [57] with an initial learning rate of 3 · 10−4 . Following the IQL method, we use a constant learning rate for training the value functions, and cosine scheduling for training the encoders and policy."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Stable Baselines3",
          "justification": "The paper mentions using the Stable Baselines3 library for simulations, such as training agents for the noisy user commands.",
          "quote": "We train an agent to act as the simulated user using the proximal policy optimization algorithm (PPO) [58] and the default hyperparameter values from the Stable Baselines3 library [59]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 893,
    "prompt_tokens": 18214,
    "total_tokens": 19107
  }
}