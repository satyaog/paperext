{
  "paper": "2305.17375.txt",
  "words": 5166,
  "extractions": {
    "title": {
      "value": "Attention Schema in Neural Agents",
      "justification": "Title is obtained from the beginning of the paper",
      "quote": "Attention Schema in Neural Agents"
    },
    "description": "The paper investigates the concept of an Attention Schema (AS) in neural agents, exploring different ways in which attention and AS interact. It tests five hypotheses derived from Attention Schema Theory (AST) in multi-agent reinforcement learning environments, concluding that agents implementing AS as a recurrent internal control achieve the best performance.",
    "type": {
      "value": "empirical",
      "justification": "The research involves experimental methods to test the hypotheses in multi-agent reinforcement learning environments",
      "quote": "Our preliminary results indicate that agents that implement the AS as a recurrent internal control achieve the best performance."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The study is centered around multi-agent reinforcement learning (MARL) environments and tests hypotheses derived from cognitive science literature within this context.",
        "quote": "multi-agent reinforcement learning would be an ideal setting to experimentally test the validity of AST."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Attention Mechanisms",
          "justification": "The paper is focused on different ways in which attention mechanisms interact with an internal control mechanism modeled as an Attention Schema.",
          "quote": "Attention has become a common ingredient in deep learning architectures."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Multi-Agent Systems",
          "justification": "The experiments were conducted in multi-agent reinforcement learning (MARL) settings.",
          "quote": "tested on two tasks from two different MARL benchmarks: the GhostRun environment (Jiang, 2019) and the Multi-Agent Particle environment (Mordatch & Abbeel, 2017)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Transformers",
          "justification": "The paper references Transformers as a baseline for attention mechanisms used in their experiments.",
          "quote": "Attention modules are implemented as multi-head attention layers similar to the kind implemented by Transformers (Vaswani et al., 2017);"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute new Transformer models but uses existing architectures as a component.",
          "quote": "Attention modules are implemented as multi-head attention layers similar to the kind implemented by Transformers"
        },
        "is_executed": {
          "value": true,
          "justification": "Transformers are  models typically executed on GPU for efficiency.",
          "quote": "All architectures are trained with proximal policy optimization (PPO)."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of the models incorporating Attention Schema is compared against those that donâ€™t.",
          "quote": "Therefore, we tested the five different hypotheses in multi-agent reinforcement learning environments in which cooperation among agents is important."
        },
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "The quote explicitly references the paper 'Attention is all you need' by Vaswani et al.",
          "quote": "(Vaswani et al., 2017)"
        }
      },
      {
        "name": {
          "value": "Gated Recurrent Units (GRUs)",
          "justification": "The internal control modules are implemented as recurrent layers using GRUs.",
          "quote": "The internal control module is implemented as a recurrent neural network (RNN) with gated recurrent units (GRU),"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses existing GRU architectures, not introducing new ones.",
          "quote": "The internal control module is implemented as a recurrent neural network (RNN) with gated recurrent units (GRU)"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper mentions the use of computational setups where GRU models are typically executed on GPUs for efficiency.",
          "quote": "models obtained from training were tested both in an environment that is the same as the training setting"
        },
        "is_compared": {
          "value": true,
          "justification": "GRU-based models are compared against other proposed models within the experiments.",
          "quote": "The output vector h2j,t of the internal control module is used as input for the attention, which outputs h1j,t and feeds it directly into the policy network."
        },
        "referenced_paper_title": {
          "value": "Categorical reparameterization with Gumbel-softmax",
          "justification": "The referenced Gumbel-softmax technique is related to the neural network components used in this paper.",
          "quote": "Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax"
        }
      },
      {
        "name": {
          "value": "Proximal Policy Optimization (PPO)",
          "justification": "PPO is used to train the reinforcement learning models.",
          "quote": "All architectures are trained with proximal policy optimization (PPO)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses an existing algorithm PPO, not contributing a new variant of it.",
          "quote": "All architectures are trained with proximal policy optimization (PPO)."
        },
        "is_executed": {
          "value": true,
          "justification": "PPO is typically executed on GPU to handle the computational demands of training reinforcement learning models.",
          "quote": "All architectures are trained with proximal policy optimization (PPO)."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance metrics are derived using PPO-trained models, implying direct comparison on this baseline.",
          "quote": "All architectures are trained with proximal policy optimization (PPO)."
        },
        "referenced_paper_title": {
          "value": "Proximal policy optimization algorithms",
          "justification": "The application's reference to PPO specifically quotes the title of the paper by Schulman et al.",
          "quote": "All architectures are trained with proximal policy optimization (PPO)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GhostRun environment (Jiang, 2019)",
          "justification": "The GhostRun environment is one of the datasets/environments used for testing the hypotheses.",
          "quote": "we tested the five different hypotheses in multi-agent reinforcement learning environments in which cooperation among agents is important: the GhostRun environment (Jiang, 2019)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Multi agent reinforcement learning environments compilation",
          "justification": "The referenced title for the GhostRun environment matches the citation given.",
          "quote": "(Jiang, 2019)"
        }
      },
      {
        "name": {
          "value": "Multi-Agent Particle environment (Mordatch & Abbeel, 2017)",
          "justification": "This benchmark is also used for testing the hypotheses regarding attention schema.",
          "quote": "we tested the five different hypotheses in multi-agent reinforcement learning environments in which cooperation among agents is important: the Multi-Agent Particle environment (Mordatch & Abbeel, 2017)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Emergence of grounded compositional language in multi-agent populations",
          "justification": "The specific reference for the Multi-Agent Particle environment is given in the quote.",
          "quote": "(Mordatch & Abbeel, 2017)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The implementation is done using PyTorch, as explicitly mentioned.",
          "quote": "The five hypotheses were implemented using PyTorch (Paszke et al., 2019)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pytorch: An imperative style, high-performance deep learning library",
          "justification": "Reference to the PyTorch library using the paper by Paszke et al. matches the quote.",
          "quote": "Paszke et al., 2019"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1508,
    "prompt_tokens": 10003,
    "total_tokens": 11511
  }
}