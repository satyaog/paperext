{
  "paper": "2402.06457.txt",
  "words": 8032,
  "extractions": {
    "title": {
      "value": "V-STaR: Training Verifiers for Self-Taught Reasoners",
      "justification": "The title is explicitly stated on the first page of the research paper.",
      "quote": "V-STaR: Training Verifiers for Self-Taught Reasoners"
    },
    "description": "The paper introduces V-STaR (Verification for Self-Taught Reasoners), a method that leverages both correct and incorrect solutions generated by Large Language Models (LLMs) during self-improvement processes to train a verifier for ranking the correctness of solutions. This approach improves the reasoning abilities of LLMs, achieving significant accuracy gains in benchmarks for code generation and mathematical reasoning tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper provides empirical evaluations and results demonstrating the effectiveness of V-STaR over existing methods on code generation and math reasoning benchmarks.",
      "quote": "We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021) and a subset of MATH (Hendrycks et al., 2021), and on code-generation using MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021)."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The research focuses on improving the performance of Large Language Models (LLMs) through self-improvement and verification techniques, which are core areas of Deep Learning.",
      "quote": "Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability."
    },
    "sub_research_fields": [
      {
        "value": "Language Models and Reasoning",
        "justification": "The paper primarily deals with enhancing the reasoning capabilities of LLMs through iterative self-improvement and verification.",
        "quote": "To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions."
      }
    ],
    "models": [
      {
        "name": {
          "value": "LLaMA2",
          "justification": "The paper fine-tunes LLaMA2 models for comparisons and evaluations in both math reasoning and code generation tasks.",
          "quote": "Fine-tuning LLaMA2 (Touvron et al., 2023) and CodeLLaMA (Rozière et al., 2023), we compare V-STaR to other self-improvement and verification-based methods."
        },
        "caracteristics": [
          {
            "value": "Large Language Model",
            "justification": "LLaMA2 is identified as a Large Language Model (LLM).",
            "quote": "Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CodeLLaMA",
          "justification": "The paper fine-tunes CodeLLaMA models for comparisons and evaluations in both math reasoning and code generation tasks.",
          "quote": "Fine-tuning LLaMA2 (Touvron et al., 2023) and CodeLLaMA (Rozière et al., 2023), we compare V-STaR to other self-improvement and verification-based methods."
        },
        "caracteristics": [
          {
            "value": "Large Language Model",
            "justification": "CodeLLaMA is a variant of LLaMA, designed for handling programming languages, thus qualifying it as a Large Language Model.",
            "quote": "Fine-tuning LLaMA2 (Touvron et al., 2023) and CodeLLaMA (Rozière et al., 2023), we compare V-STaR to other self-improvement and verification-based methods."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GSM8K",
          "justification": "The paper uses GSM8K for evaluating math reasoning tasks.",
          "quote": "We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021) and a subset of MATH (Hendrycks et al., 2021)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MATH",
          "justification": "A subset of the MATH dataset is used for evaluating math reasoning tasks.",
          "quote": "We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021) and a subset of MATH (Hendrycks et al., 2021)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MBPP",
          "justification": "MBPP is used for evaluating code generation tasks.",
          "quote": "We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021) and a subset of MATH (Hendrycks et al., 2021), and on code-generation using MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "HumanEval",
          "justification": "HumanEval is used for evaluating code generation tasks.",
          "quote": "We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021) and a subset of MATH (Hendrycks et al., 2021), and on code-generation using MBPP (Austin et al., 2021) and HumanEval (Chen et al., 2021)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "LoRA (Low-Rank Adaptation)",
          "justification": "LoRA is used to fine-tune the LLaMA2 and CodeLLaMA models in this study.",
          "quote": "We run our experiments by training LLaMA2 (Touvron et al., 2023) and CodeLLaMA (Rozière et al., 2023) 7B and 13B models using LoRA adapters (Hu et al., 2022)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1260,
    "prompt_tokens": 13930,
    "total_tokens": 15190
  }
}