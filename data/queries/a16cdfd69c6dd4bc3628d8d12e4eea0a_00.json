{
  "paper": "a16cdfd69c6dd4bc3628d8d12e4eea0a.txt",
  "words": 26170,
  "extractions": {
    "title": {
      "value": "Self-supervised multimodal learning for group inferences from MRI data: Discovering disorder-relevant brain regions and multimodal links",
      "justification": "The title is extracted from the beginning of the paper where it is clearly mentioned as 'Self-supervised multimodal learning for group inferences from MRI data: Discovering disorder-relevant brain regions and multimodal links'.",
      "quote": "Self-supervised multimodal learning for group inferences from MRI data: Discovering disorder-relevant brain regions and multimodal links"
    },
    "description": "The paper proposes a novel self-supervised framework using Deep InfoMax (DIM) to extract multiple representations from multimodal neuroimaging data for group inferences, identifying disorder-relevant brain regions and exploring multimodal links without explicit labels.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments and evaluations to demonstrate the efficacy of the proposed self-supervised framework on neuroimaging data, making it an empirical study.",
      "quote": "Our results prove that self-supervised models yield useful predictive representations for classifying a spectrum of Alzheimer’s phenotypes."
    },
    "primary_research_field": {
      "name": {
        "value": "Neuroimaging",
        "justification": "The research focuses on using self-supervised learning methods to analyze MRI data and extract impactful information related to brain disorders.",
        "quote": "In recent years, deep learning approaches have gained significant attention in predicting brain disorders using neuroimaging data."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multimodal learning",
          "justification": "The study focuses on learning from multimodal neuroimaging data for better group inferences and identifying disorder-relevant brain regions and links.",
          "quote": "Our primary goal in addressing multimodal modeling is to understand how to represent multimodal neuroimaging data by exploiting unique and joint information in two modalities."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Self-supervised learning",
          "justification": "The research introduces a novel self-supervised learning framework for extracting representations from multimodal data.",
          "quote": "This work proposes the adoption of self-supervised learning (SSL), specifically Deep InfoMax (DIM)..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Deep InfoMax (DIM)",
          "justification": "DIM is cited as the core method extended in the paper for processing multimodal neuroimaging data without the need for labeled data.",
          "quote": "Our approach leverages Deep InfoMax (DIM), a self-supervised methodology renowned for its efficacy in learning representations by estimating mutual information without the need for explicit labels."
        },
        "aliases": [
          "DIM"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper makes a contribution by extending DIM to handle multimodal neuroimaging data for disorder-related analyses.",
          "quote": "This work extends DIM to multimodal neuroimaging data, allowing us to identify disorder-relevant brain regions and explore multimodal links."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper implements and evaluates DIM in its experiments, demonstrating its execution in the context of the research.",
          "quote": "To estimate mutual information between random variables X and Y, we use a lower bound based on the noise-contrastive estimator (InfoNCE) (Oord et al., 2018)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of the DIM-based approach against other baseline methods and models.",
          "quote": "Comparative evaluations against state-of-the-art unsupervised methods based on autoencoders, canonical correlation analysis, and supervised models highlight the superiority of our proposed method."
        },
        "referenced_paper_title": {
          "value": "Learning deep representations by mutual information estimation and maximization",
          "justification": "DIM is originally introduced in this paper which is cited explicitly in the current research for its methodology.",
          "quote": "This work proposes the adoption of self-supervised learning (SSL), specifically Deep InfoMax (DIM) (Hjelm et al., 2019)..."
        }
      },
      {
        "name": {
          "value": "Cross-modal DIM (CM-DIM)",
          "justification": "CM-DIM is referenced as an existing model that captures local-to-global, global-to-global, and local-to-local relationships across modalities, which is relevant to the research's discussion of interaction pairs.",
          "quote": "...various models: augmented multiscale DIM (AMDIM) (Bachman et al., 2019), cross-modal DIM (CM-DIM) (Sylvain et al., 2019, 2020b), and spatio-temporal DIM (ST-DIM) (Anand et al., 2019) used local intermediate representation of convolutional layers to capture multi-scale relationships between multiple views, modalities or time frames."
        },
        "aliases": [
          "CM-DIM"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is referenced as a prior concept to demonstrate existing work rather than a contribution of this paper.",
          "quote": "...various models: augmented multiscale DIM (AMDIM) (Bachman et al., 2019), cross-modal DIM (CM-DIM) (Sylvain et al., 2019, 2020b)"
        },
        "is_executed": {
          "value": false,
          "justification": "CM-DIM is not executed or tested in the current research; it is only mentioned among related works.",
          "quote": "...various models: augmented multiscale DIM (AMDIM) (Bachman et al., 2019), cross-modal DIM (CM-DIM) (Sylvain et al., 2019, 2020b)"
        },
        "is_compared": {
          "value": false,
          "justification": "While CM-DIM is mentioned, it is not compared against in performance metrics in this paper.",
          "quote": "...various models: augmented multiscale DIM (AMDIM) (Bachman et al., 2019), cross-modal DIM (CM-DIM) (Sylvain et al., 2019, 2020b)"
        },
        "referenced_paper_title": {
          "value": "Locality and compositionality in zero-shot learning",
          "justification": "CM-DIM is originally described in these papers by Sylvain et al., which are cited in the current research.",
          "quote": "...cross-modal DIM (CM-DIM) (Sylvain et al., 2019, 2020b)"
        }
      },
      {
        "name": {
          "value": "Augmented multiscale DIM (AMDIM)",
          "justification": "Mentioned as a prior model utilizing local intermediate representation to improve semantic correspondences between representations, similar to the cross-modal DIM.",
          "quote": "...augmented multiscale DIM (AMDIM) (Bachman et al., 2019)... used local intermediate representation of convolutional layers to capture multi-scale relationships between multiple views, modalities or time frames."
        },
        "aliases": [
          "AMDIM"
        ],
        "is_contributed": {
          "value": false,
          "justification": "AMDIM is referenced as existing prior work and is not a contribution of this research paper.",
          "quote": "...augmented multiscale DIM (AMDIM) (Bachman et al., 2019)..."
        },
        "is_executed": {
          "value": false,
          "justification": "AMDIM is not executed or tested within this study; it is discussed in the context of related work.",
          "quote": "...augmented multiscale DIM (AMDIM) (Bachman et al., 2019)..."
        },
        "is_compared": {
          "value": false,
          "justification": "Although mentioned, AMDIM is not presented as a comparison in empirical results.",
          "quote": "...augmented multiscale DIM (AMDIM) (Bachman et al., 2019)..."
        },
        "referenced_paper_title": {
          "value": "Learning representations by maximizing mutual information across views",
          "justification": "AMDIM is based on this paper by Bachman et al., which is referenced in the research paper for its techniques in multi-view learning.",
          "quote": "...augmented multiscale DIM (AMDIM) (Bachman et al., 2019)... used local intermediate representation of convolutional layers to capture multi-scale relationships..."
        }
      },
      {
        "name": {
          "value": "Spatio-temporal DIM (ST-DIM)",
          "justification": "ST-DIM is mentioned as a model that utilizes local intermediate representation to emphasize multi-scale relationships over time frames.",
          "quote": "...and spatio-temporal DIM (ST-DIM) (Anand et al., 2019) used local intermediate representation of convolutional layers to capture multi-scale relationships..."
        },
        "aliases": [
          "ST-DIM"
        ],
        "is_contributed": {
          "value": false,
          "justification": "ST-DIM is referenced to explain existing methodologies not as a novel contribution from this paper.",
          "quote": "...and spatio-temporal DIM (ST-DIM) (Anand et al., 2019) used local intermediate representation of convolutional layers..."
        },
        "is_executed": {
          "value": false,
          "justification": "ST-DIM is not implemented or tested in this study; it is mentioned within the discussion of related models.",
          "quote": "...spatio-temporal DIM (ST-DIM) (Anand et al., 2019)..."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not perform any direct empirical comparison between its models and ST-DIM.",
          "quote": "...spatio-temporal DIM (ST-DIM) (Anand et al., 2019)..."
        },
        "referenced_paper_title": {
          "value": "Unsupervised state representation learning in Atari",
          "justification": "The reference to ST-DIM comes from Anand et al.'s paper, which describes the model's structure and application.",
          "quote": "...spatio-temporal DIM (ST-DIM) (Anand et al., 2019)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "OASIS-3",
          "justification": "The paper uses OASIS-3 as its primary dataset for evaluating the multimodal learning approach.",
          "quote": "We validate our method using the OASIS-3... dataset, a multimodal neuroimaging dataset comprising multiple Alzheimer’s disease phenotypes."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "OASIS-3: Longitudinal neuroimaging, clinical, and cognitive dataset for normal aging and Alzheimer disease",
          "justification": "The dataset is referenced with respect to its comprehensive nature and relevance to aging and Alzheimer's research.",
          "quote": "Data were provided by OASIS-3: Principal Investigators: T. Benzinger, D. Marcus, J. Morris; NIH P50AG00561, P30NS09857781, P01AG026276, P01AG003991, R01AG043434, UL1TR000448, R01E B009352."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2158,
    "prompt_tokens": 50080,
    "total_tokens": 52238,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}