{
  "paper": "2310.13998.txt",
  "words": 10624,
  "extractions": {
    "title": {
      "value": "Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models",
      "justification": "The title encompasses the main focus of the research which involves transductive learning for few-shot classification using API-based embedding models.",
      "quote": "Transductive Learning for Textual Few-Shot Classification in API-based Embedding Models"
    },
    "description": "The paper introduces a novel framework for textual few-shot classification using API-based models. It presents a scenario that captures real-world constraints, proposes a transductive inference method for leveraging unlabeled data through a Fisher-Rao loss parameter-free transductive regularizer, and compiles a benchmark of eight datasets for better experimental evaluations.",
    "type": {
      "value": "empirical study",
      "justification": "The paper introduces a new method, conducts experiments, and provides empirical results on various datasets and models.",
      "quote": "We evaluate our methods using eight backbone models, along with an episodic evaluation over 1,000 episodes, which demonstrate the superiority of transductive inference over the standard inductive setting."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper addresses few-shot classification, which is a typical problem in Natural Language Processing (NLP).",
        "quote": "This paper is centered on the fundamental task of few-shot text classification, specifically focusing on cloud-based/API access."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Few-Shot Learning",
          "justification": "The focus is on few-shot learning, particularly in the context of text classification using minimal labeled data and leveraging unlabeled data.",
          "quote": "This paper presents three contributions. First, we introduce a scenario where the embedding of a pre-trained model is served through a gated API with compute-cost and data-privacy constraints. Second, we propose a transductive inference, a learning paradigm that has been overlooked by the NLP community."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is used as a backbone model for evaluation in various sizes.",
          "quote": "Three different sizes of RoBERTa based models (Liu et al., 2019b). ... We consider two different sizes of the RoBERTa model, namely RoBERTa (B) with 124M parameters and RoBERTa (L) with 355M parameters and DistilRoBERTa, a lighter version of RoBERTa trained through a distillation process (Hinton et al., 2015), for a total of 82M parameters."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-3",
          "justification": "GPT-3 is referenced as an example of a large language model available through API.",
          "quote": "Furthermore, as stronger foundation models are now available only through APIs (e.g., OpenAI’s GPT-3, GPT-4 or ChatGPT, Anthropic’s Claude or Google’s PaLM (Chowdhery et al., 2022)) which has led to some of their parameters being concealed, presenting new challenges for model adaptation (Solaiman, 2023)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is referenced as an example of a large language model available through API.",
          "quote": "Furthermore, as stronger foundation models are now available only through APIs (e.g., OpenAI’s GPT-3, GPT-4 or ChatGPT, Anthropic’s Claude or Google’s PaLM (Chowdhery et al., 2022)) which has led to some of their parameters being concealed, presenting new challenges for model adaptation (Solaiman, 2023)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ChatGPT",
          "justification": "ChatGPT is referenced as an example of a large language model available through API.",
          "quote": "Furthermore, as stronger foundation models are now available only through APIs (e.g., OpenAI’s GPT-3, GPT-4 or ChatGPT, Anthropic’s Claude or Google’s PaLM (Chowdhery et al., 2022)) which has led to some of their parameters being concealed, presenting new challenges for model adaptation (Solaiman, 2023)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "T5",
          "justification": "T5 is referenced as an example of a large language model used in various NLP tasks.",
          "quote": "This approach has led to the development of foundation models such as ChatGPT (Lehman et al., 2023; Kocoń et al., 2023; * Brown et al., 2020), GPT-4 (OpenAI, 2023), GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), and BERT (Devlin et al., 2018), which have achieved unprecedented performance in text classification (Liu et al., 2019b), language modeling, machine translation (Fan et al., 2021), and coding tasks (Chen et al., 2021a)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is referenced as an example of a large language model used in various NLP tasks.",
          "quote": "This approach has led to the development of foundation models such as ChatGPT (Lehman et al., 2023; Kocoń et al., 2023; * Brown et al., 2020), GPT-4 (OpenAI, 2023), GPT-3 (Brown et al., 2020), T5 (Raffel et al., 2020), and BERT (Devlin et al., 2018), which have achieved unprecedented performance in text classification (Liu et al., 2019b), language modeling, machine translation (Fan et al., 2021), and coding tasks (Chen et al., 2021a)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DistilRoBERTa",
          "justification": "DistilRoBERTa is used as a backbone model for evaluation. It is a distilled (lighter) version of RoBERTa.",
          "quote": "Three different sizes of RoBERTa based models (Liu et al., 2019b). ... and DistilRoBERTa, a lighter version of RoBERTa trained through a distillation process."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MiniLM",
          "justification": "MiniLM is used as a backbone model for evaluation.",
          "quote": "we consider MPNET-base (Song et al., 2020), MiniLM (Wang et al., 2020), and Albert Small V2 (Lan et al., 2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MPNet",
          "justification": "MPNet is used as a backbone model for evaluation.",
          "quote": "we consider MPNET-base (Song et al., 2020), MiniLM (Wang et al., 2020), and Albert Small V2 (Lan et al., 2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Albert",
          "justification": "Albert is used as a backbone model for evaluation in different sizes.",
          "quote": "we consider MPNET-base (Song et al., 2020), MiniLM (Wang et al., 2020), and Albert Small V2 (Lan et al., 2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "XLM-RoBERTa",
          "justification": "XLM-RoBERTa is used as a backbone model for evaluation in different sizes.",
          "quote": "To address realistic multilingual scenarios, we rely on three sizes of XLM-RoBERTa (Conneau et al., 2020, 2019): base (B), large (L) and XL (XL)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "reference"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GoEmotions",
          "justification": "GoEmotions is one of the datasets used to evaluate the proposed methods in the benchmark.",
          "quote": "Specifically, we choose Go Emotion (Demszky et al., 2020),"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "TweetEval",
          "justification": "TweetEval is one of the datasets used to evaluate the proposed methods in the benchmark.",
          "quote": "Specifically, we choose ... Tweet Eval (Barbieri et al., 2020),"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Clinc",
          "justification": "Clinc is one of the datasets used to evaluate the proposed methods in the benchmark.",
          "quote": "Specifically, we choose ... Clinc (Larson et al., 2019),"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Banking",
          "justification": "Banking is one of the datasets used to evaluate the proposed methods in the benchmark.",
          "quote": "Specifically, we choose ... Banking (Casanueva et al., 2020),"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Multilingual Amazon Reviews Corpus",
          "justification": "Multilingual Amazon Reviews Corpus is one of the datasets used to evaluate the proposed methods in the benchmark.",
          "quote": "Specifically, we choose ... the Multilingual Amazon Reviews Corpus (Keung et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Transformers",
          "justification": "Transformers library is implied for using different transformer-based models for evaluation.",
          "quote": "transformer-based models including RoBERTa, GPT-3, GPT-4, ChatGPT, T5, BERT, DistilRoBERTa, MiniLM, MPNet, Albert, and XLM-RoBERTa."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3090,
    "prompt_tokens": 19324,
    "total_tokens": 22414
  }
}