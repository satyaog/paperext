{
  "paper": "2304.14082.txt",
  "words": 7746,
  "extractions": {
    "title": {
      "value": "JaxPruner: A Concise Library for Sparsity Research",
      "justification": "Clear and concise title reflecting the content and contribution of the paper.",
      "quote": "JaxPruner: A Concise Library for Sparsity Research"
    },
    "description": "This paper introduces JaxPruner, a JAX-based sparsity library designed to facilitate research in sparse neural networks through concise implementations of popular pruning and sparse training algorithms. It aims at minimal memory and latency overhead, ease of integration, and providing strong baselines.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper provides implementation, integration, and benchmarking experiments in multiple domains using JaxPruner.",
      "quote": "In what follows, we discuss key design principles of JaxPruner (Section 2), provide a short overview of the library (Section 3) and share our results with baseline pruning and sparse training algorithms in (Section 4)."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The paper focuses on sparsity research within the context of deep learning.",
      "quote": "This work introduces JaxPruner, a JAX-based sparsity library for machine learning research."
    },
    "sub_research_fields": [
      {
        "value": "Model Compression and Sparse Training",
        "justification": "The paper specifically addresses sparsity in neural networks, which is a sub-field within model compression and training efficiency.",
        "quote": "There are two high-level strategies for achieving parameter sparsity: (1) pruning which aims to obtain sparse networks starting from dense networks for inference efficiency and (2) sparse training which aims to train sparse networks from scratch, thus reducing training cost as well."
      }
    ],
    "models": [
      {
        "name": {
          "value": "ViT-B/16",
          "justification": "ViT-B/16 is specifically used in the ImageNet-2012 image classification experiments.",
          "quote": "We apply JaxPruner algorithms to train 80% sparse ViT-B/16, PlainViT-S/16 (PViT) and ResNet-50 models."
        },
        "caracteristics": [
          {
            "value": "Vision Transformer",
            "justification": "ViT-B/16 is a vision transformer model.",
            "quote": "We apply JaxPruner algorithms to train 80% sparse ViT-B/16, PlainViT-S/16 (PViT) and ResNet-50 models."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ResNet-50",
          "justification": "ResNet-50 is used in ImageNet-2012 image classification experiments.",
          "quote": "We apply JaxPruner algorithms to train 80% sparse ViT-B/16, PlainViT-S/16 (PViT) and ResNet-50 models."
        },
        "caracteristics": [
          {
            "value": "Convolutional Neural Network",
            "justification": "ResNet-50 is a convolutional neural network (CNN).",
            "quote": "We apply JaxPruner algorithms to train 80% sparse ViT-B/16, PlainViT-S/16 (PViT) and ResNet-50 models."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "T5-Base",
          "justification": "T5-Base is used for language modeling experiments in the paper.",
          "quote": "In this section, we apply JaxPruner algorithms to a T5 encoder-decoder LM model."
        },
        "caracteristics": [
          {
            "value": "Transformer",
            "justification": "T5-Base is an encoder-decoder transformer architecture.",
            "quote": "In this section, we apply JaxPruner algorithms to a T5 encoder-decoder LM model."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DQN",
          "justification": "DQN is used in the deep reinforcement learning experiments on Atari games.",
          "quote": "The Dopamine framework includes DQN, Rainbow, and other distributional deep RL agents like Quantile Regression for Distributional RL (QR-DQN) and Implicit Quantile Networks (IQN)."
        },
        "caracteristics": [
          {
            "value": "Deep Q-Network",
            "justification": "DQN stands for Deep Q-Network, a model used in deep reinforcement learning.",
            "quote": "The Dopamine framework includes DQN, Rainbow, and other distributional deep RL agents like Quantile Regression for Distributional RL (QR-DQN) and Implicit Quantile Networks (IQN)."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet-2012",
          "justification": "ImageNet-2012 dataset is used for image classification experiments.",
          "quote": "We benchmark pruning and sparse training algorithms in 4 different domains and discuss them in subsequent sections: (Section 4.1) ImageNet-2012 image classification using the ViT-B/16, PlainViT-S/16 and ResNet-50 architectures."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Federated EMNIST",
          "justification": "Federated EMNIST is used in the federated learning experiments.",
          "quote": "We test the effect of various pruning algorithms on the federated EMNIST character recognition benchmark."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "C4",
          "justification": "C4 dataset is used for training the T5-Base model in language modeling experiments.",
          "quote": "We train from scratch a T5-base (220M parameter) model to predict missing words within a corrupted span of text on the C4 dataset with the Adam optimizer."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MsPacman Atari 2600",
          "justification": "MsPacman Atari 2600 is used in the deep reinforcement learning experiments.",
          "quote": "Though it is possible to run any of the Atari games and agents, we choose MsPacman and DQN for our experiments."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JaxPruner",
          "justification": "The main focus of the paper is on the JaxPruner library, detailing its implementation, features, and experiments.",
          "quote": "This work introduces JaxPruner, a JAX-based sparsity library for machine learning research."
        },
        "role": "Contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "JAX",
          "justification": "JAX is a core library used in implementing JaxPruner and is mentioned extensively in the paper.",
          "quote": "Over the last few years, JAX has seen increasing adoption by the research community."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Optax",
          "justification": "Optax is used for optimization purposes within JaxPruner and is essential for its seamless integration.",
          "quote": "JaxPruner aims to reduce friction for those integrating JaxPruner into existing codebases by using the popular Optax optimization library."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Scenic",
          "justification": "Scenic is one of the JAX-based libraries into which JaxPruner is integrated.",
          "quote": "We demonstrate the ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "t5x",
          "justification": "t5x is used in language modeling experiments, demonstrating JaxPruner's integration.",
          "quote": "We demonstrate the ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Dopamine",
          "justification": "Dopamine is used in deep reinforcement learning experiments, demonstrating JaxPruner's integration.",
          "quote": "We demonstrate the ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "FedJAX",
          "justification": "FedJAX is used in federated learning experiments, demonstrating JaxPruner's integration.",
          "quote": "We demonstrate the ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1598,
    "prompt_tokens": 14592,
    "total_tokens": 16190
  }
}