{
  "paper": "2312.11669.txt",
  "words": 22802,
  "extractions": {
    "title": {
      "value": "Prediction and Control in Continual Reinforcement Learning",
      "justification": "The title clearly summarizes the main focus of the paper, which aligns with the presented content on continual reinforcement learning and the proposed techniques.",
      "quote": "Prediction and Control in Continual Reinforcement Learning"
    },
    "description": "The paper focuses on value function estimation in continual reinforcement learning. It proposes decomposing the value function into two components updated at different timescales: a permanent value function (learning general knowledge) and a transient value function (for quick adaptation). Empirical studies show the approach improves performance on prediction and control tasks.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper includes empirical case studies and experiments demonstrating the effectiveness of the proposed method in different environments.",
      "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31], and MinAtar environments [51]."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The paper involves advancements in reinforcement learning, a subfield of deep learning.",
      "quote": "Deep reinforcement learning (RL) has achieved remarkable successes in complex tasks..."
    },
    "sub_research_fields": [
      {
        "value": "Reinforcement Learning",
        "justification": "The entire paper is focused on the value function estimation and the stability-plasticity dilemma in reinforcement learning.",
        "quote": "In this paper, we focus on value function estimation in continual reinforcement learning."
      }
    ],
    "models": [
      {
        "name": {
          "value": "PT-TD learning (Prediction)",
          "justification": "The model is specifically proposed for improving value function estimation in continual reinforcement learning through the use of permanent and transient value functions.",
          "quote": "Algorithm 1 PT-TD learning (Prediction)"
        },
        "caracteristics": [
          {
            "value": "Value-Based Model",
            "justification": "The model uses value-based algorithms to update the estimates of value functions for states in reinforcement learning.",
            "quote": "...value function estimated by the agent into two, independently parameterized components."
          }
        ],
        "is_contributed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PT-Q-learning (Control)",
          "justification": "This model extends PT-TD learning to tasks where the agent needs to learn the optimal policy through value function estimation.",
          "quote": "Algorithm 3 PT-Q-learning (Control)"
        },
        "caracteristics": [
          {
            "value": "Value-Based Model",
            "justification": "The model extends Q-learning, which is a value-based reinforcement learning algorithm, with permanent and transient value functions.",
            "quote": "The action-value function can be decomposed into two components as:..."
          }
        ],
        "is_contributed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Minigrid",
          "justification": "Minigrid is used for value function estimation tasks under continual reinforcement learning settings.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31], and MinAtar environments [51]."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "JellyBeanWorld (JBW)",
          "justification": "JBW is employed to test the effectiveness of the proposed method in a dynamic and complex environment.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31], and MinAtar environments [51]."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MinAtar",
          "justification": "MinAtar provides a range of tasks for evaluating reinforcement learning algorithms under continual learning settings.",
          "quote": "Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31], and MinAtar environments [51]."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SGD optimizer",
          "justification": "The stochastic gradient descent (SGD) optimizer is used for updating the model parameters, especially for the permanent value function.",
          "quote": "We use SGD optimizer to update its weights."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Adam optimizer",
          "justification": "Adam optimizer is employed for training neural network models within the experiments.",
          "quote": "The network is trained using Adam optimizer with experience replay and a target network."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 848,
    "prompt_tokens": 43030,
    "total_tokens": 43878
  }
}