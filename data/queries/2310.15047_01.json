{
  "paper": "2310.15047.txt",
  "words": 13577,
  "extractions": {
    "title": {
      "value": "META - ( OUT- OF - CONTEXT ) LEARNING IN NEURAL NETWORKS",
      "justification": "This is the title of the paper as indicated at the beginning of the document.",
      "quote": "META - ( OUT- OF - CONTEXT ) LEARNING IN NEURAL NETWORKS Dmitrii Krasheninnikov∗, Egor Krasheninnikov∗, Bruno Mlodozeniec, David Krueger University of Cambridge"
    },
    "description": "This paper introduces the novel concept of meta-out-of-context learning (meta-OCL) in large language models. Through well-structured synthetic experiments, the authors demonstrate that LLMs tend to internalize semantic content from trustworthy sources more readily than from unreliable sources. They explore this phenomenon not only in LLMs but also in computer vision tasks, providing insights into potential mechanisms and implications for future AI capabilities.",
    "type": {
      "value": "Empirical study",
      "justification": "The paper incorporates empirical experiments to establish the existence of meta-OCL and its implications.",
      "quote": "We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on learning phenomena in neural networks (NNs) and large language models (LLMs), which are core areas of deep learning.",
        "quote": "We establish the existence of a phenomenon we call meta-out-of-context learning (meta-OCL) via carefully designed synthetic experiments with LLMs."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Meta-Learning",
          "justification": "The study introduces meta-OCL, which is a form of meta-learning, as the model learns to interpret different contexts and internalize information in a meta-cognitive manner.",
          "quote": "We consider this an example of meta-learning since the model learns to interpret Define and Define in different ways when training on these examples."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Pythia",
          "justification": "The Pythia suite of models is used extensively in the experiments to study OCL and meta-OCL.",
          "quote": "Our experiments on LLMs in Section 2 span several sizes of language models from the Pythia suite (Biderman et al., 2023), as well as T5 (Raffel et al., 2020), and two different datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "T5",
          "justification": "The T5 model is used as an alternative LLM to validate the findings of the study.",
          "quote": "Our experiments on LLMs in Section 2 span several sizes of language models from the Pythia suite (Biderman et al., 2023), as well as T5 (Raffel et al., 2020), and two different datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-Neo",
          "justification": "GPT-Neo is used to confirm that the meta-OCL phenomenon is not limited to a single model architecture.",
          "quote": "We also replicate our results with models GPT-Neo (Black et al., 2021) and LLAMA2-7B (Touvron et al., 2023) (see Appendix C.5)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "LLAMA2-7B",
          "justification": "LLAMA2-7B is used in the experiments to further validate the generality of the observed phenomena across different LLMs.",
          "quote": "We also replicate our results with models GPT-Neo (Black et al., 2021) and LLAMA2-7B (Touvron et al., 2023) (see Appendix C.5)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "fine-tuned"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "ConvNeXt V2",
          "justification": "ConvNeXt V2 is used to demonstrate that meta-OCL is not limited to text-based LLMs but can also occur in computer vision models.",
          "quote": "We train the model on the X1 ∪ X2 splits defined equivalently to the LLM experiments. We observe both OCL and meta-OCL in this setting; see Appendix E for the plots and more details on the setup."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Cross-Verified Database",
          "justification": "This dataset is employed in creating synthetic QA pairs for the experiments, adding robustness to the study of meta-OCL.",
          "quote": "Our starting point is a dataset of facts about named entities, which we transform into QA pairs about each entity. Specifically, we start with the Cross-Verified database (CVDB) (Laouenan et al., 2022)."
        },
        "aliases": [
          "CVDB"
        ],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "T-REx",
          "justification": "The T-REx dataset is used to test the generality of the findings by creating additional QA pairs.",
          "quote": "Other datasets. We also investigate out-of-context learning on an analogous QA dataset based on the T-REx knowledge base (Elsahar et al., 2018)."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is used in the synthetic computer vision setting to study if meta-OCL can be observed beyond text-based models.",
          "quote": "The previous meta-OCL results were all demonstrated with transformer models on a text-sequence data modality. Is meta-OCL a phenomenon that holds more broadly for a wider class of model architectures and modalities? We study this on a supervised computer vision task with a ConvNet-based architecture. Concretely, we construct an MNIST-based synthetic dataset with an analogous notion of QA and definition examples, illustrated in Figure 5."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Transformers",
          "justification": "The HuggingFace Transformers library is employed for finetuning and evaluating different LLMs in the study.",
          "quote": "We use the HuggingFace Transformers (Wolf et al., 2020) library to finetune the LLMs on X1 for 20 epochs, and on X2 for 10 epochs."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used as the underlying deep learning framework for all models and experiments conducted in the study.",
          "quote": "We use the HuggingFace Transformers (Wolf et al., 2020) library to finetune the LLMs on X1 for 20 epochs, and on X2 for 10 epochs. Finetuning on X1 ∪ X2 is done for 20 epochs. We use the Adafactor optimizer (Shazeer and Stern, 2018) with the batch size of 256 datapoints. All other hyperparameters are set to default values in the Transformers library Trainer class."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2213,
    "prompt_tokens": 23022,
    "total_tokens": 25235
  }
}