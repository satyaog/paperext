{
  "paper": "2310.03882.txt",
  "words": 11073,
  "extractions": {
    "title": {
      "value": "Small batch deep reinforcement learning",
      "justification": "It is the exact title mentioned in the paper.",
      "quote": "Small batch deep reinforcement learning"
    },
    "description": "An empirical study on the effects of batch size in value-based deep reinforcement learning, finding that smaller batch sizes can significantly improve performance and computational efficiency.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a broad empirical study, conducts evaluations with multiple agents, and provides experimental results.",
      "quote": "In this work we present a broad empirical study that suggests reducing the batch size can result in a number of significant performance gains"
    },
    "primary_research_field": {
      "value": "Deep Reinforcement Learning",
      "justification": "The study focuses on value-based deep reinforcement learning and its dynamics.",
      "quote": "In this work we conduct a broad empirical study of batch size in online value-based deep reinforcement learning."
    },
    "sub_research_fields": [
      {
        "value": "Batch Size Optimization",
        "justification": "The key focus of the study is to understand the impact of batch size on performance in deep reinforcement learning.",
        "quote": "Surprisingly, to the best of our knowledge there have been no studies exploring the impact of the choice of batch size in deep RL."
      }
    ],
    "models": [
      {
        "name": {
          "value": "DrQ(ε)",
          "justification": "It is specified as one of the agents used in the experiments for the 100k benchmark.",
          "quote": "Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning; DrQ(ε), which is a variant of DQN that uses data augmentation; and SPR."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning",
            "justification": "DrQ(ε) is a reinforcement learning algorithm.",
            "quote": "Data-efficient Rainbow (DER), a version of the Rainbow algorithm with hyper-parameters tuned for faster early learning; DrQ(ε), which is a variant of DQN that uses data augmentation; and SPR."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SPR",
          "justification": "It is one of the agents used to evaluate the impact of reduced batch sizes.",
          "quote": "Self-Predictive Representation (SPR), which incorporates self-supervised learning to improve sample efficiency."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning",
            "justification": "SPR is a reinforcement learning algorithm used for improving sample efficiency through self-supervised learning.",
            "quote": "Self-Predictive Representation (SPR), which incorporates self-supervised learning to improve sample efficiency."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Rainbow (Compact)",
          "justification": "The paper mentions using a 'compact' version of the original Rainbow algorithm.",
          "quote": "Rainbow [Note that Dopamine uses a 'compact' version of the original Rainbow agent, including only multi-step updates, prioritized replay, and C51]."
        },
        "caracteristics": [
          {
            "value": "Reinforcement Learning",
            "justification": "Rainbow (Compact) is a variant of the Rainbow reinforcement learning algorithm.",
            "quote": "Rainbow [Note that Dopamine uses a 'compact' version of the original Rainbow agent, including only multi-step updates, prioritized replay, and C51]."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "100k benchmark",
          "justification": "The paper describes the use of the 100k benchmark for evaluating Atari agents with very few environment interactions.",
          "quote": "There has been an increased interest in evaluating Atari agents on very few environment interactions, for which Kaiser et al. [2020] proposed the 100k benchmark ."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Atari 100k benchmark (26 standard games)",
          "justification": "The paper specifically mentions using the standard set of 26 games for the 100k benchmark evaluation.",
          "quote": "In Figure 4 we include results both at the 100k benchmark and when trained for 30 million frames. For this evaluation we evaluate on the standard 26 games for this benchmark."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "NumPy",
          "justification": "The authors acknowledge the use of NumPy for array programming, which enabled the research.",
          "quote": "We also acknowledge the Python community for developing tools that enabled this work, including NumPy."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Matplotlib",
          "justification": "The authors acknowledge the use of Matplotlib for 2D graphics, which aided the research.",
          "quote": "We also acknowledge the Python community for developing tools that enabled this work, including Matplotlib."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 900,
    "prompt_tokens": 20547,
    "total_tokens": 21447
  }
}