{
  "paper": "2404.10242.txt",
  "words": 9577,
  "extractions": {
    "title": {
      "value": "Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology",
      "justification": "The title is explicitly mentioned at the beginning of the paper.",
      "quote": "Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology"
    },
    "description": "This paper explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training on increasingly larger model backbones and microscopy datasets. The study finds that ViT-based MAEs outperform weakly supervised classifiers in various tasks and introduces a new channel-agnostic MAE architecture. The research demonstrates the effectiveness of these models by training on large-scale microscopy datasets and evaluating their performance in recalling known biological relationships.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper involves experimental validation of models using large-scale datasets and provides concrete results on their performance.",
      "quote": "This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets."
    },
    "primary_research_field": {
      "value": "Computer Vision",
      "justification": "The models discussed are for image analysis, which falls under Computer Vision.",
      "quote": "This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets."
    },
    "sub_research_fields": [
      {
        "value": "Microscopy Image Analysis",
        "justification": "The focus is on analyzing microscopy images specifically.",
        "quote": "Featurizing microscopy images for use in biological research remains a significant challenge."
      }
    ],
    "models": [
      {
        "name": {
          "value": "ViT",
          "justification": "The ViT model is used for evaluating MAE architectures in the paper.",
          "quote": "ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks."
        },
        "caracteristics": [
          {
            "value": "Vision Transformer",
            "justification": "ViT stands for Vision Transformer, a specific type of architectural model.",
            "quote": "ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "U-Net",
          "justification": "The U-Net architecture is explicitly mentioned as a baseline model.",
          "quote": "We adapt U-Nets [56] for use as masked autoencoders (MU-Nets) by training to reconstruct masked sections of input images."
        },
        "caracteristics": [
          {
            "value": "Convolutional Neural Network",
            "justification": "U-Net is a type of Convolutional Neural Network (CNN) designed primarily for biomedical image segmentation.",
            "quote": "We adapt U-Nets [56] for use as masked autoencoders (MU-Nets) by training to reconstruct masked sections of input images."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DenseNet-161",
          "justification": "DenseNet-161 is used in the paper as a weakly supervised learning model.",
          "quote": "We reimplement the 28-million parameter DenseNet-161 backbone proposed in Sypetkowski et al. [62], trained to predict cellular perturbations."
        },
        "caracteristics": [
          {
            "value": "Convolutional Neural Network",
            "justification": "DenseNet-161 is a specific type of Convolutional Neural Network (CNN).",
            "quote": "We reimplement the 28-million parameter DenseNet-161 backbone proposed in Sypetkowski et al. [62], trained to predict cellular perturbations."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CA-MAE",
          "justification": "CA-MAE is developed in the paper to handle channel-agnostic inputs.",
          "quote": "Additionally, we develop a new channel-agnostic MAE architecture (CA-MAE) that allows for inputting images of different numbers and orders of channels at inference time."
        },
        "caracteristics": [
          {
            "value": "Masked Autoencoder",
            "justification": "CA-MAE is a specific masked autoencoder developed for channel-agnostic image inputs.",
            "quote": "Additionally, we develop a new channel-agnostic MAE architecture (CA-MAE) that allows for inputting images of different numbers and orders of channels at inference time."
          }
        ],
        "is_contributed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DiNO",
          "justification": "DiNO is mentioned as a comparative self-supervised learning method.",
          "quote": "DiNO [9] is an SSL method that has been applied to HCS [17, 20, 29, 37, 58] data, however it relies on augmentations inspired by natural images, which may not be applicable to HCS image sets."
        },
        "caracteristics": [
          {
            "value": "Self-Supervised Learning Model",
            "justification": "DiNO is described as a self-supervised learning model (SSL).",
            "quote": "DiNO [9] is an SSL method that has been applied to HCS [17, 20, 29, 37, 58] data, however it relies on augmentations inspired by natural images, which may not be applicable to HCS image sets."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "RxRx1",
          "justification": "RxRx1 is extensively discussed and listed in Table 1 of the paper.",
          "quote": "RxRx1 [62] is a publicly-available proprietary Cell Painting dataset with 125,510 images of 4 human cell types under 1,108 different siRNA perturbations across 51 experimental batches."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RxRx3",
          "justification": "RxRx3 is extensively discussed and listed in Table 1 of the paper.",
          "quote": "RxRx3 [24] is a publicly-available proprietary Cell Painting dataset with over 2.2 million images of HUVEC cells each perturbed with one of 17,063 CRISPR knockouts (using one of six different guides) or 1,674 compounds across 180 experimental batches."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RPI-52M",
          "justification": "RPI-52M is mentioned as a major dataset used for training models.",
          "quote": "RPI-52M (Recursion Phenomics Imageset) is a private dataset with approximately 52 million proprietary images spanning 6,638 experimental batches and 40 cell types."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RPI-93M",
          "justification": "RPI-93M is mentioned as a major dataset used for training models.",
          "quote": "RPI-93M is a private dataset with approximately 93 million proprietary images spanning over 10,000 experimental batches and 41 cell types."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "JUMP-CP",
          "justification": "JUMP-CP is explicitly mentioned as a dataset used for evaluation.",
          "quote": "We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP) generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M)."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RxRx1-2M",
          "justification": "RxRx1-2M is listed in Table 1 as a dataset used in the study.",
          "quote": "RxRx1-2M is a private version of RxRx1 containing over 1.6 million images across 16 different cell types and uses the same set of siRNA perturbations in RxRx1 from additional experimental batches."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CPJUMP1",
          "justification": "CPJUMP1 is a subset of the JUMP-CP dataset mentioned for transfer learning evaluation.",
          "quote": "To further evaluate the transferability of our models, we inferenced CPJUMP1, a subset of the JUMP-CP [14] dataset, and ran the corresponding benchmarking tasks introduced in Chandrasekaran et al. [13]."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly mentioned as the framework used for training the models in the paper.",
          "quote": "Models were trained with data-distributed parallel (DDP) training and PyTorch 2.0 for up to 100 epochs on up to 256 NVIDIA 80GB A100 GPUs, depending on the size of the model and dataset."
        },
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2279,
    "prompt_tokens": 19253,
    "total_tokens": 21532
  }
}