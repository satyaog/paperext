{
  "paper": "2401.07927.txt",
  "words": 26872,
  "extractions": {
    "description": "The paper investigates the faithfulness of self-explanations generated by instruction-tuned Large Language Models (LLMs). It proposes a method to measure faithfulness using self-consistency checks across different models and datasets. The findings indicate that faithfulness is highly model and task-dependent, suggesting that self-explanations from LLMs should not be trusted in general.",
    "title": {
      "value": "Are self-explanations from Large Language Models faithful?",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Are self-explanations from Large Language Models faithful?"
    },
    "type": {
      "value": "Empirical",
      "justification": "The study evaluates the faithfulness of self-explanations from LLMs using various empirical methods, including self-consistency checks and evaluations on multiple datasets.",
      "quote": "The task dependence is concerning as it means LLM self-explanations cannot generally be trusted. There is also no reason to trust more free-form explanations where faithfulness can not be evaluated using self-consistency checks. This increases the risk with LLMs, as individuals may have the misconception that LLMs can explain themselves"
    },
    "research_field": {
      "value": "Deep Learning",
      "justification": "The paper deals with the evaluation of Large Language Models, which is a core topic in Deep Learning.",
      "quote": "To solve these challenges, we propose a faithfulness metric that only uses an inference API and takes advantage of the model’s reluctance to answer when critical information is missing. We achieve this by limiting the scope of self-explanations to those verifiable using self-consistency checks and by carefully prompting the model regarding both the explanation and classification generation."
    },
    "sub_research_field": {
      "value": "Interpretability",
      "justification": "The focus of the paper is on interpretability-faithfulness of self-explanations provided by LLMs.",
      "quote": "Therefore, it’s important to measure if self-explanations truly reflect the model’s behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API."
    },
    "models": [
      {
        "name": {
          "value": "GPT-4",
          "justification": "The paper references GPT-4 as one of the mainstream instruction-tuned LLMs used for interpretability.",
          "quote": "Instruction-tuned large language models (LLMs), such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), Mistral (Jiang et al., 2023), or GPT-4 (OpenAI, 2023), are increasingly becoming mainstream among the general population, due to their capabilities and availability."
        },
        "role": "Referenced",
        "type": {
          "value": "Large Language Model",
          "justification": "GPT-4 is a widely recognized Large Language Model developed by OpenAI.",
          "quote": "Instruction-tuned large language models (LLMs), such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), Mistral (Jiang et al., 2023), or GPT-4 (OpenAI, 2023), are increasingly becoming mainstream among the general population, due to their capabilities and availability."
        },
        "mode": "Inference"
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 665,
    "prompt_tokens": 41407,
    "total_tokens": 42072
  }
}