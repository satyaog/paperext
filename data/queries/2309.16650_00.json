{
  "paper": "2309.16650.txt",
  "words": 10191,
  "extractions": {
    "title": {
      "value": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning",
      "justification": "This is the title provided at the beginning of the research paper.",
      "quote": "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning"
    },
    "description": "The paper proposes ConceptGraphs, an open-vocabulary graph-structured representation for 3D scenes tailored for robot perception and planning. By leveraging 2D foundation models and fusing their output to 3D through multiview association, the approach creates a semantically rich and compact 3D representation useful for complex task execution like navigation and manipulation.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper presents experiments, implementations, and evaluations of the proposed ConceptGraphs system on various robotic platforms and real-world tasks.",
      "quote": "We implement and demonstrate ConceptGraphs on a number of real-world robotics tasks across wheeled and legged mobile robot platforms."
    },
    "primary_research_field": {
      "value": "Deep Learning",
      "justification": "The paper leverages foundation models, large vision-language models, and large language models which are key elements in deep learning research.",
      "quote": "ConceptGraphs is built by leveraging 2D foundation models and fusing their output to 3D by multiview association."
    },
    "sub_research_fields": [
      {
        "value": "Computer Vision",
        "justification": "The research deals with 3D scene representation, object detection, segmentation, and spatial reasoning, all of which fall under the domain of computer vision.",
        "quote": "We propose a novel object-centric mapping system that integrates geometric cues from traditional 3D mapping systems and semantic cues from 2D foundation models."
      }
    ],
    "models": [
      {
        "name": {
          "value": "ConceptGraphs",
          "justification": "This is the main model proposed and named in the paper.",
          "quote": "We propose ConceptGraphs, an open-vocabulary and object-centric 3D representation for robot perception and planning."
        },
        "caracteristics": [
          {
            "value": "Deep Learning Model",
            "justification": "ConceptGraphs leverage foundation models, embeddings, and language models for 3D scene understanding.",
            "quote": "ConceptGraphs constructs a map, a 3D scene graph."
          }
        ],
        "is_contributed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Segment Anything (SAM)",
          "justification": "The paper uses Segment Anything for segmentation tasks.",
          "quote": "Our experiments use SegmentAnything (SAM) [33] as the segmentation model Seg(·)"
        },
        "caracteristics": [
          {
            "value": "Segmentation Model",
            "justification": "SAM is used for segmentation tasks within the ConceptGraphs framework.",
            "quote": "Our experiments use SegmentAnything (SAM) [33] as the segmentation model Seg(·)"
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is used for obtaining visual descriptors.",
          "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i =Embed(Itrgb , mt,i )."
        },
        "caracteristics": [
          {
            "value": "Visual Feature Extractor",
            "justification": "CLIP is employed for extracting visual features from images.",
            "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31]) to obtain a visual descriptor ft,i."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DINO",
          "justification": "DINO is used alongside CLIP for obtaining visual descriptors.",
          "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (CLIP [31], DINO [53]) to obtain a visual descriptor ft,i =Embed(Itrgb , mt,i )."
        },
        "caracteristics": [
          {
            "value": "Visual Feature Extractor",
            "justification": "DINO is employed for extracting visual features from images.",
            "quote": "Each extracted mask mt,i is then passed to a visual feature extractor (DINO [53]) to obtain a visual descriptor ft,i."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "LLaVA",
          "justification": "LLaVA is used as the vision-language model for generating captions.",
          "quote": "We use LLaVA [55] as the vision-language model LVLM"
        },
        "caracteristics": [
          {
            "value": "Vision-Language Model",
            "justification": "LLaVA generates object captions based on vision-language inputs.",
            "quote": "We use LLaVA [55] as the vision-language model LVLM"
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is employed as the language model for semantic understanding and querying.",
          "quote": "We use ... GPT-4 [32] (gpt-4-0613) for our LLM."
        },
        "caracteristics": [
          {
            "value": "Language Model",
            "justification": "GPT-4 is utilized for understanding and processing language queries.",
            "quote": "We use ... GPT-4 [32] (gpt-4-0613) for our LLM."
          }
        ],
        "is_contributed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_executed": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Replica",
          "justification": "The paper uses Replica data scenes for evaluating scene graph accuracy.",
          "quote": "We first evaluate the accuracy of the 3D scene graphs output by the ConceptGraphs system in Table I. For each scene in the Replica dataset [56], we report scene graph accuracy metrics."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "AI2Thor",
          "justification": "AI2Thor is used for simulation experiments.",
          "quote": "We showcase this with a … localization and remapping task in the AI2Thor [63], [64] simulation environment."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Open3d SLAM",
          "justification": "Open3d SLAM is used for building pointcloud maps.",
          "quote": "For our navigation experiments with the Jackal robot. Our robot is equipped with a VLP-16 lidar and a foward-facing Realsense D435i camera. We begin by building a pointcloud of the REAL Lab using the onboard VLP-16 and Open3d SLAM [68]."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RTAB-Map",
          "justification": "RTAB-Map is used for obtaining camera poses and the scene point cloud.",
          "quote": "We then stage two separate scenes with different objects: one for object search and another for traversability estimation. In both cases, we map the scene with an Azure Kinect Camera and rely on RTAB-Map [69] to obtain camera poses and the scene point cloud."
        },
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1663,
    "prompt_tokens": 16993,
    "total_tokens": 18656
  }
}