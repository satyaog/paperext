{
  "paper": "2306.14808.txt",
  "words": 13970,
  "extractions": {
    "title": {
      "value": "Maximum State Entropy Exploration using Predecessor and Successor Representations",
      "justification": "It is the exact title of the paper provided by the user.",
      "quote": "Maximum State Entropy Exploration using Predecessor and Successor Representations"
    },
    "description": "The paper introduces ηψ-Learning, an algorithm designed to improve exploration strategies in reinforcement learning by efficiently utilizing past episodic experiences. By maximizing the entropy of the state visitation distribution in a finite-length trajectory, ηψ-Learning aids in methodical exploration.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments to demonstrate the effectiveness of ηψ-Learning in various environments, which indicates an empirical nature.",
      "quote": "Our experiments demonstrate the efficacy of ηψ-Learning to strategically explore the environment and maximize the state coverage with limited samples."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper discusses an algorithm that improves the exploration strategies in Reinforcement Learning, which is a subfield of Deep Learning.",
        "quote": "In this work, we propose ηψ-Learning, a method to learn efficient exploratory policies by conditioning on past episodic experience to make the next exploratory move."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The focus of the paper is on improving exploration strategies specifically within the field of Reinforcement Learning.",
          "quote": "The domain of exploration in Reinforcement Learning (RL) focuses on discovering an agent’s environment via intrinsic motivation to accelerate learning optimal policies."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ηψ-Learning",
          "justification": "The paper proposes ηψ-Learning as the main algorithm for learning efficient exploration strategies.",
          "quote": "In this paper, we present ηψ-Learning, an algorithm to learn an exploration policy that methodically searches through a task."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 437,
    "prompt_tokens": 21970,
    "total_tokens": 22407
  }
}