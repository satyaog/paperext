{
  "paper": "2305.19452.txt",
  "words": 9099,
  "extractions": {
    "title": {
      "value": "Bigger, Better, Faster: Human-level Atari with human-level efficiency",
      "justification": "The title is directly taken from the research paper.",
      "quote": "Bigger, Better, Faster: Human-level Atari with human-level efficiency"
    },
    "description": "This paper introduces a new value-based RL agent, called BBF, which achieves super-human performance in the Atari 100K benchmark. BBF scales the neural networks used for value estimation in a sample-efficient manner and incorporates various design choices to enable this scaling. The authors also discuss moving the goalposts for sample-efficient RL research on the ALE benchmark.",
    "type": {
      "value": "Empirical Study",
      "justification": "The paper conducts extensive analyses and experiments to evaluate the BBF model on the Atari 100K benchmark and compares it with other models.",
      "quote": "We conduct extensive analyses of these design choices and provide insights for future work."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Reinforcement Learning",
        "justification": "The paper focuses on value-based RL agents and their performance in the Atari 100K benchmark, which falls under deep reinforcement learning.",
        "quote": "We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model-Free Reinforcement Learning",
          "justification": "The paper emphasizes the development and evaluation of a model-free RL agent, BBF, in a sample-efficient manner.",
          "quote": "While human-level efficiency has been obtained by the model-based EfficientZero agent (Ye et al., 2021), it has remained elusive for model-free RL agents."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BBF",
          "justification": "The BBF model is the main contribution of the paper, and it's thoroughly analyzed and compared with other RL agents.",
          "quote": "We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "AlphaStar",
          "justification": "Referenced as an example of successful RL methods.",
          "quote": "... playing complex games at a human or super-human level, such as ... AlphaStar (Vinyals et al., 2019) ..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "N/A"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DER",
          "justification": "Used for performance comparison in the analysis.",
          "quote": "Figure 2: Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... DER (Van Hasselt et al., 2019) ..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "DrQ",
          "justification": "Used for performance comparison in the analysis.",
          "quote": "Figure 2: Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... DrQ (Kostrikov et al., 2020) ..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "SPR",
          "justification": "Used for performance comparison in the analysis.",
          "quote": "Figure 2: Comparing Atari 100K performance and computational cost of our model-free BBF agent to ... SPR (Schwarzer et al., 2021) ..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Used"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "Trained"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "OpenAI Five",
          "justification": "The model is referenced as an example of a successful RL method.",
          "quote": "... playing complex games at a human or super-human level, such as OpenAI Five (Berner et al., 2019), ..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "N/A"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "AlphaGo",
          "justification": "Referenced as an example of successful RL methods.",
          "quote": "... playing complex games at a human or super-human level, such as AlphaGo (Silver et al., 2016), ..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "Referenced"
        },
        "is_executed": {
          "value": false,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "N/A"
        },
        "is_inference_only": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "is_compared": {
          "value": false,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Kinetics-700",
          "justification": "The dataset is referenced in the paper for performance evaluation in the broader context of RL.",
          "quote": "... the learned policies may be unable to handle distribution shifts when interacting with the real environment (Levine et al., 2020)..."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "Discussed in the context of general performance evaluation standards.",
          "quote": "... improving generalization in RL using datasets like CIFAR-10 ..."
        },
        "aliases": [],
        "role": "Referenced",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JAX",
          "justification": "Used for implementing some of the models and experiments in the paper.",
          "quote": "... we would also like to thank the Python community for developing tools that enabled this work, including JAX (Bradbury et al., 2018) ..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "NumPy",
          "justification": "Used for numerical operations in the implementation.",
          "quote": "... we would also like to thank the Python community for developing tools that enabled this work, including NumPy (Harris et al., 2020) ..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Matplotlib",
          "justification": "Used for plotting and visualizations in the paper.",
          "quote": "... we would also like to thank the Python community for developing tools that enabled this work, including ... Matplotlib (Hunter, 2007) ..."
        },
        "aliases": [],
        "role": "Used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1827,
    "prompt_tokens": 17369,
    "total_tokens": 19196
  }
}