{
  "title": {
    "value": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset",
    "justification": "This is the exact title of the research paper provided by the user.",
    "quote": "The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"
  },
  "description": "This paper details the creation and analysis of the BigScience ROOTS corpus, a comprehensive multilingual dataset consisting of 1.6TB of text spanning 59 languages. It documents the data curation efforts, including the roles of various working groups, the preprocessing and filtering steps, as well as the ethical considerations behind the project. The dataset was used to train the 176-billion-parameter BLOOM language model.",
  "type": {
    "value": "Empirical Study",
    "justification": "The paper focuses on the empirical creation, curation, and analysis of a large multilingual dataset. It includes specific methods for data cleaning, filtering, and deduplication, and provides statistical analyses of the dataset.",
    "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages."
  },
  "primary_research_field": {
    "name": {
      "value": "Natural Language Processing",
      "justification": "The research focuses on creating a multilingual corpus for training large language models, which is a core area of Natural Language Processing.",
      "quote": "As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings."
    },
    "aliases": [
      "NLP"
    ]
  },
  "sub_research_fields": [
    {
      "name": {
        "value": "Multilingual",
        "justification": "The sub-field focuses on multilingual datasets and their use in training large-scale language models, aligning with the paper's goal of creating and analyzing a multilingual dataset for the BLOOM language model.",
        "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Opencollaboration Open-access Multilingual language model."
      },
      "aliases": []
    },
    {
      "name": {
        "value": "Large Language Models",
        "justification": "",
        "quote": ""
      },
      "aliases": []
    }
  ],
  "models": [
    {
      "name": {
        "value": "BLOOM",
        "justification": "The BLOOM model is trained using the ROOTS corpus, making it the main model discussed in the paper.",
        "quote": "This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM)(BigScience Workshop, 2022) language model"
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Trained"
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "GPT-3",
        "justification": "The paper references GPT-3 as a comparative model to the objectives they had with BLOOM.",
        "quote": "One of the founding goals of BigScience was to train an open-access, massively multilingual LLM, comparable in scale to GPT-3 (Brown et al., 2020)."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "trained"
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "T5",
        "justification": "The paper mentions T5 in the context of discussing the datasets used in training language models like T5.",
        "quote": "Opt: Open Pre-trained Transformer Language Models, mC4 (Raffel et al., 2020), which have powered the T5 family of models."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "trained"
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "GPT-NeoX 20B",
        "justification": "",
        "quote": "Figure 7: Tokens per byte for each English-language component for tokenizers trained on this corpus (BLOOM), the Pile (GPT-NeoX 20B) and C4 (T5)."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "CC100",
        "justification": "CC100 is mentioned as a dataset used for multilingual modeling.",
        "quote": "CC100 (Conneau et al., 2020) which has seen heavy use for multilingual modeling."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ROOTS",
        "justification": "The ROOTS dataset is the main contribution of the paper, meticulously created and processed as a large-scale multilingual text corpus.",
        "quote": "assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages."
      },
      "aliases": [],
      "role": "Contributed",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "the Pile",
        "justification": "CC100 is mentioned as a dataset used for multilingual modeling.",
        "quote": "Figure 7: Tokens per byte for each English-language component for tokenizers trained on this corpus (BLOOM), the Pile (GPT-NeoX 20B) and C4 (T5)."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "C4",
        "justification": "CC100 is mentioned as a dataset used for multilingual modeling.",
        "quote": "Figure 7: Tokens per byte for each English-language component for tokenizers trained on this corpus (BLOOM), the Pile (GPT-NeoX 20B) and C4 (T5)."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "mC4",
        "justification": "CC100 is mentioned as a dataset used for multilingual modeling.",
        "quote": "Exceptions include the Pile (Gao et al., 2020), a curated corpus of datasets for language modeling that has become widely used for training state-of-the-art English-language models (Lieber et al., 2021; Smith et al., 2022; Black et al., 2022; Zhang et al., 2022), and C4 and mC4 (Raffel et al., 2020; Xue et al., 2020), which have powered the T5 family of models; CC100 (Conneau et al., 2020) which has seen heavy use for multilingual modeling; and OSCAR (Ortiz Suárez et al., 2019), which has enabled monolingual non-English models"
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "OSCAR",
        "justification": "CC100 is mentioned as a dataset used for multilingual modeling.",
        "quote": "Exceptions include the Pile (Gao et al., 2020), a curated corpus of datasets for language modeling that has become widely used for training state-of-the-art English-language models (Lieber et al., 2021; Smith et al., 2022; Black et al., 2022; Zhang et al., 2022), and C4 and mC4 (Raffel et al., 2020; Xue et al., 2020), which have powered the T5 family of models; CC100 (Conneau et al., 2020) which has seen heavy use for multilingual modeling; and OSCAR (Ortiz Suárez et al., 2019), which has enabled monolingual non-English models"
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Common Crawl",
        "justification": "The paper describes using data from the Common Crawl as part of their dataset processing.",
        "quote": "we retrieved pages corresponding to the target domain names from 18 snapshots archived by Common Crawl in 2020 and 2021 in Web ARChive (WARC) format (Mohr et al., 2008)."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "Indic NLP Library",
        "justification": "Indic NLP Library is used for sentence tokenization for multiple Indian languages as part of the data processing pipeline.",
        "quote": "For Bengalic, Gujarati, Hindi, Kannada, Malayalam, Marathi, Punjabi, Tamil, and Telugu, we use the Indic NLP library tokenizer (Kunchukuttan, 2020)."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "KenLM",
        "justification": "KenLM is used to train 5-gram models after tokenization for OSCAR data filtering.",
        "quote": "KenLM 5-gram models after tokenization (Heafield, 2011) on Wikipedia article openings for every language that was extracted from OSCAR."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "NLTK",
        "justification": "NLTK is used for sentence tokenization in multiple languages as part of the data processing pipeline.",
        "quote": "For English, French, Portuguese, and Spanish, we use the NLTK tokenizer (Bird et al., 2009)."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "SentencePiece",
        "justification": "SentencePiece is used to train unigram tokenizers for multiple languages for OSCAR data filtering.",
        "quote": "Following Wenzek et al. (2020), we trained SentencePiece unigram tokenizers (Kudo, 2018)."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Stanza",
        "justification": "Stanza is used for sentence tokenization in multiple languages as part of the data processing pipeline.",
        "quote": "For Arabic, Catalan, Basque, Indonesian, and Chinese (both simplified and traditional), we use the Stanza tokenizer (Qi et al., 2020)."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Underthesea",
        "justification": "Underthesea is used for sentence tokenization for the Vietnamese language as part of the data processing pipeline.",
        "quote": "For Vietnamese, we use the Underthesea tokenizer."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ]
}