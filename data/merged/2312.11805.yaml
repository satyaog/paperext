title:
  value: 'Gemini: A Family of Highly Capable Multimodal Models'
  justification: The title is explicitly mentioned at the beginning of the paper.
  quote: 'Gemini: A Family of Highly Capable Multimodal Models'
description: This research paper introduces the Gemini family of multimodal models with Ultra, Pro, and Nano variants. These
  models demonstrate state-of-the-art performance in various multimodal tasks, including image, audio, video, and text understanding.
  Evaluations indicate significant advances in 30 out of 32 benchmarks examined.
type:
  value: Empirical Study
  justification: The paper primarily reports on the development and evaluation of models, which are empirical activities.
  quote: We present detailed evaluations of the pre- and post-trained Gemini model family... covering well-studied benchmarks
    across text, code, image, audio and video.
primary_research_field:
  name:
    value: Deep Learning
    justification: The paper is focused on the development and application of multimodal models, a core area of deep learning.
    quote: The Gemini family advances state-of-the-art in large-scale language modeling... image understanding... audio processing...
      and video understanding.
  aliases: []
sub_research_fields:
- name:
    value: Multimodal Learning
    justification: The research contributions and evaluations are specifically targeted towards multimodal tasks.
    quote: This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image,
      audio, video, and text understanding.
  aliases: []
- name:
    value: Multimodal Reasoning and Representation
    justification: The paper's emphasis on the models' ability to reason across different modalities and represent different
      data types highlights its alignment with the sub-field of multimodal reasoning and representation.
    quote: The new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide
      variety of use cases.
  aliases: []
models:
- name:
    value: Gemini Nano
    justification: The model is explicitly mentioned as a part of the new family of Gemini models.
    quote: The Gemini family consists of Ultra, Pro, and Nano sizes.
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: Contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Gemini Pro
    justification: The model is explicitly mentioned as a part of the new family of Gemini models.
    quote: The Gemini family consists of Ultra, Pro, and Nano sizes.
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: Contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Gemini Ultra
    justification: The model is explicitly mentioned as a part of the new family of Gemini models.
    quote: The Gemini family consists of Ultra, Pro, and Nano sizes.
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: Contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: PaLM 2
    justification: The paper references PaLM 2 for comparative evaluation of the Gemini models.
    quote: We compare pre- and post-trained Gemini Pro and Ultra models to a suite of external LLMs and our previous best
      model PaLM 2 across a series of text-based academic benchmarks covering reasoning, reading comprehension, STEM, and
      coding.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-4
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-3.5
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Claude 2
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Inflection-2
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Grok 1
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: LLAMA-2
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: GSM8K
    justification: The dataset is mentioned directly in the context of evaluating Gemini Ultra's performance in mathematics.
    quote: For the grade-school math benchmark, GSM8K (Cobbe et al., 2021), we find Gemini Ultra reaches 94.4% accuracy with
      chain-of-thought prompting and self-consistency (Wang et al., 2022) compared to the previous best accuracy of 92% with
      the same prompting technique.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HumanEval
    justification: This dataset is used for evaluating code generation abilities of Gemini Ultra.
    quote: For example, on HumanEval, a standard code-completion benchmark (Chen et al., 2021) mapping function descriptions
      to Python implementations, instruction-tuned Gemini Ultra correctly implements 74.4% of problems.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MMLU
    justification: The dataset is explicitly mentioned as a benchmark where Gemini Ultra achieved state-of-the-art performance.
    quote: "Gemini Ultra is the first model to achieve human-expert performance on MMLU (Hendrycks et al., 2021a) \u2014 a\
      \ prominent benchmark testing knowledge and reasoning via a suite of exams."
  aliases:
  - Massive Multitask Language Understanding
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: NaturalQuestions
    justification: The dataset is mentioned within the context of Gemini models outperforming on multiple text-based academic
      benchmarks.
    quote: In particular, Gemini Ultra achieves... highest accuracy when evaluated on high-resource, mid-resource and low-resource
      languages, scoring 74.4% on NaturalQuestions.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: BooIQ
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TydiQA
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: BIG-Bench-Hard
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MBPP
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MATH
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MMLU
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: WMT 23
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MGSM
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: XLsum
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Wikilingua
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MMMU
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TextVQA
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DocVQA
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ChartQA
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: InfographicVQA
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MathVista
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: AI2D
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VQAv2
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: XM-3600
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VATEX
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VATEX ZH
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: YouCook2
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: NextQA
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ActivityNet-QA
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Perception Test MCQA
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Multilingual Librispeech
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: FLEURS
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VoxPopuli
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: CoVoST 2
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Natural2Code
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DROP
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HellaSwag
    justification: ''
    quote: ''
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: JAX
    justification: The framework is explicitly mentioned as a critical part of the training infrastructure.
    quote: "The \u2018single controller\u2019 programming model of JAX (Bradbury et al., 2018) and Pathways (Barham et al.,\
      \ 2022) allows a single Python process to orchestrate the entire training run, dramatically simplifying the development\
      \ workflow."
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Pathways
    justification: The framework is explicitly mentioned as a critical part of the training infrastructure.
    quote: "The \u2018single controller\u2019 programming model of JAX (Bradbury et al., 2018) and Pathways (Barham et al.,\
      \ 2022) allows a single Python process to orchestrate the entire training run, dramatically simplifying the development\
      \ workflow."
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Pathways
    justification: Pathways is mentioned as part of the infrastructure used to support the training of Gemini models
    quote: "The \u2019single controller\u2019 programming model of Jax (Bradbury et al., 2018) and Pathways (Barham et al.,\
      \ 2022) allows a single Python process to orchestrate the entire training run, dramatically simplifying the development\
      \ workflow."
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
