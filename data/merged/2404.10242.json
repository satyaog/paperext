{
  "title": {
    "value": "Masked Autoencoders are Scalable Learners of Cellular Biology",
    "justification": "The title is clearly mentioned at the start of the paper.",
    "quote": "Masked Autoencoders are Scalable Learners of Cellular Biology"
  },
  "description": "This paper explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when applied to microscopy images in biological research. It introduces a new channel-agnostic MAE architecture and demonstrates the effectiveness of MAEs, particularly ViT-based MAEs, in improving the recall of known biological relationships. The study uses various large-scale microscopy datasets and proposes novel methodologies like Fourier domain reconstruction to stabilize MAE training. The goal is to create robust foundation models for cellular biology that can advance drug discovery.",
  "type": {
    "value": "Empirical Study",
    "justification": "The paper involves experimental evaluations of different models on large-scale datasets, exhibiting characteristics typical of an empirical study.",
    "quote": "Our results show that ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks, achieving as much as a 11.5% relative improvement... We train masked autoencoders (MAEs)... employing a novel channel-agnostic MAE..."
  },
  "primary_research_field": {
    "value": "Computer Vision",
    "justification": "The models discussed are for image analysis, which falls under Computer Vision.",
    "quote": "This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders (MAEs) when training with increasingly larger model backbones and microscopy datasets."
  },
  "sub_research_fields": [
    {
      "value": "Microscopy",
      "justification": "The focus is on analyzing microscopy images specifically.",
      "quote": "Featurizing microscopy images for use in biological research remains a significant challenge."
    },
    {
      "value": "Image Analysis",
      "justification": "",
      "quote": ""
    }
  ],
  "models": [
    {
      "name": {
        "value": "CA-MAE ViT-B/16",
        "justification": "CA-MAE is developed in the paper to handle channel-agnostic inputs.",
        "quote": "Table 3 shows results for three channel- agnostic MAEs (Sec. 4.2.2)."
      },
      "caracteristics": [
        {
          "value": "self-supervised masked autoencoders",
          "justification": "CA-MAE is a specific masked autoencoder developed for channel-agnostic image inputs.",
          "quote": "This work explores the scaling properties of weakly supervised clas- sifiers and self-supervised masked autoencoders (MAEs)"
        }
      ],
      "is_contributed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "WSL DN161",
        "justification": "Comparative Table",
        "quote": ""
      },
      "caracteristics": [
        {
          "value": "Weakly Supervised Learning Model",
          "justification": "none",
          "quote": ""
        }
      ],
      "is_contributed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "CA-MAE ViT-L/16",
        "justification": "CA-MAE is developed in the paper to handle channel-agnostic inputs.",
        "quote": ""
      },
      "caracteristics": [
        {
          "value": "Masked Autoencoder",
          "justification": "CA-MAE is a specific masked autoencoder developed for channel-agnostic image inputs.",
          "quote": ""
        }
      ],
      "is_contributed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "DenseNet-161",
        "justification": "Comparative Table",
        "quote": "Comparative Table"
      },
      "caracteristics": [
        {
          "value": "Dense Convolutional Network",
          "justification": "none",
          "quote": ""
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "DiNO ViT-S/8",
        "justification": "DiNO is mentioned as a comparative self-supervised learning method.",
        "quote": "DiNO [9] is an SSL method that has been applied to HCS [17, 20, 29, 37, 58] data, however it relies on augmentations inspired by natural images, which may not be applicable to HCS image sets."
      },
      "caracteristics": [
        {
          "value": "Self-Supervised Learning Model",
          "justification": "DiNO is described as a self-supervised learning model (SSL).",
          "quote": "DiNO [9] is an SSL method that has been applied to HCS [17, 20, 29, 37, 58] data, however it relies on augmentations inspired by natural images, which may not be applicable to HCS image sets."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MU-Net-L",
        "justification": "The U-Net architecture is explicitly mentioned as a baseline model.",
        "quote": "We adapt U-Nets [56] for use as masked autoencoders (MU-Nets) by training to reconstruct masked sections of input images."
      },
      "caracteristics": [
        {
          "value": "Convolutional Neural Network",
          "justification": "U-Net is a type of Convolutional Neural Network (CNN) designed primarily for biomedical image segmentation.",
          "quote": "We adapt U-Nets [56] for use as masked autoencoders (MU-Nets) by training to reconstruct masked sections of input images."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "U-Net",
        "justification": "The U-Net architecture is explicitly mentioned as a baseline model.",
        "quote": "We adapt U-Nets [56] for use as masked autoencoders (MU-Nets) by training to reconstruct masked sections of input images."
      },
      "caracteristics": [
        {
          "value": "Convolutional Neural Network",
          "justification": "U-Net is a type of Convolutional Neural Network (CNN) designed primarily for biomedical image segmentation.",
          "quote": "We adapt U-Nets [56] for use as masked autoencoders (MU-Nets) by training to reconstruct masked sections of input images."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ViT",
        "justification": "The ViT model is used for evaluating MAE architectures in the paper.",
        "quote": "ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks."
      },
      "caracteristics": [
        {
          "value": "Vision Transformer",
          "justification": "ViT stands for Vision Transformer, a specific type of architectural model.",
          "quote": "ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks."
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ViT-S/16",
        "justification": "Comparative Table",
        "quote": ""
      },
      "caracteristics": [
        {
          "value": "Vision Transformer",
          "justification": "ViT stands for Vision Transformer, a type of model used in the paper.",
          "quote": ""
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ViT-B/16",
        "justification": "Comparative Table",
        "quote": ""
      },
      "caracteristics": [
        {
          "value": "Vision Transformer",
          "justification": "ViT stands for Vision Transformer, a type of model used in the paper.",
          "quote": ""
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ViT-B/8",
        "justification": "Comparative Table",
        "quote": ""
      },
      "caracteristics": [
        {
          "value": "Vision Transformer",
          "justification": "ViT stands for Vision Transformer, a type of model used in the paper.",
          "quote": ""
        }
      ],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "CPJUMP1",
        "justification": "CPJUMP1 is a subset of the JUMP-CP dataset mentioned for transfer learning evaluation.",
        "quote": "To further evaluate the transferability of our models, we inferenced CPJUMP1, a subset of the JUMP-CP [14] dataset, and ran the corresponding benchmarking tasks introduced in Chandrasekaran et al. [13]."
      },
      "role": "contributed",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "JUMP-CP",
        "justification": "JUMP-CP is explicitly mentioned as a dataset used for evaluation.",
        "quote": "We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP) generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M)."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HCS Dataset RPI-52M",
        "justification": "RPI-52M is mentioned as a major dataset used for training models.",
        "quote": "RPI-52M (Recursion Phenomics Imageset) is a private dataset with approximately 52 million proprietary images spanning 6,638 experimental batches and 40 cell types."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HCS Dataset RPI-93M",
        "justification": "RPI-93M is mentioned as a major dataset used for training models.",
        "quote": "RPI-93M is a private dataset with approximately 93 million proprietary images spanning over 10,000 experimental batches and 41 cell types."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HCS Dataset RxRx1",
        "justification": "RxRx1 is mentioned multiple times as a dataset used in the study.",
        "quote": "RxRx1 [62] is a publicly-available proprietary Cell Painting dataset with 125,510 images of 4 human cell types under 1,108 different siRNA perturbations across 51 experimental batches. A unique feature of this dataset is that it is comprised entirely of siRNA perturbations"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HCS Dataset RxRx1-2M",
        "justification": "RxRx1-2M is listed in Table 1 as a dataset used in the study.",
        "quote": "RxRx1-2M is a private version of RxRx1 containing over 1.6 million images across 16 different cell types and uses the same set of siRNA perturbations in RxRx1 from additional experimental batches."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HCS Dataset RxRx3",
        "justification": "RxRx3 is another dataset used in the study, as explicitly mentioned.",
        "quote": "RxRx3 [24] is a publicly-available proprietary Cell Painting dataset with over 2.2 million images of HUVEC cells each perturbed with one of 17,063 CRISPR knockouts (using one of six different guides) or 1,674 compounds across 180 experimental batches"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Imagenet-21k",
        "justification": "Mentioned in comparative table",
        "quote": "Imagenet-21k"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "PyTorch",
        "justification": "PyTorch is explicitly mentioned as the framework used for training the models.",
        "quote": "Models were trained with data-distributed parallel (DDP) training and PyTorch 2.0 for up to 100 epochs on up to 256 NVIDIA 80GB A100 GPUs, depending on the size of the model and dataset."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ]
}