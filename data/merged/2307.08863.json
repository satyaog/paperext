{
  "title": {
    "value": "Meta-Value Learning: A General Framework for Learning with Learning Awareness",
    "justification": "The title succinctly reflects the main contribution and scope of the paper.",
    "quote": "META -VALUE L EARNING : A G ENERAL F RAMEWORK FOR L EARNING WITH L EARNING AWARENESS"
  },
  "description": "This paper introduces Meta-Value Learning (MeVa), a novel method for gradient-based learning in multi-agent systems that uses a meta-game approach to judge joint policies by their long-term prospects through a meta-value function.",
  "type": {
    "value": "Empirical Study",
    "justification": "The paper evaluates the proposed MeVa method using experiments on several environments and compares its performance with other existing methods.",
    "quote": "We analyze the behavior of our method on a toy game and compare to prior work on repeated matrix games."
  },
  "primary_research_field": {
    "name": {
      "value": "Deep Learning",
      "justification": "The paper deals with deep learning techniques in the context of multi-agent systems and reinforcement learning.",
      "quote": "We study P -player differentiable games f : RP ×N 7→ RP that map vectors of policies xi to vectors of expected returns yi"
    },
    "aliases": []
  },
  "sub_research_fields": [
    {
      "name": {
        "value": "Multi-Agent",
        "justification": "The paper specifically discusses learning in multi-agent systems and the interactions between agents.",
        "quote": "Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model which does not account for the interaction between agents’ learning processes."
      },
      "aliases": []
    },
    {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "",
        "quote": ""
      },
      "aliases": []
    }
  ],
  "models": [
    {
      "name": {
        "value": "LOLA",
        "justification": "LOLA is used as a comparison point to demonstrate the improvements made by MeVa.",
        "quote": "We take inspiration from the recent work Learning with Opponent-Learning Awareness (LOLA Foerster et al. (2018a;c)), the first general learning algorithm to find tit-for-tat on IPD."
      },
      "aliases": [
        "Learning with Opponent-Learning Awareness"
      ],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Used"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Trained"
      },
      "is_inference_only": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Meta-Value Learning",
        "justification": "MeVa is the primary model introduced and explored in the research paper.",
        "quote": "The resulting method, MeVa, is consistent and far-sighted."
      },
      "aliases": [
        "MeVa"
      ],
      "is_contributed": {
        "value": true,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Contributed"
      },
      "is_executed": {
        "value": true,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Trained"
      },
      "is_inference_only": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Naive",
        "justification": "Naive Learning is used as a baseline for comparison among other methods.",
        "quote": "The naive application of gradient descent (see §2.1) fails to find tit-for-tat on the IPD unless initialized sufficiently close to it."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Used"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Trained"
      },
      "is_inference_only": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MFOS",
        "justification": "",
        "quote": "Table 2: Head-to-head comparison of meta-policies on repeated matrix games"
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Used"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Trained"
      },
      "is_inference_only": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MMAML",
        "justification": "",
        "quote": "Table 2: Head-to-head comparison of meta-policies on repeated matrix games"
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Used"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Trained"
      },
      "is_inference_only": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "Chicken Game",
        "justification": "The Chicken Game is another environment used in the paper to test the performance of MeVa.",
        "quote": "On the Chicken Game (Table 2c), LOLA exploits every opponent except M-MAML, but does poorly against itself (also observed by Lu et al. (2022))."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Iterated Matching Pennies",
        "justification": "IMP is also used to evaluate the MeVa method, focusing on its ability to handle different game dynamics.",
        "quote": "On Iterated Matching Pennies (Table 2b), MeVa exploits naive and LOLA learners, moreso than M-FOS."
      },
      "aliases": [
        "IMP"
      ],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Iterated Prisoner's Dilemma",
        "justification": "IPD is used to evaluate the effectiveness of the proposed MeVa method.",
        "quote": "In a tournament on repeated matrix games (§5.2), MeVa exhibits opponent-shaping behavior, including ZD-extortion (Press & Dyson, 2012) on the IPD."
      },
      "aliases": [
        "IPD"
      ],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "JAX",
        "justification": "JAX is used for scientific computing and model implementation in the paper.",
        "quote": "We used the JAX (Bradbury et al., 2018) library for scientific computing."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ]
}