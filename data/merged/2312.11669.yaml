title:
  value: Prediction and Control in Continual Reinforcement Learning
  justification: The title clearly summarizes the main focus of the paper, which aligns with the presented content on continual
    reinforcement learning and the proposed techniques.
  quote: Prediction and Control in Continual Reinforcement Learning
description: 'The paper focuses on value function estimation in continual reinforcement learning. It proposes decomposing
  the value function into two components updated at different timescales: a permanent value function (learning general knowledge)
  and a transient value function (for quick adaptation). Empirical studies show the approach improves performance on prediction
  and control tasks.'
type:
  value: empirical
  justification: The paper includes empirical case studies and experiments demonstrating the effectiveness of the proposed
    method in different environments.
  quote: Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31],
    and MinAtar environments [51].
primary_research_field:
  name:
    value: Reinforcement Learning
    justification: ''
    quote: ''
  aliases: []
sub_research_fields:
- name:
    value: Continual Reinforcement Learning
    justification: The paper specifically focuses on continual learning in reinforcement learning environments.
    quote: "Let S be the set of possible states and A the set of actions. At each timestep t, the agent takes action At \u2208\
      \ A in state St according to its (stochastic) policy \u03C0... This quantity can be estimated with a function approximator\
      \ parameterized by w, for example using TD learning."
  aliases: []
- name:
    value: Reinforcement Learning
    justification: The entire paper is focused on the value function estimation and the stability-plasticity dilemma in reinforcement
      learning.
    quote: In this paper, we focus on value function estimation in continual reinforcement learning.
  aliases: []
models:
- name:
    value: PT-Q-learning
    justification: The paper introduces PT-Q-learning as a part of their continual reinforcement learning approach.
    quote: PT-Q-learning (Control)
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: PT-TD learning
    justification: The paper introduces PT-TD learning as a part of their continual reinforcement learning approach.
    quote: PT-TD learning (Prediction)
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DQN
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: ''
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: JellyBeanWorld
    justification: JBW is employed to test the effectiveness of the proposed method in a dynamic and complex environment.
    quote: Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31],
      and MinAtar environments [51].
  aliases:
  - JBW
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MinAtar
    justification: MinAtar provides a range of tasks for evaluating reinforcement learning algorithms under continual learning
      settings.
    quote: Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31],
      and MinAtar environments [51].
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Minigrid
    justification: Minigrid is used for value function estimation tasks under continual reinforcement learning settings.
    quote: Empirical case studies of the proposed approaches in simple gridworlds, Minigrid [11], JellyBeanWorld (JBW) [31],
      and MinAtar environments [51].
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries: []
