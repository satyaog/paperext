title:
  value: Improving Automatic VQA Evaluation Using Large Language Models
  justification: The title directly reflects the main focus and contribution of the paper.
  quote: Improving Automatic VQA Evaluation Using Large Language Models
description: This paper proposes a new metric for automatic Visual Question Answering (VQA) evaluation named LAVE (LLM-Assisted
  VQA Evaluation), which leverages instruction-tuned large language models (LLMs) to better correlate with human judgment
  compared to existing metrics.
type:
  value: empirical
  justification: The paper conducts experiments analyzing the effectiveness of the proposed LAVE metric by collecting human
    judgments and comparing them to existing metrics.
  quote: We demonstrate the proposed metric better correlates with human judgment compared to existing metrics across several
    VQA models and benchmarks.
primary_research_field:
  name:
    value: Computer Vision
    justification: The paper deals with Visual Question Answering (VQA), a key benchmark in the field of Computer Vision.
    quote: Visual question answering (VQA) (Antol et al. 2015) has become an essential benchmark for assessing the progress
      of multimodal vision-language systems.
  aliases: []
sub_research_fields:
- name:
    value: Visual Question Answering
    justification: The study focuses specifically on improving the evaluation of Visual Question Answering systems.
    quote: We propose a novel automatic VQA evaluation metric, LAVE (LLM-Assisted VQA Evaluation), which leverages the in-context
      learning capabilities of instruction-tuned LLMs.
  aliases: []
models:
- name:
    value: BLIP
    justification: BLIP is used as another representative VQA model for finetuning and evaluation.
    quote: We also include BLIP (Li et al. 2022) finetuned on VQAv2 (BLIPVQA) and VG-QA (BLIPVG), which represents the finetuning-OOD
      paradigm.
  aliases:
  - BLIP-VQA
  - BLIP-VG
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: BLIP-2
    justification: BLIP is used as another representative VQA model for finetuning and evaluation.
    quote: 'We evaluate LAVE on answers generated by several VQA models across multi- ple VQA benchmarks. In particular, we
      consider two rep- resentative state-of-the-art VQA models: BLIP-2 Flan-T5- XXL (Li et al. 2023b) and PromptCap GPT-3
      (Hu et al.  2022).'
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: PromptCap
    justification: PromptCap is used as a state-of-the-art VQA model in evaluations.
    quote: 'We evaluate LAVE on answers generated by several VQA models across multi- ple VQA benchmarks. In particular, we
      consider two rep- resentative state-of-the-art VQA models: BLIP-2 Flan-T5- XXL (Li et al. 2023b) and PromptCap GPT-3
      (Hu et al.  2022).'
  aliases:
  - PromptCap GPT-3
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Flan-T5
    justification: ''
    quote: 'We evaluate LAVE on answers generated by several VQA models across multi- ple VQA benchmarks. In particular, we
      consider two rep- resentative state-of-the-art VQA models: BLIP-2 Flan-T5- XXL (Li et al. 2023b) and PromptCap GPT-3
      (Hu et al.  2022).'
  aliases:
  - Flan-T5-XXL
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-3.5-Turbo
    justification: ''
    quote: "To demonstrate LAVE\u2019s robustness across different LLMs, we also consider Vicuna-v1.3 (Chiang et al. 2023)\
      \ and GPT-3.5-Turbo (aka ChatGPT (OpenAI 2022))."
  aliases:
  - ChatGPT (OpenAI 2022)
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Vicuna-v1.3
    justification: ''
    quote: "To demonstrate LAVE\u2019s robustness across different LLMs, we also consider Vicuna-v1.3 (Chiang et al. 2023)\
      \ and GPT-3.5-Turbo (aka ChatGPT (OpenAI 2022))."
  aliases:
  - Vicuna
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: OK-VQA
    justification: The OK-VQA dataset is mentioned as one of the benchmarks for evaluating the proposed metric, LAVE.
    quote: 'We use these VQA models to generate answers for three VQA datasets: VQAv2 (Goyal et al. 2017), VG-QA (Krishna
      et al. 2017) and OK-VQA (Marino et al. 2019).'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VG-QA
    justification: The VG-QA dataset is used for evaluating the performance of LAVE.
    quote: 'We use these VQA models to generate answers for three VQA datasets: VQAv2 (Goyal et al. 2017), VG-QA (Krishna
      et al. 2017) and OK-VQA (Marino et al. 2019).'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VQAv2
    justification: VQAv2 is one of the VQA datasets used for evaluation.
    quote: Our results demonstrate the proposed metric better correlates with human judgment compared to existing metrics
      across several VQA models and benchmarks.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: HuggingFace Transformers
    justification: The paper mentions using the HuggingFace Transformers library for implementation purposes.
    quote: "We leverage the HuggingFace Transformers\u2019 (Wolf et al. 2020) implementation of Flan-T5 and LLaMA (for Vicuna),\
      \ and use GPT-3.5-Turbo through OpenAI\u2019s API1."
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
