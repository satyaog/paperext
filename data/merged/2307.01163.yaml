title:
  value: Improving Language Plasticity via Pretraining with Active Forgetting
  justification: The title accurately captures the paper's focus on using active forgetting to enhance the adaptability of
    PLMs to new languages.
  quote: Improving Language Plasticity via Pretraining with Active Forgetting
description: This paper proposes an active forgetting mechanism during the pretraining of language models to improve their
  adaptability to new languages. By regularly resetting the embedding layer during pretraining, the authors encourage the
  model to better learn new embeddings, similar to a meta-learning effect. Experiments demonstrate faster convergence and
  better performance in low-data scenarios, especially for languages distant from English.
type:
  value: empirical
  justification: The paper conducts experiments with RoBERTa models to validate the effectiveness of the proposed active forgetting
    mechanism, making it an empirical study.
  quote: Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence
    during language adaptation, but also outperform standard ones in a low-data regime, particularly for languages that are
    distant from English.
primary_research_field:
  name:
    value: Natural Language Processing
    justification: The paper focuses on pretrained language models and their application to cross-lingual tasks, which are
      key areas within NLP.
    quote: Pretrained language models (PLMs) are today the primary model for natural language processing.
  aliases:
  - NLP
sub_research_fields:
- name:
    value: Cross-lingual
    justification: The study aims to enhance the adaptability of PLMs to new languages, making it a contribution to cross-lingual
      transfer learning.
    quote: We study whether this forgetting approach creates a PLM that can easily rewire...to an unseen (possibly distant)
      language.
  aliases: []
- name:
    value: Transfer Learning
    justification: The study aims to enhance the adaptability of PLMs to new languages, making it a contribution to cross-lingual
      transfer learning.
    quote: We study whether this forgetting approach creates a PLM that can easily rewire...to an unseen (possibly distant)
      language.
  aliases: []
- name:
    value: Pretrained language models
    justification: ''
    quote: Pretrained language models (PLMs) are today the primary model for natural language processing
  aliases:
  - PLMs
models:
- name:
    value: RoBERTa-base
    justification: RoBERTa-base is used as the primary model for implementing and testing the proposed active forgetting mechanism.
    quote: we choose to pretrain RoBERTa-base Liu et al. [2019], a 12-layer transformer-based model, on English CC100 [Conneau
      et al., 2020].
  aliases: []
  is_contributed:
    value: false
    justification: ''
    quote: ''
  is_executed:
    value: true
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: CC100
    justification: CC100 is used as the primary dataset for pretraining the RoBERTa-base model.
    quote: we choose to pretrain RoBERTa-base Liu et al. [2019], a 12-layer transformer-based model, on English CC100 [Conneau
      et al., 2020].
  aliases:
  - English CC100
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MLQA
    justification: MLQA is used to evaluate the model's performance on cross-lingual question-answering tasks.
    quote: Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation
      corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline
      by large margins...+33.8% on MLQA.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MultiNLI
    justification: The MultiNLI dataset was used for task adaptation in English.
    quote: In the task adapt stage, both models were finetuned for 10 epochs on the English task data, specifically MultiNLI
      [Williams et al., 2018] for the NLI task.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: SQuAD
    justification: The SQuAD dataset was used for task adaptation in English.
    quote: In the task adapt stage, both models were finetuned for 10 epochs on the English task data, specifically ... SQUAD
      Rajpurkar et al. [2016] for the QA task.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: XNLI
    justification: XNLI is used to evaluate the model's performance on cross-lingual natural language inference tasks.
    quote: 'Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation
      corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline
      by large margins: average gains of +21.2% on XNLI.'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: XQuAD
    justification: XQuAD is used to evaluate the model's performance on cross-lingual question-answering tasks.
    quote: Our zero-shot evaluations on several cross-lingual transfer benchmarks show that for cases where unlabeled adaptation
      corpus for the unseen language has as few as 5 million tokens (a low-data regime), forgetting PLMs outperforms the baseline
      by large margins...+60.9% on XQuAD.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: fairseq
    justification: The experiments were implemented using the fairseq library, which is widely used for sequence modeling.
    quote: Our experiments were implemented using fairseq [Ott et al., 2019].
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
