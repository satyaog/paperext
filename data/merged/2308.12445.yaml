title:
  value: An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems
  justification: The title is clearly stated at the beginning of the paper.
  quote: An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems
description: This paper proposes a novel self-healing approach for Deep Reinforcement Learning (DRL) systems called Dr. DRL,
  which integrates intentional forgetting mechanisms into Continual Learning (CL) to address issues like catastrophic forgetting
  and slow convergence in DRL systems. This approach selectively forgets minor behaviors in order to prioritize the DRL system's
  key problem-solving skills, thereby accelerating adaptation to environmental drifts and improving overall performance.
type:
  value: empirical study
  justification: The paper includes experimental evaluations of the proposed approach using different DRL algorithms and environments.
  quote: In this paper, we propose Dr. DRL, a self-healing approach for DRL systems that integrates a novel mechanism of intentional
    forgetting into vanilla CL (i.e., standard CL) to overcome its main issues... To demonstrate the effectiveness of Dr.
    DRL, we evaluate it on purposefully drifted gym environments with different drifting intensities.
primary_research_field:
  name:
    value: Deep reinforcement learning
    justification: ''
    quote: "Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook.\
      \ As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which\
      \ of- ten occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach\
      \ for adapting the DRL agent in response to the environment\u2019s conditions shifts."
  aliases:
  - DRL
sub_research_fields:
- name:
    value: Continual Learning
    justification: ''
    quote: "Deep reinforcement learning (DRL) is increasingly applied in large-scale productions like Netflix and Facebook.\
      \ As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which\
      \ of- ten occur in constantly-changing production settings. Continual Learning (CL) is the inherent self-healing approach\
      \ for adapting the DRL agent in response to the environment\u2019s conditions shifts."
  aliases:
  - CL
models:
- name:
    value: Deep Q-Learning
    justification: The paper mentions the use of Deep Q-Learning (DQN) as one of the DRL algorithms used for evaluation.
    quote: three well-established DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal
      Policy Optimization (PPO) [33].
  aliases:
  - DQN
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: evaluation
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Dr. DRL
    justification: Dr. DRL is the main model proposed in the paper.
    quote: In this paper, we propose Dr. DRL, a self-healing approach for DRL systems that integrates a novel mechanism of
      intentional forgetting into vanilla CL (i.e., standard CL) to overcome its main issues.
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: training
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Proximal Policy Optimization
    justification: The paper mentions the use of Proximal Policy Optimization (PPO) as one of the DRL algorithms used for
      evaluation.
    quote: three well-established DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal
      Policy Optimization (PPO) [33].
  aliases:
  - PPO
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: evaluation
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Soft Actor-Critic
    justification: The paper mentions the use of Soft Actor-Critic (SAC) as one of the DRL algorithms used for evaluation.
    quote: three well-established DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal
      Policy Optimization (PPO) [33].
  aliases:
  - SAC
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: evaluation
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Acrobot
    justification: The paper evaluates the proposed approach using the Acrobot environment.
    quote: In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym [27] environments...
      CartePole [28], MountainCar [29], and Acrobot [30]
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: CartPole
    justification: The paper evaluates the proposed approach using the CartPole environment.
    quote: In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym [27] environments...
      CartePole [28], MountainCar [29], and Acrobot [30]
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MountainCar
    justification: The paper evaluates the proposed approach using the MountainCar environment.
    quote: In order to demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym [27] environments...
      CartePole [28], MountainCar [29], and Acrobot [30]
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: TensorFlow
    justification: The paper states that the implementation supports TensorFlow and uses it for the empirical evaluation.
    quote: We implemented our approach as an open-source tool using Python 3.7 [49] and it supports Tensorflow (version 2.4.4)
      [50].
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Gym library
    justification: ''
    quote: We evaluated environments from the Gym library (version 0.23.1) [27]
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
