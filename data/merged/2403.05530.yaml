title:
  value: 'Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context'
  justification: The title is clearly stated on the first page of the paper.
  quote: 'Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context'
description: "In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient\
  \ multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of\
  \ tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect\
  \ recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video\
  \ QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra\u2019s state-of-the-art performance across a broad\
  \ set of benchmarks."
type:
  value: empirical
  justification: The paper provides empirical results on the performance of Gemini 1.5 Pro and compares it with other models.
  quote: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, ... achieving high performance
    in long-context tasks across text, video, and audio.
primary_research_field:
  name:
    value: Deep Learning
    justification: The paper focuses on developing and evaluating a deep learning model, specifically a multimodal mixture-of-experts
      model.
    quote: 'We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro. This is our first release from Gemini
      1.5, a new family of highly-capable multimodal models which incorporates a novel mixture-of-experts architecture...'
  aliases:
  - DL
sub_research_fields:
- name:
    value: Multimodal Learning
    justification: The research involves a model handling multiple data modalities including text, video, and audio.
    quote: Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained
      information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs),
      and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours
      of video, and almost five days long of audio.
  aliases: []
models:
- name:
    value: Claude 2.1
    justification: This model is used for comparison in the experiments evaluating long-context capabilities.
    quote: "Gemini 1.5 Pro achieves a 100% recall at 200k tokens, surpassing Claude 2.1\u2019s 98%. This 100% recall is maintained\
      \ up to 530k tokens, and recall is 99.7% at 1M tokens."
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Claude 3
    justification: This model is used for comparison in the experiments evaluating long-context capabilities.
    quote: ''
  aliases:
  - Claude 3 Haiku
  - Claude 3 Sonnet
  - Claude 3 Opus
  - Claude 3.0
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_compared:
    value: true
    justification: ''
    quote: Figure 12 compares the capabilities of the Gemini 1.5 Pro, Gemini 1.5 Flash, GPT-4 Turbo 01-25-  2024 and 04-09-2024
      models, Claude 3 models, and Claude 2.1 on MRCR.
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-4
    justification: This model is used for comparison in the experiments evaluating long-context capabilities.
    quote: Gemini 1.5 Pro in the half book setting outperforms GPT-4 Turbo and Claude 3 on the same  setup by a wide margin;
      see Tables 4 and 5.
  aliases:
  - GPT-4 Turbo
  - GPT-4V
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Gemini 1.0
    justification: We compare Gemini 1.5 Pro against Gemini 1.0 Pro.
    quote: Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks...
  aliases:
  - Gemini 1.0 Ultra
  - Gemini 1.0 Pro
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Gemini 1.5
    justification: This is the main model introduced and evaluated in the paper.
    quote: 'We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro.'
  aliases:
  - Gemini 1.5 Pro
  - Gemini 1.5 Flash
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: Contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: LLaMA
    justification: This model is used for comparison in terms of training data emission.
    quote: We found that LLaMA and Mistral emit training data at a rate around 0.1% with this attack.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Mistral
    justification: This model is used for comparison in terms of training data emission.
    quote: We found that LLaMA and Mistral emit training data at a rate around 0.1% with this attack.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Universal Speech Model
    justification: This model is used for comparison in automatic speech recognition tasks.
    quote: We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023)... achieving a WER of 8.8%.
  aliases:
  - USM
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Whisper
    justification: This model is used for comparison in automatic speech recognition tasks.
    quote: Note that ASR tasks report a word error rate (WER) metric, where a lower number is better. The Table 5 below shows
      that... Whisper is not robust to long segments and hence requires audio to be segmented every 30 seconds to achieve
      a WER of 7.3%.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: BigBench
    justification: This dataset is used to evaluate reasoning capabilities.
    quote: On reasoning tasks, 1.5 Pro outperforms 1.0 Pro by a large margin and shows a comparable performance to 1.0 Ultra,
      slightly underperforming on DROP and slightly outperforming on BBH.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DROP
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: OpenEQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Graduate-Level Google-Proof Q&A
    justification: ''
    quote: ''
  aliases:
  - GPQA
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MMLU
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MMMU
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HumanEval
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ActivityNet-QA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Natural2Code
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: V* Bench
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MathViSTA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: 1H-VideoQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: PhysicsFinals
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HiddenMath
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Functional MATH
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: AMC 2022-23
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GSM8K
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Hellaswag
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: AI2D
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: AIME2024
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ChemicalDiagramQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ChartQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: BetterChartQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DocVQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TextVQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VQAv2
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: YouYube
    justification: ''
    quote: The post-training data of the model contains 5  head languages, resulting thus in slight regressions on multi-lingual
      datasets that are not head heavy  (e.g., YouYube, FLEURS and Covost 2).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: YouCook2
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: COVOST-2
    justification: This dataset is used for evaluating speech-to-text translation capabilities.
    quote: On CoVoST-2 we evaluate on translating speech in 20 languages into English, reporting on the subset of languages
      that were seen by the model during pre-training.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DUDE
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TAT-DQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: InfographicVQA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VATEX
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: VATEX ZH
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Blink
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Realworld QA
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: IMO-Bench
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: EgoSchema
    justification: This dataset is used for evaluating video understanding capabilities.
    quote: The publicly available question answering benchmark with the longest videos is EgoSchema (Mangalam et al., 2023)...
      we introduce a new benchmark, 1H-VideoQA, composed of 125 five-way multiple-choice questions over public videos 40-105
      minutes long.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: FLEURS
    justification: This dataset is used to evaluate automatic speech recognition in multiple languages.
    quote: On FLEURS we evaluate a subset of 55 languages for which we have coverage our training data... On AST we report
      BLEU scores.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: "Les Mis\xE9rables"
    justification: This dataset is used for long-document question answering tasks, as described in the paper.
    quote: "In this section we present experiments on question answering, we create questions using the book 'Les Mis\xE9\
      rables' (by Victor Hugo) and test the model\u2019s ability to answer them correctly when the entire 1,462 page book\
      \ (i.e., 710K tokens) is provided as input."
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MATH
    justification: This dataset is used to evaluate mathematical problem-solving capabilities.
    quote: We find that 1.5 Pro consistently outperforms both 1.0 Ultra and 1.0 Pro on grade-school math (i.e., GSM8K) and
      even shows material improvement over the more demanding benchmarks where there is more headroom for improvement, i.e.,
      +3.5% over 1.0 Ultra for middle- and high-school math problems (i.e., Hendrycks MATH)...
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Math Odyssey
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: WMT23
    justification: This dataset is used for evaluating multilingual translation capabilities.
    quote: "For our multilingual evaluations we use a multilingual math reasoning (MGSM; Shi et al., 2023a) benchmark and\
      \ a machine translation benchmark (WMT23; Kocmi et al., 2023) which was constructed after the model\u2019s training\
      \ data cut-off hence minimizing test set leakage risks."
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MGSM
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: JAX
    justification: The training of Gemini 1.5 Pro was done using JAX.
    quote: Training was done using JAX (Bradbury et al., 2018) and ML Pathways (Dean, 2021).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: difflib
    justification: This library is used to compute a string similarity.
    quote: SequenceMatcher ratio as implemented in https://docs.python.org/3/library/difflib.html
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: SymPy
    justification: The SymPy library is used for solving mathematical problems and integrating into the evaluation of the
      model.
    quote: One promising approach to improve performance involves prompting models to generate solutions  using Python libraries
      like SymPy.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
