title:
  value: 'Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context'
  justification: The title is clearly stated on the first page of the paper.
  quote: 'Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context'
description: "In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient\
  \ multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of\
  \ tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect\
  \ recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video\
  \ QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra\u2019s state-of-the-art performance across a broad\
  \ set of benchmarks."
type:
  value: Empirical Study
  justification: The paper provides empirical results on the performance of Gemini 1.5 Pro and compares it with other models.
  quote: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, ... achieving high performance
    in long-context tasks across text, video, and audio.
primary_research_field:
  name:
    value: Deep Learning
    justification: The paper focuses on developing and evaluating a deep learning model, specifically a multimodal mixture-of-experts
      model.
    quote: 'We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro. This is our first release from Gemini
      1.5, a new family of highly-capable multimodal models which incorporates a novel mixture-of-experts architecture...'
  aliases: []
sub_research_fields:
- name:
    value: Multimodal Learning
    justification: The research involves a model handling multiple data modalities including text, video, and audio.
    quote: Gemini 1.5 Pro is built to handle extremely long contexts; it has the ability to recall and reason over fine-grained
      information from up to at least 10M tokens. This scale is unprecedented among contemporary large language models (LLMs),
      and enables the processing of long-form mixed-modality inputs including entire collections of documents, multiple hours
      of video, and almost five days long of audio.
  aliases: []
models:
- name:
    value: Claude 2.1
    justification: This model is used for comparison in the experiments evaluating long-context capabilities.
    quote: "Gemini 1.5 Pro achieves a 100% recall at 200k tokens, surpassing Claude 2.1\u2019s 98%. This 100% recall is maintained\
      \ up to 530k tokens, and recall is 99.7% at 1M tokens."
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-4 Turbo
    justification: This model is used for comparison in the experiments evaluating long-context capabilities.
    quote: "Gemini 1.5 Pro sets a new state-of-the-art of 64.5% accuracy on EgoSchema using only 16 frames (vs 55.6% for GPT-4V\
      \ (Bala\u017Eevi\u0107 et al., 2024))."
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-4V
    justification: This model is used for comparison in the video needle-in-a-haystack task.
    quote: "Gemini 1.5 Pro sets a new state-of-the-art of 64.5% accuracy on EgoSchema using only 16 frames (vs 55.6% for GPT-4V\
      \ (Bala\u017Eevi\u0107 et al., 2024))."
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Gemini 1.0 Ultra
    justification: This model is used for comparison with the proposed model, Gemini 1.5 Pro.
    quote: Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks...
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Gemini 1.5 Pro
    justification: This is the main model introduced and evaluated in the paper.
    quote: 'We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro.'
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: Contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Gemini 1.5 Pro
    justification: It is the main model introduced and discussed throughout the paper.
    quote: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro.
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: LLaMA
    justification: This model is used for comparison in terms of training data memorization.
    quote: We found that LLaMA and Mistral emit training data at a rate around 0.1% with this attack.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: USM
    justification: This model is used for comparison in automatic speech recognition tasks.
    quote: We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023)... achieving a WER of 8.8%.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Whisper
    justification: This model is used for comparison in automatic speech recognition tasks.
    quote: Note that ASR tasks report a word error rate (WER) metric, where a lower number is better. The Table 5 below shows
      that... Whisper is not robust to long segments and hence requires audio to be segmented every 30 seconds to achieve
      a WER of 7.3%.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Used
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: BigBench
    justification: This dataset is used to evaluate reasoning capabilities.
    quote: On reasoning tasks, 1.5 Pro outperforms 1.0 Pro by a large margin and shows a comparable performance to 1.0 Ultra,
      slightly underperforming on DROP and slightly outperforming on BBH.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: COVOST-2
    justification: This dataset is used for evaluating speech-to-text translation capabilities.
    quote: On CoVoST-2 we evaluate on translating speech in 20 languages into English, reporting on the subset of languages
      that were seen by the model during pre-training.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: EgoSchema
    justification: This dataset is used for evaluating video understanding capabilities.
    quote: The publicly available question answering benchmark with the longest videos is EgoSchema (Mangalam et al., 2023)...
      we introduce a new benchmark, 1H-VideoQA, composed of 125 five-way multiple-choice questions over public videos 40-105
      minutes long.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: FLEURS
    justification: This dataset is used to evaluate automatic speech recognition in multiple languages.
    quote: On FLEURS we evaluate a subset of 55 languages for which we have coverage our training data... On AST we report
      BLEU scores.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: "Les Mis\xE9rables"
    justification: This dataset is used for long-document QA tasks in the paper.
    quote: We test the model's ability to answer them correctly when the entire 1,462 page book (i.e., 710K tokens) is provided
      as input.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: "Les Mis\xE9rables"
    justification: This dataset is used for long-document question answering tasks, as described in the paper.
    quote: "In this section we present experiments on question answering, we create questions using the book 'Les Mis\xE9\
      rables' (by Victor Hugo) and test the model\u2019s ability to answer them correctly when the entire 1,462 page book\
      \ (i.e., 710K tokens) is provided as input."
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MATH
    justification: This dataset is used to evaluate mathematical problem-solving capabilities.
    quote: We find that 1.5 Pro consistently outperforms both 1.0 Ultra and 1.0 Pro on grade-school math (i.e., GSM8K) and
      even shows material improvement over the more demanding benchmarks where there is more headroom for improvement, i.e.,
      +3.5% over 1.0 Ultra for middle- and high-school math problems (i.e., Hendrycks MATH)...
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MATH
    justification: The paper evaluates model performance on this specific dataset to assess mathematical problem-solving capabilities.
    quote: We find that 1.5 Pro consistently outperforms both 1.0 Ultra and 1.0 Pro on grade-school math (i.e., GSM8K) and
      even shows material improvement over the more demanding benchmarks where there is more headroom for improvement, i.e.,
      +3.5% over 1.0 Ultra for middle- and high-school math problems (i.e., Hendrycks MATH).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: WMT23
    justification: This dataset is used for evaluating multilingual translation capabilities.
    quote: "For our multilingual evaluations we use a multilingual math reasoning (MGSM; Shi et al., 2023a) benchmark and\
      \ a machine translation benchmark (WMT23; Kocmi et al., 2023) which was constructed after the model\u2019s training\
      \ data cut-off hence minimizing test set leakage risks."
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: JAX
    justification: The training of Gemini 1.5 Pro was done using JAX.
    quote: Training was done using JAX (Bradbury et al., 2018) and ML Pathways (Dean, 2021).
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: JAX
    justification: JAX is used for training Gemini 1.5 Pro, as stated in the paper.
    quote: Training was done using JAX (Bradbury et al., 2018) and ML Pathways (Dean, 2021).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: SymPy
    justification: The SymPy library is used for solving mathematical problems and integrating into the evaluation of the
      model.
    quote: 'We present a simple example of MATH Intermediate Algebra problem where the solution involved SymPy. Here is the
      Python code to solve the problem: ... import sympy as sp'
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
