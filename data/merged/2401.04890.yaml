title:
  value: 'Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse Actions, Interventions and Sparse Temporal
    Dependencies'
  justification: This is the exact title of the paper as given by the user.
  quote: 'Nonparametric Partial Disentanglement via Mechanism Sparsity: Sparse Actions, Interventions and Sparse Temporal
    Dependencies'
description: This research paper proposes mechanism sparsity regularization as a principle for disentanglement in machine
  learning models. The authors introduce a method to induce disentanglement by simultaneously learning the latent factors
  and the sparse causal graphical model that explains them. They develop a nonparametric identifiability theory to formalize
  this principle and guarantee partial disentanglement by enforcing sparse temporal dependencies and sparse influence from
  auxiliary variables.
type:
  value: theoretical
  justification: The paper is focused on developing a new theoretical framework for disentanglement through mechanism sparsity
    regularization and proves identifiability results based on this framework.
  quote: We propose a representation learning method that induces disentanglement by simultaneously learning the latent factors
    and the sparse causal graphical model that explains them. We develop a nonparametric identifiability theory that formalizes
    this principle and shows that the latent factors can be recovered by regularizing the learned causal graph to be sparse.
primary_research_field:
  name:
    value: Deep Learning
    justification: The paper deals with representation learning and identifiability theory within the context of deep learning
      and causal inference.
    quote: This work introduces a novel principle for disentanglement we call mechanism sparsity regularization, which applies
      when the latent factors of interest depend sparsely on observed auxiliary variables and/or past latent factors. We propose
      a representation learning method that induces disentanglement by simultaneously learning the latent factors and the
      sparse causal graphical model that explains them.
  aliases: []
sub_research_fields:
- name:
    value: Representation Learning
    justification: The paper specifically deals with disentanglement in the context of representation learning, aiming to
      recover latent factors and their causal relations.
    quote: This is closely related to the problem of disentanglement (Bengio et al., 2013; Higgins et al., 2017; Locatello
      et al., 2020) which also aims at extracting interpretable variables from high-dimensional observations, but without
      the emphasis on modelling their causal relations.
  aliases: []
models:
- name:
    value: Variational Autoencoder
    justification: The paper uses a VAE-based approach to implement mechanism sparsity regularization in training models for
      disentanglement.
    quote: Lastly, we propose an estimation procedure based on variational autoencoders and a sparsity constraint and demonstrate
      it on various synthetic datasets.
  aliases:
  - VAE
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: training
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Synthetic datasets
    justification: The paper utilizes various synthetic datasets to demonstrate the effectiveness of the proposed methods
      and validate the theoretical results.
    quote: Lastly, we propose an estimation procedure based on variational autoencoders and a sparsity constraint and demonstrate
      it on various synthetic datasets.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries: []
