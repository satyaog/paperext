title:
  value: Toward Metrics for Differentiating Out-of-Distribution Sets
  justification: The title is explicitly mentioned on the first page of the paper.
  quote: Toward Metrics for Differentiating Out-of-Distribution Sets
description: The paper addresses the challenge of classifying out-of-distribution (OOD) samples with confidence using Convolutional
  Neural Networks (CNNs). It proposes a method to differentiate OOD sets based on generalization errors and introduces three
  efficient metrics (Softmax-based Entropy, Coverage Ratio, and Coverage Distance) to select the most effective OOD sets for
  training well-generalized, calibrated CNN models. The effectiveness is demonstrated across various image and audio classification
  tasks.
type:
  value: empirical
  justification: The paper involves conducting numerous empirical experiments to evaluate the proposed metrics and validate
    the effectiveness of OOD sets for training CNN models.
  quote: "We empirically verify that the most protective OOD sets \u2013 selected according to our metrics..."
primary_research_field:
  name:
    value: Deep Learning
    justification: The research focuses on improving the performance of Deep Learning models by addressing the issue of out-of-distribution
      samples.
    quote: Vanilla CNNs, as uncalibrated classifiers, suffer from classifying out-of-distribution (OOD) samples nearly as
      confidently as in-distribution samples.
  aliases: []
sub_research_fields:
- name:
    value: Out-Of-Distribution Detection
    justification: The core of the paper is to develop metrics and methods to improve OOD sample detection and classification
      using CNN models.
    quote: Our main goal in this paper is to characterizing properties of OOD sets for recognizing a proper one for training
      an end-to-end A-CNN and calibrated vanilla CNN with high detection rate on unseen OOD sets...
  aliases: []
models:
- name:
    value: Vanilla VGG
    justification: ''
    quote: 'Table 1: The influence of selected most and least protective OOD sets on inducing well-generalized A-CNNs with
      high OOD detection rates.'
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ''
    quote: 'Table 1: The influence of selected most and least protective OOD sets on inducing well-generalized A-CNNs with
      high OOD detection rates.'
  is_compared:
    value: true
    justification: ''
    quote: 'Table 1: The influence of selected most and least protective OOD sets on inducing well-generalized A-CNNs with
      high OOD detection rates.'
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Augmented VGG
    justification: ''
    quote: 'Table 1: The influence of selected most and least protective OOD sets on inducing well-generalized A-CNNs with
      high OOD detection rates.'
  aliases:
  - A-VGG
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Used
  is_executed:
    value: true
    justification: ''
    quote: 'Table 1: The influence of selected most and least protective OOD sets on inducing well-generalized A-CNNs with
      high OOD detection rates.'
  is_compared:
    value: true
    justification: ''
    quote: 'Table 1: The influence of selected most and least protective OOD sets on inducing well-generalized A-CNNs with
      high OOD detection rates.'
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Augmented-CNN (A-CNN)
    justification: This model is explicitly developed in the research to handle OOD sample detection by adding an extra rejection
      class.
    quote: We exploit A-CNN as an end-to-end model for OOD detection task.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ''
    quote: In our experiments, we consider two types of end-to-end approaches, i.e., an A-CNN and a confidence-calibrated
      vanilla CNN, for detecting OOD set
  is_compared:
    value: true
    justification: ''
    quote: In our experiments, we consider two types of end-to-end approaches, i.e., an A-CNN and a confidence-calibrated
      vanilla CNN, for detecting OOD set
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Calibrated CNN
    justification: Calibrated CNNs are trained for making uncertain predictions for OOD samples while confidently classifying
      in-distribution ones.
    quote: For instance, calibrated vanilla CNNs [15, 13, 20] are trained to make uncertain predictions for OOD samples while
      still confidently and correctly classifying in-distribution ones.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ''
    quote: In our experiments, we consider two types of end-to-end approaches, i.e., an A-CNN and a confidence-calibrated
      vanilla CNN, for detecting OOD set
  is_compared:
    value: true
    justification: ''
    quote: In our experiments, we consider two types of end-to-end approaches, i.e., an A-CNN and a confidence-calibrated
      vanilla CNN, for detecting OOD set
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Vanilla CNN
    justification: Vanilla CNN is used as a baseline model in the experiments conducted to evaluate the effectiveness of the
      proposed metrics.
    quote: It has been shown that state-of-the-art (vanilla) deep neural networks (e.g., CNN) are uncalibrated such that they
      are making predictions for OOD samples with a confidence that is as high as those of in-distribution samples.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Used
  is_executed:
    value: true
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: CIFAR-10
    justification: The CIFAR-10 dataset is used as an in-distribution dataset in several experiments conducted in the paper.
    quote: We conduct a series of experiments on several classification tasks including two image benchmarks, namely CIFAR-10
      and SVHN, and one audio benchmark, namely Urban-Sound [28].
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: CIFAR-100
    justification: Used as an out-of-distribution dataset in the experiments.
    quote: Like in [21], for each of these in-distribution task, various naturalistic OOD sets are considered; for image classification
      tasks, we consider LSUN, ISUN, CIFAR-100 and TinyIma-geNet as OOD sets and Gaussian noise as a synthetic OOD set.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ECS
    justification: ECS is used as an out-of-distribution dataset for audio classification tasks.
    quote: For audio classification task with 10 classes, i.e., Urban-Sound, OOD sets considered are TuT [25], Google Command
      [32] and ECS (Environmental Sound Classification) [26], as well as white-noise sound as a synthetic OOD set.
  aliases:
  - Environmental Sound Classification
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Google Command
    justification: The Google Command dataset is used as an out-of-distribution dataset in the audio classification experiments.
    quote: For audio classification task with 10 classes, i.e., Urban-Sound, OOD sets considered are TuT [25], Google Command
      [32] and ECS (Environmental Sound Classification) [26], as well as white-noise sound as a synthetic OOD set.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ISUN
    justification: ISUN is used as an out-of-distribution dataset in the experiments.
    quote: Like in [21], for each of these in-distribution task, various naturalistic OOD sets are considered; for image classification
      tasks, we consider LSUN, ISUN, CIFAR-100 and TinyIma- geNet as OOD sets and Gaussian noise as a synthetic OOD set.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: LSUN
    justification: LSUN is used as an out-of-distribution dataset in the experiments.
    quote: Like in [21], for each of these in-distribution task, various naturalistic OOD sets are considered; for image classification
      tasks, we consider LSUN, ISUN, CIFAR-100 and TinyIma- geNet as OOD sets and Gaussian noise as a synthetic OOD set.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: SVHN
    justification: SVHN is both used as an in-distribution dataset and an out-of-distribution dataset in various experiments.
    quote: It has also been shown in [19], where using SVHN as OOD set for CIFAR-10 is leading to an A-CNN with inferior generalization
      properties.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TinyImageNet
    justification: TinyImageNet is used as an out-of-distribution dataset for evaluating the models.
    quote: Like in [21], for each of these in-distribution task, various naturalistic OOD sets are considered; for image classification
      tasks, we consider LSUN, ISUN, CIFAR-100 and TinyIma-geNet as OOD sets and Gaussian noise as a synthetic OOD set.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TuT
    justification: The TuT dataset is used as an out-of-distribution dataset in the audio classification experiments.
    quote: OOD sets considered are TuT [25], Google Command [32] and ECS (Environmental Sound Classification) [26], as well
      as white-noise sound as a synthetic OOD set.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Urban-Sound
    justification: The Urban-Sound dataset is used to verify the effectiveness of the proposed metrics in audio classification
      tasks.
    quote: We conduct a series of experiments on several classification tasks including two image benchmarks, namely CIFAR-10
      and SVHN, and one audio benchmark, namely Urban-Sound.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries: []
