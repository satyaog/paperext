{
  "title": {
    "value": "A Survey on Deep Learning for Theorem Proving",
    "justification": "The title succinctly describes the content and focus of the paper.",
    "quote": "A Survey on Deep Learning for Theorem Proving"
  },
  "description": "This paper offers a comprehensive survey of deep learning techniques applied to the field of theorem proving, reviewing existing approaches, available datasets, evaluation metrics, and suggesting future research directions.",
  "type": {
    "value": "theoretical",
    "justification": "The paper provides a thorough review and analysis of existing deep learning methods, datasets, and performance metrics within the specific field of theorem proving, but doesnt run any experiments.",
    "quote": "This paper presents a pioneering comprehensive survey of deep learning for theorem proving by offering i) a thorough review of existing approaches... ii) a meticulous summary of available datasets... iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art."
  },
  "primary_research_field": {
    "name": {
      "value": "Mathematics",
      "justification": "The survey focuses on the application of deep learning techniques within the domain of theorem proving.",
      "quote": "(@fabrice removed the quote)"
    },
    "aliases": []
  },
  "sub_research_fields": [
    {
      "name": {
        "value": "Theorem Proving",
        "justification": "The sub-research field explicitly narrows down to the intersection of theorem proving and deep learning methods.",
        "quote": "Exploring learning-based approaches for theorem proving has been a long-standing research focus..."
      },
      "aliases": []
    }
  ],
  "models": [
    {
      "name": {
        "value": "AlphaGeometry",
        "justification": "AlphaGeometry is noted for its application in the domain of geometry theorem proving.",
        "quote": "Notably, AlphaGeometry (Trinh et al., 2024) trains a decoder-only Transformer to predict the auxiliary constructions in the proofs of International Mathematical Olympiad (IMO) geometry problems."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "training"
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Codex",
        "justification": "Codex is discussed for its use in autoformalization.",
        "quote": "Wu et al. (2022); Azerbayev et al. (2023); Jiang et al. (2023a) explore advanced LLMs like Codex and GPT-4 (Achiam et al., 2023) for informalization..."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "inference"
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "GPT-4",
        "justification": "GPT-4 is highlighted for its contributions to autoformalization and proof generation.",
        "quote": "Using GPT-4, MMA (Jiang et al., 2023a) informalizes all theorem statements in Archive of Formal Proofs and mathlib..."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "inference"
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "GPT-f",
        "justification": "GPT-f is listed in the context of proofstep generation and various models have been referenced including GPT-3 and GPT-4.",
        "quote": "Specifically, GPT-f (Polu & Sutskever, 2020) first apply a conditional language modeling objective to train decoder-only Transformers to generate a proof step..."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Inference"
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "NaturalProver",
        "justification": "NaturalProver is described as employing constrained decoding to aid in proof generation, leveraging GPT-3.",
        "quote": "NaturalProver (Welleck et al., 2022) trains GPT-3 (Brown et al., 2020) with constrained decoding to encourage using retrieved references in the proof steps."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Inference"
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "PaLM",
        "justification": "PaLM is mentioned as being studied for autoformalization.",
        "quote": "Wu et al. (2022); Agrawal et al. (2022); Gadgil et al. (2022) study the prospects of autoformalization using PaLM (Chowdhery et al., 2023)..."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "inference"
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Thor",
        "justification": "Thor is mentioned as a model that integrates language models with automated theorem provers, enhancing the process of generating proofs.",
        "quote": "Thor (Jiang et al., 2022) adds a <hammer> token to learn when to invoke an ATP tool for premise selection to simplify the proof."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Referenced"
      },
      "is_executed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Inference"
      },
      "is_compared": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "CoqGym",
        "justification": "CoqGym is recognized specifically as a large-scale dataset for Coq proofs.",
        "quote": "with CoqGym constructing a dataset from 123 projects encompassing 71k proofs."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "GamePad",
        "justification": "GamePad is highlighted as a Coq dataset for formal proofs.",
        "quote": "Notable datasets for Coq include Gamepad (Huang et al., 2019)..."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "PISA",
        "justification": "",
        "quote": "For Isabelle, datasets like IsarStep (Li et al., 2021a), PISA (Jiang et al., 2021), and Magnushammer (Mikuła et al., 2024) are built on the Archive of Formal Proofs and Isabelle Standard Library, where PISA extracts 183K theorems and 2.16M proof steps."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Magnushammer",
        "justification": "",
        "quote": "For Isabelle, datasets like IsarStep (Li et al., 2021a), PISA (Jiang et al., 2021), and Magnushammer (Mikuła et al., 2024) are built on the Archive of Formal Proofs and Isabelle Standard Library, where PISA extracts 183K theorems and 2.16M proof steps."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HolStep",
        "justification": "HolStep is noted as a dataset for the HOL Light proof assistant, facilitating research in premise selection and proof generation.",
        "quote": "Notable datasets for Coq include Gamepad, CoqGym, and PRISM... Datasets for other proof assistants include HolStep (Kaliszyk et al., 2017) and HOList (Bansal et al., 2019) for HOL Light"
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HOList",
        "justification": "HOList is noted as a dataset for the HOL Light proof assistant, facilitating research in premise selection and proof generation.",
        "quote": "Notable datasets for Coq include Gamepad, CoqGym, and PRISM... Datasets for other proof assistants include HolStep (Kaliszyk et al., 2017) and HOList (Bansal et al., 2019) for HOL Light"
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "IsarStep",
        "justification": "IsarStep is mentioned for its utility in formal theorem proving within Isabelle.",
        "quote": "For Isabelle, datasets like IsarStep (Li et al., 2021a)... are built on the Archive of Formal Proofs and Isabelle Standard Library..."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MiniF2F",
        "justification": "MiniF2F is highlighted as a dataset that contains Olympiad-level problems manually formalized in multiple proof systems.",
        "quote": "Notably, MiniF2F (Zheng et al., 2022) manually formalizes 488 Olympiad-level problems across 4 proof systems and equally splits them into a validation set and a test set."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "NL-PS",
        "justification": "NL-PS is mentioned as a dataset designed for premise selection from natural language.",
        "quote": "NL-PS (Ferreira & Freitas, 2020a) first builds a natural language premise selection dataset sourced from ProofWiki."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MATcH",
        "justification": "mentioned as a dataset for matching using the MREC corpus.",
        "quote": "MATcH (Li et al., 2023) constructs over 180k statement-proof pairs for matching using the MREC corpus."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "NaturalProofs",
        "justification": "NaturalProofs is referred to as a dataset that includes data from various sources for better diversity.",
        "quote": "Similarly, NaturalProofs (Welleck et al., 2021) further incorporates data from Stacks and textbooks, resulting in a dataset with roughly 25k examples."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "NaturalProofs-Gen",
        "justification": "NaturalProofs-Gen is referred to as a variant of NaturalProofs",
        "quote": "Adapted from it, NaturalProofs-Gen (Welleck et al., 2022) contains around 14.5k theorems for informal proof generation."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "PRISM",
        "justification": "PRISM is mentioned as one of the datasets that contain formal theorems and proofs extracted for the Coq proof assistant.",
        "quote": "Notable datasets for Coq include Gamepad, CoqGym, and PRISM (Reichel et al., 2023), with CoqGym constructing a dataset from 123 projects encompassing 71k proofs."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ProofNet",
        "justification": "ProofNet is mentioned as a dataset for formalizing IMO and undergraduate-level problems.",
        "quote": "ProofNet (Azerbayev et al., 2023) formalizes the theorem statements of IMO and undergraduate-level problems in Lean."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "LeanStep",
        "justification": "",
        "quote": "LeanStep (Han et al., 2022), LeanDojo (Yang et al., 2023), and MLFMF (Bauer et al., 2023) leverage the mathlib library (mathlib Community, 2020) in Lean. In particular, LeanDojo extracts over 98k theorems and proofs with 130k premises from mathlib."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "LeanDojo",
        "justification": "",
        "quote": "LeanStep (Han et al., 2022), LeanDojo (Yang et al., 2023), and MLFMF (Bauer et al., 2023) leverage the mathlib library (mathlib Community, 2020) in Lean. In particular, LeanDojo extracts over 98k theorems and proofs with 130k premises from mathlib."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MLFMF",
        "justification": "",
        "quote": "LeanStep (Han et al., 2022), LeanDojo (Yang et al., 2023), and MLFMF (Bauer et al., 2023) leverage the mathlib library (mathlib Community, 2020) in Lean. In particular, LeanDojo extracts over 98k theorems and proofs with 130k premises from mathlib."
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MPTP2078",
        "justification": "",
        "quote": "MPTP2078 (Alama et al., 2014)"
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Mizar40",
        "justification": "",
        "quote": "Mizar40 (Kaliszyk & Urban, 2015b)"
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "M2K",
        "justification": "",
        "quote": "M2K (Kaliszyk et al., 2018) for Mizar"
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "Coq",
        "justification": "Coq is specifically identified as a proof assistant used extensively in the research.",
        "quote": "Coq (Barras et al., 1999)"
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HOL Light",
        "justification": "HOL Light is recognized for its role as a proof assistant in formal theorem proving.",
        "quote": "HOL Light (Harrison, 1996)"
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Isabelle",
        "justification": "Isabelle is highlighted as a proof assistant utilized in interactive theorem proving.",
        "quote": "Isabelle (Paulson, 1994), HOL Light (Harrison, 1996), Coq (Barras et al., 1999), and Lean (Moura & Ullrich, 2021)"
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Lean",
        "justification": "Lean is highlighted as one of the major proof assistants used in formal theorem proving.",
        "quote": "Lean (Moura & Ullrich, 2021)"
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Vampire",
        "justification": "Vampire is described as an automated theorem proving system employed extensively in the field.",
        "quote": "Saturation-based theorem provers, including E (Schulz, 2002) and Vampire (Ková́cs & Voronkov, 2013), mainly operate on first-order logic (FOL)..."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "leanCoP",
        "justification": "leanCoP is a tableau-based method referenced for its approach in automated theorem proving.",
        "quote": "Similarly, geometric ATP systems such as GEX (Chou et al., 2000) prove geometry problems by iteratively applying deduction rules. Other approaches, such as tableau-based methods like leanCoP (Otten & Bibel, 2003)..."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "leanRoP",
        "justification": "leanRoP is included for its relevance in connection-based theorem proving methods.",
        "quote": "Other approaches, such as tableau-based methods like leanCoP... and connection tableau methods like leanRoP (Gauthier et al., 2020), use other forms of proof calculi for proof construction."
      },
      "aliases": [],
      "role": "Referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "mathlib",
        "justification": "mathlib is mentioned as a significant library used within the Lean proof assistant.",
        "quote": "Lean's mathlib library (mathlib Community, 2020)"
      },
      "aliases": [],
      "role": "referenced",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ]
}