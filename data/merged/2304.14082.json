{
  "title": {
    "value": "JaxPruner: A Concise Library for Sparsity Research",
    "justification": "The title accurately reflects the main subject and purpose of the paper which is introducing the JaxPruner library for sparsity research.",
    "quote": "This work introduces JaxPruner, a JAX-based sparsity library for machine learning research."
  },
  "description": "This paper introduces JaxPruner, a JAX-based sparsity library designed to facilitate research in sparse neural networks through concise implementations of popular pruning and sparse training algorithms. It aims at minimal memory and latency overhead, ease of integration, and providing strong baselines.",
  "type": {
    "value": "Empirical Study",
    "justification": "The paper includes empirical benchmark results of JaxPruner on multiple standard datasets and demonstrates the utility of the library through various experiments.",
    "quote": "We benchmark pruning and sparse training algorithms in 4 different domains and discuss them in subsequent sections"
  },
  "primary_research_field": {
    "name": {
      "value": "Deep Learning",
      "justification": "The paper focuses on developing and evaluating a library for deep learning models, specifically for sparsity research in neural networks.",
      "quote": "JaxPruner aims to accelerate research on sparse neural networks..."
    },
    "aliases": []
  },
  "sub_research_fields": [
    {
      "name": {
        "value": "Model Compression",
        "justification": "The paper specifically addresses sparsity in neural networks, which is a sub-field within model compression and training efficiency.",
        "quote": "There are two high-level strategies for achieving parameter sparsity: (1) pruning which aims to obtain sparse networks starting from dense networks for inference efficiency and (2) sparse training which aims to train sparse networks from scratch, thus reducing training cost as well."
      },
      "aliases": []
    },
    {
      "name": {
        "value": "Sparse Training",
        "justification": "",
        "quote": ""
      },
      "aliases": []
    },
    {
      "name": {
        "value": "Pruning",
        "justification": "",
        "quote": ""
      },
      "aliases": []
    }
  ],
  "models": [
    {
      "name": {
        "value": "DQN",
        "justification": "This model is used for deep reinforcement learning experiments within the Dopamine library.",
        "quote": "Though it is possible to run any of the Atari games and agents, we choose MsPacman and DQN for our experiments."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Used"
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ResNet-50",
        "justification": "This model is used in the empirical evaluation for image classification tasks.",
        "quote": "We train 80% sparse ResNet-50 models on ImageNet..."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Used"
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "T5",
        "justification": "This model is used for the language modeling benchmarks in the experiments.",
        "quote": "We also build a JaxPruner integration with the t5x library... we prune 80% of the weights... of our LM architecture"
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Used"
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ViT-B/16",
        "justification": "This model is used in the benchmarks for image classification experiments within the paper.",
        "quote": "We apply JaxPruner algorithms to train 80% sparse ViT-B/16..."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "Used"
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "PlainViT-S/16+",
        "justification": "",
        "quote": "We benchmark pruning and sparse training algorithms in 4 different domains and discuss them in sub- sequent sections: •(Section 4.1) ImageNet-2012 [36] image classification using the ViT-B/16 [37], PlainViT-S/16 [38] and ResNet-50 [39] architectures. •(Section 4.2) Federated EMNIST [40] character recognition using a CNN with dropout [41]. •(Section 4.3) C4 language modelling using the T5-Base encoder-decoder transformer architec- ture [42, 43]. •(Section 4.4) a DQN agent [44] with a convolutional backbone trained on the MsPacman Atari 2600 game [45]."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
        "quote": "Trained"
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "CNN",
        "justification": "",
        "quote": "We benchmark pruning and sparse training algorithms in 4 different domains and discuss them in sub- sequent sections: •(Section 4.1) ImageNet-2012 [36] image classification using the ViT-B/16 [37], PlainViT-S/16 [38] and ResNet-50 [39] architectures. •(Section 4.2) Federated EMNIST [40] character recognition using a CNN with dropout [41]. •(Section 4.3) C4 language modelling using the T5-Base encoder-decoder transformer architec- ture [42, 43]. •(Section 4.4) a DQN agent [44] with a convolutional backbone trained on the MsPacman Atari 2600 game [45]."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "",
        "quote": ""
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "C4",
        "justification": "The dataset is used for language modeling benchmarks with the T5-Base model.",
        "quote": "We train from scratch a T5-base (220M parameter) model to predict missing words within a corrupted span of text on the C4 dataset"
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Federated EMNIST (MNIST)",
        "justification": "The dataset is used for character recognition tasks in the federated learning benchmarks.",
        "quote": "We test the effect of various pruning algorithms on the federated EMNIST character recognition benchmark [40]"
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "ImageNet-2012",
        "justification": "The ImageNet-2012 dataset is used for benchmarking ViT-B/16, PlainViT-S/16, and ResNet-50 models.",
        "quote": "We benchmark pruning and sparse training algorithms in 4 different domains... ImageNet-2012 [36] image classification using the ViT-B/16, PlainViT-S/16 (PViT) and ResNet-50 architectures."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MsPacman Atari 2600",
        "justification": "The dataset is used with the DQN model for deep reinforcement learning benchmarks.",
        "quote": "We benchmark pruning and sparse training algorithms in 4 different domains and discuss them in sub- sequent sections: •(Section 4.1) ImageNet-2012 [36] image classification using the ViT-B/16 [37], PlainViT-S/16 [38] and ResNet-50 [39] architectures. •(Section 4.2) Federated EMNIST [40] character recognition using a CNN with dropout [41]. •(Section 4.3) C4 language modelling using the T5-Base encoder-decoder transformer architec- ture [42, 43]. •(Section 4.4) a DQN agent [44] with a convolutional backbone trained on the MsPacman Atari 2600 game [45]."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": [
    {
      "name": {
        "value": "Dopamine",
        "justification": "Dopamine is one of the libraries integrated with JaxPruner to conduct experiments in deep reinforcement learning.",
        "quote": "Speficically, we integrate JaxPruner with Scenic [29], T5x [30], Dopamine [31] and FedJAX [6]."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "FedJAX",
        "justification": "FedJAX is one of the libraries integrated with JaxPruner for facilitating research specifically in the domain of federated learning.",
        "quote": "Speficically, we integrate JaxPruner with Scenic [29], T5x [30], Dopamine [31] and FedJAX [6]."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "JAX",
        "justification": "JAX is the foundational library on which JaxPruner is built, and it is utilized for its features such as function transformations and gradient calculations.",
        "quote": "JAX [3] has seen increasing adoption by the research community...functions like taking gradients... reduces the time required for implementing complex ideas"
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "JaxPruner",
        "justification": "JaxPruner is the main library introduced and discussed in the paper for sparsity research.",
        "quote": "This work introduces JaxPruner, a JAX-based sparsity library for machine learning research."
      },
      "aliases": [],
      "role": "Contributed",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Optax",
        "justification": "Optax is an optimization library used in conjunction with JaxPruner for various pruning and sparse training algorithms.",
        "quote": "Algorithms implemented in JaxPruner share a com- mon API and works seamlessly with Optax, a widely-used optimization library in JAX"
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "Scenic",
        "justification": "Scenic is one of the libraries integrated with JaxPruner for facilitating research and it is used in the examples provided.",
        "quote": "Speficically, we integrate JaxPruner with Scenic [29], T5x [30], Dopamine [31] and FedJAX [6]."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "t5x",
        "justification": "t5x is one of the libraries integrated with JaxPruner for facilitating research specifically in the domain of language modeling.",
        "quote": "Speficically, we integrate JaxPruner with Scenic [29], T5x [30], Dopamine [31] and FedJAX [6]."
      },
      "aliases": [],
      "role": "Used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ]
}