title:
  value: 'DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning'
  justification: The title is explicitly mentioned at the beginning of the paper.
  quote: 'DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning'
description: This paper introduces DP-RDM, a differentially private retrieval-augmented diffusion model for generating high-quality
  image samples with rigorous privacy guarantees. The proposed algorithm generates images from text prompts without fine-tuning
  the model on private datasets, thereby addressing sample-level memorization issues in text-to-image diffusion models. The
  method is validated on datasets like MS-COCO, Shutterstock, and CIFAR-10, showing significant improvements.
type:
  value: empirical
  justification: The paper details an experiment involving the adaptation of diffusion models to private domains, evaluates
    the performance, and provides empirical results.
  quote: ''
primary_research_field:
  name:
    value: Text2Image
    justification: ''
    quote: Text-to-image diffusion models (Ho et al., 2020; Song et al., 2020) enable highly customizable image generation,
      producing photo-realistic image samples that can be instructed through text prompting.
  aliases: []
sub_research_fields:
- name:
    value: Generative Models
    justification: The paper discusses the development and use of generative models, specifically diffusion models for generating
      images.
    quote: We develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating
      high-quality image samples while providing provable privacy guarantees.
  aliases: []
- name:
    value: Diffusion Model
    justification: ''
    quote: ''
  aliases: []
- name:
    value: RAG
    justification: ''
    quote: ''
  aliases:
  - Retrieval-Augmented Generation
  - RAG
- name:
    value: Differentially Private
    justification: ''
    quote: ''
  aliases:
  - Differentially Private retrieval-augmented model
  - DP-RDM
models:
- name:
    value: ViT
    justification: From paper Blattmann et al. (2022). It's a bit tricky to know...
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: ''
    quote: ''
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: CIFAR-10
    justification: CIFAR-10 is used for evaluating the model.
    quote: "we evaluate our DP-RDM on three datasets\u2014CIFAR-10, MS-COCO and Shutterstock"
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ImageNet FB
    justification: ImageNet FB is mentioned as a dataset used in the experiments.
    quote: 'trained on face-blurred ImageNet, using different private retrieval datasets at inference time: Shutterstock,
      MS-COCO with face-blurring (FB), ImageNet FB, and CIFAR-10.'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MS-COCO
    justification: MS-COCO is used for evaluating the model.
    quote: "we evaluate our DP-RDM on three datasets\u2014CIFAR-10, MS-COCO and Shutterstock"
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Shutterstock
    justification: Shutterstock is used for evaluating the model.
    quote: "we evaluate our DP-RDM on three datasets\u2014CIFAR-10, MS-COCO and Shutterstock"
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries: []
