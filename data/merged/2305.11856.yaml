title:
  value: 'Video Killed the HD-Map: Predicting Multi-Agent Behavior Directly From Aerial Images'
  justification: It is the title of the paper.
  quote: 'Video Killed the HD-Map: Predicting Multi-Agent Behavior Directly From Aerial Images'
description: This paper introduces an aerial image-based map (AIM) representation that requires minimal annotation and provides
  rich road context information for multi-agent behavior modeling in autonomous driving. It integrates AIM into a differentiable
  driving simulator and demonstrates competitive multi-agent trajectory prediction performance compared to high-definition
  maps.
type:
  value: Empirical study
  justification: The paper conducts experiments and provides evaluation metrics for multi-agent trajectory prediction using
    the AIM representation.
  quote: We evaluate multi-agent trajectory prediction using the AIM by incorporating it into a differentiable driving simulator
    as an image-texture-based differentiable rendering module.
primary_research_field:
  name:
    value: Deep Learning
    justification: The paper leverages deep learning models and techniques for multi-agent trajectory prediction.
    quote: Recent studies have emphasized the use of learned models to generate more realistic behavior for controlled agents
      like pedestrians and surrounding vehicles.
  aliases: []
sub_research_fields:
- name:
    value: Trajectory Prediction
    justification: The paper focuses on predicting future trajectories of multiple traffic agents using AIM representation.
    quote: In this study, we investigate the performance of behavioral models learned using aerial imagery instead of HD maps...
      We evaluate multi-agent trajectory prediction using the AIM...
  aliases: []
models:
- name:
    value: ITRA
    justification: The paper discusses and evaluates the ITRA model for multi-agent trajectory prediction.
    quote: We use ITRA [3] to investigate the validity of our primary claim. ITRA uses a conditional variational recurrent
      neural network (CVRNN) [16] model followed by a bicycle kinematic model [17] to jointly predict the next state of each
      agent in the scene.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: training
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ITRA-AIM
    justification: A variant of the ITRA model that uses aerial image-based map (AIM) for multi-agent trajectory prediction.
    quote: ITRA-AIM demonstrates competitive performance compared to ITRA-HDM
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: Contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ITRA-HDM
    justification: A variant of the ITRA model that uses high-definition maps (HDM) for multi-agent trajectory prediction.
    quote: ITRA-AIM demonstrates competitive performance compared to ITRA-HDM
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ResNet-18
    justification: The paper experiments with a ResNet-18 backbone to encode the AIM representation.
    quote: We use an identical CNN encoder for encoding AIM which consists of a 4-layer CNN model for our ITRA-AIM model but
      also experiment with a ResNet-18 backbone on the vehicle dataset to encode the AIM representation...
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: training
  is_inference_only:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Argoverse
    justification: The Argoverse dataset is mentioned as a source of HD maps for comparison.
    quote: Examples of HD maps from public motion planning datasets for (a) Argoverse [10] and (b) Nuplan.
  aliases: []
  role: referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Bing aerial imagery
    justification: The dataset is used to test the flexibility of the AIM representation.
    quote: We acquired an aerial image from Bing aerial imagery of the same location as Fig. 4d to construct a larger AIM.
  aliases: []
  role: Referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: INTERACTION Dataset
    justification: "This dataset is similar to the one recorded using a drone from a bird\u2019s-eye view in the study."
    quote: "We record a dataset of human behavior in traffic scenes with a drone from a bird\u2019s-eye view, in a manner\
      \ similar to [11]"
  aliases: []
  role: Referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Nuplan
    justification: The Nuplan dataset is referred to in the context of HD map annotations.
    quote: Examples of HD maps from public motion planning datasets for (a) Argoverse [10] and (b) Nuplan.
  aliases: []
  role: referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Waymo Open Motion Dataset
    justification: The dataset is used for multi-agent behavior modeling and provides sensory data for agent tracks.
    quote: Typically, learning behavior models requires data consisting of the high-definition (HD) map for the given location
      and extracted agent tracks.
  aliases: []
  role: Referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: PyTorch
    justification: PyTorch is used as the deep learning framework for the implementation of models.
    quote: Our image-texture-based differentiable rendering module is designed to be differentiable and efficient as it supports
      rendering in batch mode using PyTorch.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: PyTorch3D
    justification: PyTorch3D is used for implementing the differentiable rendering module.
    quote: We incorporate AIM into a differentiable simulator by implementing a custom differentiable renderer with PyTorch3D.
  aliases: []
  role: Used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
