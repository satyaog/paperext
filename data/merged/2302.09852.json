{
  "title": {
    "value": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection",
    "justification": "Extracted from the title of the paper.",
    "quote": "Unsupervised Layer-wise Score Aggregation for Textual OOD Detection"
  },
  "description": "The paper presents a novel unsupervised method for Out-of-Distribution (OOD) detection for textual data by aggregating layer-wise anomaly scores from a model's encoder. The authors demonstrate that the last layer's representation is not always the most effective for OOD detection and propose an automatic aggregation method to leverage all hidden layers without requiring access to OOD samples.",
  "type": {
    "value": "empirical study",
    "justification": "The paper involves conducting experiments to evaluate the performance of their proposed method on a new benchmark dataset and comparing it with existing methods.",
    "quote": "We conduct extensive experiments on our newly proposed benchmark: We introduce MILTOOD-C A MultI Lingual Text OOD detection benchmark for Classification tasks"
  },
  "primary_research_field": {
    "name": {
      "value": "Out-of-distribution",
      "justification": "",
      "quote": "Out-of-distribution (OOD) detection is a rapidly growing field due to new robustness and security requirements driven by an increased number of AI-based systems"
    },
    "aliases": ["OOD"]
  },
  "sub_research_fields": [
    {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper specifically addresses OOD detection in textual data, which falls under the sub-field of Natural Language Processing (NLP).",
        "quote": "Although OOD detection has attracted much attention in computer vision (Huang et al. 2022; Wang et al. 2022c; Fang et al. 2022), few studies focused on textual data"
      },
      "aliases": ["NLP"]
    }
  ],
  "models": [
    {
      "name": {
        "value": "BERT",
        "justification": "The experiments involve fine-tuning BERT models for various benchmark tests.",
        "quote": "To ensure that our results are consistent not only across tasks and shifts, but also across model architec- tures, we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al.  2018) (base, large and multilingual versions), DISTILBERT (Sanh et al. 2019) and RoBERTa (Liu et al. 2019) (base and large versions) fine-tuned on each task."
      },
      "aliases": ["BERT-large", "BERT-multilingual"],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "used"
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "DISTILBERT",
        "justification": "The paper mentions using DISTILBERT models for their experiments.",
        "quote": "To ensure that our results are consistent not only across tasks and shifts, but also across model architec- tures, we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al.  2018) (base, large and multilingual versions), DISTILBERT (Sanh et al. 2019) and RoBERTa (Liu et al. 2019) (base and large versions) fine-tuned on each task."
      },
      "aliases": [],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "used"
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "RoBERTa",
        "justification": "RoBERTa models are used in the experiments as per the detailed mention in the paper.",
        "quote": "To ensure that our results are consistent not only across tasks and shifts, but also across model architec- tures, we train classifiers based on 6 different Transformer (Vaswani et al. 2017) decoders: BERT (Devlin et al.  2018) (base, large and multilingual versions), DISTILBERT (Sanh et al. 2019) and RoBERTa (Liu et al. 2019) (base and large versions) fine-tuned on each task."
      },
      "aliases": ["RoBERTa-large"],
      "is_contributed": {
        "value": false,
        "justification": "Role:['contributed', 'used', 'referenced']",
        "quote": "used"
      },
      "is_executed": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "is_compared": {
        "value": true,
        "justification": "",
        "quote": ""
      },
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "datasets": [
    {
      "name": {
        "value": "SetFit/20-newsgroups",
        "justification": "The 20 Newsgroups dataset is mentioned as one of the datasets that the proposed method is evaluated on.",
        "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
      },
      "aliases": ["20-newsgroups"],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "IMDB",
        "justification": "IMDB dataset is used for evaluation in the paper.",
        "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
      },
      "aliases": [],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MILTOOD-C",
        "justification": "MILTOOD-C is the new dataset proposed in the paper for multilingual textual OOD detection.",
        "quote": "We introduce MILTOOD-C A MultI Lingual Text OOD detection benchmark for Classification tasks"
      },
      "aliases": [],
      "role": "contributed",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "SST2",
        "justification": "SST2 is among the datasets used for text classification tasks for evaluation of the proposed method.",
        "quote": "It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classification (i.e., 20Newsgroup (Joachims 1996))"
      },
      "aliases": [],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "TREC",
        "justification": "",
        "quote": "We relied on the benchmark proposed by Zhou, Liu, and Chen (2021); Hendrycks et al. (2020). It features three types of IN-DS: sentiment analysis (i.e., SST2 (Socher et al. 2013), IMDB (Maas et al. 2011)), topic classi- fication (i.e., 20Newsgroup (Joachims 1996)) and question answering (i.e., TREC-10 and TREC-50 (Li and Roth 2002))"
      },
      "aliases": ["TREC-10", "TREC-50"],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "AmazonScience/massive",
        "justification": "",
        "quote": "We also included the Massive (FitzGerald et al. 2022) dataset and the Banking (Casanueva et al. 2020) for a larger num- ber of classes and NLI datasets (i.e., RTE (Burger and Ferro 2005; Hickl et al. 2006) and MNLI (Williams, Nangia, and Bowman 2018)) following Colombo et al. (2022)."
      },
      "aliases": ["Massive"],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "banking77",
        "justification": "",
        "quote": "We also included the Massive (FitzGerald et al. 2022) dataset and the Banking (Casanueva et al. 2020) for a larger num- ber of classes and NLI datasets (i.e., RTE (Burger and Ferro 2005; Hickl et al. 2006) and MNLI (Williams, Nangia, and Bowman 2018)) following Colombo et al. (2022)."
      },
      "aliases": ["Banking"],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "RTE",
        "justification": "",
        "quote": "We also included the Massive (FitzGerald et al. 2022) dataset and the Banking (Casanueva et al. 2020) for a larger num- ber of classes and NLI datasets (i.e., RTE (Burger and Ferro 2005; Hickl et al. 2006) and MNLI (Williams, Nangia, and Bowman 2018)) following Colombo et al. (2022)."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "MNLI",
        "justification": "",
        "quote": "We also included the Massive (FitzGerald et al. 2022) dataset and the Banking (Casanueva et al. 2020) for a larger num- ber of classes and NLI datasets (i.e., RTE (Burger and Ferro 2005; Hickl et al. 2006) and MNLI (Williams, Nangia, and Bowman 2018)) following Colombo et al. (2022)."
      },
      "aliases": [],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "HINT3",
        "justification": "",
        "quote": "To go one step further in terms of number of classes, we considered HINT3(Arora et al. 2020) and clink (Larson et al. 2019a)."
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "SefFit/emotion",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "aliases": [],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "glue",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "aliases": [],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "super-glue",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "go-emotions",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "aliases": [],
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "clinc-oos",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "xquad",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "catalonia-independence",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "muchocine",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "allocine",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "paws-x",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "swiss-judgment-prediction",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "xnli",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "tweet-sentiment-multilingual",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    },
    {
      "name": {
        "value": "xstance",
        "justification": "",
        "quote": "Table 3: List of Datasets"
      },
      "role": "used",
      "referenced_paper_title": {
        "value": "",
        "justification": "",
        "quote": ""
      }
    }
  ],
  "libraries": []
}