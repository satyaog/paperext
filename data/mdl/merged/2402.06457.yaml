title:
  value: 'V-STaR: Training Verifiers for Self-Taught Reasoners'
  justification: This is the official title of the research paper.
  quote: 'V-STaR: Training Verifiers for Self-Taught Reasoners'
description: This paper introduces V-STaR, an approach designed to train verifiers by utilizing both correct and incorrect
  solutions generated during the self-improvement process of Large Language Models (LLMs). V-STaR contrasts itself with existing
  methods by leveraging Direct Preference Optimization to improve verifier and generator performance iteratively. The empirical
  results exhibit significant enhancements in test accuracy over prior methods in code generation and math reasoning benchmarks.
type:
  value: empirical
  justification: The paper involves empirical evaluations of V-STaR against baseline methods in various tasks, primarily focused
    on experimental results.
  quote: We empirically evaluate V-STaR on math reasoning... and on code-generation...
primary_research_field:
  name:
    value: Self-improvement for large language models
    justification: The paper focuses on methods to improve reasoning abilities and verification processes in Large Language
      Models, which are a subfield of Deep Learning.
    quote: To improve the reasoning performance of LLMs...
  aliases: []
sub_research_fields:
- name:
    value: Language Models and Reasoning
    justification: The paper primarily deals with enhancing the reasoning capabilities of LLMs through iterative self-improvement
      and verification.
    quote: To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated
      during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions.
  aliases: []
- name:
    value: Model Training and Optimization
    justification: The core contribution of the paper is improving the training process of verifiers and generators using
      techniques like Direct Preference Optimization.
    quote: To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated
      during the self-improvement process to train a verifier using DPO...
  aliases: []
models:
- name:
    value: V-STaR
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: fine-tuned
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: STaR
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: fine-tuned
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: CodeLLaMA
    justification: CodeLLaMA is another model used in the experiments for code generation benchmarks.
    quote: "Fine-tuning LLaMA2 (Touvron et al., 2023) and CodeLLaMA (Rozie\u0300re et al., 2023), we compare V-STaR to other\
      \ self-improvement and verification-based methods, as well a non-iterative V-STaR baseline that uses the same number\
      \ of generation samples to bootstrap a generator and verifier."
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: fine-tuned
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: LLaMA2
    justification: The paper uses LLaMA2 as one of the baseline models for comparison and experimentation.
    quote: train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference
      time to select one solution among many candidate solutions. Running VSTaR for multiple iterations results in progressively
      better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and
      verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: fine-tuned
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: GSM8K
    justification: The GSM8K dataset is used for evaluating the models on math reasoning tasks.
    quote: We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021)...
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HumanEval
    justification: HumanEval is another dataset used for evaluating code generation tasks.
    quote: "We also evaluate the transfer generalization performance of V-STaR using Hendrycks\u2019 MATH (Hendrycks et al.,\
      \ 2021) HumanEval (Chen et al., 2021)."
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MATH
    justification: The MATH dataset is used as another benchmark to evaluate the transfer generalization performance of the
      models for math reasoning.
    quote: We empirically evaluate V-STaR on math reasoning using GSM8K (Cobbe et al., 2021) and a subset of MATH (Hendrycks
      et al., 2021)...
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MBPP
    justification: The MBPP dataset is used for training and evaluating models on code generation tasks.
    quote: '...and on code-generation using MBPP (Austin et al., 2021)...'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries: []
