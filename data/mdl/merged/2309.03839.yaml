title:
  value: Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning
  justification: This is the title as given in the document.
  quote: Bootstrapping Adaptive Human-Machine Interfaces with Offline Reinforcement Learning
description: The paper presents an offline reinforcement learning algorithm, ORBIT (Offline RL-Bootstrapped InTerface), which
  optimizes human-machine interfaces for sequential decision-making tasks like robotic teleoperation. The method combines
  offline pre-training with online fine-tuning to improve the interface's performance, particularly in noisy and high-dimensional
  command settings such as eye-gaze-based control.
type:
  value: empirical
  justification: The research includes user studies with participants performing tasks using the proposed ORBIT method and
    compares the performance with baseline methods.
  quote: "To evaluate ORBIT\u2019s ability to learn an effective interface from real user data, we conduct a user study with\
    \ 12 participants who perform a simulated navigation task by using their eye gaze to modulate a 128dimensional command\
    \ signal from their webcam (illustrated in Fig. 3 and 5)."
primary_research_field:
  name:
    value: Reinforcement Learning
    justification: The paper introduces a reinforcement learning algorithm and discusses its application and evaluation in
      sequential decision-making tasks.
    quote: Recent work on human-in-the-loop reinforcement learning ... lifts these assumptions, but still assumes access to
      either a task-agnostic reward function, task distribution, or prior interface.
  aliases: []
sub_research_fields:
- name:
    value: Human-in-the-loop
    justification: The research focuses on interaction with users, especially using their feedback and commands to improve
      the learning algorithm.
    quote: "To address the high cost of user data, we pre-train value functions on a large offline dataset, and distill them\
      \ into a policy (i.e., an interface) that infers the user\u2019s desired action from the user\u2019s command signal."
  aliases: []
- name:
    value: Reinforcement Learning
    justification: ''
    quote: ''
  aliases: []
models:
- name:
    value: ORBIT
    justification: ORBIT is the main model and contribution described in the paper.
    quote: We call our method the Offline RL-Bootstrapped InTerface (ORBIT).
  aliases:
  - Offline RL-Bootstrapped InTerface
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: COACH
    justification: ''
    quote: "Prior work on COACH [42], [43], TAMER [44], [45], and preference learning [46]\u2013[50] trains RL agents from\
      \ human feedback."
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TAMER
    justification: ''
    quote: "Prior work on COACH [42], [43], TAMER [44], [45], and preference learning [46]\u2013[50] trains RL agents from\
      \ human feedback."
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: preference learning
    justification: ''
    quote: "Prior work on COACH [42], [43], TAMER [44], [45], and preference learning [46]\u2013[50] trains RL agents from\
      \ human feedback."
  aliases: []
  is_contributed:
    value: false
    justification: ''
    quote: ''
  is_executed:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Expert User Data on Navigation Task
    justification: The paper describes collecting a large dataset from an expert user performing navigation tasks using a
      directional interface.
    quote: Hence, we collect a large offline dataset of 1200 episodes of an expert user (the first author) performing tasks
      using their eye gaze and the default directional interface.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Lunar Lander
    justification: The paper describes using simulated user commands on the Lunar Lander game to evaluate their model.
    quote: We further evaluate on a simulated Sawyer pushing task with eye gaze control, and the Lunar Lander game with simulated
      user commands...
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Simulated User Data on Navigation Task
    justification: The paper presents results from navigation tasks using simulated noisy user commands, to validate their
      algorithm's components.
    quote: To perform ablations of ORBIT at a scale that would be impractical for a user study, we simulate noisy, high dimensional
      user commands xt for the navigation task...
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: Stable Baselines3
    justification: The paper mentions using the Stable Baselines3 library for simulations, such as training agents for the
      noisy user commands.
    quote: We train an agent to act as the simulated user using the proximal policy optimization algorithm (PPO) [58] and
      the default hyperparameter values from the Stable Baselines3 library [59].
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
