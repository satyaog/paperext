title:
  value: 'The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset'
  justification: This is the exact title of the research paper provided by the user.
  quote: 'The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset'
description: This paper details the creation and analysis of the BigScience ROOTS corpus, a comprehensive multilingual dataset
  consisting of 1.6TB of text spanning 59 languages. It documents the data curation efforts, including the roles of various
  working groups, the preprocessing and filtering steps, as well as the ethical considerations behind the project. The dataset
  was used to train the 176-billion-parameter BLOOM language model.
type:
  value: empirical
  justification: The paper focuses on the empirical creation, curation, and analysis of a large multilingual dataset. It includes
    specific methods for data cleaning, filtering, and deduplication, and provides statistical analyses of the dataset.
  quote: This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible
    Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages.
primary_research_field:
  name:
    value: Natural Language Processing
    justification: The research focuses on creating a multilingual corpus for training large language models, which is a core
      area of Natural Language Processing.
    quote: As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing,
      especially in multilingual settings.
  aliases:
  - NLP
sub_research_fields:
- name:
    value: Multilingual
    justification: The sub-field focuses on multilingual datasets and their use in training large-scale language models, aligning
      with the paper's goal of creating and analyzing a multilingual dataset for the BLOOM language model.
    quote: This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible
      Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to
      train the 176-billion-parameter BigScience Large Opencollaboration Open-access Multilingual language model.
  aliases: []
- name:
    value: Large Language Models
    justification: ''
    quote: ''
  aliases: []
models:
- name:
    value: BLOOM
    justification: The BLOOM model is trained using the ROOTS corpus, making it the main model discussed in the paper.
    quote: This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible
      Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to
      train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM)(BigScience Workshop,
      2022) language model
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-3
    justification: The paper references GPT-3 as a comparative model to the objectives they had with BLOOM.
    quote: One of the founding goals of BigScience was to train an open-access, massively multilingual LLM, comparable in
      scale to GPT-3 (Brown et al., 2020).
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: T5
    justification: The paper mentions T5 in the context of discussing the datasets used in training language models like T5.
    quote: 'Opt: Open Pre-trained Transformer Language Models, mC4 (Raffel et al., 2020), which have powered the T5 family
      of models.'
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-NeoX 20B
    justification: ''
    quote: 'Figure 7: Tokens per byte for each English-language component for tokenizers trained on this corpus (BLOOM), the
      Pile (GPT-NeoX 20B) and C4 (T5).'
  aliases: []
  is_contributed:
    value: false
    justification: ''
    quote: ''
  is_executed:
    value: false
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: CC100
    justification: CC100 is mentioned as a dataset used for multilingual modeling.
    quote: CC100 (Conneau et al., 2020) which has seen heavy use for multilingual modeling.
  aliases: []
  role: referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ROOTS
    justification: The ROOTS dataset is the main contribution of the paper, meticulously created and processed as a large-scale
      multilingual text corpus.
    quote: assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning
      59 languages.
  aliases: []
  role: contributed
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: the Pile
    justification: CC100 is mentioned as a dataset used for multilingual modeling.
    quote: 'Figure 7: Tokens per byte for each English-language component for tokenizers trained on this corpus (BLOOM), the
      Pile (GPT-NeoX 20B) and C4 (T5).'
  aliases: []
  role: referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: C4
    justification: CC100 is mentioned as a dataset used for multilingual modeling.
    quote: 'Figure 7: Tokens per byte for each English-language component for tokenizers trained on this corpus (BLOOM), the
      Pile (GPT-NeoX 20B) and C4 (T5).'
  aliases: []
  role: referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: mC4
    justification: CC100 is mentioned as a dataset used for multilingual modeling.
    quote: "Exceptions include the Pile (Gao et al., 2020), a curated corpus of datasets for language modeling that has become\
      \ widely used for training state-of-the-art English-language models (Lieber et al., 2021; Smith et al., 2022; Black\
      \ et al., 2022; Zhang et al., 2022), and C4 and mC4 (Raffel et al., 2020; Xue et al., 2020), which have powered the\
      \ T5 family of models; CC100 (Conneau et al., 2020) which has seen heavy use for multilingual modeling; and OSCAR (Ortiz\
      \ Su\xE1rez et al., 2019), which has enabled monolingual non-English models"
  aliases: []
  role: referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: OSCAR
    justification: CC100 is mentioned as a dataset used for multilingual modeling.
    quote: "Exceptions include the Pile (Gao et al., 2020), a curated corpus of datasets for language modeling that has become\
      \ widely used for training state-of-the-art English-language models (Lieber et al., 2021; Smith et al., 2022; Black\
      \ et al., 2022; Zhang et al., 2022), and C4 and mC4 (Raffel et al., 2020; Xue et al., 2020), which have powered the\
      \ T5 family of models; CC100 (Conneau et al., 2020) which has seen heavy use for multilingual modeling; and OSCAR (Ortiz\
      \ Su\xE1rez et al., 2019), which has enabled monolingual non-English models"
  aliases: []
  role: referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Common Crawl
    justification: The paper describes using data from the Common Crawl as part of their dataset processing.
    quote: we retrieved pages corresponding to the target domain names from 18 snapshots archived by Common Crawl in 2020
      and 2021 in Web ARChive (WARC) format (Mohr et al., 2008).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: Indic NLP Library
    justification: Indic NLP Library is used for sentence tokenization for multiple Indian languages as part of the data processing
      pipeline.
    quote: For Bengalic, Gujarati, Hindi, Kannada, Malayalam, Marathi, Punjabi, Tamil, and Telugu, we use the Indic NLP library
      tokenizer (Kunchukuttan, 2020).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: KenLM
    justification: KenLM is used to train 5-gram models after tokenization for OSCAR data filtering.
    quote: KenLM 5-gram models after tokenization (Heafield, 2011) on Wikipedia article openings for every language that was
      extracted from OSCAR.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: NLTK
    justification: NLTK is used for sentence tokenization in multiple languages as part of the data processing pipeline.
    quote: For English, French, Portuguese, and Spanish, we use the NLTK tokenizer (Bird et al., 2009).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: SentencePiece
    justification: SentencePiece is used to train unigram tokenizers for multiple languages for OSCAR data filtering.
    quote: Following Wenzek et al. (2020), we trained SentencePiece unigram tokenizers (Kudo, 2018).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Stanza
    justification: Stanza is used for sentence tokenization in multiple languages as part of the data processing pipeline.
    quote: For Arabic, Catalan, Basque, Indonesian, and Chinese (both simplified and traditional), we use the Stanza tokenizer
      (Qi et al., 2020).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Underthesea
    justification: Underthesea is used for sentence tokenization for the Vietnamese language as part of the data processing
      pipeline.
    quote: For Vietnamese, we use the Underthesea tokenizer.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
