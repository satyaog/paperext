title:
  value: 'Teacher-Student Architecture for Knowledge Distillation: A Survey'
  justification: The provided title accurately represents the focus of the paper, which surveys the Teacher-Student architecture
    for knowledge distillation.
  quote: 'Teacher-Student Architecture for Knowledge Distillation: A Survey'
description: The paper provides a comprehensive review of Teacher-Student architectures for various knowledge distillation
  (KD) objectives, including knowledge compression, expansion, adaptation, and enhancement. It discusses different knowledge
  representations and optimization objectives, summarizes representative learning algorithms and effective distillation schemes,
  and explores recent applications of Teacher-Student architectures in multiple domains such as classification, recognition,
  generation, ranking, and regression. The paper aims to provide insights and guidelines for designing, learning, and applying
  Teacher-Student architectures across various distillation objectives.
type:
  value: empirical
  justification: The paper conducts a comprehensive review and summarizes practical applications, representative learning
    algorithms, and effective distillation schemes, making it an empirical study.
  quote: Different from the existing KD surveys that primarily focus on knowledge compression, this survey first explores
    Teacher-Student architectures across multiple distillation objectives. This survey presents an introduction to various
    knowledge representations and their corresponding optimization objectives.
primary_research_field:
  name:
    value: Knowledge Distillation
    justification: ''
    quote: ''
  aliases: []
sub_research_fields:
- name:
    value: Teacher-Student Architecture
    justification: ''
    quote: ''
  aliases: []
models:
- name:
    value: BERT
    justification: BERT is used as an example of a model in the text classification tasks within the NLP domain, and it's
      referenced in the context of Knowledge Distillation.
    quote: Tang et al. [153] suggest task-specific KD from a BERT teacher model to a bidirectional long short-term memory
      network for sentence classification and matching.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Bert-LSTM
    justification: Bidirectional long short-term memory network (Bi-LSTM) is used as a student model to which knowledge is
      distilled from a BERT model.
    quote: Tang et al. [15] compress BERT [16] to a much light-weight Bi-LSTM [17] for the task of natural language processing.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MuST
    justification: Multi-task Self-Training (MuST) is described for multi-task learning in the context of knowledge distillation.
    quote: Ghiasi et al [10] propose a multi-task self-training (MuST) strategy which uses multiple independent teacher models
      to train one multi-task student model.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: training
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TinyBERT
    justification: TinyBERT is mentioned in the context of compressing models for natural language understanding.
    quote: Jiao et al. [173] propose TinyBERT, a transformer-based KD, to accelerate the inference speed.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: CIFAR10
    justification: CIFAR10 is mentioned alongside CIFAR100 as a common dataset used for evaluating the performance of lightweight
      student networks in knowledge distillation.
    quote: Notably, CIFAR100 and CIFAR10 datasets [32] are commonly employed for evaluation purposes.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: CIFAR100
    justification: CIFAR100 is mentioned as a common dataset used for evaluating the performance of lightweight student networks
      in knowledge distillation.
    quote: Notably, CIFAR100 and CIFAR10 datasets [32] are commonly employed for evaluation purposes.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ImageNet
    justification: ImageNet is used as an example in the knowledge expansion objective where the teacher model is initially
      trained on ImageNet.
    quote: "Xie et al. [5] initially train the teacher model on ImageNet [25], then incorporate a privately collected unlabelled\
      \ dataset, and use the teacher model\u2019s prediction as pseudo ground truth to train a larger student model."
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries: []
