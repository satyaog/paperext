title:
  value: Characterizing and Classifying Developer Forum Posts with their Intentions
  justification: This title accurately reflects the core subject and purpose of the paper, which is to classify developer
    forum posts according to their intentions.
  quote: Characterizing and Classifying Developer Forum Posts with their Intentions
description: This paper proposes an intention detection framework to classify developer forum posts by their intentions (such
  as asking for help, sharing information, etc.) using a transformer-based pre-trained model. The proposed model significantly
  outperforms existing state-of-the-art methods in classifying post intentions.
type:
  value: empirical
  justification: The paper describes an empirical study involving the collection and manual analysis of a dataset from developer
    forums, as well as the development and evaluation of a transformer-based model for intention classification.
  quote: '...we manually annotate the intentions of posts following a rigorous process according to the resulting taxonomy
    of technical forum post intentions. ... we propose an intention prediction framework for technical online posts.'
primary_research_field:
  name:
    value: Natural Language Processing
    justification: The paper focuses on classifying posts from online forums using transformer-based language models, a prominent
      method in NLP.
    quote: In the framework, we employ transformer-based pre-trained language models to generate embeddings for both title
      and description of posts.
  aliases:
  - NLP
sub_research_fields:
- name:
    value: Text Classification
    justification: The core task in the paper is to classify developer forum posts according to their intentions, which falls
      under the category of text classification.
    quote: Our work is performed on a dataset of forum posts provided by our industrial partner that covers multiple developer
      communities (e.g., Stack Overflow, Discourse forums, etc.). Furthermore, we manually annotate the intentions of posts
      following a rigorous process according to the resulting taxonomy of technical forum post intentions. Based on the findings
      and insights from the qualitative study, we propose an intention prediction framework for technical online posts.
  aliases: []
- name:
    value: Text Classification
    justification: ''
    quote: "Keywords Developer Forum \xB7 Online Community \xB7 Intention \xB7 Tag Recommendation."
  aliases: []
models:
- name:
    value: ALBERT
    justification: ALBERT is explicitly mentioned as one of the models tested in the study.
    quote: We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including
      ALBERT.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: BERT
    justification: BERT is one of the pre-trained language models used in the proposed framework.
    quote: We compare the performance of six variants of our intention detection framework with transformer-based PTMs...
      We use the pooler output of the PTMs, which corresponds to the representation of the first token... As by fine-tuning
      this layer with our task, the quality of embedding may be improved for our downstream task.
  aliases:
  - BERT-base
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: BERTOverflow
    justification: BERTOverflow is explicitly mentioned as one of the models tested in the study.
    quote: We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including
      BERTOverflow.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: CodeBERT
    justification: CodeBERT is explicitly mentioned as one of the models tested in the study.
    quote: We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including
      CodeBERT.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DistilBERT
    justification: DistilBERT is explicitly mentioned as one of the models tested in the study.
    quote: We compare the performance of six variants of our intention detection framework with transformer-based PTMs, including
      DistilBERT.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: RoBERTa
    justification: RoBERTa, another pre-trained language model, is evaluated in the proposed framework.
    quote: RoBERTa (Liu et al., 2019) modified some hyper-parameters and training tasks while maintaining the original BERT
      architecture.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Discourse forums
    justification: The dataset also includes posts from Discourse forums, contributing to the diversity of the technical communities
      covered.
    quote: 'The dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities),
      mainly from three different platforms: Stack Exchange, Lithium forums and Discourse Forums.'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Lithium forums
    justification: Lithium forums are another source of developer posts included in the dataset.
    quote: 'The dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities),
      mainly from three different platforms: Stack Exchange, Lithium forums and Discourse Forums.'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Stack Exchange
    justification: The dataset includes posts from Stack Overflow, a major source of technical discussions among developers.
    quote: 'The dump contains primary posts (initial topic-setting posts) from different sources (i.e., online communities),
      mainly from three different platforms: Stack Exchange, Lithium forums and Discourse Forums.'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: Hugging Face
    justification: The Hugging Face library provided the transformer-based pre-trained models used in the study.
    quote: We compare the performances of six variants of our framework with the PTMs mentioned above. We leverage the PTMs
      released in the online community Hugging Face (Wolf et al., 2019) in our experiments.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
