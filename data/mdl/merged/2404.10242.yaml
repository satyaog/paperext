title:
  value: Masked Autoencoders are Scalable Learners of Cellular Biology
  justification: The title is clearly mentioned at the start of the paper.
  quote: Masked Autoencoders are Scalable Learners of Cellular Biology
description: This paper explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders
  (MAEs) when applied to microscopy images in biological research. It introduces a new channel-agnostic MAE architecture and
  demonstrates the effectiveness of MAEs, particularly ViT-based MAEs, in improving the recall of known biological relationships.
  The study uses various large-scale microscopy datasets and proposes novel methodologies like Fourier domain reconstruction
  to stabilize MAE training. The goal is to create robust foundation models for cellular biology that can advance drug discovery.
type:
  value: empirical
  justification: The paper involves experimental evaluations of different models on large-scale datasets, exhibiting characteristics
    typical of an empirical study.
  quote: Our results show that ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks, achieving as
    much as a 11.5% relative improvement... We train masked autoencoders (MAEs)... employing a novel channel-agnostic MAE...
primary_research_field:
  name:
    value: Computer Vision
    justification: The models discussed are for image analysis, which falls under Computer Vision.
    quote: This work explores the scaling properties of weakly supervised classifiers and self-supervised masked autoencoders
      (MAEs) when training with increasingly larger model backbones and microscopy datasets.
  aliases: []
sub_research_fields:
- name:
    value: Microscopy
    justification: The focus is on analyzing microscopy images specifically.
    quote: Featurizing microscopy images for use in biological research remains a significant challenge.
  aliases: []
- name:
    value: Image Analysis
    justification: ''
    quote: ''
  aliases: []
models:
- name:
    value: CA-MAE ViT-B/16
    justification: CA-MAE is developed in the paper to handle channel-agnostic inputs.
    quote: Table 3 shows results for three channel- agnostic MAEs (Sec. 4.2.2).
  aliases:
  - CA-MAE ViT-B/16
  - CA-MAE ViT-B/16+
  - CA-MAE ViT-L/16+
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: training
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DN161
    justification: Comparative Table
    quote: ''
  aliases:
  - WSL DN161
  - DenseNet-161
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: training
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DiNO ViT-S/8
    justification: DiNO is mentioned as a comparative self-supervised learning method.
    quote: DiNO [9] is an SSL method that has been applied to HCS [17, 20, 29, 37, 58] data, however it relies on augmentations
      inspired by natural images, which may not be applicable to HCS image sets.
  aliases:
  - CP-DiNO 1640
  - DiNO ViT-S/8
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: Emerging Properties in Self-Supervised Vision Transformers
    justification: ''
    quote: ''
- name:
    value: MU-Net-L
    justification: The U-Net architecture is explicitly mentioned as a baseline model.
    quote: We adapt U-Nets [56] for use as masked autoencoders (MU-Nets) by training to reconstruct masked sections of input
      images.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: U-Net
    justification: The U-Net architecture is explicitly mentioned as a baseline model.
    quote: We adapt U-Nets [56] for use as masked autoencoders (MU-Nets) by training to reconstruct masked sections of input
      images.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ViT
    justification: The ViT model is used for evaluating MAE architectures in the paper.
    quote: ViT-based MAEs outperform weakly supervised classifiers on a variety of tasks.
  aliases:
  - ViT-L/8
  - ViT-S/8
  - ViT-B/8
  - ViT-B/16
  - ViT-L/16
  - ViT-S/16
  - ViT
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: CPJUMP1
    justification: CPJUMP1 is a subset of the JUMP-CP dataset mentioned for transfer learning evaluation.
    quote: To further evaluate the transferability of our models, we inferenced CPJUMP1, a subset of the JUMP-CP [14] dataset,
      and ran the corresponding benchmarking tasks introduced in Chandrasekaran et al. [13].
  aliases: []
  role: contributed
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: JUMP-CP
    justification: JUMP-CP is explicitly mentioned as a dataset used for evaluation.
    quote: We demonstrate that CA-MAEs effectively generalize by inferring and evaluating on a microscopy image dataset (JUMP-CP)
      generated under different experimental conditions with a different channel structure than our pretraining data (RPI-93M).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HCS Dataset RPI-52M
    justification: RPI-52M is mentioned as a major dataset used for training models.
    quote: RPI-52M (Recursion Phenomics Imageset) is a private dataset with approximately 52 million proprietary images spanning
      6,638 experimental batches and 40 cell types.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HCS Dataset RPI-93M
    justification: RPI-93M is mentioned as a major dataset used for training models.
    quote: RPI-93M is a private dataset with approximately 93 million proprietary images spanning over 10,000 experimental
      batches and 41 cell types.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HCS Dataset RxRx1
    justification: RxRx1 is mentioned multiple times as a dataset used in the study.
    quote: RxRx1 [62] is a publicly-available proprietary Cell Painting dataset with 125,510 images of 4 human cell types
      under 1,108 different siRNA perturbations across 51 experimental batches. A unique feature of this dataset is that it
      is comprised entirely of siRNA perturbations
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HCS Dataset RxRx1-2M
    justification: RxRx1-2M is listed in Table 1 as a dataset used in the study.
    quote: RxRx1-2M is a private version of RxRx1 containing over 1.6 million images across 16 different cell types and uses
      the same set of siRNA perturbations in RxRx1 from additional experimental batches.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: HCS Dataset RxRx3
    justification: RxRx3 is another dataset used in the study, as explicitly mentioned.
    quote: RxRx3 [24] is a publicly-available proprietary Cell Painting dataset with over 2.2 million images of HUVEC cells
      each perturbed with one of 17,063 CRISPR knockouts (using one of six different guides) or 1,674 compounds across 180
      experimental batches
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Imagenet-21k
    justification: Mentioned in comparative table
    quote: Imagenet-21k
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: PyTorch
    justification: PyTorch is explicitly mentioned as the framework used for training the models.
    quote: Models were trained with data-distributed parallel (DDP) training and PyTorch 2.0 for up to 100 epochs on up to
      256 NVIDIA 80GB A100 GPUs, depending on the size of the model and dataset.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
