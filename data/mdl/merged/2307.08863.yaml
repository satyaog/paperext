title:
  value: 'Meta-Value Learning: A General Framework for Learning with Learning Awareness'
  justification: The title succinctly reflects the main contribution and scope of the paper.
  quote: 'META -VALUE L EARNING : A G ENERAL F RAMEWORK FOR L EARNING WITH L EARNING AWARENESS'
description: This paper introduces Meta-Value Learning (MeVa), a novel method for gradient-based learning in multi-agent systems
  that uses a meta-game approach to judge joint policies by their long-term prospects through a meta-value function.
type:
  value: empirical
  justification: The paper evaluates the proposed MeVa method using experiments on several environments and compares its performance
    with other existing methods.
  quote: We analyze the behavior of our method on a toy game and compare to prior work on repeated matrix games.
primary_research_field:
  name:
    value: Deep Learning
    justification: The paper deals with deep learning techniques in the context of multi-agent systems and reinforcement learning.
    quote: "We study P -player differentiable games f : RP \xD7N 7\u2192 RP that map vectors of policies xi to vectors of\
      \ expected returns yi"
  aliases: []
sub_research_fields:
- name:
    value: Multi-Agent
    justification: The paper specifically discusses learning in multi-agent systems and the interactions between agents.
    quote: "Gradient-based learning in multi-agent systems is difficult because the gradient derives from a first-order model\
      \ which does not account for the interaction between agents\u2019 learning processes."
  aliases: []
- name:
    value: Reinforcement Learning
    justification: ''
    quote: ''
  aliases: []
models:
- name:
    value: LOLA
    justification: LOLA is used as a comparison point to demonstrate the improvements made by MeVa.
    quote: We take inspiration from the recent work Learning with Opponent-Learning Awareness (LOLA Foerster et al. (2018a;c)),
      the first general learning algorithm to find tit-for-tat on IPD.
  aliases:
  - Learning with Opponent-Learning Awareness
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Meta-Value Learning
    justification: MeVa is the primary model introduced and explored in the research paper.
    quote: The resulting method, MeVa, is consistent and far-sighted.
  aliases:
  - MeVa
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: Contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Naive
    justification: Naive Learning is used as a baseline for comparison among other methods.
    quote: "The naive application of gradient descent (see \xA72.1) fails to find tit-for-tat on the IPD unless initialized\
      \ sufficiently close to it."
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MFOS
    justification: ''
    quote: 'Table 2: Head-to-head comparison of meta-policies on repeated matrix games'
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MMAML
    justification: ''
    quote: 'Table 2: Head-to-head comparison of meta-policies on repeated matrix games'
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Chicken Game
    justification: The Chicken Game is another environment used in the paper to test the performance of MeVa.
    quote: On the Chicken Game (Table 2c), LOLA exploits every opponent except M-MAML, but does poorly against itself (also
      observed by Lu et al. (2022)).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Iterated Matching Pennies
    justification: IMP is also used to evaluate the MeVa method, focusing on its ability to handle different game dynamics.
    quote: On Iterated Matching Pennies (Table 2b), MeVa exploits naive and LOLA learners, moreso than M-FOS.
  aliases:
  - IMP
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Iterated Prisoner's Dilemma
    justification: IPD is used to evaluate the effectiveness of the proposed MeVa method.
    quote: "In a tournament on repeated matrix games (\xA75.2), MeVa exhibits opponent-shaping behavior, including ZD-extortion\
      \ (Press & Dyson, 2012) on the IPD."
  aliases:
  - IPD
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: JAX
    justification: JAX is used for scientific computing and model implementation in the paper.
    quote: We used the JAX (Bradbury et al., 2018) library for scientific computing.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
