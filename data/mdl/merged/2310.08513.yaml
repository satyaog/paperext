title:
  value: How Connectivity Structure Shapes Rich and Lazy Learning in Neural Circuits
  justification: This is the exact title of the paper as it appears at the beginning of the document.
  quote: HOW CONNECTIVITY STRUCTURE SHAPES RICH AND LAZY LEARNING IN NEURAL CIRCUITS
description: This paper investigates the impact of initial weight structures, particularly the effective rank, on the learning
  dynamics of neural networks. Through both theoretical and empirical analysis, the study finds that high-rank initializations
  yield lazier learning, whereas low-rank initializations foster richer learning. The findings are validated using recurrent
  neural networks on neuroscience-relevant tasks and experimentally-driven initial connectivity patterns.
type:
  value: empirical
  justification: The paper is mostly theoretical but contains a section with empirical experiments.
  quote: ''
primary_research_field:
  name:
    value: Deep Learning Optimization
    justification: The study applies to deep learning, focusing on how connectivity structures in neural networks influence
      learning dynamics.
    quote: In theoretical neuroscience, recent work leverages deep learning tools to explore how some network attributes critically
      influence its learning dynamics.
  aliases: []
sub_research_fields:
- name:
    value: Neural Network Learning Dynamics
    justification: The paper specifically addresses how the learning dynamics of neural networks are influenced by their initial
      weight structures, which falls under the study of neural network learning dynamics.
    quote: here we investigate how the structure of the initial weights \\u2014 in particular their effective rank \\u2014
      influences the network learning regime
  aliases: []
- name:
    value: Neural Network Initialization
    justification: ''
    quote: ''
  aliases: []
- name:
    value: Neuroscience
    justification: The paper explores the implications of neural connectivity structures on learning dynamics, which is a
      key topic within neuroscience.
    quote: Our research highlights the pivotal role of initial weight structures in shaping learning regimes, with implications
      for metabolic costs of plasticity and risks of catastrophic forgetting.
  aliases: []
models:
- name:
    value: Recurrent Neural Networks
    justification: The paper uses RNNs to validate their theoretical findings on learning dynamics.
    quote: We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known
      neuroscience tasks.
  aliases:
  - RNNs
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: train
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Neurogym
    justification: The dataset is used to validate the findings of the paper through various neuroscience tasks.
    quote: We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known
      neuroscience tasks (Figure 1) ... including ... tasks implemented with Neurogym.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Sequential MNIST
    justification: The dataset is used to validate the findings of the paper on a standard machine learning benchmark task.
    quote: We validate our theoretical findings in recurrent neural networks (RNNs) through numerical experiments on well-known
      neuroscience tasks (Figure 1) and demonstrate the applicability to different initial connectivity structures extracted
      from neuroscience data ... and the well-known machine learning benchmark sequential MNIST (sMNIST).
  aliases:
  - sMNIST
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: PyTorch
    justification: The library is used to implement and run the neural network models and experiments described in the paper.
    quote: Our code is accessible at https://github.com/Helena-Yuhan-Liu/BioRNN_RichLazy. We used PyTorch Version 1.10.2
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
