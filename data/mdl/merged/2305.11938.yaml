title:
  value: 'XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages'
  justification: The title is clearly mentioned at the beginning and in the header of the paper.
  quote: 'XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages'
description: The paper presents XTREME-UP, a benchmark designed to evaluate the capabilities of language models on under-represented
  languages (ULs) using scarce data. It focuses on user-centric tasks prevalent among high-resource languages. The benchmark
  comprises 88 languages and nine key technologies, including OCR, ASR, MT, autocomplete, semantic parsing, and transliteration.
  The paper provides baseline results and recommends the development of more inclusive multilingual NLP technologies.
type:
  value: empirical
  justification: The study provides experimental results using the X TREME -U P benchmark to evaluate different language models.
  quote: We evaluate commonly used models on the benchmark.
primary_research_field:
  name:
    value: Natural Language Processing
    justification: The paper focuses on evaluating NLP capabilities such as ASR, OCR, MT, and more, across multiple languages.
    quote: XTREME-UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric
      technologies including ASR, OCR, MT, and information access tasks that are of general utility.
  aliases:
  - NLP
sub_research_fields:
- name:
    value: Multilingual
    justification: The focus is on under-represented languages and tasks that can be tackled using textual data, specifically
      for languages with scarce data.
    quote: XTREME-UP focuses on under-represented languages and user-centric tasks, creating new data for under-represented
      tasks and languages.
  aliases: []
- name:
    value: Low-Resource
    justification: ''
    quote: ''
  aliases: []
- name:
    value: Natural Language Processing
    justification: ''
    quote: ''
  aliases:
  - NLP
- name:
    value: under-represented languages
    justification: ''
    quote: ''
  aliases:
  - UL
models:
- name:
    value: ByT5
    justification: ByT5 is discussed and evaluated within the XTREME-UP benchmark, particularly emphasizing the byte-based
      approaches.
    quote: ByT5-base (Xue et al., 2022), a byte-based multilingual encoder-decoder model.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Flan-PaLM
    justification: Flan-PaLM is highlighted as an in-context learning model evaluated in the XTREME-UP benchmark.
    quote: For the in-context learning setting, we employ Flan-PaLM (Chung et al., 2022), an instruction-tuned version of
      PaLM (Chowdhery et al., 2022).
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: mT5
    justification: mT5 is explicitly mentioned and evaluated across various tasks within the XTREME-UP benchmark.
    quote: The data for each language is sub-sampled to emulate data sizes that can be realistically annotated within a reasonable
      time frame... We evaluate mT5-base (Xue et al., 2021).
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Dakshina
    justification: Used for the transliteration task in the benchmark.
    quote: Most of the data for the task comes from the romanized full-string subset of the Dakshina dataset (Roark et al.,
      2020), in which 10,000 Wikipedia sentences written in the native scripts of the 12 languages were human-romanized by
      native speakers, resulting in parallel sentences in the native and Latin scripts.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: FLEURS
    justification: FLEURS is used for evaluating ASR tasks in the benchmark.
    quote: We employ the FLEURS dataset (Conneau et al., 2023) consisting of recordings in 102 languages for sentences from
      FLORES-101 (Goyal et al., 2022), which were translated from English Wikipedia to 101 languages.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: FLORES-101
    justification: Adapted for machine translation tasks in the benchmark.
    quote: "The dataset is adapted from FLORES-101 (Goyal et al., 2022), repurposing half of the dataset\u2019s original development\
      \ set as a training set."
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MasakhaNER
    justification: The dataset provides data for the named entity recognition task in the benchmark.
    quote: The dataset contains processed data from MasakhaNER (Adelani et al., 2021) and MasakhaNER 2.0 (Adelani et al.,
      2022).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MasakhaNER 2.0
    justification: The dataset provides data for the named entity recognition task in the benchmark.
    quote: The dataset contains processed data from MasakhaNER (Adelani et al., 2021) and MasakhaNER 2.0 (Adelani et al.,
      2022).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: TyDi QA
    justification: Used for evaluating question answering tasks in the X TREME -U P benchmark.
    quote: In the in-language QA task, both the question and passage are in the same language. In this task, original questions
      and passages are from the TyDi QA dataset (Clark et al., 2020).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: XOR-TyDi QA
    justification: ''
    quote: 'Original datset names: TyDi QA, XOR-TyDi QA'
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Universal Dependencies
    justification: This dataset was used to test models' predictive capabilities rather than their memorization capabilities.
    quote: We process high-quality natural language data from Universal Dependencies (de Marneffe et al., 2021), which we
      deduplicate against mC4 (Xue et al., 2021), the most common multilingual pre-training corpus in order to test models
      predictive rather than memorization capabilities.
  aliases:
  - UD
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Google Books
    justification: ''
    quote: We retrieve books that are in the public domain on Google Books.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: UL-OCR
    justification: ''
    quote: Dataset title UL-OCR
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MTOP
    justification: ''
    quote: For XTREME-UP, we adapted the MTOP (Li et al., 2021) test dataset
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: T5X
    justification: T5X was used to train models on the XTREME-UP benchmark.
    quote: Models were trained using seqio and T5X (Roberts et al., 2022) on TPUs (Kumar et al., 2019).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: seqio
    justification: seqio was used for training models on the benchmark.
    quote: Models were trained using seqio and T5X (Roberts et al., 2022) on TPUs (Kumar et al., 2019).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
