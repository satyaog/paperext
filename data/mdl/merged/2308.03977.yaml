title:
  value: 'PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning'
  justification: The title is clearly stated on the first page of the provided document.
  quote: 'PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning'
description: "The paper introduces a new generation of photorealistic synthetic datasets called PUG (Photorealistic Unreal\
  \ Graphics), designed for representation learning research. Leveraging the Unreal Engine, these datasets provide granular\
  \ control over various factors like pose, background, size, texture, and lighting, aiming to bridge the gap between synthetic\
  \ and real image data. The paper discusses the creation of four key datasets\u2014PUG Animals, PUG ImageNet, PUG SPAR, and\
  \ PUG AR4T\u2014demonstrating their utility in evaluating and fine-tuning vision-language models, as well as assessing model\
  \ robustness and out-of-distribution generalization."
type:
  value: empirical
  justification: The paper extensively discusses the implementation of synthetic datasets and their applications in various
    experimental settings, demonstrating empirical results.
  quote: In this paper, we demonstrate the potential of PUG to enable more rigorous evaluations of vision models.
primary_research_field:
  name:
    value: Computer Vision
    justification: The paper focuses on the creation and utilization of synthetic image datasets for representation learning
      in vision models.
    quote: We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic
      Unreal Graphics) environments and datasets for representation learning.
  aliases: []
sub_research_fields:
- name:
    value: Representation Learning
    justification: The datasets and environments are designed to improve representation learning by providing controllable
      and realistic synthetic data.
    quote: We introduce the Photorealistic Unreal Graphics (PUG) environments, a family of 3D graphics environments that leverage
      Unreal Engine for rendering image data for representation learning research.
  aliases: []
- name:
    value: Synthetic Data for Representation Learning
    justification: The paper specifically addresses the use of synthetic datasets to enhance representation learning, focusing
      on aspects such as photorealism and controllability.
    quote: 'In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation
      of interactive environments for representation learning research, that offer both controllability and realism.'
  aliases: []
models:
- name:
    value: BLIP
    justification: BLIP is mentioned in the context of evaluating vision-language models similar to other models like CLIP.
    quote: 'First, we feed images and their corresponding captions to 9 pretrained vision-language models including multiple
      CLIP models Radford et al. [2021], NegCLIP Yuksekgonul et al. [2023] Flava Singh et al. [2022], BLIP Li et al. [2022b]
      and X-VLM Zeng et al. [2021] and collect their embeddings of PUG: Animals images and created captions.'
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: negCLIP
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Flava
    justification: ''
    quote: ''
  aliases:
  - Flava (PMD 70M)
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: CLIP
    justification: ''
    quote: ''
  aliases:
  - CLIPViTB32 (2B)
  - CLIPViTB32 (400M)
  - CLIPViTL14 (2B)
  - CLIPViTL14 (400M)
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: X-VLM
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: DINOv2
    justification: DINOv2 is mentioned in the context of assessing model robustness across various factors.
    quote: We also provide a collection of objects with mappings to classes in the popular ImageNet dataset, enabling researchers
      to probe the robustness of SoTA vision models without retraining, such as DINOv2.
  aliases:
  - DINOv2 (LVD-142M)
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ResNet50
    justification: ResNet50 is clearly mentioned in the context of experiments evaluating model robustness to various factors.
    quote: In Figure 3, we present our results training a ResNet50 with different held out factors.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ResNet101
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ViT-L/14
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ViT-B/14
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: ViT-B/32
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: BiT (JFT300M)
    justification: ''
    quote: ''
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Swin-B
    justification: 'Swin-B is discussed in the context of robustness evaluations on the PUG: ImageNet dataset.'
    quote: For example, the pretrained ViT-B32 trained on ImageNet-21k is better on the ImageNet validation set compared to
      a Swin-B, but offers worse robustness across all factors.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: referenced
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: ImageNet
    justification: ImageNet is frequently mentioned as a standard dataset for training and evaluating models.
    quote: The main purpose of this dataset is to provide a novel useful benchmark, paralleling ImageNet, but for fine-grained
      evaluation of the robustness of image classifiers, along several factors of variation.
  aliases: []
  role: referenced
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: 'PUG: Animals'
    justification: 'PUG: Animals is one of the primary datasets introduced and used for various experiments in the paper.'
    quote: 'We present PUG: Animals for research on out-of-distribution (OOD) generalization and to study the representational
      space of foundation models.'
  aliases: []
  role: contributed
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: 'PUG: AR4T'
    justification: 'The paper discusses PUG: AR4T as a dataset for fine-tuning vision-language models.'
    quote: 'In addition, we introduce PUG: AR4T for fine-tuning vision-language models and use it to demonstrate the reliability
      of PUG: SPAR in contrast to other benchmarks.'
  aliases: []
  role: contributed
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: 'PUG: ImageNet'
    justification: 'The paper introduces PUG: ImageNet specifically as an additional test set for robustness evaluations.'
    quote: 'We introduce PUG: ImageNet as an additional robustness test set to ImageNet, containing a rich set of factor changes
      such as pose, background, size, texture, and lighting.'
  aliases: []
  role: contributed
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: 'PUG: SPAR'
    justification: 'PUG: SPAR is introduced for evaluating vision-language models.'
    quote: 'We introduce PUG: SPAR for evaluating vision-language models. We use it to demonstrate how synthetic data can
      be utilized to address known benchmark limitations.'
  aliases: []
  role: contributed
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: TorchMultiverse
    justification: TorchMultiverse is introduced as a Python library used for creating and manipulating the PUG datasets.
    quote: In addition to pre-rendered static image datasets, we also introduce the TorchMultiverse python library, which
      offers a simple python interface to enable easily controlled dataset creation from any given PUG environment.
  aliases: []
  role: contributed
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Transformers
    justification: ''
    quote: ''
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: CLIP
    justification: ''
    quote: CLIP models all come from OpenAI CLIP https://github.com/openai/CLIP. We use the versions with ResNet50, ResNet101,
      ViT-L/14, ViT-B/16, ViT-B/32
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
