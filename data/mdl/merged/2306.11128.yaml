title:
  value: 'CAMMARL : Conformal Action Modeling in Multi Agent Reinforcement Learning'
  justification: The title is clearly stated at the top of the text.
  quote: 'CAMMARL : Conformal Action Modeling in Multi Agent Reinforcement Learning'
description: "This research proposes a novel multi-agent reinforcement learning (MARL) algorithm named CAMMARL. The main idea\
  \ is to model the actions of other agents as conformal prediction sets, which provide high-probability guarantees of containing\
  \ the true actions. The algorithm aims to optimize an autonomous agent\u2019s decision-making capabilities in multi-agent\
  \ environments. The performance of CAMMARL is demonstrated through various experiments in cooperative multi-agent tasks."
type:
  value: empirical
  justification: The paper includes several experiments to demonstrate the performance of the proposed CAMMARL algorithm in
    different multi-agent environments.
  quote: Through several experiments in two fully cooperative multi-agent tasks, we show that CAMMARL elevates the capabilities
    of an autonomous agent in MARL by modeling conformal prediction sets over the behavior of other agents.
primary_research_field:
  name:
    value: Deep Learning
    justification: The focus is on a novel algorithm for multi-agent reinforcement learning, a subfield of machine learning
      which is inherently a part of deep learning.
    quote: In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL, which involves
      modeling the actions of other agents in different situations in the form of confident sets.
  aliases:
  - DL
sub_research_fields:
- name:
    value: Multi-Agent
    justification: The core contribution of the paper is an algorithm designed for multi-agent reinforcement learning scenarios.
    quote: In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL , which involves
      modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their
      true actions with a high probability.
  aliases: []
- name:
    value: Reinforcement Learning
    justification: The study specifically deals with reinforcement learning and extends it to multi-agent settings.
    quote: Multi-agent reinforcement learning (MARL). Numerous deep MARL research works that focus on partial observability
      in fully cooperative settings indirectly involve reasoning about the intentions of teammates or opponents in an environment.
  aliases:
  - RL
models:
- name:
    value: Conformal Action Modeling in Multi Agent Reinforcement Learning
    justification: This is the main model proposed by the authors.
    quote: In this article, we propose a novel multi-agent reinforcement learning (MARL) algorithm CAM MARL , which involves
      modeling the actions of other agents in different situations in the form of confident sets, i.e., sets containing their
      true actions with a high probability.
  aliases:
  - CAMMARL
  is_contributed:
    value: true
    justification: Role:['contributed', 'used', 'referenced']
    quote: contributed
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: trained
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: proximal policy optimization
    justification: ''
    quote: We use proximal policy optimization (PPO) (Schulman et al., 2017) to update the decision-making policy for both
      the RL agents
  aliases:
  - PPO
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: multi-layer perceptron
    justification: ''
    quote: For the individual actor and critic networks, we used 2 fully-connected multi-layer perceptron (MLP) layers.
  aliases:
  - MLP
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ''
    quote: ''
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: Cooperative Navigation
    justification: This dataset/environment is used in the experiments to test CAMMARL.
    quote: 'We focus on four cooperative multi-agent environments: Cooperative Navigation.'
  aliases:
  - CN
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Google Football
    justification: This dataset/environment is used in the experiments to test CAMMARL.
    quote: Finally, we also run CAMMARL in Google Football, where 3 agents try to score a goal against a defender and a keeper
      in a game of football.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Level-based foraging
    justification: This dataset/environment is used in the experiments to test CAMMARL.
    quote: 'We focus on four cooperative multi-agent environments: Level-based Foraging.'
  aliases:
  - LBF
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Pressure Plate
    justification: This dataset/environment is used in the experiments to test CAMMARL.
    quote: Finally, we also run CAMMARL in Google Football, where 3 agents try to score a goal against a defender and a keeper
      in a game of football.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries: []
