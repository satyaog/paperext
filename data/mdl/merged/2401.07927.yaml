title:
  value: Are self-explanations from Large Language Models faithful?
  justification: The extracted title matches exactly as given in the user prompt.
  quote: Are self-explanations from Large Language Models faithful?
description: The paper investigates the faithfulness of self-explanations generated by instruction-tuned Large Language Models
  (LLMs). It proposes a method to measure faithfulness using self-consistency checks across different models and datasets.
  The findings indicate that faithfulness is highly model and task-dependent, suggesting that self-explanations from LLMs
  should not be trusted in general.
type:
  value: empirical
  justification: The research involves experimental evaluation of language models and their explanations, making it empirical.
  quote: In this paper, we evaluate the faithfulness of the following types of self-explanations.
primary_research_field:
  name:
    value: Natural Language Processing
    justification: The paper primarily deals with the interpretability and faithfulness of explanations generated by NLP models.
    quote: Instruction-tuned large language models (LLMs), such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023),
      Mistral (Jiang et al., 2023), or GPT4 (OpenAI, 2023), are increasingly becoming mainstream among the general population,
      due to their capabilities and availability.
  aliases:
  - NLP
sub_research_fields:
- name:
    value: Interpretability
    justification: The focus of the paper is on evaluating the faithfulness of self-explanations generated by LLMs, which
      falls under interpretability in NLP.
    quote: "Our investigation reveals that self-explanations\u2019 faithfulness is highly model and dataset-dependent."
  aliases:
  - interpretability-faithfulness
models:
- name:
    value: Falcon
    justification: Falcon is one of the models used for evaluating faithfulness in self-explanations.
    quote: For example, with sentiment classification, counterfactuals are more faithful for Llama2, importance measures for
      Mistral, and redaction for Falcon 40B.
  aliases:
  - Falcon-7B
  - Falcon-40B
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Llama2
    justification: The paper explicitly mentions using Llama2 for evaluating self-explanations.
    quote: For example, with sentiment classification, counterfactuals are more faithful for Llama2.
  aliases:
  - Llama2-70B
  - Llama2-7b
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: true
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: true
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Llama2-7b
    justification: ''
    quote: List of models used in this paper. All models are publicly available.
  aliases: []
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: Mistral
    justification: Mistral is another model evaluated for its faithfulness in providing self-explanations.
    quote: For example, with sentiment classification, counterfactuals are more faithful for Llama2, importance measures for
      Mistral, and redaction for Falcon 40B.
  aliases:
  - Mistral v0.1-7B
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: used
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: inference
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: GPT-4
    justification: The paper references GPT-4 as one of the mainstream instruction-tuned LLMs used for interpretability.
    quote: Instruction-tuned large language models (LLMs), such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023),
      Mistral (Jiang et al., 2023), or GPT-4 (OpenAI, 2023), are increasingly becoming mainstream among the general population,
      due to their capabilities and availability.
  aliases:
  - chatgpt
  is_contributed:
    value: false
    justification: Role:['contributed', 'used', 'referenced']
    quote: Referenced
  is_executed:
    value: false
    justification: ModelMode:['trained', 'fine-tuned', 'inference']
    quote: Inference
  is_compared:
    value: false
    justification: ''
    quote: ''
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
datasets:
- name:
    value: IMDB
    justification: The paper uses the IMDB dataset for evaluating faithfulness in sentiment classification tasks.
    quote: For example, regarding Llama2 (70B), counterfactuals only work with IMDB.
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: MCTest
    justification: The paper uses the MCTest dataset for evaluating faithfulness in multi-choice classification tasks.
    quote: multi-choice classification (bAbI and MCTest Weston et al. 2016; Richardson et al. 2013)
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: RTE
    justification: The paper uses the RTE dataset for evaluating faithfulness in two-paragraph classification tasks.
    quote: two-paragraph classification (RTE Dagan et al. 2006).
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
- name:
    value: bAbI
    justification: The paper uses the bAbI dataset for multi-choice classification tasks.
    quote: multi-choice classification (bAbI...Weston et al. 2016)
  aliases: []
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
libraries:
- name:
    value: HuggingFace
    justification: The paper uses the Hugging Face library for model implementation and experimentation.
    quote: All generation inferences were made using Text Generation Inference (TGI) version 1.1.0 by HuggingFace (https://github.com/huggingface/
      text-generation-inference).
  aliases:
  - Text Generation Inference
  - TGI
  role: used
  referenced_paper_title:
    value: ''
    justification: ''
    quote: ''
