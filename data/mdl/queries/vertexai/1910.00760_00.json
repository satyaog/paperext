{
  "paper": "1910.00760.txt",
  "words": 7599,
  "extractions": {
    "title": {
      "value": "Efficient Graph Generation with Graph Recurrent Attention Networks",
      "justification": "This is the title of the given research paper.",
      "quote": "Efficient Graph Generation with Graph Recurrent Attention Networks"
    },
    "description": "The paper introduces Graph Recurrent Attention Networks (GRANs), a family of deep generative models for graphs. GRANs generate graphs one block of nodes and edges at a time, using Graph Neural Networks (GNNs) with attention to capture dependencies between generated and to-be-generated parts of the graph. The model handles node orderings by marginalizing over a family of canonical orderings and parameterizes the output distribution per block using a mixture of Bernoullis to capture edge correlations. Experiments on synthetic and real-world graph datasets demonstrate that GRANs achieve state-of-the-art time efficiency and sample quality, even for large graphs with up to 5,000 nodes.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a new model and conducts experiments to evaluate its performance, making it an empirical research paper.",
      "quote": "On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models."
    },
    "primary_research_field": {
      "name": {
        "value": "graph generative models",
        "justification": "The paper focuses on developing a new deep generative model for graphs.",
        "quote": "We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs)."
      },
      "aliases": [
        "graph generative models"
      ]
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "Graph Recurrent Attention Networks",
          "justification": "The paper introduces Graph Recurrent Attention Networks (GRANs) as a new family of efficient and expressive deep generative models of graphs.",
          "quote": "We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs)."
        },
        "aliases": [
          "GRAN",
          "GRANs",
          "Graph Recurrent Attention Networks",
          "Graph Recurrent Attention Network"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The authors propose a novel graph generative model called GRAN in this paper.",
          "quote": "In this paper, we propose an efficient auto-regressive graph generative model, called Graph Recurrent Attention Network (GRAN), which overcomes the shortcomings of previous approaches."
        },
        "is_executed": {
          "value": true,
          "justification": "The authors provide experimental results for their proposed GRAN model on synthetic and real-world graph datasets.",
          "quote": "On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper benchmarks GRANs against several other graph generative models, including GraphVAE, GraphRNN, and GraphRNN-S.",
          "quote": "In this section we empirically verify the effectiveness of our model on both synthetic and real graph datasets with drastically varying sizes and characteristics."
        },
        "referenced_paper_title": {
          "value": "Efficient Graph Generation with Graph Recurrent Attention Networks",
          "justification": "This is the paper we are currently analyzing.",
          "quote": "Efficient Graph Generation with Graph Recurrent Attention Networks"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 13908
  }
}