{
  "paper": "2403.05530.txt",
  "words": 30556,
  "extractions": {
    "title": {
      "value": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "justification": "The title is explicitly provided in the paper.",
      "quote": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
    },
    "description": "This research paper introduces Gemini 1.5 Pro, a multimodal AI model by Google. Gemini 1.5 Pro specializes in understanding extremely long contexts of up to 10 million tokens, encompassing various modalities like text, video, and audio. Notably, it achieves near-perfect recall in retrieval tasks and showcases advancements in long-document QA, long-video QA, and long-context ASR, while matching or surpassing the performance of its predecessor, Gemini 1.0 Ultra, on various benchmarks. The paper emphasizes the model\\'s efficiency, its novel mixture-of-experts architecture, and its ability to translate a low-resource language like Kalamang solely by processing its grammar manual. The authors highlight the need for new evaluation benchmarks and methodologies to properly assess the capabilities of such advanced models.",
    "type": {
      "value": "empirical",
      "justification": "The paper primarily presents empirical research based on experiments and benchmark results demonstrating the capabilities of the new model, Gemini 1.5 Pro.",
      "quote": "To measure the effectiveness of our model’s long-context capabilities, we conduct experiments on both synthetic and real-world tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Artificial Intelligence",
        "justification": "The primary focus of the paper is on the development and capabilities of a multimodal AI model, emphasizing its understanding and processing of long context information across various modalities like text, video, and audio.",
        "quote": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Large Language Models",
          "justification": "The paper centers around Large Language Models, focusing on their evolution towards longer context windows and the introduction of Gemini 1.5 Pro as a significant advancement in this field.",
          "quote": "The ability to model data of increasingly longer contexts has tracked the development of more general and capable language models, from the now toy 2-gram language model proposed by Shannon (1948), to the modern n-gram models of the 1990s & 2000s typically constrained to 5 tokens of context (Brants et al., 2007; Chen and Goodman, 1999; Jelinek, 1998; Kneser and Ney, 1995), to recurrent neural networks language models from the 2010s which could effectively condition on hundreds of tokens (Jozefowicz et al., 2016; Mikolov et al., 2010), to the modern Transformer (Vaswani et al., 2017) which can condition on hundreds of thousands of tokens (Anthropic, 2023a). Gemini 1.5 Pro continues this trend by extending language model context lengths by over an order of magnitude."
        },
        "aliases": [
          "LLM",
          "Large language model"
        ]
      },
      {
        "name": {
          "value": "Multimodal AI",
          "justification": "The paper heavily emphasizes the multimodal nature of Gemini 1.5 Pro, highlighting its capabilities in processing and understanding information across various modalities such as text, audio, and video.",
          "quote": "We present our latest multimodal model from the Gemini line: Gemini 1.5 Pro."
        },
        "aliases": [
          "Multimodal AI"
        ]
      },
      {
        "name": {
          "value": "Long-context AI",
          "justification": "The paper focuses on long-context understanding as a key advancement in AI, highlighting Gemini 1.5 Pro's ability to process and recall information from extremely long contexts, marking a significant departure from previous models with limited context windows.",
          "quote": "This is our first release from Gemini 1.5, a new family of highly-capable multimodal models which incorporates a novel mixture-of-experts architecture as well as major advances in training and serving infrastructure that allow it to push the boundary of efficiency, reasoning, and long-context performance."
        },
        "aliases": [
          "Long-context AI",
          "long-context understanding"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Gemini 1.5 Pro",
          "justification": "The name of the model is explicitly provided in the title and throughout the text.",
          "quote": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
        },
        "aliases": [
          "Gemini 1.5 Pro",
          "Gemini 1.5",
          "1.5 Pro"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper introduces Gemini 1.5 Pro as a new model from Google.",
          "quote": "In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides extensive experimental results, demonstrating the execution of Gemini 1.5 Pro on various tasks.",
          "quote": "To measure the effectiveness of our model’s long-context capabilities, we conduct experiments on both synthetic and real-world tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Gemini 1.5 Pro with several other LLMs including GPT-4 Turbo, Claude 2.1, Gemini 1.0 Ultra, and Gemini 1.0 Pro.",
          "quote": "Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra’s state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro’s long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k)."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "No referenced paper directly introducing Gemini 1.5 Pro was found.",
          "quote": "None"
        }
      },
      {
        "name": {
          "value": "Gemini 1.0 Pro",
          "justification": "The model name is explicitly stated in the text.",
          "quote": "Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train."
        },
        "aliases": [
          "Gemini 1.0 Pro",
          "1.0 Pro"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Gemini 1.0 Pro is a previous model released by Google.",
          "quote": "Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper presents experimental results obtained by executing Gemini 1.0 Pro on various tasks.",
          "quote": "To measure the effectiveness of our model’s long-context capabilities, we conduct experiments on both synthetic and real-world tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Gemini 1.5 Pro with Gemini 1.0 Pro on various tasks.",
          "quote": "Gemini 1.5 Pro surpasses Gemini 1.0 Pro and performs at a similar level to 1.0 Ultra on a wide array of benchmarks while requiring significantly less compute to train."
        },
        "referenced_paper_title": {
          "value": "Gemini: a family of highly capable multimodal models",
          "justification": "The paper references the Gemini 1.0 technical report.",
          "quote": "We refer readers to the Gemini 1.0 Technical Report (Gemini-Team et al., 2023) for further information."
        }
      },
      {
        "name": {
          "value": "Gemini 1.0 Ultra",
          "justification": "The model is explicitly mentioned in the text.",
          "quote": "A host of improvements made across nearly the entire model stack (architecture, data, optimization and systems) allows Gemini 1.5 Pro to achieve comparable quality to Gemini 1.0 Ultra (see Section 5), while using significantly less training compute and being significantly more efficient to serve."
        },
        "aliases": [
          "Gemini 1.0 Ultra",
          "1.0 Ultra"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Gemini 1.0 Ultra is a previously released model by Google.",
          "quote": "A host of improvements made across nearly the entire model stack (architecture, data, optimization and systems) allows Gemini 1.5 Pro to achieve comparable quality to Gemini 1.0 Ultra (see Section 5), while using significantly less training compute and being significantly more efficient to serve."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper presents experimental results obtained by executing Gemini 1.0 Ultra.",
          "quote": "Overall, we find that Gemini 1.5 Pro greatly surpasses Gemini 1.0 Pro, performing better on the vast majority of benchmarks (i.e., 29/33), increasing the margin in particular for Math, Science and Reasoning (+38.4%), Multilinguality (+22.3%), Video Understanding (+16.9%), Image Understanding (+6.5%), and Code (+8.9%) (see Table 7 for breakdowns). However, a more striking comparison is the one with Gemini 1.0 Ultra, a state-of-the-art model across many capabilities. Despite Gemini 1.5 Pro using significantly less training compute and being more efficient to serve, we find Gemini 1.5 Pro to perform better on more than half of the benchmarks (19/33), in particular on text (12/15) and many of the vision benchmarks (6/13)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of Gemini 1.5 Pro with Gemini 1.0 Ultra on different benchmarks.",
          "quote": "The ability to model data of increasingly longer contexts has tracked the development of more general and capable language models, from the now toy 2-gram language model proposed by Shannon (1948), to the modern n-gram models of the 1990s & 2000s typically constrained to 5 tokens of context (Brants et al., 2007; Chen and Goodman, 1999; Jelinek, 1998; Kneser and Ney, 1995), to recurrent neural networks language models from the 2010s which could effectively condition on hundreds of tokens (Jozefowicz et al., 2016; Mikolov et al., 2010), to the modern Transformer (Vaswani et al., 2017) which can condition on hundreds of thousands of tokens (Anthropic, 2023a). Gemini 1.5 Pro continues this trend by extending language model context lengths by over an order of magnitude."
        },
        "referenced_paper_title": {
          "value": "Gemini: a family of highly capable multimodal models",
          "justification": "The paper references the Gemini 1.0 technical report.",
          "quote": "We refer readers to the Gemini 1.0 Technical Report (Gemini-Team et al., 2023) for further information."
        }
      },
      {
        "name": {
          "value": "Claude 2.1",
          "justification": "Both model names, Claude 2 and Claude 2.1 are explicitly stated in the text.",
          "quote": "Notably, among the state-of-the-art LLMs, Anthropic has successfully extended the context of their text-only Claude 2 model to 100k tokens, while OpenAI has recently released GPT-4 Turbo reaching 128k tokens. Finally, the latest addition to the series was Claude 2.1 with a context window of 200k tokens."
        },
        "aliases": [
          "Claude 2",
          "Claude 2.1"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Claude 2 and Claude 2.1 are models developed by Anthropic.",
          "quote": "Notably, among the state-of-the-art LLMs, Anthropic has successfully extended the context of their text-only Claude 2 model to 100k tokens, while OpenAI has recently released GPT-4 Turbo reaching 128k tokens. Finally, the latest addition to the series was Claude 2.1 with a context window of 200k tokens."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper doesn't directly execute Claude 2 or 2.1 but references and compares their performance.",
          "quote": "Compared to Claude 2.1 with a 200k token context window, Gemini 1.5 Pro achieves a 100% recall at 200k tokens, surpassing Claude 2.1’s 98%."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Gemini 1.5 Pro with Claude 2 and Claude 2.1 in terms of context length and performance.",
          "quote": "Notably, among the state-of-the-art LLMs, Anthropic has successfully extended the context of their text-only Claude 2 model to 100k tokens, while OpenAI has recently released GPT-4 Turbo reaching 128k tokens. Finally, the latest addition to the series was Claude 2.1 with a context window of 200k tokens."
        },
        "referenced_paper_title": {
          "value": "Model Card and Evaluations for Claude Models",
          "justification": "The paper cites a report on Claude models.",
          "quote": "Anthropic. Model Card and Evaluations for Claude Models, 2023a."
        }
      },
      {
        "name": {
          "value": "GPT-4 Turbo",
          "justification": "The model name is explicitly mentioned in the text.",
          "quote": "Notably, among the state-of-the-art LLMs, Anthropic has successfully extended the context of their text-only Claude 2 model to 100k tokens, while OpenAI has recently released GPT-4 Turbo reaching 128k tokens. Finally, the latest addition to the series was Claude 2.1 with a context window of 200k tokens."
        },
        "aliases": [
          "GPT-4 Turbo"
        ],
        "is_contributed": {
          "value": false,
          "justification": "GPT-4 Turbo is a model from OpenAI.",
          "quote": "Notably, among the state-of-the-art LLMs, Anthropic has successfully extended the context of their text-only Claude 2 model to 100k tokens, while OpenAI has recently released GPT-4 Turbo reaching 128k tokens. Finally, the latest addition to the series was Claude 2.1 with a context window of 200k tokens."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses the GPT-4 Turbo API in its evaluations.",
          "quote": "For reference, we report results for GPT-4 Turbo up to the 128K sequence length supported by their API."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Gemini 1.5 Pro with GPT-4 Turbo in terms of context length and performance.",
          "quote": "Notably, among the state-of-the-art LLMs, Anthropic has successfully extended the context of their text-only Claude 2 model to 100k tokens, while OpenAI has recently released GPT-4 Turbo reaching 128k tokens. Finally, the latest addition to the series was Claude 2.1 with a context window of 200k tokens."
        },
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "The paper references the GPT-4 technical report.",
          "quote": "OpenAI. GPT-4 Technical Report. 2023."
        }
      },
      {
        "name": {
          "value": "GPT-4V",
          "justification": "Both model names, GPT-4 and GPT-4V, are explicitly stated in the text.",
          "quote": "As Figure 8 shows, Gemini 1.5 Pro successfully answers this question across a breadth of video lengths and a range of randomly inserted needle locations in the 10.5 hour video. In contrast, the GPT-4V API supports video lengths only up to around the first 3 minutes."
        },
        "aliases": [
          "GPT-4",
          "GPT-4V"
        ],
        "is_contributed": {
          "value": false,
          "justification": "GPT-4 and GPT-4V are models developed by OpenAI.",
          "quote": "As Figure 8 shows, Gemini 1.5 Pro successfully answers this question across a breadth of video lengths and a range of randomly inserted needle locations in the 10.5 hour video. In contrast, the GPT-4V API supports video lengths only up to around the first 3 minutes."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper leverages the GPT-4V API in its evaluations.",
          "quote": "As Figure 8 shows, Gemini 1.5 Pro successfully answers this question across a breadth of video lengths and a range of randomly inserted needle locations in the 10.5 hour video. In contrast, the GPT-4V API supports video lengths only up to around the first 3 minutes."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Gemini 1.5 Pro with GPT-4 and its vision variant GPT-4V in tasks like Video Haystack and Long-context Video QA.",
          "quote": "As Figure 8 shows, Gemini 1.5 Pro successfully answers this question across a breadth of video lengths and a range of randomly inserted needle locations in the 10.5 hour video. In contrast, the GPT-4V API supports video lengths only up to around the first 3 minutes."
        },
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "The paper references the GPT-4 technical report.",
          "quote": "OpenAI. GPT-4 Technical Report. 2023."
        }
      },
      {
        "name": {
          "value": "Universal Speech Model",
          "justification": "The full name of the model and its acronym are provided in the text.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (OpenAI, 2023)."
        },
        "aliases": [
          "USM",
          "Universal Speech Model"
        ],
        "is_contributed": {
          "value": false,
          "justification": "USM is a model developed by Google.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (OpenAI, 2023)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses USM in its evaluations.",
          "quote": "The Table 5 below shows that the 1.0 Pro model, when evaluated on transcribing 15-minute videos without segmentation, has a WER of 100% due to a mismatch between training and testing audio lengths. When we segment the videos every 30 seconds and pass the textual content of the language model across each segment boundary, the 1.0 Pro model can achieve a WER of 7.8%. The USM model with a CTC decoder, while robust to long segments, achieves a WER of 8.8%."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Gemini 1.5 Pro with the Universal Speech Model (USM) in the context of Automatic Speech Recognition.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (OpenAI, 2023)."
        },
        "referenced_paper_title": {
          "value": "Google usm: Scaling automatic speech recognition beyond 100 languages",
          "justification": "The paper cites the paper introducing USM.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (OpenAI, 2023)."
        }
      },
      {
        "name": {
          "value": "Whisper",
          "justification": "The model name is explicitly mentioned in the text.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (OpenAI, 2023). Note that ASR tasks report a word error rate (WER) metric, where a lower number is better."
        },
        "aliases": [
          "Whisper"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Whisper is a model developed by OpenAI.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (OpenAI, 2023). Note that ASR tasks report a word error rate (WER) metric, where a lower number is better."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses Whisper in its evaluations.",
          "quote": "Specifically, to compare against Whisper, we chunk the audio input into 30 second segments, transcribe the audio using the model to produce a text transcript, concatenate the transcripts for each chunk, and finally prompt GPT-4 Turbo to find the “secret keyword” given the text transcript."
        },
        "is_compared": {
          "value": true,
          "justification": "Gemini 1.5 Pro's performance is compared with Whisper in Automatic Speech Recognition and Audio Haystack tasks.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (OpenAI, 2023). Note that ASR tasks report a word error rate (WER) metric, where a lower number is better."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "No specific paper is referenced for Whisper, but a general reference to OpenAI's work is provided.",
          "quote": "We also report performance with the Universal Speech Model (USM) (Zhang et al., 2023) and Whisper (OpenAI, 2023). Note that ASR tasks report a word error rate (WER) metric, where a lower number is better."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 59327
  }
}