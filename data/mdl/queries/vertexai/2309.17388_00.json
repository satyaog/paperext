{
  "paper": "2309.17388.txt",
  "words": 9712,
  "extractions": {
    "title": {
      "value": "TREE CROSS ATTENTION",
      "justification": "The paper's title is extracted from the provided abstract.",
      "quote": "T REE C ROSS ATTENTION"
    },
    "description": "The paper introduces Tree Cross Attention (TCA), a more efficient alternative to standard Cross Attention mechanisms in deep learning. TCA organizes data into a tree structure, enabling logarithmic-time information retrieval during inference. The authors demonstrate TCA",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical results to validate the proposed TCA method, making it empirical in nature.",
      "quote": "We show empirically that Tree Cross Attention (TCA) performs comparable to Cross Attention across various classification and uncertainty regression tasks while being significantly more token-efficient."
    },
    "primary_research_field": {
      "name": {
        "value": "machine learning",
        "justification": "The paper falls under the broader domain of machine learning, as evident from its focus on improving a core aspect of deep learning models.",
        "quote": "With the rapid growth in applications of machine learning, an important objective is to make inference efficient both in terms of compute and memory."
      },
      "aliases": [
        "machine learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The paper explicitly addresses challenges related to deep learning, particularly the efficiency of attention mechanisms.",
          "quote": "Furthermore, with the rapid growth in low-memory/compute domains (e.g. IoT devices) and the popularity of attention mechanisms in recent years, there is a strong incentive to design more efficient attention mechanisms for performing inference."
        },
        "aliases": [
          "Deep Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ReTreever",
          "justification": "The model is named ReTreever in the paper.",
          "quote": "Leveraging TCA, we introduce ReTreever, a flexible architecture for token-efficient inference."
        },
        "aliases": [
          "ReTreever"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The authors introduce ReTreever as a novel architecture leveraging TCA.",
          "quote": "Leveraging TCA, we introduce ReTreever, a flexible architecture for token-efficient inference."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides experimental results for ReTreever on multiple benchmark datasets.",
          "quote": "In our experiments, we show (1) TCA achieves results competitive to that of Cross Attention while only requiring a logarithmic number of tokens, (2) ReTreever outperforms Perceiver IO on various classification and uncertainty estimation tasks while using the same number of tokens"
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares ReTreever with Perceiver IO, showing superior performance in various tasks.",
          "quote": "Furthermore, we compare ReTreever against Perceiver IO,\\nshowing significant gains while using the same number of tokens for inference."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No referenced paper title is available for this model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Tree Cross Attention",
          "justification": "The paper refers to the model as Tree Cross Attention, abbreviated as TCA.",
          "quote": "In this work, we propose (1) Tree Cross Attention (TCA), a replacement for Cross Attention that performs retrieval, scaling logarithmically O(log(N )) with the number of tokens."
        },
        "aliases": [
          "TCA",
          "Tree Cross Attention"
        ],
        "is_contributed": {
          "value": true,
          "justification": "TCA is the core contribution of the paper, presented as a novel method.",
          "quote": "In this work, we propose (1) Tree Cross Attention (TCA), a replacement for Cross Attention that performs retrieval, scaling logarithmically O(log(N )) with the number of tokens."
        },
        "is_executed": {
          "value": true,
          "justification": "The authors provide experimental results for TCA, demonstrating its effectiveness in practice.",
          "quote": "In our experiments, we show (1) TCA achieves results competitive to that of Cross Attention while only requiring a logarithmic number of tokens"
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares TCA with standard Cross Attention, highlighting its efficiency advantages.",
          "quote": "We show empirically that Tree Cross Attention (TCA) performs comparable to Cross Attention across various classification and uncertainty regression tasks while being significantly more token-efficient."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No referenced paper title is available for this model.",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 18008
  }
}