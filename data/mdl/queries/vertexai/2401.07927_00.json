{
  "paper": "2401.07927.txt",
  "words": 26872,
  "extractions": {
    "title": {
      "value": "Are self-explanations from Large Language Models faithful?",
      "justification": "The title of the paper is mentioned in the beginning.",
      "quote": "Are self-explanations from Large Language Models faithful?"
    },
    "description": "This research paper investigates the faithfulness of self-explanations from Large Language Models (LLMs) using self-consistency checks. It examines counterfactual, importance measure, and redaction explanations across different models (Llama2, Falcon, and Mistral) and tasks (sentiment classification, multi-choice classification, and two-paragraph classification). The study finds that faithfulness is explanation, model, and task-dependent, highlighting the need for caution in trusting LLM self-explanations.",
    "type": {
      "value": "empirical",
      "justification": "The paper is empirical as it performs experiments and analysis on different LLMs and datasets.",
      "quote": "Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The main research field is Natural Language Processing (NLP) as it investigates the interpretability and faithfulness of LLMs.",
        "quote": "Post-hoc Interpretability for Neural NLP: A Survey."
      },
      "aliases": []
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "Llama2",
          "justification": "Llama2 is one of the LLMs analyzed in the paper.",
          "quote": "For example, with sentiment classification, counterfactuals are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute any new LLM.",
          "quote": "None"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses existing LLMs to test their faithfulness.",
          "quote": "We demonstrate our approach on four datasets with varying tasks: sentiment classification (IMDB Maas et al. 2011), multi-choice classification (bAbI and MCTest Weston et al. 2016; Richardson et al. 2013), and two-paragraph classification (RTE Dagan et al. 2006)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the faithfulness of different LLMs like Llama2, Falcon, and Mistral.",
          "quote": "Additionally, we apply the approach to Llama2 (70B, 7B), Falcon (40B, 7B), and Mistral (7B)."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
          "justification": "The paper references the Llama2 paper.",
          "quote": "such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), Mistral (Jiang et al., 2023), or GPT4 (OpenAI, 2023), are increasingly becoming mainstream among the general population, due to their capabilities and availability."
        }
      },
      {
        "name": {
          "value": "Falcon",
          "justification": "Falcon is one of the LLMs analyzed in the paper.",
          "quote": "For example, with sentiment classification, counterfactuals are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute any new LLM.",
          "quote": "None"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses existing LLMs to test their faithfulness.",
          "quote": "We demonstrate our approach on four datasets with varying tasks: sentiment classification (IMDB Maas et al. 2011), multi-choice classification (bAbI and MCTest Weston et al. 2016; Richardson et al. 2013), and two-paragraph classification (RTE Dagan et al. 2006)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the faithfulness of different LLMs like Llama2, Falcon, and Mistral.",
          "quote": "Additionally, we apply the approach to Llama2 (70B, 7B), Falcon (40B, 7B), and Mistral (7B)."
        },
        "referenced_paper_title": {
          "value": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
          "justification": "The paper references the Falcon paper.",
          "quote": "such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), Mistral (Jiang et al., 2023), or GPT4 (OpenAI, 2023), are increasingly becoming mainstream among the general population, due to their capabilities and availability."
        }
      },
      {
        "name": {
          "value": "Mistral",
          "justification": "Mistral is one of the LLMs analyzed in the paper.",
          "quote": "For example, with sentiment classification, counterfactuals are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute any new LLM.",
          "quote": "None"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses existing LLMs to test their faithfulness.",
          "quote": "We demonstrate our approach on four datasets with varying tasks: sentiment classification (IMDB Maas et al. 2011), multi-choice classification (bAbI and MCTest Weston et al. 2016; Richardson et al. 2013), and two-paragraph classification (RTE Dagan et al. 2006)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the faithfulness of different LLMs like Llama2, Falcon, and Mistral.",
          "quote": "Additionally, we apply the approach to Llama2 (70B, 7B), Falcon (40B, 7B), and Mistral (7B)."
        },
        "referenced_paper_title": {
          "value": "Mistral 7B",
          "justification": "The paper references the Mistral paper.",
          "quote": "such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), Mistral (Jiang et al., 2023), or GPT4 (OpenAI, 2023), are increasingly becoming mainstream among the general population, due to their capabilities and availability."
        }
      },
      {
        "name": {
          "value": "GPT4",
          "justification": "GPT4 is mentioned as a popular LLM.",
          "quote": "such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), Mistral (Jiang et al., 2023), or GPT4 (OpenAI, 2023), are increasingly becoming mainstream among the general population, due to their capabilities and availability."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute any new LLM.",
          "quote": "None"
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not execute GPT4, only mentions it.",
          "quote": "None"
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not compare GPT4 to other models in terms of faithfulness, only mentions its popularity.",
          "quote": "None"
        },
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "The paper references the GPT4 technical report.",
          "quote": "such as Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), Mistral (Jiang et al., 2023), or GPT4 (OpenAI, 2023), are increasingly becoming mainstream among the general population, due to their capabilities and availability."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 44176
  }
}