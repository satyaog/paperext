{
  "paper": "1910.02344.txt",
  "words": 9506,
  "extractions": {
    "title": {
      "value": "Neural Multisensory Scene Inference",
      "justification": "The title of the paper clearly reflects its core focus.",
      "quote": "Neural Multisensory Scene Inference"
    },
    "description": "This paper focuses on learning multisensory 3D scene representations, particularly through vision and haptics, using a novel Generative Multisensory Network (GMN). The model leverages an Amortized Product-of-Experts (APoE) approach to handle missing modalities and efficiently infer scene representations from various sensory inputs. Experiments are conducted in a simulated Multisensory Embodied 3D-Scene Environment (MESE) with Shepard-Metzler objects and a virtual MPL hand. The authors demonstrate the effectiveness of their approach in cross-modal generation, density estimation, and downstream classification tasks, highlighting its robustness to missing sensory information and its ability to integrate multimodal data for improved 3D scene understanding.",
    "type": {
      "value": "empirical",
      "justification": "This paper presents a novel model and evaluates its performance through experiments, making it empirical in nature.",
      "quote": "Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations from arbitrary combinations of modalities and perform accurate cross-modal generation."
    },
    "primary_research_field": {
      "name": {
        "value": "3D Scene Understanding",
        "justification": "The paper's primary focus is on learning representations of 3D scenes using multiple sensory inputs, aligning with the field of 3D scene understanding. The emphasis on 'embodied' agents further suggests a connection to embodied learning.",
        "quote": "Our goal is to understand 3D scenes by learning a metamodal representation of the scene through the interaction of multiple sensory modalities such as vision, haptics, and auditory inputs. In particular,\\nmotivated by human multisensory processing (Deneve & Pouget, 2004; Shams & Seitz, 2008; Murray\\n& Wallace, 2011), we consider a setting where the model infers a scene from experiences of a set of modalities and then to generate another set of modalities given a query for the generation."
      },
      "aliases": [
        "3D scene understanding",
        "multisensory representation learning",
        "embodied learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Cross-modal Generation",
          "justification": "The core focus on generating sensory outputs from different modalities given inputs from other modalities directly translates to the field of cross-modal generation and inference.",
          "quote": "Multimodal data are associated with many interesting learning problems, e.g. cross-modal inference, zero-shot learning or weakly-supervised learning."
        },
        "aliases": [
          "cross-modal generation",
          "cross-modal inference",
          "multimodal data"
        ]
      },
      {
        "name": {
          "value": "Generative Modeling",
          "justification": "The use of probabilistic models and latent representations to generate sensory data aligns with the concepts of generative modeling.",
          "quote": "Regarding these, latent variable models have provided effective solutions: from a model with global latent variable shared among all modalities (Suzuki et al., 2016) to hierarchical latent structures (Hsu &\\nGlass, 2018) and scalable inference networks with Product-of-Experts (PoE) (Hinton, 2002; Wu &\\nGoodman, 2018; Kurle et al., 2018)."
        },
        "aliases": [
          "generative modeling",
          "probabilistic latent variable models"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Generative Multisensory Network",
          "justification": "The paper refers to the model as \\\"Generative Multisensory Network (GMN)\\\".",
          "quote": "Motivated by the above desiderata, we propose the Generative Multisensory Network (GMN) for neural multisensory scene inference and rendering."
        },
        "aliases": [
          "GMN"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper proposes the GMN model as its primary contribution.",
          "quote": "Motivated by the above desiderata, we propose the Generative Multisensory Network (GMN) for neural multisensory scene inference and rendering."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper describes experiments and results using the GMN model.",
          "quote": "Experimental results demonstrate that the proposed model can efficiently infer robust modality-invariant 3D-scene representations from arbitrary combinations of modalities and perform accurate cross-modal generation."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper introduces and focuses on evaluating the GMN.",
          "quote": "Motivated by the above desiderata, we propose the Generative Multisensory Network (GMN) for neural multisensory scene inference and rendering."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No other paper is referenced as contributing to the GMN model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Amortized Product-of-Experts",
          "justification": "The paper refers to the model as  \\\"Amortized Product-of-Experts (APoE)\\\".",
          "quote": "To deal with the limitations of PoE, we introduce the Amortized Product-of-Experts (APoE)."
        },
        "aliases": [
          "APoE"
        ],
        "is_contributed": {
          "value": true,
          "justification": "This paper introduces APoE as a novel method within the GMN model.",
          "quote": "To deal with the limitations of PoE, we introduce the Amortized Product-of-Experts (APoE)."
        },
        "is_executed": {
          "value": true,
          "justification": "The APoE method is implemented and used within the experiments of the GMN model.",
          "quote": "In GMN, we introduce the Amortized Product-of-Experts (APoE) in order to deal with the problem of missing-modalities while resolving the space complexity problem of standard Product-of-Experts."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of APoE against a standard Product-of-Experts (PoE) approach.",
          "quote": "To deal with the limitations of PoE, we introduce the Amortized Product-of-Experts (APoE)."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "No other paper is referenced as contributing to the APoE model.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Product-of-Experts",
          "justification": "The paper refers to the model as \\\"Product-of-Experts (PoE)\\\".",
          "quote": "To deal with the limitations of PoE, we introduce the Amortized Product-of-Experts (APoE)."
        },
        "aliases": [
          "PoE"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't introduce PoE but utilizes it as a basis for comparison.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The paper implements and evaluates a PoE model to compare against APoE.",
          "quote": "We also provide a comparison to PoE version of the model in terms of computation speed and memory footprint."
        },
        "is_compared": {
          "value": true,
          "justification": "PoE is used as a baseline comparison for the proposed APoE approach in the paper.",
          "quote": "To deal with the limitations of PoE, we introduce the Amortized Product-of-Experts (APoE)."
        },
        "referenced_paper_title": {
          "value": "Training products of experts by minimizing contrastive divergence",
          "justification": "The paper references Hinton (2002) as the origin of the PoE model.",
          "quote": "While this could achieve our goal at the functional level, it comes at a computational cost of increased space and time complexity w.r.t. the number of modalities. This is particularly problematic when we want to employ diverse sensory modalities (as in, e.g., robotics) or if each expert has to be a powerful\\n(hence expensive both in computation and storage) model like the 3D scene inference task (Eslami et al., 2018), where it is necessary to use the powerful ConvDraw network to represent the complex 3D scene."
        }
      },
      {
        "name": {
          "value": "Generative Query Network",
          "justification": "The paper refers to the model as \\\"Generative Query Network (GQN)\\\", \\\"C-GQN\\\", and \\\"ConvDraw\\\".",
          "quote": "A simple way to do this is to follow the Generative Query Network (GQN) (Eslami et al., 2018) approach: each context query-sense pair (vn , xn ) is encoded to rn = fenc P(vn , xn )\\nand summed (or averaged) to obtain permutation-invariant context representation r = n rn . A ConvDRAW module (Gregor et al., 2016) is then used to sample z from r."
        },
        "aliases": [
          "GQN",
          "C-GQN",
          "ConvDraw"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper doesn't introduce GQN or ConvDraw, but builds upon them in its own model.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The authors implement and evaluate a GQN variant as a baseline.",
          "quote": "As a baseline model, we use a GQN variant (Kumar et al., 2018) (discussed in Section 2.3)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper primarily compares against a GQN variant and uses ConvDraw within its architecture.",
          "quote": "As a baseline model, we use a GQN variant (Kumar et al., 2018) (discussed in Section 2.3)."
        },
        "referenced_paper_title": {
          "value": "Neural scene representation and rendering",
          "justification": "The paper cites Eslami et al. (2018) for GQN and Gregor et al. (2016) for ConvDraw.",
          "quote": "A simple way to do this is to follow the Generative Query Network (GQN) (Eslami et al., 2018) approach: each context query-sense pair (vn , xn ) is encoded to rn = fenc P(vn , xn )\\nand summed (or averaged) to obtain permutation-invariant context representation r = n rn . A ConvDRAW module (Gregor et al., 2016) is then used to sample z from r."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 17737
  }
}