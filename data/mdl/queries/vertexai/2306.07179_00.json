{
  "paper": "2306.07179.txt",
  "words": 45669,
  "extractions": {
    "title": {
      "value": "Benchmarking Neural Network Training Algorithms",
      "justification": "The title of the paper is \\\"Benchmarking Neural Network Training Algorithms\\\".",
      "quote": "Benchmarking Neural Network Training Algorithms"
    },
    "description": "This paper presents a benchmark for comparing deep learning optimization algorithms called AlgoPerf: Training Algorithms. The benchmark includes workloads covering image classification, speech recognition, machine translation, MRI reconstruction, click-through rate prediction, and chemical property prediction tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper focuses on empirical benchmarking of training algorithms for deep neural networks.",
      "quote": "In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms"
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on benchmarking training algorithms for deep neural networks.",
        "quote": "Benchmarking Neural Network Training Algorithms"
      },
      "aliases": [
        "Deep Learning"
      ]
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "DLRMsmall",
          "justification": "The authors refer to the model as DLRMsmall.",
          "quote": "Criteo 1TB DLRMsmall"
        },
        "aliases": [
          "DLRM",
          "DLRMsmall"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the DLRMsmall model, it was originally proposed in Naumov et al. (2019).",
          "quote": "Naumov et al. (2019)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the DLRMsmall model in their benchmark.",
          "quote": "Criteo 1TB DLRMsmall"
        },
        "is_compared": {
          "value": true,
          "justification": "The DLRMsmall model is used as a benchmark workload for click-through rate prediction on the Criteo 1TB dataset.",
          "quote": "Criteo 1TB DLRMsmall"
        },
        "referenced_paper_title": {
          "value": "Deep Learning Recommendation Model for Personalization and Recommendation Systems",
          "justification": "The authors cite the paper \\\"Deep Learning Recommendation Model for Personalization and Recommendation Systems\\\" when mentioning DLRM.",
          "quote": "Naumov et al. (2019)"
        }
      },
      {
        "name": {
          "value": "U-Net",
          "justification": "The authors refer to the model as U-Net.",
          "quote": "fastMRI U-Net"
        },
        "aliases": [
          "U-Net"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the U-Net model, it was originally proposed in Ronneberger et al. (2015).",
          "quote": "Ronneberger et al. (2015)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the U-Net model in their benchmark.",
          "quote": "fastMRI U-Net"
        },
        "is_compared": {
          "value": true,
          "justification": "The U-Net model is used as a benchmark workload for MRI reconstruction on the fastMRI dataset.",
          "quote": "fastMRI U-Net"
        },
        "referenced_paper_title": {
          "value": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
          "justification": "The authors cite the paper \\\"U-Net: Convolutional Networks for Biomedical Image Segmentation\\\" when mentioning U-Net.",
          "quote": "Ronneberger et al. (2015)"
        }
      },
      {
        "name": {
          "value": "ResNet-50",
          "justification": "The authors refer to the model as ResNet-50.",
          "quote": "ImageNet ResNet-50"
        },
        "aliases": [
          "ResNet-50"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the ResNet-50 model, it was originally proposed in He et al. (2016a).",
          "quote": "He et al. (2016a)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the ResNet-50 model in their benchmark.",
          "quote": "ImageNet ResNet-50"
        },
        "is_compared": {
          "value": true,
          "justification": "The ResNet-50 model is used as a benchmark workload for image classification on the ImageNet dataset.",
          "quote": "ImageNet ResNet-50"
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "The authors cite the paper \\\"Deep Residual Learning for Image Recognition\\\" when mentioning ResNet-50.",
          "quote": "He et al. (2016a)"
        }
      },
      {
        "name": {
          "value": "ViT",
          "justification": "The authors refer to the model as ViT.",
          "quote": "ImageNet ViT"
        },
        "aliases": [
          "ViT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the ViT model, it was originally proposed in Dosovitskiy et al. (2021).",
          "quote": "Dosovitskiy et al. (2021)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the ViT model in their benchmark.",
          "quote": "ImageNet ViT"
        },
        "is_compared": {
          "value": true,
          "justification": "The ViT model is used as a benchmark workload for image classification on the ImageNet dataset.",
          "quote": "ImageNet ViT"
        },
        "referenced_paper_title": {
          "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "justification": "The authors cite the paper \\\"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\\\" when mentioning ViT.",
          "quote": "Dosovitskiy et al. (2021)"
        }
      },
      {
        "name": {
          "value": "Conformer",
          "justification": "The authors refer to the model as Conformer.",
          "quote": "LibriSpeech Conformer"
        },
        "aliases": [
          "Conformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the Conformer model, it was originally proposed in Gulati et al. (2020).",
          "quote": "Gulati et al. (2020)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the Conformer model in their benchmark.",
          "quote": "LibriSpeech Conformer"
        },
        "is_compared": {
          "value": true,
          "justification": "The Conformer model is used as a benchmark workload for speech recognition on the LibriSpeech dataset.",
          "quote": "LibriSpeech Conformer"
        },
        "referenced_paper_title": {
          "value": "Conformer: Convolution-augmented Transformer for Speech Recognition",
          "justification": "The authors cite the paper \\\"Conformer: Convolution-augmented Transformer for Speech Recognition\\\" when mentioning Conformer.",
          "quote": "Gulati et al. (2020)"
        }
      },
      {
        "name": {
          "value": "DeepSpeech",
          "justification": "The authors refer to the model as DeepSpeech.",
          "quote": "LibriSpeech DeepSpeech"
        },
        "aliases": [
          "DeepSpeech",
          "DeepSpeech 2"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the DeepSpeech model, it was originally proposed in Amodei et al. (2016).",
          "quote": "Amodei et al. (2016)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the DeepSpeech model in their benchmark.",
          "quote": "LibriSpeech DeepSpeech"
        },
        "is_compared": {
          "value": true,
          "justification": "The DeepSpeech model is used as a benchmark workload for speech recognition on the LibriSpeech dataset.",
          "quote": "LibriSpeech DeepSpeech"
        },
        "referenced_paper_title": {
          "value": "Deep Speech 2: End-to-End Speech Recognition in English and Mandarin",
          "justification": "The authors cite the paper \\\"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\\\" when mentioning DeepSpeech 2.",
          "quote": "Amodei et al. (2016)"
        }
      },
      {
        "name": {
          "value": "GNN",
          "justification": "The authors refer to the model as GNN.",
          "quote": "OGBG GNN"
        },
        "aliases": [
          "GNN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the GNN model, it is a generic model architecture.",
          "quote": ""
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the GNN model in their benchmark.",
          "quote": "OGBG GNN"
        },
        "is_compared": {
          "value": true,
          "justification": "The GNN model is used as a benchmark workload for molecular property prediction on the OGBG dataset.",
          "quote": "OGBG GNN"
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The authors do not provide a reference for the GNN model architecture.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Transformer",
          "justification": "The authors refer to the model as Transformer.",
          "quote": "WMT Transformer"
        },
        "aliases": [
          "Transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the Transformer model, it was originally proposed in Vaswani et al. (2017).",
          "quote": "Vaswani et al. (2017)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the Transformer model in their benchmark.",
          "quote": "WMT Transformer"
        },
        "is_compared": {
          "value": true,
          "justification": "The Transformer model is used as a benchmark workload for translation on the WMT dataset.",
          "quote": "WMT Transformer"
        },
        "referenced_paper_title": {
          "value": "Attention is All you Need",
          "justification": "The authors cite the paper \\\"Attention is All you Need\\\" when mentioning Transformer.",
          "quote": "Vaswani et al. (2017)"
        }
      },
      {
        "name": {
          "value": "Wide ResNet 28-10",
          "justification": "The authors refer to the model as Wide ResNet 28-10.",
          "quote": "Wide ResNet 28-10 architecture"
        },
        "aliases": [
          "Wide ResNet",
          "Wide ResNet 28-10"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the Wide ResNet 28-10 model, it was originally proposed in Zagoruyko and Komodakis (2016).",
          "quote": "Zagoruyko and Komodakis (2016)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the Wide ResNet 28-10 model in their sensitivity analysis experiment.",
          "quote": "Wide ResNet (Zagoruyko and Komodakis, 2016)"
        },
        "is_compared": {
          "value": true,
          "justification": "The Wide ResNet 28-10 model is used as an example for demonstrating the sensitivity of optimizers to model architecture.",
          "quote": "Wide ResNet (Zagoruyko and Komodakis, 2016)"
        },
        "referenced_paper_title": {
          "value": "Wide Residual Networks",
          "justification": "The authors cite the paper \\\"Wide Residual Networks\\\" when mentioning Wide ResNet.",
          "quote": "Zagoruyko and Komodakis (2016)"
        }
      },
      {
        "name": {
          "value": "200-layer ResNetV2",
          "justification": "The authors refer to the model as a 200-layer ResNetV2.",
          "quote": "200-layer ResNetV2 architecture"
        },
        "aliases": [
          "ResNetV2",
          "ResNet-200"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the 200-layer ResNetV2 model, it was originally proposed in He et al. (2016b).",
          "quote": "He et al. (2016b)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the 200-layer ResNetV2 model in their sensitivity analysis experiment.",
          "quote": "200-layer ResNetV2 architecture (He et al., 2016b)."
        },
        "is_compared": {
          "value": true,
          "justification": "The 200-layer ResNetV2 model is used as an example for demonstrating the sensitivity of optimizers to model architecture.",
          "quote": "200-layer ResNetV2 architecture (He et al., 2016b)."
        },
        "referenced_paper_title": {
          "value": "Identity Mappings in Deep Residual Networks",
          "justification": "The authors cite the paper \\\"Identity Mappings in Deep Residual Networks\\\" when mentioning ResNetV2.",
          "quote": "He et al. (2016b)"
        }
      },
      {
        "name": {
          "value": "Post-Layer Norm Transformer",
          "justification": "The authors refer to the model as the Post-Layer Norm Transformer.",
          "quote": "Post-Layer Norm (Post-LN) Transformer"
        },
        "aliases": [
          "Post-Layer Norm Transformer",
          "Post-LN Transformer",
          "Post-LN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the Post-Layer Norm Transformer model, it was originally proposed in Vaswani et al. (2017).",
          "quote": "Vaswani et al. (2017)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the Post-Layer Norm Transformer model in their sensitivity analysis experiment.",
          "quote": "Post-Layer Norm (Post-LN) Transformer (Vaswani et al., 2017)"
        },
        "is_compared": {
          "value": true,
          "justification": "The Post-Layer Norm Transformer model is used as an example for demonstrating the sensitivity of optimizers to model architecture.",
          "quote": "Post-Layer Norm (Post-LN) Transformer (Vaswani et al., 2017)"
        },
        "referenced_paper_title": {
          "value": "Attention is All you Need",
          "justification": "The authors cite the paper \\\"Attention is All you Need\\\" when mentioning the Post-Layer Norm Transformer.",
          "quote": "Vaswani et al. (2017)"
        }
      },
      {
        "name": {
          "value": "Pre-Layer Norm Transformer",
          "justification": "The authors refer to the model as the Pre-Layer Norm Transformer.",
          "quote": "Pre-Layer Norm (Pre-LN) Transformer"
        },
        "aliases": [
          "Pre-Layer Norm Transformer",
          "Pre-LN Transformer",
          "Pre-LN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The authors of the paper did not contribute the Pre-Layer Norm Transformer model, it was originally proposed in Xiong et al. (2020).",
          "quote": "Xiong et al. (2020)"
        },
        "is_executed": {
          "value": true,
          "justification": "The authors of the paper used the Pre-Layer Norm Transformer model in their sensitivity analysis experiment.",
          "quote": "Pre-Layer Norm (Pre-LN) Transformer (Xiong et al., 2020)"
        },
        "is_compared": {
          "value": true,
          "justification": "The Pre-Layer Norm Transformer model is used as an example for demonstrating the sensitivity of optimizers to model architecture.",
          "quote": "Pre-Layer Norm (Pre-LN) Transformer (Xiong et al., 2020)"
        },
        "referenced_paper_title": {
          "value": "On Layer Normalization in the Transformer Architecture",
          "justification": "The authors cite the paper \\\"On Layer Normalization in the Transformer Architecture\\\" when mentioning the Pre-Layer Norm Transformer.",
          "quote": "Xiong et al. (2020)"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 79987
  }
}