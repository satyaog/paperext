{
  "paper": "2308.12445.txt",
  "words": 10320,
  "extractions": {
    "title": {
      "value": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems",
      "justification": "This is the title of the research paper.",
      "quote": "An Intentional Forgetting-Driven Self-Healing Method For Deep Reinforcement Learning Systems"
    },
    "description": "This research paper proposes Dr. DRL, a novel self-healing approach for Deep Reinforcement Learning (DRL) systems. The approach addresses the issue of environmental drifts, which can cause undesirable behaviors in DRL systems operating in constantly changing production settings. Dr. DRL integrates a novel mechanism of intentional forgetting into standard Continual Learning (CL) to overcome limitations such as catastrophic forgetting and slow convergence. The paper evaluates Dr. DRL on various drifted environments using established DRL algorithms, demonstrating its effectiveness in reducing healing time, improving adaptability, and enhancing obtained rewards.",
    "type": {
      "value": "empirical",
      "justification": "The paper proposes and evaluates a novel method, making it empirical.",
      "quote": "In this paper, we propose Dr. DRL, a self-healing approach for DRL that leverages intentional forgetting [20, 21] combined with continual learning in order to optimize the agent’s plasticity and accelerate its adaptation to drifted environments."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Reinforcement Learning",
        "justification": "The paper focuses on self-healing approaches for deep reinforcement learning systems, specifically addressing environmental drifts.",
        "quote": "In this paper, we propose Dr. DRL, a self-healing approach for DRL that leverages intentional forgetting [20, 21] combined with continual learning in order to optimize the agent’s plasticity and accelerate its adaptation to drifted environments."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Healing",
          "justification": "The paper specifically focuses on self-healing in the context of DRL systems.",
          "quote": "In this paper, we propose Dr. DRL, a self-healing approach for DRL that leverages intentional forgetting [20, 21] combined with continual learning in order to optimize the agent’s plasticity and accelerate its adaptation to drifted environments."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Environmental Drifts",
          "justification": "The paper focuses on addressing environmental drifts in DRL systems as a key challenge.",
          "quote": "As with most data-driven systems, DRL systems can exhibit undesirable behaviors due to environmental drifts, which often occur in constantly-changing production settings."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The core idea of the paper is integrating intentional forgetting with continual learning to enhance DRL self-healing.",
          "quote": "Dr. DRL deliberately erases the DRL system’s minor behaviors to systematically prioritize the adaptation of the key problemsolving skills."
        },
        "aliases": [
          "CL"
        ]
      },
      {
        "name": {
          "value": "Intentional Forgetting",
          "justification": "Intentional forgetting is a key contribution and the core mechanism employed in the proposed Dr. DRL approach.",
          "quote": "In this paper, we propose Dr. DRL, a self-healing approach for DRL that leverages intentional forgetting [20, 21] combined with continual learning in order to optimize the agent’s plasticity and accelerate its adaptation to drifted environments."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Dr. DRL",
          "justification": "The paper names its proposed approach as 'Dr. DRL'.",
          "quote": "In this paper, we propose Dr. DRL, a self-healing approach for DRL that leverages intentional forgetting [20, 21] combined with continual learning in order to optimize the agent’s plasticity and accelerate its adaptation to drifted environments."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Dr. DRL is the novel approach proposed in this paper.",
          "quote": "In this paper, we propose Dr. DRL, a self-healing approach for DRL that leverages intentional forgetting [20, 21] combined with continual learning in order to optimize the agent’s plasticity and accelerate its adaptation to drifted environments."
        },
        "is_executed": {
          "value": true,
          "justification": "Dr. DRL is evaluated on different environments.",
          "quote": "To demonstrate the effectiveness of Dr. DRL, we evaluate it on purposefully drifted gym [27] environments with different drifting intensities."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Dr. DRL, which leverages DQN, with vanilla CL.",
          "quote": "Using well-established DRL algorithms, Dr. DRL is compared with vanilla CL on various drifted environments."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "The paper does not reference a previously published paper for Dr. DRL.",
          "quote": "None"
        }
      },
      {
        "name": {
          "value": "DQN",
          "justification": "DQN is an established DRL algorithm used in the paper.",
          "quote": "Deep Q-Learning (DQN) [31, 48] is a popular value-based algorithm that leverages neural network to approximate the action-value function, Q."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute to DQN, SAC, or PPO, but uses them for comparison.",
          "quote": "None"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses and evaluates the performance of these algorithms in different environments.",
          "quote": "Dr. DRL is able to reduce, on average, the healing time and fine-tuning episodes by, respectively, 18.74% and 17.72%."
        },
        "is_compared": {
          "value": true,
          "justification": "This is supported by the quote: '... three wellestablished DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33].'",
          "quote": "three wellestablished DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33]."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "The paper references [31] for the DQN algorithm.",
          "quote": "Deep Q-Learning (DQN) [31, 48] is a popular value-based algorithm that leverages neural network to approximate the action-value function, Q."
        }
      },
      {
        "name": {
          "value": "SAC",
          "justification": "SAC is an established DRL algorithm used in the paper.",
          "quote": "Soft Actor Critic (SAC) [32] is a hybrid DRL algorithm."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute to DQN, SAC, or PPO, but uses them for comparison.",
          "quote": "None"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses and evaluates the performance of these algorithms in different environments.",
          "quote": "Dr. DRL is able to reduce, on average, the healing time and fine-tuning episodes by, respectively, 18.74% and 17.72%."
        },
        "is_compared": {
          "value": true,
          "justification": "This is supported by the quote: '... three wellestablished DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33].'",
          "quote": "three wellestablished DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33]."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "The paper references [32] for the SAC algorithm.",
          "quote": "Soft Actor Critic (SAC) [32] is a hybrid DRL algorithm."
        }
      },
      {
        "name": {
          "value": "PPO",
          "justification": "PPO is an established DRL algorithm used in the paper.",
          "quote": "Proximal Policy Optimization (PPO) [33] is a state-of-theart policy-based algorithm whose goal is to maximize policy optimization without compromising performance."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not contribute to DQN, SAC, or PPO, but uses them for comparison.",
          "quote": "None"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses and evaluates the performance of these algorithms in different environments.",
          "quote": "Dr. DRL is able to reduce, on average, the healing time and fine-tuning episodes by, respectively, 18.74% and 17.72%."
        },
        "is_compared": {
          "value": true,
          "justification": "This is supported by the quote: '... three wellestablished DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33].'",
          "quote": "three wellestablished DRL algorithms, namely Deep Q-Learning (DQN) [31], Soft Actor-Critic (SAC) [32], and Proximal Policy Optimization (PPO) [33]."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "The paper references [33] for the PPO algorithm.",
          "quote": "Proximal Policy Optimization (PPO) [33] is a state-of-theart policy-based algorithm whose goal is to maximize policy optimization without compromising performance."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 19092
  }
}