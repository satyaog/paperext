{
  "paper": "2312.14279.txt",
  "words": 16269,
  "extractions": {
    "title": {
      "value": "Characterizing and Classifying Developer Forum Posts with their Intentions",
      "justification": "The title of the paper is \"Characterizing and Classifying Developer Forum Posts with their Intentions.\"",
      "quote": "Characterizing and Classifying Developer Forum Posts with their Intentions"
    },
    "description": "This paper analyzes the intentions behind posts on developer forums like Stack Overflow. The authors find that posts often have multiple intentions, with \"How-to\" being the most common. They propose an intention detection framework based on transformer-based pre-trained language models, such as BERT and RoBERTa, and achieve promising results. Their work has implications for improving content organization and recommendation systems in online technical communities.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments and analyzes a dataset of forum posts. This approach aligns with empirical research.",
      "quote": "Through manual labeling and analysis on a sampled post dataset extracted from online forums, we understand the relevance between the constitution of posts (code, error messages) and their intentions."
    },
    "primary_research_field": {
      "name": {
        "value": "Empirical Software Engineering",
        "justification": "The paper focuses on analyzing and understanding developer forum posts, which falls under the research field of Empirical Software Engineering.",
        "quote": "Empirical Software Engineering"
      },
      "aliases": []
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "The paper names BERT multiple times.",
          "quote": "Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) is a transformer-based architecture that is capable of capturing long dependency in natural language, and various transformer-based PTMs have been achieving state-of-the-art results in different natural language tasks (Jin et al., 2020)."
        },
        "aliases": [
          "BERT",
          "BERTbase"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses BERT but doesn’t contribute to it.",
          "quote": "We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides the results of executing BERT for intention detection.",
          "quote": "Table 7 shows the results of our experiments on different variants of our proposed intention detection framework with different PTMs."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares six different pre-trained models for intention detection, including BERT.",
          "quote": "We compare the performances of six variants of our framework with the PTMs mentioned above."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "The paper references the BERT paper.",
          "quote": "Devlin et al. (2018)"
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "The paper names RoBERTa multiple times.",
          "quote": "RoBERTa (Liu et al., 2019)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses RoBERTa but doesn’t contribute to it.",
          "quote": "We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides the results of executing RoBERTa for intention detection.",
          "quote": "Table 7 shows the results of our experiments on different variants of our proposed intention detection framework with different PTMs."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares six different pre-trained models for intention detection, including RoBERTa.",
          "quote": "We compare the performances of six variants of our framework with the PTMs mentioned above."
        },
        "referenced_paper_title": {
          "value": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "justification": "The paper references the RoBERTa paper.",
          "quote": "Liu et al. (2019)"
        }
      },
      {
        "name": {
          "value": "ALBERT",
          "justification": "The paper names ALBERT multiple times.",
          "quote": "ALBERT (Lan et al., 2019)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses ALBERT but doesn’t contribute to it.",
          "quote": "We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides the results of executing ALBERT for intention detection.",
          "quote": "Table 7 shows the results of our experiments on different variants of our proposed intention detection framework with different PTMs."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares six different pre-trained models for intention detection, including ALBERT.",
          "quote": "We compare the performances of six variants of our framework with the PTMs mentioned above."
        },
        "referenced_paper_title": {
          "value": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "justification": "The paper references the ALBERT paper.",
          "quote": "Lan et al. (2019)"
        }
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "The paper names DistilBERT multiple times.",
          "quote": "DistilBERT (Sanh et al., 2019)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses DistilBERT but doesn’t contribute to it.",
          "quote": "We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides the results of executing DistilBERT for intention detection.",
          "quote": "Table 7 shows the results of our experiments on different variants of our proposed intention detection framework with different PTMs."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares six different pre-trained models for intention detection, including DistilBERT.",
          "quote": "We compare the performances of six variants of our framework with the PTMs mentioned above."
        },
        "referenced_paper_title": {
          "value": "DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter",
          "justification": "The paper references the DistilBERT paper.",
          "quote": "Sanh et al. (2019)"
        }
      },
      {
        "name": {
          "value": "BERTOverflow",
          "justification": "The paper names BERTOverflow multiple times.",
          "quote": "BERTOverflow (Tabassum et al., 2020)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses BERTOverflow but doesn’t contribute to it.",
          "quote": "We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides the results of executing BERTOverflow for intention detection.",
          "quote": "Table 7 shows the results of our experiments on different variants of our proposed intention detection framework with different PTMs."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares six different pre-trained models for intention detection, including BERTOverflow.",
          "quote": "We compare the performances of six variants of our framework with the PTMs mentioned above."
        },
        "referenced_paper_title": {
          "value": "Code and Named Entity Recognition in StackOverflow",
          "justification": "The paper references the BERTOverflow paper.",
          "quote": "Tabassum et al. (2020)"
        }
      },
      {
        "name": {
          "value": "CodeBERT",
          "justification": "The paper names CodeBERT multiple times.",
          "quote": "CodeBERT (Feng et al., 2020)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses CodeBERT but doesn’t contribute to it.",
          "quote": "We leverage the PTMs released in the online community Hugging Face (Wolf et al., 2019) in our experiments."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides the results of executing CodeBERT for intention detection.",
          "quote": "Table 7 shows the results of our experiments on different variants of our proposed intention detection framework with different PTMs."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares six different pre-trained models for intention detection, including CodeBERT.",
          "quote": "We compare the performances of six variants of our framework with the PTMs mentioned above."
        },
        "referenced_paper_title": {
          "value": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages",
          "justification": "The paper references the CodeBERT paper.",
          "quote": "Feng et al. (2020)"
        }
      },
      {
        "name": {
          "value": "Random Forest Binary Classifiers",
          "justification": "The paper refers to the model as \"Random Forest Binary Classifiers.\"",
          "quote": "Baseline 1: Random Forest Binary Classifiers are used in Beyer et al. (2020) to classify Stack Overflow posts into seven intention categories."
        },
        "aliases": [
          "Random Forest Binary Classifiers"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses an existing random forest model as a baseline, but does not contribute to the development of the model itself.",
          "quote": "In our work, we follow the preprocessing and configurations from their work and train a set of random forest classifiers for our intention categories with our dataset as the baseline model."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides results of executing the Random Forest model.",
          "quote": "Table 9 shows the performances of the baseline models and two best-performing variants from RQ1 after fine-tuning."
        },
        "is_compared": {
          "value": true,
          "justification": "The authors compare their intention detection framework with a baseline approach based on random forest binary classifiers.",
          "quote": "In this research question, we have two goals: The first objective is to examine if the performance of our approach can be further improved by finetuning the PTMs with the intention detection task. We chose two baselines for our study. The first one, proposed by Beyer et al. (2020), uses a set of random forest binary classifiers for QA post intention detection."
        },
        "referenced_paper_title": {
          "value": "What Kind of Questions Do Developers Ask on Stack Overflow? A Comparison of Automated Approaches to Classify Posts into Question Categories",
          "justification": "The paper references Beyer et al. (2020) for using random forest in intention detection.",
          "quote": "Beyer et al. (2020)"
        }
      },
      {
        "name": {
          "value": "CNN-based approach",
          "justification": "The paper refers to the model as a \"CNN-based approach\" as well as a \"Convolutional Neural Network (CNN)-based approach\".",
          "quote": "Baseline 2: A CNN-based approach is introduced by Huang et al. (2018) for the task of extracting intentions from issue reports on GitHub. This approach applies a CNN-based network and classifies sentences from issue reports into seven pre-defined intention categories."
        },
        "aliases": [
          "CNN-based approach",
          "Convolutional Neural Network (CNN)-based approach"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses an existing CNN-based model as a baseline and does not modify it.",
          "quote": "In our work, we utilize the same CNN architecture while we substitute the cross entropy loss with the BCE loss to adapt to our task, which requires a multi-label output."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper presents the results obtained from using the CNN-based model.",
          "quote": "Table 9 shows the performances of the baseline models and two best-performing variants from RQ1 after fine-tuning."
        },
        "is_compared": {
          "value": true,
          "justification": "The authors use a CNN-based model as one of their baseline methods for comparison.",
          "quote": "We chose two baselines for our study. [...] The second baseline is a convolution neural network (CNN)-based approach from Huang et al. (2018) which is designed for extracting intentions from GitHub issue reports."
        },
        "referenced_paper_title": {
          "value": "Automating Intention Mining",
          "justification": "The authors reference Huang et al. (2018) when discussing their choice of the CNN-based approach.",
          "quote": "Huang et al. (2018)"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 27493
  }
}