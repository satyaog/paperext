{
  "paper": "2310.03419.txt",
  "words": 10200,
  "extractions": {
    "title": {
      "value": "Pre-Training and Fine-Tuning Generative Flow Networks",
      "justification": "The title of the paper is \\\"Pre-Training and Fine-Tuning Generative Flow Networks\\\".",
      "quote": "Pre-Training and Fine-Tuning Generative Flow Networks Ling Pan1∗ Moksh Jain1 Kanika Madan1 Yoshua Bengio1"
    },
    "description": "The paper introduces a novel approach for reward-free pre-training of Generative Flow Networks (GFlowNets) called Outcome-Conditioned GFlowNet (OC-GFN). The authors propose to pre-train OC-GFN in a self-supervised manner to reach any targeted outcomes. They then adapt it to new reward functions for downstream tasks by learning an amortized predictor, which enables efficient fine-tuning. The paper demonstrates the effectiveness of the approach through experiments on GridWorld, Bit Sequence Generation, TF Bind Generation, RNA Generation, and Antimicrobial Peptide Generation, showing improvements in mode discovery, diversity, and top-k scores.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents a novel method and validates its effectiveness through experiments, classifying it as empirical research.",
      "quote": ""
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on Generative Flow Networks, which fall under the domain of Deep Learning.",
        "quote": ""
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Unsupervised Pre-Training",
          "justification": "The paper proposes a method for \\\"unsupervised pre-training of GFlowNets\\\".",
          "quote": "In this paper, we propose a novel method for reward-free unsupervised pre-training of GFlowNets."
        },
        "aliases": [
          "unsupervised pre-training",
          "Unsupervised Pre-Training"
        ]
      },
      {
        "name": {
          "value": "Fine-Tuning",
          "justification": "The paper focuses on \\\"efficiently fine-tuning\\\" pre-trained GFlowNets.",
          "quote": "The authors propose to pre-train OC-GFN in a self-supervised manner to reach any targeted outcomes. They then adapt it to new reward functions for downstream tasks by learning an amortized predictor, which enables efficient fine-tuning."
        },
        "aliases": [
          "fine-tuning",
          "Fine-Tuning"
        ]
      },
      {
        "name": {
          "value": "Generative Flow Networks",
          "justification": "The paper focuses on \\\"pre-training of Generative Flow Networks (GFlowNets)\\\".",
          "quote": "In this paper, we propose a novel method for reward-free unsupervised pre-training of GFlowNets."
        },
        "aliases": [
          "Generative Flow Networks",
          "GFlowNets",
          "GFlowNet"
        ]
      },
      {
        "name": {
          "value": "Goal-Conditioned Reinforcement Learning",
          "justification": "The paper draws inspiration from \\\"Goal-conditioned Reinforcement Learning\\\".",
          "quote": "We formulate the problem of pre-training GFlowNets as a self-supervised problem of learning an outcome-conditioned GFlowNet (OC-GFN) which learns to reach any input target outcomes, inspired by the success of goal-conditioned reinforcement learning in Figure 1: The unsupervised pre-training generalizing to a variety of tasks when it is tasked phase of outcome-conditioned GFlowNet."
        },
        "aliases": [
          "Goal-Conditioned Reinforcement Learning"
        ]
      },
      {
        "name": {
          "value": "Amortized Inference",
          "justification": "The paper introduces an \\\"amortized predictor\\\" to handle intractable marginalization during adaptation.",
          "quote": "We propose a novel alternative by learning a predictor that amortizes this marginalization, allowing efficient fine-tuning of the OC-GFN to downstream tasks."
        },
        "aliases": [
          "Amortized Inference"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Outcome-Conditioned GFlowNet",
          "justification": "The paper introduces a novel approach for reward-free pre-training of Generative Flow Networks (GFlowNets) called Outcome-Conditioned GFlowNet (OC-GFN).",
          "quote": "In this paper, we propose a novel method for reward-free unsupervised pre-training of GFlowNets. We formulate the problem of pre-training GFlowNets as a self-supervised problem of learning an outcome-conditioned GFlowNet (OC-GFN) which learns to reach any input target outcomes, inspired by the success of goal-conditioned reinforcement learning in Figure 1: The unsupervised pre-training generalizing to a variety of tasks when it is tasked phase of outcome-conditioned GFlowNet."
        },
        "aliases": [
          "GFlowNets",
          "Generative Flow Networks",
          "GFlowNet",
          "OC-GFN",
          "Outcome-conditioned GFlowNet",
          "Outcome-Conditioned GFlowNets"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper proposes a novel method for unsupervised pre-training of Generative Flow Networks (GFlowNets) called Outcome-Conditioned GFlowNet (OC-GFN).",
          "quote": "In this paper, we propose a novel method for reward-free unsupervised pre-training of GFlowNets. We formulate the problem of pre-training GFlowNets as a self-supervised problem of learning an outcome-conditioned GFlowNet (OC-GFN) which learns to reach any input target outcomes, inspired by the success of goal-conditioned reinforcement learning in Figure 1: The unsupervised pre-training generalizing to a variety of tasks when it is tasked phase of outcome-conditioned GFlowNet."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides experimental results of the proposed model on GridWorld, Bit Sequence Generation, TF Bind Generation, RNA Generation, and Antimicrobial Peptide Generation.",
          "quote": "Extensive experimental results validate the efficacy of our approach, demonstrating the effectiveness of pre-training the OC-GFN, and its ability to swiftly adapt to downstream tasks and discover modes more efficiently."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares OC-GFN with several baselines, including training GFN from scratch, Metropolis-Hastings-MCMC, and Deep Q-Networks (DQN).",
          "quote": "We compare the proposed approach against strong baselines including training a GFN from scratch (Bengio et al., 2023), Metropolis-Hastings-MCMC (Dai et al., 2020), and Deep Q-Networks (DQN) (Mnih et al., 2015)."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "I cannot find the referenced paper in the context.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Generative Augmented Flow Networks",
          "justification": "The paper refers to the model as \\\"Generative Augmented Flow Networks (GAFlowNets)\\\".",
          "quote": "Generative Augmented Flow Networks (GAFlowNets; Pan et al., 2023b) incorporate intrinsic intermediate rewards represented as augmented flows in the flow network to drive exploration, where ri (s → s′ ) is specified by intrinsic motivation (Burda et al., 2018), yielding the following variant of detailed balance."
        },
        "aliases": [
          "GAFN",
          "Generative Augmented Flow Networks",
          "GAFlowNets"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses an existing method, GAFN, which is not contributed in this work.",
          "quote": "Outcome generation We can train OC-GFN by conditioning outcomes-conditioned flows and policies on a specified outcome y , and we study how to generate them autotelicly. It is worth noting that we need to train it with full-support over y . We propose to leverage GAFN (Pan et al., 2023b) with the introduction of augmented flows that enable efficient reward-free exploration purely by intrinsic motivation (Burda et al., 2018)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper utilizes GAFN to generate diverse outcomes for training OC-GFN.",
          "quote": "Outcome generation We can train OC-GFN by conditioning outcomes-conditioned flows and policies on a specified outcome y , and we study how to generate them autotelicly. It is worth noting that we need to train it with full-support over y . We propose to leverage GAFN (Pan et al., 2023b) with the introduction of augmented flows that enable efficient reward-free exploration purely by intrinsic motivation (Burda et al., 2018)."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper uses GAFN for exploration and compares its performance with and without other components of the proposed method.",
          "quote": "Generative Augmented Flow Networks (GAFlowNets; Pan et al., 2023b) incorporate intrinsic intermediate rewards represented as augmented flows in the flow network to drive exploration, where ri (s → s′ ) is specified by intrinsic motivation (Burda et al., 2018), yielding the following variant of detailed balance."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "I cannot find the referenced paper in the context.",
          "quote": ""
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 21133
  }
}