{
  "paper": "2401.11237.txt",
  "words": 11221,
  "extractions": {
    "title": {
      "value": "CLOSING THE GAP BETWEEN TD LEARNING AND SUPERVISED LEARNING – A GENERALISATION POINT OF VIEW",
      "justification": "The title of the paper, as stated in the provided text.",
      "quote": "C LOSING THE G AP BETWEEN TD L EARNING AND S UPERVISED L EARNING – A G ENERALISATION P OINT OF V IEW"
    },
    "description": "This research paper explores the differences between reinforcement learning (RL) and supervised learning (SL), particularly focusing on the \"stitching\" property. The authors argue that stitching, the ability to combine experiences to solve new tasks, is crucial for RL agents. They demonstrate that outcome-conditioned behavioral cloning (OCBC) methods, despite their success, lack this stitching ability. They propose a temporal data augmentation technique to enhance OCBC methods by enabling them to learn from state-goal pairs not encountered during training, thereby improving their combinatorial generalization and stitching capabilities.",
    "type": {
      "value": "empirical",
      "justification": "The paper primarily focuses on empirical analysis and experiments to investigate the stitching property in RL and SL.",
      "quote": "Our empirical results support the theory: we demonstrate that prior RL methods based on SL (DT [2] and RvS [3]) fail to perform stitching, even when trained on abundant quantities of data."
    },
    "primary_research_field": {
      "name": {
        "value": "reinforcement learning",
        "justification": "The paper focuses on analyzing the differences and similarities between reinforcement learning (RL) and supervised learning (SL), especially in the context of 'stitching.'",
        "quote": "Some reinforcement learning (RL) algorithms can stitch pieces of experience to solve a task never seen before during training."
      },
      "aliases": [
        "RL",
        "reinforcement learning"
      ]
    },
    "sub_research_fields": [],
    "models": [
      {
        "name": {
          "value": "Decision Transformer",
          "justification": "Decision Transformer, abbreviated as DT.",
          "quote": "Decision transformer: Reinforcement learning via sequence modeling."
        },
        "aliases": [
          "DT",
          "Decision Transformer",
          "transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Decision Transformer (DT) is not a novel contribution of this paper but is used as a baseline model.",
          "quote": "Our work shows that combinatorial generalisation is also required to solve tasks in the context of RL."
        },
        "is_executed": {
          "value": true,
          "justification": "This paper benchmarks Decision Transformer (DT) on multiple experiments.",
          "quote": "Our empirical results support the theory: we demonstrate that prior RL methods based on SL (DT [2] and RvS [3]) fail to perform stitching, even when trained on abundant quantities of data."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Decision Transformer (DT) with temporal difference (TD) learning methods.",
          "quote": "Many recent methods view RL as a purely SL problem of mapping input states and desired goals, to optimal actions [1–3]. These methods have gained a lot of attention due to their simplicity and scalability [4]."
        },
        "referenced_paper_title": {
          "value": "Decision transformer: Reinforcement learning via sequence modeling.",
          "justification": "The paper references the original Decision Transformer paper.",
          "quote": "Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084–15097, 2021."
        }
      },
      {
        "name": {
          "value": "RvS",
          "justification": "The paper refers to the model by the name 'RvS.'",
          "quote": "RvS [3] shows that a simple SL-based algorithm can surpass the performance of TD algorithms."
        },
        "aliases": [
          "RvS"
        ],
        "is_contributed": {
          "value": false,
          "justification": "RvS is not a new model introduced in this paper but is used as a baseline algorithm.",
          "quote": "Our work shows that combinatorial generalisation is also required to solve tasks in the context of RL."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper benchmarks RvS in multiple experiments.",
          "quote": "Our empirical results support the theory: we demonstrate that prior RL methods based on SL (DT [2] and RvS [3]) fail to perform stitching, even when trained on abundant quantities of data."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares RvS with temporal difference (TD) learning algorithms.",
          "quote": "Our empirical results support the theory: we demonstrate that prior RL methods based on SL (DT [2] and RvS [3]) fail to perform stitching, even when trained on abundant quantities of data."
        },
        "referenced_paper_title": {
          "value": "Rvs: What is essential for offline rl via supervised learning?",
          "justification": "The paper cites the original RvS research paper.",
          "quote": "Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "cached_content_token_count": 0,
    "candidates_token_count": 0,
    "prompt_token_count": 0,
    "total_token_count": 19151
  }
}