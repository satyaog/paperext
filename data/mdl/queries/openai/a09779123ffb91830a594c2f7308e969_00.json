{
  "paper": "a09779123ffb91830a594c2f7308e969.txt",
  "words": 8907,
  "extractions": {
    "title": {
      "value": "On Learning Fairness and Accuracy on Multiple Subgroups",
      "justification": "The title of the paper is explicitly mentioned at the beginning and reflects the paper's focus on fairness and accuracy across multiple subgroups.",
      "quote": "On Learning Fairness and Accuracy on Multiple Subgroups"
    },
    "description": "This paper proposes a new framework for learning a fair predictor that ensures group sufficiency while maintaining prediction accuracy across multiple subgroups. It formulates the problem as a bilevel optimization and evaluates the approach on real-world datasets, demonstrating improvements in fairness and accuracy.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments on real-world datasets to demonstrate the empirical effectiveness of the proposed framework.",
      "quote": "We evaluate the proposed framework on real-world datasets. Empirical evidence suggests the consistently improved fair predictions, as well as the comparable accuracy to the baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Fairness in Machine Learning",
        "justification": "The paper focuses on developing fair learning algorithms and controlling group sufficiency in machine learning models.",
        "quote": "In this work, we focus on the criteria of group sufficiency, which ensures that the conditional expectation of ground-truth label is identical across different subgroups."
      },
      "aliases": [
        "Fair AI"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper addresses optimization through bilevel optimization strategies for fair learning.",
          "quote": "We develop a principled algorithm and formulate the problem as a bilevel optimization."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper evaluates the proposed algorithm on NLP datasets, such as Amazon reviews and toxic comments.",
          "quote": "We evaluate the proposed algorithm on two real-world NLP datasets that have shown prediction disparities w.r.t. group sufficiency."
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DistilBERT",
          "justification": "DistilBERT is used as a base model for learning embeddings in the experiments conducted in the paper.",
          "quote": "In the implementation, we first adopt DistilBERT to learn the embedding with dimension R ^{768}."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "DistilBERT is an existing model and not a contribution of this paper.",
          "quote": "In the implementation, we first adopt DistilBERT to learn the embedding with dimension R ^{768}."
        },
        "is_executed": {
          "value": true,
          "justification": "DistilBERT is actively used in experiments to process the datasets.",
          "quote": "In the implementation, we first adopt DistilBERT to learn the embedding with dimension R ^{768}."
        },
        "is_compared": {
          "value": false,
          "justification": "DistilBERT is not compared as the paper's focus is not on comparing different language models, but rather on fairness objectives.",
          "quote": "We first adopt DistilBERT to learn the embedding."
        },
        "referenced_paper_title": {
          "value": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
          "justification": "The paper should refer to the original paper on DistilBERT for its theoretical foundation and application.",
          "quote": "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Amazon review dataset",
          "justification": "The Amazon review dataset is used to evaluate sentiment prediction as a part of the experiments.",
          "quote": "We adopt Amazon review dataset, which aims to predict the sentiment (classification) from the review."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Justifying recommendations using distantly-labeled reviews and fine-grained aspects",
          "justification": "The paper cites the source of the Amazon review dataset used for the experiments.",
          "quote": "Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-labeled reviews and fine-grained aspects."
        }
      },
      {
        "name": {
          "value": "Toxic Comment dataset",
          "justification": "The Toxic Comment dataset is used to experiment with text classification related to toxicity detection.",
          "quote": "We also use the toxic comment dataset to predict the text comment being toxic or not."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Nuanced metrics for measuring unintended bias with real data for text classification",
          "justification": "The paper refers to the source of the Toxic Comment dataset used in the experiments.",
          "quote": "Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "Although not explicitly mentioned in the excerpt provided, commonly used libraries such as PyTorch would be necessary to implement deep learning models and algorithms.",
          "quote": "We develop a practical learning algorithm that can be applied to a wide range of differentiable and parametric models, including neural networks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in machine learning: a survey",
          "justification": "PyTorch is fundamental for executing neural network operations in this context.",
          "quote": "Atilim Gunes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automatic differentiation in machine learning: a survey."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1102,
    "prompt_tokens": 16286,
    "total_tokens": 17388,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}