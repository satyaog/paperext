{
  "paper": "2402.08609.txt",
  "words": 9010,
  "extractions": {
    "title": {
      "value": "Mixtures of Experts Unlock Parameter Scaling for Deep RL",
      "justification": "The paper aims to unlock parameter scaling for deep reinforcement learning by incorporating mixtures of experts into value-based networks.",
      "quote": "Mixtures of Experts Unlock Parameter Scaling for Deep RL"
    },
    "description": "The paper investigates the use of Mixture-of-Experts (MoE) modules, notably Soft MoEs, in value-based deep reinforcement learning networks to improve parameter scalability and performance across various model sizes and training regimes.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experiments and analyses to demonstrate the performance improvements achieved by using MoE in deep RL models, which provides empirical evidence towards developing scaling laws.",
      "quote": "This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on applying mixtures of experts to improve parameter scaling and performance in deep reinforcement learning networks.",
        "quote": "While deep networks are critical to any successful application of RL in complex environments, their design and learning dynamics in RL remain a mystery."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The paper applies deep learning techniques, specifically deep neural networks with MoE modules, to reinforcement learning tasks.",
          "quote": "Deep Reinforcement Learning (RL) ‚Äì the combination of reinforcement learning algorithms with deep neural networks ‚Äì has proven effective at producing agents."
        },
        "aliases": [
          "DL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DQN",
          "justification": "The paper uses DQN as one of the base architectures to investigate the performance improvements with MoE.",
          "quote": "We focus our investigation on DQN (Mnih et al., 2015) and Rainbow, two value-based agents that have formed the basis of a large swath of modern deep RL research."
        },
        "aliases": [
          "Deep Q-Network"
        ],
        "is_contributed": {
          "value": false,
          "justification": "DQN is used as a baseline model for comparison and is not introduced by this paper.",
          "quote": "We focus our investigation on DQN (Mnih et al., 2015) and Rainbow."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes empirical evaluations based on experiments conducted using DQN.",
          "quote": "Our implementation, included with this submission, is built on the Dopamine library."
        },
        "is_compared": {
          "value": true,
          "justification": "DQN's performance is compared to the variations that include MoE modules.",
          "quote": "As Figure 1 demonstrates, Soft MoE provides clear performance gains, and these gains increase with the number of experts."
        },
        "referenced_paper_title": {
          "value": "Human-level control through deep reinforcement learning",
          "justification": "The DQN model is referenced in the context of its original introduction by Mnih et al. in 2015.",
          "quote": "DQN by Mnih et al. (2015) uses neural networks with parameters ùúÉ."
        }
      },
      {
        "name": {
          "value": "Rainbow",
          "justification": "Rainbow is used as a baseline model to evaluate and compare the effects of integrating MoEs.",
          "quote": "We focus our investigation on DQN (Mnih et al., 2015) and Rainbow, two value-based agents that have formed the basis of a large swath of modern deep RL research."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Rainbow is utilized as a baseline for comparison and further experimentation with MoE modules.",
          "quote": "Rainbow was shown to significantly outperform DQN and is an important baseline in deep RL research."
        },
        "is_executed": {
          "value": true,
          "justification": "Rainbow, as a baseline, is used to run experiments and evaluate performance with MoE.",
          "quote": "All experiments were run on NVIDIA Tesla P100 GPUs, and each took on average 4 days to complete."
        },
        "is_compared": {
          "value": true,
          "justification": "Rainbow's performance is extensively compared to models integrated with MoE to show improvements.",
          "quote": "As Figure 1 demonstrates, Soft MoE provides clear performance gains, and these gains increase with the number of experts."
        },
        "referenced_paper_title": {
          "value": "Rainbow: Combining Improvements in Deep Reinforcement Learning",
          "justification": "Rainbow is referenced as an already existing work used for comparison.",
          "quote": "Rainbow (Hessel et al., 2018) extended the original DQN algorithm with multiple algorithmic components."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari 2600",
          "justification": "The paper evaluates models on the Atari 2600 game suite as part of its empirical evaluation.",
          "quote": "We evaluate on 20 games from the Arcade Learning Environment (ALE), a collection of a diverse and challenging pixel-based environments."
        },
        "aliases": [
          "ALE"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
          "justification": "This is the cited reference for the Atari 2600 dataset used in reinforcement learning benchmarks.",
          "quote": "We evaluate on 20 games from the Arcade Learning Environment (ALE),... (Bellemare et al., 2013)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Dopamine",
          "justification": "The Dopamine library is used in the implementation of the paper's experiments.",
          "quote": "Our implementation, included with this submission, is built on the Dopamine library."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Dopamine: A Research Framework for Deep Reinforcement Learning",
          "justification": "The library is referenced as a framework used in conducting the research experiments.",
          "quote": "Our implementation, included with this submission, is built on the Dopamine library (Castro et al., 2018)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1157,
    "prompt_tokens": 16771,
    "total_tokens": 17928,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}