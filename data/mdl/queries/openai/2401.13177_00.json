{
  "paper": "2401.13177.txt",
  "words": 10929,
  "extractions": {
    "title": {
      "value": "Deep Learning Model Reuse in the HuggingFace Community: Challenges, Benefit and Trends",
      "justification": "The title is explicitly stated at the beginning of the paper.",
      "quote": "Deep Learning Model Reuse in the HuggingFace Community: Challenges, Benefit and Trends"
    },
    "description": "This paper conducts an empirical study on the reuse of Deep Learning Pre-Trained Models (PTMs) within the HuggingFace community. It analyzes the challenges, benefits, and trends in model reuse by examining discussions and data on the HuggingFace platform, using both qualitative and quantitative methods.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves both qualitative and quantitative analysis of data collected from the HuggingFace platform, which is characteristic of empirical research.",
      "quote": "To address this gap, we conducted an extensive mixed-methods empirical study by focusing on discussion forums and the model hub of HuggingFace."
    },
    "primary_research_field": {
      "name": {
        "value": "Software Engineering",
        "justification": "The paper focuses on software reuse and community discussions in the context of Deep Learning, making it a research work within Software Engineering.",
        "quote": "Index Termsâ€”Software Reuse, Pre-Trained Models, Model Hubs, Software Supply Chain, Deep Learning Models"
      },
      "aliases": [
        "SE"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper discusses the use of Pre-Trained Models, particularly in NLP tasks such as language understanding and generation, which is a major part of the analysis conducted.",
          "quote": "Pre-training Deep Learning (DL) models with huge amounts of data was first successfully applied to computer vision and NLP tasks."
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is mentioned as one of the most popular and discussed models in the paper.",
          "quote": "BERT, some BERT-based models (DistilBERT, RoBERTa), GPT2, T5, and BART are among the most popular ones, with BERT being the most discussed and most uploaded model type over time."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BERT is a well-known pre-existing model and is referenced alongside other similar models.",
          "quote": "BERT, some BERT-based models (DistilBERT, RoBERTa), GPT2, T5, and BART are among the most popular ones."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper discusses trends and usage rather than executing these models as a part of the study.",
          "quote": "our qualitative analysis, we present a taxonomy of the challenges and benefits associated with PTM reuse within this community."
        },
        "is_compared": {
          "value": true,
          "justification": "BERT is compared in terms of its popularity and trends related to its reuse within the community.",
          "quote": "BERT, being the most discussed and most uploaded model type over time with a noticeable distance from others."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "BERT's original research paper is well-known and widely referenced alongside other models in similar contexts.",
          "quote": "BERT, some BERT-based models (DistilBERT, RoBERTa), GPT2, T5, and BART are among the most popular ones."
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is mentioned as a popular BERT-based model discussed in the community.",
          "quote": "BERT, some BERT-based models (DistilBERT, RoBERTa)... are among the most popular ones."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "RoBERTa was developed prior to this study and is discussed as an existing model within the community.",
          "quote": "BERT, some BERT-based models (DistilBERT, RoBERTa), GPT2, T5, and BART are among the most popular ones."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper focuses on discussions and trends rather than execution details.",
          "quote": "our qualitative analysis, we present a taxonomy of the challenges and benefits associated with PTM reuse within this community."
        },
        "is_compared": {
          "value": true,
          "justification": "RoBERTa is compared in relation to its presence and discussions in the community, similar to other popular models.",
          "quote": "BERT, some BERT-based models (DistilBERT, RoBERTa)... are among the most popular ones."
        },
        "referenced_paper_title": {
          "value": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "justification": "RoBERTa's reference paper is well-known for pre-training optimizations over BERT.",
          "quote": "BERT, some BERT-based models (DistilBERT, RoBERTa)... are among the most popular ones."
        }
      },
      {
        "name": {
          "value": "GPT-2",
          "justification": "GPT-2 is highlighted as a major model often discussed in the HF community.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GPT-2 is a well-established model referenced as part of the community's focus, not a new contribution from the paper.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "is_executed": {
          "value": false,
          "justification": "The study does not detail execution of these models but rather reviews their usage and discussion.",
          "quote": "our qualitative analysis, we present a taxonomy of the challenges and benefits associated with PTM reuse within this community."
        },
        "is_compared": {
          "value": true,
          "justification": "GPT-2 is evaluated in terms of how frequently it is discussed compared to others.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "referenced_paper_title": {
          "value": "Language Models are Unsupervised Multitask Learners",
          "justification": "GPT-2's reference paper is pivotal in the context of language models and is widely recognized.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        }
      },
      {
        "name": {
          "value": "T5",
          "justification": "T5 is listed as one of the frequently discussed models within the HF community.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "T5 is pre-existing and commonly referenced, not newly contributed by this paper.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper discusses trends rather than providing execution insights.",
          "quote": "our qualitative analysis, we present a taxonomy of the challenges and benefits associated with PTM reuse within this community."
        },
        "is_compared": {
          "value": true,
          "justification": "T5's popularity and trends are compared within the community context.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "referenced_paper_title": {
          "value": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.",
          "justification": "T5's development and abilities are documented in its main reference work.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        }
      },
      {
        "name": {
          "value": "BART",
          "justification": "BART is indicated as being widely discussed and a model of significance in the community.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BART exists as a known model prior to the paper and is among the established PTMs.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "is_executed": {
          "value": false,
          "justification": "The research does not focus on execution but on social and usage dynamics in the community.",
          "quote": "our qualitative analysis, we present a taxonomy of the challenges and benefits associated with PTM reuse within this community."
        },
        "is_compared": {
          "value": true,
          "justification": "BART is compared for its usage trends and community discussion similar to other models.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        },
        "referenced_paper_title": {
          "value": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "justification": "The referenced paper for BART is essential for understanding its application in translation and text tasks.",
          "quote": "GPT2, T5, and BART are among the most popular ones."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "HuggingFace Datasets",
          "justification": "The paper mentions that HF provides a datasets hub alongside the model hub.",
          "quote": "HF also provides a dataset hub and a space hub and different libraries to facilitate their usage with the models."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Datasets documentation on Hugging Face",
          "justification": "The dataset hub mentioned is specific to Hugging Face repositories.",
          "quote": "HF also provides a dataset hub and a space hub and different libraries to facilitate their usage with the models."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Transformers",
          "justification": "HuggingFace Transformers is explicitly mentioned as a library provided by HF for PTM reuse.",
          "quote": "HuggingFace is a PTM reuse platform that provides open-source resources and services to the community. They provide several libraries for reusing PTMs, the most notable of which is HuggingFace Transformers."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Transformers - HuggingFace documentation",
          "justification": "The HF Transformers library is well-documented in the HuggingFace ecosystem.",
          "quote": "HuggingFace is a PTM reuse platform that provides open-source resources and services to the community. They provide several libraries for reusing PTMs."
        }
      },
      {
        "name": {
          "value": "Evaluate",
          "justification": "The Evaluate library by Hugging Face is mentioned in the context of model evaluation.",
          "quote": "Evaluate: A library for easily evaluating machine learning models and datasets."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Evaluate - HuggingFace documentation",
          "justification": "The referenced paper is the HuggingFace documentation related to the Evaluate tool.",
          "quote": "Evaluate: A library for easily evaluating machine learning models and datasets."
        }
      },
      {
        "name": {
          "value": "Miro",
          "justification": "Miro is used as a collaborative visual workspace in the methodology for thematic analysis.",
          "quote": "For further analysis, we exported these codes to Miro which is a collaborative visual workspace for managing ideas."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Miro board for visual collaboration",
          "justification": "Though a known tool for visualization, in this context it is used in the methodological framework of the paper.",
          "quote": "For further analysis, we exported these codes to Miro which is a collaborative visual workspace for managing ideas."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2281,
    "prompt_tokens": 17853,
    "total_tokens": 20134,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}