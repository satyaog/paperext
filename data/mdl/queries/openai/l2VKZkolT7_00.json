{
  "paper": "l2VKZkolT7.txt",
  "words": 10757,
  "extractions": {
    "title": {
      "value": "Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Feature Likelihood Divergence: Evaluating the Generalization of Generative Models Using Samples"
    },
    "description": "The paper proposes a new metric called Feature Likelihood Divergence (FLD) for evaluating deep generative models, addressing limitations of current methods by incorporating novelty, fidelity, and diversity. The authors demonstrate its efficacy through empirical evaluations on various image datasets and model classes.",
    "type": {
      "value": "empirical",
      "justification": "The paper is empirical in nature as it introduces a new metric and demonstrates its performance through various experiments and evaluations.",
      "quote": "We empirically demonstrate the ability of FLD to identify overfitting problem cases, even when previously proposed metrics fail."
    },
    "primary_research_field": {
      "name": {
        "value": "Generative Models",
        "justification": "The paper focuses on evaluating the generalization of generative models through a new metric (FLD).",
        "quote": "Generative modeling is one of the fastest-growing areas of deep learning, with success stories spanning the artificial intelligence spectrum."
      },
      "aliases": [
        "Generative Modeling"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Evaluation Metrics",
          "justification": "The core contribution of the paper is a new evaluation metric for generative models.",
          "quote": "We propose the feature likelihood divergence (FLD): a novel sample-based metric that captures sample fidelity, diversity, and novelty."
        },
        "aliases": [
          "Evaluation Methods"
        ]
      },
      {
        "name": {
          "value": "Image Synthesis",
          "justification": "The evaluations are performed on various image datasets, and the generative models in question produce high-dimensional, photo-realistic images.",
          "quote": "We also extensively evaluate FLD on various image datasets and model classes, demonstrating its ability to match intuitions of previous metrics like FID while offering a more comprehensive evaluation of generative models."
        },
        "aliases": [
          "Image Generation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Inception-v3",
          "justification": "Inception-v3 is used as one of the pre-trained models for mapping samples to a perceptually meaningful feature space.",
          "quote": "Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 or DINOv2."
        },
        "aliases": [
          "Inception V3"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The Inception-v3 model is not contributed by this paper but is used as part of the methodology.",
          "quote": "Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 or DINOv2."
        },
        "is_executed": {
          "value": true,
          "justification": "Inception-v3 is utilized in the experimental setup of the paper.",
          "quote": "Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 or DINOv2."
        },
        "is_compared": {
          "value": false,
          "justification": "The model is not compared to other models in a performance sense but is used for feature extraction.",
          "quote": "Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 or DINOv2."
        },
        "referenced_paper_title": {
          "value": "Rethinking the inception architecture for computer vision",
          "justification": "This is the foundational paper for the Inception-v3 model.",
          "quote": "Rethinking the inception architecture for computer vision"
        }
      },
      {
        "name": {
          "value": "DINOv2",
          "justification": "DINOv2 is used as one of the pre-trained models for mapping samples to a perceptually meaningful feature space.",
          "quote": "Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 or DINOv2."
        },
        "aliases": [
          "DINO v2"
        ],
        "is_contributed": {
          "value": false,
          "justification": "DINOv2 is not contributed by this paper but is used as part of the methodology.",
          "quote": "Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 or DINOv2."
        },
        "is_executed": {
          "value": true,
          "justification": "DINOv2 is utilized in the experimental setup of the paper.",
          "quote": "Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 or DINOv2."
        },
        "is_compared": {
          "value": false,
          "justification": "The model is not compared to other models in a performance sense but is used for feature extraction.",
          "quote": "Intuitively, FLD achieves these goals by first mapping samples to a perceptually meaningful feature space such as a pre-trained Inception-v3 or DINOv2."
        },
        "referenced_paper_title": {
          "value": "Learning robust visual features without supervision",
          "justification": "This is the foundational paper for the DINOv2 model.",
          "quote": "Learning robust visual features without supervision"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR10",
          "justification": "CIFAR10 is one of the datasets used for evaluation in the paper.",
          "quote": "For example, on CIFAR10, the current standard FID computation uses 50k generated samples and 50k training samples from the dataset, a practice that does not take into account overfitting."
        },
        "aliases": [
          "CIFAR-10"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The CIFAR-10 dataset",
          "justification": "This is the foundational paper for the CIFAR-10 dataset.",
          "quote": "The CIFAR-10 dataset. online: http://www. cs. toronto. edu/kriz/cifar. html"
        }
      },
      {
        "name": {
          "value": "FFHQ",
          "justification": "FFHQ is one of the datasets used for evaluation in the paper.",
          "quote": "For datasets, we evaluate a variety of popular natural image benchmarks in CIFAR10, FFHQ and ImageNet."
        },
        "aliases": [
          "Flickr-Faces-HQ"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "A style-based generator architecture for generative adversarial networks",
          "justification": "This is the foundational paper for the FFHQ dataset.",
          "quote": "A style-based generator architecture for generative adversarial networks."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is one of the datasets used for evaluation in the paper.",
          "quote": "For datasets, we evaluate a variety of popular natural image benchmarks in CIFAR10, FFHQ and ImageNet."
        },
        "aliases": [
          "ImageNet Large Scale Visual Recognition Challenge"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A large-scale hierarchical image database",
          "justification": "This is the foundational paper for the ImageNet dataset.",
          "quote": "ImageNet: A large-scale hierarchical image database."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is mentioned in the references and is likely used for implementing the models and evaluations.",
          "quote": "TorchVision: PyTorch’s Computer Vision library."
        },
        "aliases": [
          "torch",
          "torchvision"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "TorchVision: PyTorch’s Computer Vision library",
          "justification": "The paper mentions the TorchVision library, which is part of the PyTorch framework.",
          "quote": "TorchVision: PyTorch’s Computer Vision library."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1534,
    "prompt_tokens": 19192,
    "total_tokens": 20726
  }
}