{
  "paper": "gpJw8f4tIU.txt",
  "words": 12492,
  "extractions": {
    "title": {
      "value": "Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL",
      "justification": "The value exactly matches the title of the paper.",
      "quote": "Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL"
    },
    "description": "This paper introduces Contrastive Retrospection (ConSpec), a new reinforcement learning (RL) algorithm designed to hone in on critical steps for success using offline contrastive learning. The algorithm can be added to existing RL setups to improve identification and generalization of these critical steps across various tasks. The paper demonstrates that ConSpec significantly improves learning efficiency and out-of-distribution generalization across diverse RL tasks including grid-world, Atari, and 3D environments.",
    "type": {
      "value": "empirical",
      "justification": "The paper demonstrates the application and effectiveness of the ConSpec algorithm through various experiments on different RL tasks.",
      "quote": "We tested ConSpec on a variety of RL tasks: grid worlds, continuous control, video games, and 3-D environments. Training was done on an RTX 8000 GPU cluster and hyperparameter choices are detailed in the Appendix A.5."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary focus of the paper is on improving reinforcement learning algorithms by introducing ConSpec, which aids in rapid learning and generalization for RL tasks.",
        "quote": " Here, we present a new RL algorithm that uses offline contrastive learning to hone in on these critical steps."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Contrastive Learning",
          "justification": "The paper introduces a novel contrastive loss to identify critical steps in reinforcement learning tasks.",
          "quote": "Contrastive Retrospection (ConSpec) learns a set of prototypes for the critical steps in a task by a novel contrastive loss."
        },
        "aliases": [
          "CL"
        ]
      },
      {
        "name": {
          "value": "Hierarchical Reinforcement Learning",
          "justification": "ConSpec's idea of identifying critical steps and prototypes is somewhat related to sub-goal discovery in hierarchical reinforcement learning.",
          "quote": "ConSpec shares with bottleneck states, hierarchical RL (HRL), and options discovery the idea that learning to hone in on a sparse number of critical steps may be beneficial."
        },
        "aliases": [
          "HRL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Contrastive Retrospection (ConSpec)",
          "justification": "The paper explicitly names the presented algorithm as Contrastive Retrospection (ConSpec), which is the core model introduced.",
          "quote": "we introduce Contrastive Retrospection (ConSpec), which can be added to any backbone RL algorithm."
        },
        "aliases": [
          "ConSpec"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper contributes ConSpec as a new model for reinforcement learning tasks.",
          "quote": "Our contributions in this paper are as follows: • We introduce a scalable algorithm (ConSpec) for rapidly honing in on critical steps."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments in the paper were conducted using GPU resources.",
          "quote": "Training was done on an RTX 8000 GPU cluster and hyperparameter choices are detailed in the Appendix A.5."
        },
        "is_compared": {
          "value": true,
          "justification": "ConSpec is compared against several other methods like PPO, SynthRs, and CURL through various experiments in the paper.",
          "quote": "The prototype that recognized picking up flat objects immediately generalized to black flat objects (Fig. 3b). Thanks to this, the agent with ConSpec solved the task with the black objects in zero-shot (Fig. 3e). This shows that ConSpec was able to learn prototypes that discovered an invariant feature among successes (flatness)"
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "ConSpec is a new algorithm introduced in this paper and does not have a previously referenced paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Grid-world",
          "justification": "The experiments included in the paper use a grid-world dataset to validate the effectiveness of ConSpec.",
          "quote": "We tested ConSpec on a variety of RL tasks: grid worlds, continuous control, video games, and 3-D environments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no specific reference paper for the grid-world dataset as it is a common environment used in RL research.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "3D environments",
          "justification": "The experiments included in the paper also involve 3D environments to test ConSpec's capabilities.",
          "quote": "We tested ConSpec on a variety of RL tasks: grid worlds, continuous control, video games, and 3-D environments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no specific reference paper for the 3D environments used as they are standard in RL research.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Atari",
          "justification": "The experiments included various Atari games to benchmark ConSpec's performance.",
          "quote": "We show that ConSpec greatly improves long-term credit assignment in a wide variety of RL tasks including grid-world, Atari, and 3D environments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Playing Atari with Deep Reinforcement Learning",
          "justification": "The foundational reference for using Atari games in RL research is the paper by Mnih et al. (2013), which introduced the idea.",
          "quote": "Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing Atari with Deep Reinforcement Learning. arXiv e-prints, art. arXiv:1312.5602, December 2013."
        }
      },
      {
        "name": {
          "value": "SilicoLabs' Unity",
          "justification": "The experiments involved creating a custom 3D environment called the 'OrangeTree' task using SilicoLabs' Unity-based game engine.",
          "quote": "To do this, we used SilicoLabs’3 Unity-based (31) game engine to create a 3-stage task that we call the \"OrangeTree\" task."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Unity: A General Platform for Intelligent Agents",
          "justification": "The reference paper for the Unity game engine by Juliani et al. (2018) is cited.",
          "quote": "Juliani, A., Berges, V.-P., Teng, E., Cohen, A., Harper, J., Elion, C., Goy, C., Gao, Y., Henry, H., Mattar, M., and Lange, D. Unity: A General Platform for Intelligent Agents. arXiv e-prints, art. arXiv:1809.02627, September 2018."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PPO (Proximal Policy Optimization)",
          "justification": "The PPO algorithm was used as a baseline in the experiments to compare against ConSpec.",
          "quote": "To further test ConSpec’s capacity for generalization, we made another variation of the OrangeTree task. During training, the agent saw flat objects that were magenta, blue, red, green, yellow, or peach (Fig. 3a). During testing, a black table or box was presented, even though the agent had never seen black objects before. The prototype that recognized picking up flat objects immediately generalized to black flat objects (Fig. 3b). Thanks to this, the agent with ConSpec solved the task with the black objects in zero-shot (Fig. 3e). This shows that ConSpec was able to learn prototypes that discovered an invariant feature among successes (flatness), and was not confused by irrelevant sensory features such as colour, permitting it to generalize to contingencies involving colours never seen before."
        },
        "aliases": [
          "PPO"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "The PPO algorithm is based on the paper by Schulman et al. (2017), which is specifically cited in the references.",
          "quote": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal Policy Optimization Algorithms. arXiv e-prints, art. arXiv:1707.06347, July 2017."
        }
      },
      {
        "name": {
          "value": "CURL (Contrastive Unsupervised Representations for Reinforcement Learning)",
          "justification": "CURL was used as another baseline to validate the performance of ConSpec.",
          "quote": "Thus, we next considered more sophisticated representation learning for RL: Contrastive Unsupervised Reinforcement Learning (CURL) (56), which outperforms prior pixel-based methods on complex tasks. However, adding CURL to PPO, too, did not solve the task (Fig. 2b), likely because CURL is not learning invariances for task success, but rather, invariances for image perturbations."
        },
        "aliases": [
          "CURL"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "CURL: Contrastive Unsupervised Representations for Reinforcement Learning",
          "justification": "The CURL algorithm is based on the paper by Srinivas et al. (2020), which is specifically cited in the references.",
          "quote": "Srinivas, A., Laskin, M., and Abbeel, P. CURL: Contrastive Unsupervised Representations for Reinforcement Learning. arXiv e-prints, art. arXiv:2004.04136, April 2020."
        }
      },
      {
        "name": {
          "value": "RIMs (Recurrent Independent Mechanisms)",
          "justification": "RIMs were tested as part of the algorithmic solution space in the experiments.",
          "quote": "Along similar lines, (56; 2; 54; 19; 21; 18; 36; 1) have begun to explore contrastive systems and binary classifiers to do imitation learning and RL. With these works, ConSpec shares the principle that transforming the RL problem into a classification task is beneficial. But these works typically use the contrastive approach to learn whole policies or value functions. In distinction to this, ConSpec uses contrastive learning for the purpose of learning prototype representations for recognizing critical steps in order to shape the reward function, thereby enabling rapid learning and generalization."
        },
        "aliases": [
          "RIM"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Recurrent Independent Mechanisms",
          "justification": "The reference paper for RIMs by Goyal et al. (2019) is cited.",
          "quote": "Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., and Schölkopf, B. Recurrent Independent Mechanisms. arXiv e-prints, art. arXiv:1909.10893, September 2019."
        }
      },
      {
        "name": {
          "value": "RND (Random Network Distillation)",
          "justification": "RND was used as a comparison point in the experimental setup to illustrate ConSpec's capabilities.",
          "quote": "We also note that RND, despite being strong enough to handle sparse reward tasks like Montezuma’s Revenge (10), also fails here as more keys are added, illustrating the point that long term credit assignment is its own distinct issue beyond exploration which ConSpec addresses and RND does not."
        },
        "aliases": [
          "RND"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Exploration by Random Network Distillation",
          "justification": "The reference paper for RND by Burda et al. (2018) is cited.",
          "quote": "Burda, Y., Edwards, H., Storkey, A., and Klimov, O. Exploration by Random Network Distillation. arXiv e-prints, art. arXiv:1810.12894, October 2018."
        }
      },
      {
        "name": {
          "value": "RUDDER: Return Decomposition for Delayed Rewards",
          "justification": "RUDDER was also used in experiments to benchmark ConSpec’s performance.",
          "quote": "We also tested ConSpec on another set of grid-world tasks with multiple contingencies, but with a different and harder to learn task structure. Again, ConSpec but not other baselines could solve these tasks (Fig. A.16), illustrating that ConSpec successfully handles a wide variety of real-life inspired scenarios with multiple contingencies."
        },
        "aliases": [
          "RUDDER",
          "Return Decomposition for Delayed Rewards"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "RUDDER: Return Decomposition for Delayed Rewards",
          "justification": "The reference paper for RUDDER by Arjona-Medina et al. (2018) is cited.",
          "quote": "Arjona-Medina, J. A., Gillhofer, M., Widrich, M., Unterthiner, T., Brandstetter, J., and Hochreiter, S. RUDDER: Return Decomposition for Delayed Rewards. arXiv e-prints, art. arXiv:1806.07857, June 2018."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 5357,
    "prompt_tokens": 44604,
    "total_tokens": 49961
  }
}