{
  "paper": "2310.17722.txt",
  "words": 17959,
  "extractions": {
    "title": {
      "value": "Large Language Models as Generalizable Policies for Embodied Tasks",
      "justification": "The title is taken from the top of the provided research paper.",
      "quote": "Large Language Models as Generalizable Policies for Embodied Tasks"
    },
    "description": "This paper explores the use of large language models (LLMs) as adaptable and generalizable policies for embodied visual tasks. The approach, named Large Language model Reinforcement Learning Policy (LLaRP), leverages reinforcement learning to teach a pre-trained frozen LLM to process text instructions and visual observations to generate actions within an environment. The study's key contributions include training the LLaRP model using reinforcement learning, evaluating its generalization abilities across unseen tasks, and introducing a new benchmark, Language Rearrangement, to facilitate further research. The results show that LLaRP achieves a success rate of 42% on 1000 unseen tasks, significantly outperforming various baselines.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves empirical evaluation of the proposed Large Language model Reinforcement Learning Policy (LLaRP) by training it and comparing its performance on various tasks against baselines.",
      "quote": "We demonstrate advanced capabilities on a diverse set of rearrangement tasks, where the input and output domains arenâ€™t just language"
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary research field is Reinforcement Learning as the paper focuses on leveraging reinforcement learning to train the Large Language model Reinforcement Learning Policy (LLaRP).",
        "quote": "We demonstrate that by utilizing Reinforcement Learning together with a pre-trained LLM and maximizing only sparse rewards, we can learn a policy that generalizes to novel language rearrangement tasks"
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Embodied AI",
          "justification": "A significant portion of the paper's focus is on applying LLMs to embodied visual tasks and interactive learning within an environment, which falls under Embodied AI.",
          "quote": "We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Vision-Language Models",
          "justification": "The paper discusses the adaptation of large language models for tasks that involve both language and visual input, classifying it under Vision-Language Models.",
          "quote": "First, we show that using a pre-trained and frozen LLM as a Vision-Language Model (VLM) policy with learned input and output adapter layers results in a policy that exhibits strong generalization capabilities."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "LLaRP",
          "justification": "LLaRP (Large Language model Reinforcement Learning Policy) is the core model proposed and evaluated in the paper.",
          "quote": "Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment"
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "LLaRP is the proposed method introduced and evaluated in the paper.",
          "quote": "Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment"
        },
        "is_executed": {
          "value": true,
          "justification": "The model is trained and evaluated using GPU resources.",
          "quote": "We convert the LLaMA weights to bfloat16. The observation encoder and action output modules represent their weights in float32. Despite running such a large policy with RL, we find that total training throughput is 700-800 steps-per-second on a full compute node of 8 GPUs"
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares LLaRP's performance against several other models including an LSTM-based policy, zero-shot applications of LLMs, and others.",
          "quote": "LLaRP is thoroughly evaluated on over 1, 000 unseen tasks spanning the above axes and attains 42% success rate, compared to 25% for an LSTM-based policy and 22% for zero-shot applications of LLMs."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The model LLaRP is introduced in this paper and does not reference another paper.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Language Rearrangement",
          "justification": "Language Rearrangement is a novel benchmark introduced in the paper, consisting of 150,000 training tasks and 1,000 testing tasks specifically designed for evaluating language-conditioned rearrangement.",
          "quote": "Finally, to aid the community in studying language-conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150, 000 training and 1, 000 testing tasks for language-conditioned rearrangement"
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "Language Rearrangement is a novel benchmark introduced in this paper.",
          "quote": "Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150, 000 training and 1, 000 testing tasks for language-conditioned rearrangement"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "LLaMA",
          "justification": "LLaMA is a pre-trained large language model that is adapted and utilized within the proposed LLaRP method.",
          "quote": "By default, we use the base LLaMA-7B V1 (Touvron et al., 2023) for the LLM in LLaRP"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Llama: Open and Efficient Foundation Language Models",
          "justification": "The reference for LLaMA is appropriately provided in the paper's related work section.",
          "quote": "By default, we use the base LLaMA-7B V1 (Touvron et al., 2023) for the LLM in LLaRP"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1245,
    "prompt_tokens": 33706,
    "total_tokens": 34951
  }
}