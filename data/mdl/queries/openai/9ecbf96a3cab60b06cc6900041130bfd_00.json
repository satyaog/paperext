{
  "paper": "9ecbf96a3cab60b06cc6900041130bfd.txt",
  "words": 12249,
  "extractions": {
    "title": {
      "value": "Learning What You Need from What You Did: Product Taxonomy Expansion with User Behaviors Supervision",
      "justification": "The title of the paper is explicitly stated at the beginning of the document.",
      "quote": "Learning What You Need from What You Did: Product Taxonomy Expansion with User Behaviors Supervision"
    },
    "description": "This paper presents a self-supervised and user behavior-oriented framework to automate the expansion of product taxonomies by incorporating user behavior data. The framework extracts and models hyponymy relations using user click logs and user-generated content. It leverages Pre-trained Language Models and Graph Neural Networks combined with Contrastive Learning, enhancing the semantic understanding of concepts and relations. Extensive experiments demonstrate its effectiveness on real-world product taxonomies.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experiments and validations performed on real-world datasets to demonstrate the effectiveness of their proposed framework.",
      "quote": "Extensive experiments on real-world product taxonomies in Meituan Platform, a leading Chinese vertical e-commerce platform..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on taxonomy expansion, which involves understanding and processing natural language constructs and relations.",
        "quote": "Pre-trained Language Models and Graph Neural Network combined with Contrastive Learning;..."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Taxonomy Expansion",
          "justification": "The main focus of the paper is on automatically expanding product taxonomies using user behavior data.",
          "quote": "...our proposed method designs relational and structural representations learned from user-generated content and user click logs to model the semantics of in-domain concepts."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Graph Neural Networks",
          "justification": "The method leverages graph neural networks to model structural information from user data.",
          "quote": "Graph Neural Network combined with Contrastive Learning; iii) to reduce the cost of dataset construction and overcome data skews..."
        },
        "aliases": [
          "GNN"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "C-BERT",
          "justification": "C-BERT is mentioned as a model enhanced from an original pre-trained language model, tailored for concept-level understanding in the domain of the paper.",
          "quote": "...an enhanced model called C(oncept)-BERT, modeling domain-specific concept knowledge."
        },
        "aliases": [
          "Concept-BERT"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model C-BERT is specifically enhanced for the purposes of this paper to better model domain-specific knowledge.",
          "quote": "Due to the lack of domain knowledge in vanilla BERT-Chinese, we acquire an enhanced model called C(oncept)-BERT..."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not indicate that C-BERT was specifically executed, but rather trained and used conceptually for modeling representations.",
          "quote": "Due to the lack of domain knowledge in vanilla BERT-Chinese..."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper uses C-BERT to improve upon the limitations of vanilla BERT, suggesting a comparison was made.",
          "quote": "...the enhanced model called C(oncept)-BERT modeling domain-specific concept knowledge."
        },
        "referenced_paper_title": {
          "value": "Pre-training with whole word masking for chinese bert",
          "justification": "The C-BERT model is built upon modifications of BERT, which relies on pre-training methods mentioned in this paper.",
          "quote": "Due to the lack of domain knowledge in vanilla BERT-Chinese..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Meituan Gourmet Food Taxonomy",
          "justification": "This dataset from the Meituan platform is used in the paper as a primary testbed for experiments.",
          "quote": "In this paper, we take the Meituan Gourmet Food Taxonomy as the major testbed for all the following experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A user-centered concept mining system for query and document understanding at Tencent",
          "justification": "The dataset is discussed as part of understanding user behavior and related studies at scale in e-commerce.",
          "quote": "A user-centered concept mining system for query and document understanding at Tencent, 2019."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is referenced as a foundational model that C-BERT builds upon for pre-training and domain-specific enhancements.",
          "quote": "Due to the lack of domain knowledge in vanilla BERT-Chinese..."
        },
        "aliases": [
          "Bidirectional Encoder Representations from Transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pre-training with whole word masking for chinese bert",
          "justification": "The paper references the concept of whole word masking in BERT, which underlies their adaptation efforts.",
          "quote": "Pre-training with whole word masking for chinese bert."
        }
      },
      {
        "name": {
          "value": "Graph Convolutional Network",
          "justification": "Graph Convolutional Network is used within the paper to update representations based on adjacent nodes within a graph.",
          "quote": "we expressly adopt Graph Convolutional Network (GCN) as follows."
        },
        "aliases": [
          "GCN"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Semi-supervised classification with graph convolutional networks",
          "justification": "The GCN cited in the methodology is supported by existing literature on semi-supervised learning.",
          "quote": "we expressly adopt Graph Convolutional Network (GCN) as follows."
        }
      },
      {
        "name": {
          "value": "InfoNCE",
          "justification": "InfoNCE loss is adopted in their contrastive learning approach for pre-training graph neural networks.",
          "quote": "The loss function is defined based on InfoNCE as follows."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Representation learning with contrastive predictive coding",
          "justification": "The influential paper on contrastive learning and representation techniques, mentioned for adopting their loss calculation.",
          "quote": "Representation learning with contrastive predictive coding."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1185,
    "prompt_tokens": 22053,
    "total_tokens": 23238,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}