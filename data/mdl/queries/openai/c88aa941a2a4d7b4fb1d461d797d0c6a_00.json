{
  "paper": "c88aa941a2a4d7b4fb1d461d797d0c6a.txt",
  "words": 12052,
  "extractions": {
    "title": {
      "value": "Think Before You Act: Decision Transformers with Working Memory",
      "justification": "The title clearly states the main focus of the paper, which is on Decision Transformers integrated with a working memory mechanism.",
      "quote": "Think Before You Act: Decision Transformers with Working Memory"
    },
    "description": "This paper introduces Decision Transformers with Memory (DT-Mem), a novel architecture incorporating a working memory module to improve the adaptability and generalization of Transformer-based models on multiple tasks. Inspired by human cognitive processes, this module stores and retrieves information to mitigate the forgetting phenomenon commonly encountered in decision-making agents. The model is evaluated in Atari games and Meta-World environments, showing improvements in training efficiency and performance over existing models.",
    "type": {
      "value": "empirical",
      "justification": "The paper evaluates the proposed DT-Mem model through experiments on Atari games and Meta-World environments, making it an empirical study.",
      "quote": "To validate our approach, we evaluate DT-Mem in two environments and compare against a set of strong baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on decision-making agents and evaluates the generalization of models in environments like Atari games, which are common in reinforcement learning research.",
        "quote": "Transformer-based Reinforcement Learning methods... to improve model online adaptability, Zheng et al. (2022) propose the Online Decision Transformer."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Memory-Augmented Neural Networks",
          "justification": "The paper introduces a memory module integrated with Decision Transformers, which falls under the scope of memory-augmented neural networks.",
          "quote": "DT-Mem builds on earlier work on memory-augmented neural networks."
        },
        "aliases": [
          "Memory Networks"
        ]
      },
      {
        "name": {
          "value": "Transformer Models",
          "justification": "The proposed DT-Mem model is based on Transformer architecture, focusing on improving decision-making and generalization.",
          "quote": "Recently, with the tremendous success of decoder-only Transformer models... an increasing number of researchers have focused on decoder-only Transformer-based decision-making agents."
        },
        "aliases": [
          "Transformer"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Decision Transformer with Memory (DT-Mem)",
          "justification": "The paper's primary contribution is the Decision Transformer with Memory (DT-Mem), designed to incorporate working memory into Decision Transformers.",
          "quote": "we introduce Decision Transformers with Memory (DT-Mem): which represents a working memory as a matrix."
        },
        "aliases": [
          "DT-Mem"
        ],
        "is_contributed": {
          "value": true,
          "justification": "DT-Mem is the main novel contribution of the paper, aimed at improving model generalization and efficiency.",
          "quote": "Inspired by this, we propose a working memory module to store, blend, and retrieve information for different downstream tasks."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper describes the execution of DT-Mem in experiments with Atari games and Meta-World tasks.",
          "quote": "To validate our approach, we evaluate DT-Mem in two environments and compare against a set of strong baselines."
        },
        "is_compared": {
          "value": true,
          "justification": "DT-Mem's performance is compared against strong baselines in experiments to highlight its advantages.",
          "quote": "Evaluation results show that the proposed method improves training efficiency and generalization in Atari games and Meta-World object manipulation tasks."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper does not explicitly provide a reference paper title for DT-Mem as it is an original contribution by the authors.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Multi-game Decision Transformer (MDT)",
          "justification": "MDT is mentioned as a baseline for comparisons in the experiments section of the paper.",
          "quote": "We compare against Multi-game Decision Transformer (MDT, Lee et al., 2022)."
        },
        "aliases": [
          "MDT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "MDT is used as a baseline model for comparison, not as a contribution of this paper.",
          "quote": "We compare against Multi-game Decision Transformer (MDT, Lee et al., 2022)."
        },
        "is_executed": {
          "value": false,
          "justification": "MDT is only mentioned as a baseline and not executed as part of the paper's experiments.",
          "quote": ""
        },
        "is_compared": {
          "value": true,
          "justification": "MDT serves as a baseline for comparison with the proposed DT-Mem model.",
          "quote": "We compare against Multi-game Decision Transformer (MDT, Lee et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "Multi-game Decision Transformer",
          "justification": "The paper references MDT in comparison studies and provides the citation for its original description.",
          "quote": "We compare against Multi-game Decision Transformer (MDT, Lee et al., 2022)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari games dataset",
          "justification": "The paper uses Atari games datasets to evaluate the DT-Mem model, as common in reinforcement learning studies.",
          "quote": "Evaluation results show that the proposed method improves training efficiency and generalization in Atari games."
        },
        "aliases": [
          "Atari"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Multi-game Decision Transformer",
          "justification": "The paper references this dataset in the context of experiments comparing it with MDT's performance, originally described in Lee et al., 2022.",
          "quote": "Due to limited compute resources and to prevent cherry-picking, we select 17 games from a total of 41 as introduced in (Lee et al., 2022)."
        }
      },
      {
        "name": {
          "value": "Meta-World ML45",
          "justification": "Meta-World ML45 is used to evaluate DT-Mem on object manipulation tasks.",
          "quote": "We evaluate the proposed method on the Meta-World environment (Yu et al., 2019)."
        },
        "aliases": [
          "Meta-World"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning",
          "justification": "The paper uses this dataset for evaluating DT-Mem's performance on generic object manipulation tasks.",
          "quote": "We evaluate the proposed method on the Meta-World environment (Yu et al., 2019)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "While the paper does not explicitly mention PyTorch, it is commonly used for implementing models like DT-Mem that require advanced neural network operations.",
          "quote": "We open source the code at https://github.com/luciferkonn/DT_Mem."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper does not provide a reference title for PyTorch, as it is typically used as an implementation tool.",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1355,
    "prompt_tokens": 20918,
    "total_tokens": 22273,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}