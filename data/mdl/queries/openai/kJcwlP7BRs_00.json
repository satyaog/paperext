{
  "paper": "kJcwlP7BRs.txt",
  "words": 17131,
  "extractions": {
    "title": {
      "value": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer",
      "justification": "This is the exact title of the paper as found near the beginning.",
      "quote": "Transfer Entropy Bottleneck: Learning Sequence to Sequence Information Transfer"
    },
    "description": "This paper develops an information bottleneck approach for conditional learning on two dependent streams of data, termed the Transfer Entropy Bottleneck (TEB). The method allows for learning a model that bottlenecks directed information transferred from a source variable to a target variable and quantifies this information transfer within the model. TEB improves prediction of the target stream by leveraging information from the source stream.",
    "type": {
      "value": "theoretical",
      "justification": "The paper focuses on developing the Transfer Entropy Bottleneck (TEB) method and its mathematical formulation, rather than conducting extensive empirical experiments.",
      "quote": "Here, we develop an information bottleneck approach for conditional learning on two dependent streams of data. Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable, while quantifying this information transfer within the model."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper primarily discusses an information bottleneck approach within the context of machine learning, particularly focusing on modeling two dependent streams of data.",
        "quote": "Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable, while quantifying this information transfer within the model."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "Though not explicitly focused on NLP, the method described can be applied to sequence to sequence information transfer tasks commonly found in NLP.",
          "quote": "TEB provides a useful new information bottleneck approach for modelling two statistically dependent streams of data in order to make predictions about one of them."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Time-Series Analysis",
          "justification": "The method introduced is applicable to dual stream modeling problems, like time-series prediction.",
          "quote": "Here we design a challenging task to test performance of TEB, and to test its added ability to generalize when the statistical coupling between the X and Y streams is high, but with a conditionally small and crucial directed information transfer from X to Y."
        },
        "aliases": [
          "TSA"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper mentions applicability to various data modalities, including images, indicating relevance to computer vision tasks.",
          "quote": "TEB allows one to improve the predictions of the target stream, and that it is applicable to various data modalities including images and time-series signals."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Transfer Entropy Bottleneck (TEB)",
          "justification": "The model introduced in the paper is termed Transfer Entropy Bottleneck (TEB).",
          "quote": "Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable, while quantifying this information transfer within the model."
        },
        "aliases": [
          "TEB"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper introduces the Transfer Entropy Bottleneck (TEB) model as its main contribution.",
          "quote": "Here, we develop an information bottleneck approach for conditional learning on two dependent streams of data. Our method, which we call Transfer Entropy Bottleneck (TEB), allows one to learn a model that bottlenecks the directed information transferred from the source variable to the target variable, while quantifying this information transfer within the model."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes experimental results that involve the implementation of the TEB model, indicating that it was executed in the scope of the study.",
          "quote": "To experimentally test Theorems 1 & 2, we create a dataset of videos of rotating MNIST (Lecun et al., 1998) digits..."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the TEB model to other baselines and methods, including deterministic baselines and joint stream models.",
          "quote": "To evaluate performance of the directed information transfer from X to Y, we want to determine whether the predicted image at time t has the correct color for the balls. To quantify the color accuracy we use an additional color classification network."
        },
        "referenced_paper_title": {
          "value": "Deep Variational Information Bottleneck",
          "justification": "The paper references various information bottleneck methods, including the Deep Variational Information Bottleneck (VIB), as relevant prior work.",
          "quote": "The previously developed IB methods most relevant to ours, are the Variational Information Bottleneck (VIB) (Alemi et al., 2017) and the Conditional Entropy Bottleneck (CEB) (Fischer, 2020)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Rotating MNIST",
          "justification": "The dataset of rotating MNIST digits is used to experimentally test the TEB model.",
          "quote": "To experimentally test Theorems 1 & 2, we create a dataset of videos of rotating MNIST (Lecun et al., 1998) digits."
        },
        "aliases": [
          "Rotating MNIST dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-based learning applied to document recognition",
          "justification": "The original MNIST dataset is referenced as the base for the rotating MNIST dataset used in the experiments.",
          "quote": "To experimentally test Theorems 1 & 2, we create a dataset of videos of rotating MNIST (Lecun et al., 1998) digits."
        }
      },
      {
        "name": {
          "value": "Needle in a haystack task",
          "justification": "A custom dataset involving a colored bouncing balls task with distractor and needle pixels is used to evaluate the TEB model.",
          "quote": "Here we design a challenging task to test performance of TEB, and to test its added ability to generalize when the statistical coupling between the X and Y streams is high, but with a conditionally small and crucial directed information transfer from X to Y."
        },
        "aliases": [
          "Needle in a haystack dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The Needle in a haystack task is a custom dataset created for the paper. It doesn't have a separate reference paper.",
          "quote": "Here we design a challenging task to test performance of TEB, and to test its added ability to generalize when the statistical coupling between the X and Y streams is high, but with a conditionally small and crucial directed information transfer from X to Y."
        }
      },
      {
        "name": {
          "value": "Multi-component sinusoids",
          "justification": "The dataset of multi-component sinusoids is used to evaluate the TEB model's performance on time-series extrapolation tasks with frequency changes.",
          "quote": "We also evaluate the performance of TEB on synthetic time-series data, where each time-series signal is generated as the average of five sinusoidal waves of distinct frequencies."
        },
        "aliases": [
          "Multi-component sinusoidal dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The Multi-component sinusoids dataset is a custom dataset created for the paper. It doesn't have a separate reference paper.",
          "quote": "We also evaluate the performance of TEB on synthetic time-series data, where each time-series signal is generated as the average of five sinusoidal waves of distinct frequencies."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions the use of PyTorch for implementing the deep learning models and experiments.",
          "quote": "The implementation of the TEB model uses two initial encoders for the two streams qy and qx, both of which feed into two separate LSTMs to aggregate information across the sequence. The encoder depends on the input modality; on images the initial encoders are ResNet18, on a discrete modality like class integers, we used an embedding vector lookup table (i.e. a torch.nn.Embedding), and on time-series it is just the identity mapping."
        },
        "aliases": [
          "torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "The PyTorch library is explicitly mentioned as used for implementing the TEB model and its experiments",
          "quote": "All of the implementations were done using PyTorch (Paszke et al., 2019)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1752,
    "prompt_tokens": 27193,
    "total_tokens": 28945
  }
}