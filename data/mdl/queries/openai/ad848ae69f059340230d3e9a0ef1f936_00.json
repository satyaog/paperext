{
  "paper": "ad848ae69f059340230d3e9a0ef1f936.txt",
  "words": 10573,
  "extractions": {
    "title": {
      "value": "INViTE: INterpret and Control Vision-Language Models with Text Explanations",
      "justification": "The title clearly reflects the focus of the paper, which is on interpreting and controlling vision-language models using text explanations.",
      "quote": "We present INViTE: a framework for INterpreting Vision Transformer’s latent tokens with Text Explanations."
    },
    "description": "This paper introduces INViTE, a framework for interpreting vision transformers by explaining latent tokens with text descriptions. The approach allows for controlling and intervening in model reasoning without additional data collection or retraining, improving robustness and addressing biases in vision-language models.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical experiments demonstrating the effectiveness of the INViTE framework in interpreting and controlling vision-language models.",
      "quote": "Visualization and empirical experiments show that our method produces text explanations for latent tokens more aligned with the ground truth."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper focuses on improving the interpretability and control of vision-language models, which is a topic in machine learning.",
        "quote": "With the advent of large-scale foundation models, transformers have become the backbone for machine learning tasks."
      },
      "aliases": [
        "AI",
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The work is centered around vision-language models, specifically involving transformers used for visual reasoning.",
          "quote": "Large-scale pre-trained vision foundation models, such as CLIP, have become de facto backbones for various vision tasks."
        },
        "aliases": [
          "Vision"
        ]
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The framework involves understanding and interpreting vision transformers using text explanations, overlapping with NLP in processing text-based data.",
          "quote": "We extend this procedure to retrieve text descriptions for all latent tokens."
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is used as a reference model within the INViTE framework for vision-language tasks.",
          "quote": "In the established CLIP model, a text description is retrieved from a set of provided vocabulary for entire image based on similarity between image and text representation."
        },
        "aliases": [
          "Contrastive Language–Image Pretraining"
        ],
        "is_contributed": {
          "value": false,
          "justification": "CLIP is used as a reference model, not contributed by this paper.",
          "quote": "In the established CLIP model..."
        },
        "is_executed": {
          "value": false,
          "justification": "The focus is not on executing CLIP but on interpreting its latent tokens with INViTE.",
          "quote": "Our approach allows text interpretation for latent tokens without any training or data collection."
        },
        "is_compared": {
          "value": false,
          "justification": "CLIP is not directly compared to other models within this context but used for illustrating the INViTE framework.",
          "quote": "In the established CLIP model..."
        },
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "This is the original title of the paper introducing CLIP.",
          "quote": "Learning transferable visual models from natural language supervision."
        }
      },
      {
        "name": {
          "value": "ViT (Vision Transformer)",
          "justification": "ViT is the type of model that the INViTE framework seeks to interpret.",
          "quote": "The vision transformer (ViT) was the first work to adapt the transformer for vision tasks."
        },
        "aliases": [
          "Vision Transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "ViT was not introduced in this paper, only utilized for interpretation.",
          "quote": "The vision transformer (ViT) was the first work to adapt the transformer for vision tasks."
        },
        "is_executed": {
          "value": false,
          "justification": "ViT is not executed per se but interpreted via the framework.",
          "quote": "We propose INViTE: an approach to INterpret Vision Transformer latent tokens using Text Explanations."
        },
        "is_compared": {
          "value": false,
          "justification": "There is no model comparison involving ViT, as it serves as the subject of interpretation rather than comparison.",
          "quote": "Vision Transformer latent tokens..."
        },
        "referenced_paper_title": {
          "value": "An image is worth 16x16 words: Transformers for image recognition at scale",
          "justification": "This is the original paper that introduced the ViT model.",
          "quote": "An image is worth 16x16 words: Transformers for image recognition at scale."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "VAW Dataset",
          "justification": "The VAW dataset is used to assess if the annotated attribute emerges in vision transformer reasoning.",
          "quote": "VAW dataset (Pham et al., 2021) is a large-scale visual attributes dataset with bounding box labels for the attribution annotation."
        },
        "aliases": [
          "Visual Attributes in the Wild"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning to predict visual attributes in the wild",
          "justification": "This is the original title of the paper introducing the VAW dataset.",
          "quote": "Learning to predict visual attributes in the wild."
        }
      },
      {
        "name": {
          "value": "UC Merced Land Use Dataset",
          "justification": "This dataset is used to evaluate fixing typographical attacks and intervening in visual reasoning.",
          "quote": "UC Merced Land Use Dataset (Yang & Newsam, 2010) contains remote sensing satellite images of 21 classes, with 100 images in each class."
        },
        "aliases": [
          "UC Merced"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Bag-of-visual-words and spatial extensions for land-use classification",
          "justification": "This is the original title of the paper introducing the UC Merced Land Use Dataset.",
          "quote": "Bag-of-visual-words and spatial extensions for land-use classification."
        }
      },
      {
        "name": {
          "value": "CelebA Dataset",
          "justification": "Used to classify the hair color task as gray or not gray, highlighting spurious correlations with gender.",
          "quote": "CelebA Dataset. (Liu et al., 2015) We use the task of classifying the hair color as gray or not gray. The label is spuriously correlated with gender."
        },
        "aliases": [
          "CelebFaces Attributes"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep learning face attributes in the wild",
          "justification": "This is the original title of the paper introducing the CelebA dataset.",
          "quote": "Deep learning face attributes in the wild."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1279,
    "prompt_tokens": 19213,
    "total_tokens": 20492,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}