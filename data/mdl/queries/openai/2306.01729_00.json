{
  "paper": "2306.01729.txt",
  "words": 18932,
  "extractions": {
    "title": {
      "value": "Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans",
      "justification": "This is the concise title of the paper provided in the document.",
      "quote": "Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans"
    },
    "description": "This paper focuses on enhancing the robustness of task-oriented dialogue agents by augmenting them with workflow names and action plans. Specifically, the paper illustrates the utilization of text2text transformers like T5-small, base, and large to improve the models' ability to generalize to unseen workflows and actions. Extensive experiments are conducted using the Action-Based Conversations Dataset (ABCD).",
    "type": {
      "value": "empirical",
      "justification": "The article primarily describes experiments and results based on applying various models to a specific dataset to demonstrate the effectiveness of the proposed method.",
      "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The main focus of the paper is on dialogue systems, which falls under Natural Language Processing (NLP).",
        "quote": "We hypothesize that performance on task-oriented dialogue can be improved by augmenting language models with prompts."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Dialogue Systems",
          "justification": "The paper concentrates on task-oriented dialogue systems, which is a specific domain within NLP.",
          "quote": "Task-oriented dialogue is difficult in part because it involves understanding user intent, collecting information from the user, executing API calls, and generating helpful and fluent responses."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "T5-small",
          "justification": "The paper mentions the use of T5-small as one of the models for experiments.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The T5-small model was used for the experiments, but it is not introduced or contributed by this paper.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented and experimented upon during the study.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of T5-small with other models like T5-base and T5-large.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "referenced_paper_title": {
          "value": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "justification": "The referenced paper describes the architecture of the T5 model used in this study.",
          "quote": "We use the T5 Pytorch implementation provided by huggingface (hf)7 and train using the hf library."
        }
      },
      {
        "name": {
          "value": "T5-base",
          "justification": "The paper mentions the use of T5-base as one of the models for experiments.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The T5-base model was used for the experiments, but it is not introduced or contributed by this paper.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented and experimented upon during the study.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of T5-base with other models like T5-small and T5-large.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "referenced_paper_title": {
          "value": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "justification": "The referenced paper describes the architecture of the T5 model used in this study.",
          "quote": "We use the T5 Pytorch implementation provided by huggingface (hf)7 and train using the hf library."
        }
      },
      {
        "name": {
          "value": "T5-large",
          "justification": "The paper mentions the use of T5-large as one of the models for experiments.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The T5-large model was used for the experiments, but it is not introduced or contributed by this paper.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented and experimented upon during the study.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of T5-large with other models like T5-small and T5-base.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "referenced_paper_title": {
          "value": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "justification": "The referenced paper describes the architecture of the T5 model used in this study.",
          "quote": "We use the T5 Pytorch implementation provided by huggingface (hf)7 and train using the hf library."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Action-Based Conversations Dataset (ABCD)",
          "justification": "The paper uses the ABCD dataset for training and evaluating the models.",
          "quote": "We perform extensive experiments on the Action-Based Conversations Dataset (ABCD) with T5-small, base and large models."
        },
        "aliases": [
          "ABCD"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Action-based conversations dataset: A corpus for building more in-depth task-oriented dialogue systems",
          "justification": "The referenced paper introduces the ABCD dataset, which is used in this study.",
          "quote": "We finetune a T5 text2text model (Raffel et al., 2020) on the multi-step tasks of the Action Based Conversations Dataset (ABCD) (Chen et al., 2021),"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face's Transformers",
          "justification": "The library was used to implement and train the T5 models.",
          "quote": "We use the T5 Pytorch implementation provided by huggingface (hf)7 and train using the hf library."
        },
        "aliases": [
          "HF Transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Hugging Face's Transformers: State-of-the-art Natural Language Processing",
          "justification": "The referenced paper describes the library used to implement the models.",
          "quote": "We use the T5 Pytorch implementation provided by huggingface (hf)7 and train using the hf library."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1522,
    "prompt_tokens": 34474,
    "total_tokens": 35996
  }
}