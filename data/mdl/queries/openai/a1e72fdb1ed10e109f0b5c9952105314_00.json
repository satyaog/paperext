{
  "paper": "a1e72fdb1ed10e109f0b5c9952105314.txt",
  "words": 9099,
  "extractions": {
    "title": {
      "value": "Mapping Parallelism in a Functional IR through Constraint Satisfaction: A Case Study on Convolution for Mobile GPUs",
      "justification": "The title is explicitly mentioned at the beginning and throughout the paper where they describe the core focus as a case study on convolution for mobile GPUs.",
      "quote": "Mapping Parallelism in a Functional IR through Constraint\nSatisfaction: A Case Study on Convolution for Mobile GPUs"
    },
    "description": "This paper presents a novel approach to mapping parallelism using constraints within a functional intermediate representation for convolutional neural networks on mobile GPUs. The authors propose automatic generation of parallelization constraints to improve memory utilization and performance, especially for large search spaces in current parallel computing architectures. Their method shows competitive performance to state-of-the-art frameworks such as TVM, while significantly reducing memory usage.",
    "type": {
      "value": "empirical",
      "justification": "The paper evaluates its proposed methods against existing implementations, performs experiments, and reports on performance metrics.",
      "quote": "This paper proposes to extract parallelization con-\nstraints automatically from a functional IR and use a\nsolver to identify valid rewriting. Using a convolutional\nneural network on a mobile GPU as a use case, this ap-\nproach matches the performance of the ARM Compute\nLibrary GEMM convolution and the TVM-generated\nkernel consuming between 2× and 3.6× less memory."
    },
    "primary_research_field": {
      "name": {
        "value": "Parallel Computing Optimization",
        "justification": "The paper focuses on optimizing parallel code execution on GPUs, especially applied to convolution operations in neural networks.",
        "quote": "This paper proposes to extract parallelization con-\nstraints automatically from a functional IR and use a\nsolver to identify valid rewriting. Using a convolutional\nneural network on a mobile GPU as a use case, this ap-\nproach matches the performance of the ARM Compute\nLibrary GEMM convolution and the TVM-generated\nkernel consuming between 2× and 3.6× less memory."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Convolutional Neural Networks",
          "justification": "The paper uses VGG-16, a CNN, as a use case for evaluating their parallelization techniques.",
          "quote": "To evaluate this new approach, the VGG-16 [19] Con-\nvolutional Neural Network (CNN) is used as a use-case\non a mobile GPU."
        },
        "aliases": [
          "CNN"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "VGG-16",
          "justification": "The paper specifically mentions using VGG-16 as a use-case for evaluating parallelization on mobile GPUs.",
          "quote": "To evaluate this new approach, the VGG-16 [19] Con-\nvolutional Neural Network (CNN) is used as a use-case\non a mobile GPU."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "VGG-16 is used as an existing model to evaluate the new method proposed in the paper, rather than being a newly developed model.",
          "quote": "To evaluate this new approach, the VGG-16 [19] Con-\nvolutional Neural Network (CNN) is used as a use-case\non a mobile GPU."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper runs experiments using VGG-16 on a mobile GPU to evaluate performance.",
          "quote": "The experimental results collected on ARM Mali GPU\nshow that this new approach outperforms the handwrit-\nten ARM Compute Library [12] direct convolution ker-\nnel by 12×."
        },
        "is_compared": {
          "value": true,
          "justification": "VGG-16 performance is compared against other methods like ARM Compute Library and TVM-generated kernels.",
          "quote": "Using a convolutional\nneural network on a mobile GPU as a use case, this ap-\nproach matches the performance of the ARM Compute\nLibrary GEMM convolution and the TVM-generated\nkernel."
        },
        "referenced_paper_title": {
          "value": "Very deep convolutional networks for large-scale image recognition",
          "justification": "The paper cites VGG-16 as introduced by Simonyan and Zisserman in their work on very deep neural networks.",
          "quote": "To evaluate this new approach, the VGG-16 [19] Con-\nvolutional Neural Network (CNN) is used as a use-case\non a mobile GPU."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "ARM Compute Library",
          "justification": "ARM Compute Library is directly mentioned as a comparison target for the evaluations within the paper.",
          "quote": "matches the performance of the ARM Compute Library GEMM convolution and the TVM-generated kernel consuming between 2× and 3.6× less memory."
        },
        "aliases": [
          "ARM-CL"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Arm Compute Library",
          "justification": "The library itself is directly referenced for its role in comparison experiments.",
          "quote": "The implementations are tuned us-\ning the ARM Compute auto-tuner. The library produces\nboth direct and GEMM-based implementations."
        }
      },
      {
        "name": {
          "value": "TVM",
          "justification": "The paper mentions using TVM-generated kernels for performance comparisons, indicating its use as a benchmark library.",
          "quote": "matches the performance of the ARM Compute Library GEMM convolution and the TVM-generated kernel consuming between 2× and 3.6× less memory."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning",
          "justification": "TVM is clearly used for comparison against the optimized kernels created in the experiments, and its reference matches the paper title.",
          "quote": "We use TVM v0.6, built with OpenCL support generated using LLVM version 4.0.0."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1168,
    "prompt_tokens": 15951,
    "total_tokens": 17119,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}