{
  "paper": "4GM3Ak3B3e.txt",
  "words": 21260,
  "extractions": {
    "title": {
      "value": "Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization",
      "justification": "The provided title matches the content of the paper.",
      "quote": "Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization"
    },
    "description": "We propose a new first-order optimization algorithm — AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent—for separable convex-concave minimax optimization. The main idea of our algorithm is to carefully leverage the structure of the minimax problem, performing Nesterov acceleration on the individual component and optimistic gradient on the coupling component.",
    "type": {
      "value": "theoretical",
      "justification": "The paper focuses on designing and analyzing theoretical aspects of optimization algorithms.",
      "quote": "In this work, we focus on designing fast or even optimal deterministic and stochastic first-order algorithms for solving convex-concave minimax problems of the form (1.1)."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization",
        "justification": "The primary focus of the paper is on optimization techniques for minimax problems.",
        "quote": "Optimization is the workhorse for machine learning (ML) and artificial intelligence."
      },
      "aliases": [
        "Optimization"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Convex Optimization",
          "justification": "The paper discusses algorithms for convex optimization problems.",
          "quote": "In this work, we focus on designing fast or even optimal deterministic and stochastic first-order algorithms for solving convex-concave minimax problems of the form (1.1)."
        },
        "aliases": [
          "Convex Optimization"
        ]
      },
      {
        "name": {
          "value": "Stochastic Optimization",
          "justification": "The paper extends the proposed algorithms to stochastic settings.",
          "quote": "We also extend our algorithm to the stochastic setting and achieve the optimal convergence rate in both bi-SC-SC and bi-C-SC settings."
        },
        "aliases": [
          "Stochastic Optimization"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "AcceleratedGradient-OptimisticGradient (AG-OG)",
          "justification": "This is the main model proposed in the paper.",
          "quote": "We propose a new first-order optimization algorithm — AcceleratedGradient-OptimisticGradient (AG-OG) Descent Ascent—for separable convex-concave minimax optimization."
        },
        "aliases": [
          "AG-OG"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is introduced as a novel contribution in the paper.",
          "quote": "We present a novel algorithm that blends acceleration dynamics based on the single-call OGDA algorithm for the coupling component and Nesterov’s acceleration for the individual component."
        },
        "is_executed": {
          "value": true,
          "justification": "The model involves computational experiments and theoretical analysis.",
          "quote": "Due to space limitations, we defer all proof details along with results of numerical experiments to the supplementary materials."
        },
        "is_compared": {
          "value": true,
          "justification": "The AG-OG model is compared with other state-of-the-art methods in the paper.",
          "quote": "In Figure 1 we plot the AG-OG algorithm and the AG-OG with restarting algorithm under three different instances."
        },
        "referenced_paper_title": {
          "value": "On the convergence of stochastic extragradient for bilinear games using restarted iteration averaging",
          "justification": "The referenced paper provides context for the comparison made in the current paper.",
          "quote": "We compared stochastic AG-OG and its restarted version (S-AG-OG) with Stochastic extragradient (SEG) SEG with restarting, respectively (cf. Li et al., 2022)."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1543,
    "prompt_tokens": 81004,
    "total_tokens": 82547
  }
}