{
  "paper": "9f694ded7abee859afedc46d5887a6fe.txt",
  "words": 11707,
  "extractions": {
    "title": {
      "value": "Grow-and-Clip: Informative-yet-Concise Evidence Distillation for Answer Explanation",
      "justification": "The title is directly taken from the paper, which is typically located at the beginning of the document.",
      "quote": "Grow-and-Clip: Informative-yet-Concise Evidence Distillation for Answer Explanation"
    },
    "description": "The paper proposes a new algorithm, the Grow-and-Clip Evidence Distillation (GCED), designed to extract informative, concise, and readable evidences for question answering tasks. It is aimed at improving the interpretability of QA models by distilling optimal evidence from contexts. The paper includes extensive experiments on QA datasets like SQuAD and TriviaQA to evaluate the algorithm's performance.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments on SQuAD and TriviaQA datasets to evaluate the effectiveness of the proposed Grow-and-Clip Evidence Distillation (GCED) algorithm.",
      "quote": "We conduct extensive experiments on the SQuAD and TriviaQA datasets with several baseline models to evaluate the effect of GCED on interpreting answers to questions."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on improving Question Answering systems, which is a key topic within Natural Language Processing.",
        "quote": "Question Answering (QA) is an important task in Natural Language Processing (NLP) and plays a vital role in many real-world applications."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Explainable Question Answering",
          "justification": "The paper is primarily focused on enhancing interpretability by extracting evidence for QA systems.",
          "quote": "Index Termsâ€”Explainable Question Answering, Evidence Distillation, Grow-and-Clip, Informative-yet-Concise Evidence"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Grow-and-Clip Evidence Distillation (GCED)",
          "justification": "The GCED algorithm is the primary contribution of the paper, designed for evidence distillation in QA systems.",
          "quote": "Furthermore, we propose Grow-and-Clip Evidence Distillation (GCED) algorithm to extract evidences from the contexts by trade-off informativeness, concise- ness, and readability."
        },
        "aliases": [
          "GCED"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The GCED model is a novel contribution of this research for improving evidence extraction in QA systems.",
          "quote": "Furthermore, we propose Grow-and-Clip Evidence Distillation (GCED) algorithm to extract evidences from the contexts by trade-off informativeness, concise- ness, and readability."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed to perform evidence distillation during experiments.",
          "quote": "We conduct extensive experiments on the SQuAD and TriviaQA datasets with several baseline models to evaluate the effect of GCED on interpreting answers to questions."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of GCED with other baseline models on QA datasets.",
          "quote": "We conduct extensive experiments on the SQuAD and TriviaQA datasets with several baseline models to evaluate the effect of GCED on interpreting answers to questions."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "The paper does not reference a previous work for the GCED model as it is the original contribution of this research.",
          "quote": "Furthermore, we propose Grow-and-Clip Evidence Distillation (GCED) algorithm to extract evidences from the contexts by trade-off informativeness, concise- ness, and readability."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SQuAD",
          "justification": "The SQuAD dataset is explicitly mentioned as one of the benchmark datasets for the experiments.",
          "quote": "We conduct extensive experiments on the SQuAD and TriviaQA datasets with several baseline models to evaluate the effect of GCED on interpreting answers to questions."
        },
        "aliases": [
          "Stanford Question Answering Dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "justification": "The paper references earlier works related to the SQuAD dataset.",
          "quote": "SQuAD [41], [42] have two versions: 1.1 and 2.0, which are both derived from Wikipedia articles."
        }
      },
      {
        "name": {
          "value": "TriviaQA",
          "justification": "The TriviaQA dataset is explicitly mentioned as one of the benchmark datasets for the experiments.",
          "quote": "We conduct extensive experiments on the SQuAD and TriviaQA datasets with several baseline models to evaluate the effect of GCED on interpreting answers to questions."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TriviaQA: A large-scale Distantly Supervised Challenge Dataset for Reading Comprehension",
          "justification": "The paper references earlier works related to the TriviaQA dataset.",
          "quote": "TriviaQA [27] contains over 650K question-answer-evidence triples."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Pytorch",
          "justification": "The paper explicitly mentions the use of Pytorch for training QA models.",
          "quote": "Moreover, we use Adam optimizer with default parameters, initialize the learning rate and batch size to 5e-5 and 8, respectively, for 3 epochs."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "None",
          "justification": "PyTorch is a widely recognized library and does not require a specific reference paper by the authors.",
          "quote": "Moreover, we use Adam optimizer with default parameters, initialize the learning rate and batch size to 5e-5 and 8, respectively, for 3 epochs."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1140,
    "prompt_tokens": 21445,
    "total_tokens": 22585,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}