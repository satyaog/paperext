{
  "paper": "2402.09327.txt",
  "words": 23510,
  "extractions": {
    "title": {
      "value": "Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization",
      "justification": "The title is directly taken from the beginning of the paper where the authors state it as 'Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization.'",
      "quote": "Information Complexity of Stochastic Convex Optimization: Applications to Generalization and Memorization"
    },
    "description": "The paper investigates the relationship between memorization and learning in the context of stochastic convex optimization (SCO). It focuses on the conditional mutual information (CMI) framework to quantify information revealed by learning algorithms. The main results include establishing the tradeoff between learning accuracy and CMI, highlighting the role of memorization, and discussing the limitations of CMI-based generalization bounds in SCO.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily focuses on theoretical frameworks, characterizations, and proofs related to the information complexity in stochastic convex optimization and does not include empirical experiments.",
      "quote": "In this work, we investigate the interplay between memorization and learning in the context of stochastic convex optimization (SCO). We define memorization via the information a learning algorithm reveals about its training data points."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper discusses foundational concepts in machine learning like generalization, memorization, and optimization problems within the context of stochastic convex optimization.",
        "quote": "In this work, we investigate the interplay between generalization and memorization in the context of stochastic convex optimization (SCO)."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Stochastic Convex Optimization",
          "justification": "The paper explicitly centers around stochastic convex optimization and its implications on learning and memorization.",
          "quote": "In this work, we investigate the interplay between generalization and memorization in the context of stochastic convex optimization (SCO)."
        },
        "aliases": [
          "SCO"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ε-Learner",
          "justification": "The ε-Learner is discussed extensively throughout the paper as a conceptual model to examine the tradeoff between accuracy and conditional mutual information in learning.",
          "quote": "We show that, in the L2 Lipschitz–bounded setting and under strong convexity, every learner with an excess error ε has CMI bounded below by Ω(1/ε2 ) and Ω(1/ε), respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The ε-Learner is used as a theoretical model to discuss existing bounds and frameworks, rather than being a novel contribution of the paper.",
          "quote": "Our proof techniques are inspired from the privacy literature and build on so-called fingerprinting lemmas."
        },
        "is_executed": {
          "value": false,
          "justification": "The ε-Learner is discussed in a theoretical context with no specific mention of computational execution within the study.",
          "quote": "We show that, in the L2 Lipschitz–bounded setting and under strong convexity, every learner with an excess error ε has CMI bounded below by Ω(1/ε2 ) and Ω(1/ε), respectively."
        },
        "is_compared": {
          "value": true,
          "justification": "The ε-Learner is compared theoretically to other learning bounds in terms of its conditional mutual information and accuracy tradeoffs.",
          "quote": "We study CMI of learners for two important classes of SCO problems."
        },
        "referenced_paper_title": {
          "value": "When is memorization of irrelevant training data necessary for high-accuracy learning?",
          "justification": "The referenced paper is one of the prior works discussing memorization in learning, which is relevant to the discussion on ε-Learner in the context of SCO.",
          "quote": "Inspired by CMI and membership inference [CCNSTT22], we have developed a framework to quantify memorization in SCO: informally, a point is considered memorized if adversary can guess correctly if this point appeared in the training set with a high confidence."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 815,
    "prompt_tokens": 41053,
    "total_tokens": 41868,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}