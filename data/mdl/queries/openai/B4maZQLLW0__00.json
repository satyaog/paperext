{
  "paper": "B4maZQLLW0_.txt",
  "words": 11493,
  "extractions": {
    "title": {
      "value": "Stateful Active Facilitator: Coordination and Environmental Heterogeneity in Cooperative Multi-Agent Reinforcement Learning",
      "justification": "This is the title of the paper as mentioned at the beginning and throughout the text.",
      "quote": "STATEFUL ACTIVE FACILITATOR: COORDINATION AND ENVIRONMENTAL HETEROGENEITY IN COOPERATIVE MULTI-AGENT REINFORCEMENT LEARNING"
    },
    "description": "The paper addresses the varying degrees of coordination and environmental heterogeneity in cooperative multi-agent reinforcement learning (MARL). It introduces HECOGrid, a suite of environments designed to empirically evaluate MARL approaches with quantitative control over coordination and heterogeneity levels. The paper also proposes the Stateful Active Facilitator (SAF), a learning approach that leverages centralized training and decentralized execution to improve performance in complex environments. The performance of SAF is compared with IPPO and MAPPO.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves empirical evaluations of the proposed approaches against existing models using the HECOGrid environments.",
      "quote": "Further, we propose a Centralized Training Decentralized Execution learning approach called Stateful Active Facilitator (SAF) that enables agents to work efficiently in high-coordination and high-heterogeneity environments through a differentiable and shared knowledge source used during training and dynamic selection from a shared pool of policies. We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on reinforcement learning in a multi-agent setting.",
        "quote": "Multi-Agent Reinforcement Learning (MARL) studies the problem of sequential decision-making in an environment with multiple actors."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Agent Systems",
          "justification": "The paper specifically deals with cooperative strategies in multi-agent environments.",
          "quote": "In cooperative multi-agent reinforcement learning, a team of agents works together to achieve a common goal."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Stateful Active Facilitator",
          "justification": "This is the primary model proposed by the authors.",
          "quote": "we propose a Centralized Training Decentralized Execution learning approach called Stateful Active Facilitator (SAF)."
        },
        "aliases": [
          "SAF"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Stateful Active Facilitator (SAF) is introduced in this paper.",
          "quote": "we propose a Centralized Training Decentralized Execution learning approach called Stateful Active Facilitator (SAF)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper states that the model was deployed for evaluation.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "is_compared": {
          "value": true,
          "justification": "SAF is numerically compared to IPPO and MAPPO.",
          "quote": "Our results show that SAF consistently outperforms the baselines across different tasks and different heterogeneity and coordination levels."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no reference to a previous paper specifically for SAF as it is a new contribution.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Independent PPO",
          "justification": "IPPO is used as a baseline for comparison.",
          "quote": "PPO, when used for independent learning in multi-agent settings (called Independent PPO or IPPO)"
        },
        "aliases": [
          "IPPO"
        ],
        "is_contributed": {
          "value": false,
          "justification": "IPPO is not a new model introduced by the paper.",
          "quote": "PPO, when used for independent learning in multi-agent settings (called Independent PPO or IPPO)"
        },
        "is_executed": {
          "value": true,
          "justification": "IPPO is used for evaluations.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "is_compared": {
          "value": true,
          "justification": "IPPO is numerically compared to the SAF model.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "referenced_paper_title": {
          "value": "Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?",
          "justification": "This paper is referenced for IPPO.",
          "quote": "de Witt et al. (2020) recently showed that PPO, when used for independent learning in multi-agent settings (called Independent PPO or IPPO) is in fact capable of beating several state-of-the-art approaches in MARL on competitive benchmarks such as StarCraft (Samvelyan et al., 2019)."
        }
      },
      {
        "name": {
          "value": "Multi-Agent PPO",
          "justification": "MAPPO is used as a baseline for comparison.",
          "quote": "Centralized Training Decentralized Execution (CTDE) such as MADDPG (Lowe et al., 2017), MAPPO (Yu et al., 2021), HAPPO and HTRPO (Kuba et al., 2021) was developed."
        },
        "aliases": [
          "MAPPO"
        ],
        "is_contributed": {
          "value": false,
          "justification": "MAPPO is not a new model introduced by the paper.",
          "quote": "Centralized Training Decentralized Execution (CTDE) such as MADDPG (Lowe et al., 2017), MAPPO (Yu et al., 2021), HAPPO and HTRPO (Kuba et al., 2021) was developed."
        },
        "is_executed": {
          "value": true,
          "justification": "MAPPO is used for evaluations.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "is_compared": {
          "value": true,
          "justification": "MAPPO is numerically compared to the SAF model.",
          "quote": "We evaluate SAF and compare its performance against baselines IPPO and MAPPO on HECOGrid."
        },
        "referenced_paper_title": {
          "value": "The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games",
          "justification": "This paper is referenced for MAPPO.",
          "quote": "Yu et al. (2021) proposes the extension PPO (Schulman et al., 2017) to a multi-agent framework in a similar manner."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "HECOGrid",
          "justification": "HECOGrid is introduced in the paper for evaluating MARL approaches.",
          "quote": "We formalize the notions of coordination level and heterogeneity level of an environment and present HECOGrid, a suite of multi-agent RL environments that facilitates empirical evaluation of different MARL approaches across different levels of coordination and environmental heterogeneity."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "HECOGrid is introduced in this paper and is not referenced from another work.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "StarCraft Multi-Agent Challenge",
          "justification": "StarCraft Multi-Agent Challenge is mentioned in the context of existing benchmarks for MARL.",
          "quote": "de Witt et al. (2020) recently showed that PPO, when used for independent learning in multi-agent settings (called Independent PPO or IPPO) is in fact capable of beating several state-of-the-art approaches in MARL on competitive benchmarks such as StarCraft (Samvelyan et al., 2019)."
        },
        "aliases": [
          "SMAC"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The StarCraft Multi-Agent Challenge",
          "justification": "This is the referenced paper title for SMAC.",
          "quote": "de Witt et al. (2020) recently showed that PPO, when used for independent learning in multi-agent settings (called Independent PPO or IPPO) is in fact capable of beating several state-of-the-art approaches in MARL on competitive benchmarks such as StarCraft (Samvelyan et al., 2019)."
        }
      },
      {
        "name": {
          "value": "MeltingPot",
          "justification": "MeltingPot is mentioned in the paper as a benchmark for MARL.",
          "quote": "Melting Point (Leibo et al., 2021) allows evaluating for out of distribution generalization, where the OOD scenarios can be defined by changing the background population of the environment."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Scalable Evaluation of Multi-Agent Reinforcement Learning with Melting Pot",
          "justification": "This is the referenced paper title for MeltingPot.",
          "quote": "Melting Point (Leibo et al., 2021) allows evaluating for out of distribution generalization, where the OOD scenarios can be defined by changing the background population of the environment."
        }
      },
      {
        "name": {
          "value": "Multi-Agent Particle Environments",
          "justification": "MPE is mentioned in the context of MARL environments.",
          "quote": "Table 1: Comparison between our newly developed HECOGrid environments and widely used multi-agent reinforcement learning environments including ... MPE (Lowe et al., 2017)"
        },
        "aliases": [
          "MPE"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments",
          "justification": "This is the referenced paper title for MPE.",
          "quote": "Table 1: Comparison between our newly developed HECOGrid environments and widely used multi-agent reinforcement learning environments including ... MPE (Lowe et al., 2017)"
        }
      },
      {
        "name": {
          "value": "DOTA 2",
          "justification": "DOTA 2 is mentioned as a benchmark for MARL.",
          "quote": "Table 1: Comparison between our newly developed HECOGrid environments and widely used multi-agent reinforcement learning environments including ... DOTA2 (Berner et al., 2019)"
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Dota 2 with Large Scale Deep Reinforcement Learning",
          "justification": "This is the referenced paper title for DOTA 2.",
          "quote": "Table 1: Comparison between our newly developed HECOGrid environments and widely used multi-agent reinforcement learning environments including ... DOTA2 (Berner et al., 2019)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 4643,
    "prompt_tokens": 41384,
    "total_tokens": 46027
  }
}