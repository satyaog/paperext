{
  "paper": "8b87618195e63201d12081249d973557.txt",
  "words": 6262,
  "extractions": {
    "title": {
      "value": "Game On, Hate Off: A Study of Toxicity in Online Multiplayer Environments",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Game On, Hate Off: A Study of Toxicity in Online Multiplayer Environments"
    },
    "description": "The paper investigates the detection and trends of toxicity in online multiplayer games. It utilizes a RoBERTa-based model to detect toxic behavior in chat data from Ubisoft games, providing insights into toxic behavior and offering strategies for healthier online communities.",
    "type": {
      "value": "empirical",
      "justification": "The research involves empirical analysis of chat data using a RoBERTa-based model to detect toxicity, thus it is an empirical study.",
      "quote": "Analyzing eight months of chat data from two Ubisoft games, we uncover patterns and trends in toxic behavior."
    },
    "primary_research_field": {
      "name": {
        "value": "Toxicity Detection in Online Gaming",
        "justification": "The main focus of the paper is on detecting and analyzing toxicity in online multiplayer gaming environments.",
        "quote": "...development of a reliable toxicity detection model that achieves an average precision score of 0.95."
      },
      "aliases": [
        "Online Gaming Toxicity"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing (NLP)",
          "justification": "The paper involves using NLP techniques such as language models for toxicity detection in chat.",
          "quote": "Toxic or harmful speech classification is a fundamental task in Natural Language Processing (NLP)."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The paper utilizes machine learning techniques, specifically mentioning deep learning models such as RoBERTa.",
          "quote": "Taking into account the performance gain due to transformer-based language models, we chose to use RoBERTa-base model..."
        },
        "aliases": [
          "ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "RoBERTa-base",
          "justification": "The paper specifically mentions the RoBERTa-base model used for toxicity detection.",
          "quote": "This study employs a RoBERTa-base model, a transformer-based machine learning technique for NLP."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "RoBERTa is not a newly contributed model by this paper; it is a pre-existing model used in the study.",
          "quote": "We initiated our work by obtaining the weights for a RoBERTa-base model [ 18 ] from Hugging-Face."
        },
        "is_executed": {
          "value": true,
          "justification": "The study executed the RoBERTa-base model for toxicity detection in chat data.",
          "quote": "This study employs a RoBERTa-base model, a transformer-based machine learning technique for NLP."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of the RoBERTa-base model was compared against other tools like Cleanspeak and Googleâ€™s Perspective API.",
          "quote": "Table 4 presents the performance of our model averaged over five different data splits for both For Honor and Rainbow Six Siege datasets."
        },
        "referenced_paper_title": {
          "value": "RoBERTa: A robustly optimized BERT pretraining approach",
          "justification": "The paper refers to the original research where RoBERTa was developed and optimized as a pre-training approach for BERT.",
          "quote": "We initiated our work by obtaining the weights for a RoBERTa-base model [ 18 ] from Hugging-Face."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "For Honor Chat Dataset",
          "justification": "The dataset includes chat data from the game For Honor, used for pre-training and fine-tuning the model.",
          "quote": "Specifically, we amassed approximately 11 million lines from For Honor and 18 million lines from Rainbow Six Siege."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper title for this dataset; it is collected for this study.",
          "quote": "Specifically, we amassed approximately 11 million lines from For Honor..."
        }
      },
      {
        "name": {
          "value": "Rainbow Six Siege Chat Dataset",
          "justification": "The dataset includes chat data from the game Rainbow Six Siege, used for pre-training and fine-tuning the model.",
          "quote": "Specifically, we amassed approximately 11 million lines from For Honor and 18 million lines from Rainbow Six Siege."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper title for this dataset; it is collected for this study.",
          "quote": "Specifically, we amassed approximately 11 million lines from For Honor and 18 million lines from Rainbow Six Siege."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face Transformers",
          "justification": "The paper mentions obtaining weights from Hugging Face for the RoBERTa-base model, indicating the use of their library.",
          "quote": "We initiated our work by obtaining the weights for a RoBERTa-base model [ 18 ] from Hugging-Face."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific referenced paper title for this library, as it is a commonly used tool for accessing pre-trained models.",
          "quote": "We initiated our work by obtaining the weights for a RoBERTa-base model [ 18 ] from Hugging-Face."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1068,
    "prompt_tokens": 11253,
    "total_tokens": 12321,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}