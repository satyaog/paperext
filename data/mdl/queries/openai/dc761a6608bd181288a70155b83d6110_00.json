{
  "paper": "dc761a6608bd181288a70155b83d6110.txt",
  "words": 11848,
  "extractions": {
    "title": {
      "value": "Web Retrieval Agents for Evidence-Based Misinformation Detection",
      "justification": "The title clearly indicates the research focus on developing agents for web retrieval specifically for evidence-based detection of misinformation.",
      "quote": "Web Retrieval Agents for Evidence-Based Misinformation Detection"
    },
    "description": "This paper introduces an agent-based approach for automated fact-checking to detect misinformation. It highlights the benefits of combining a large language model (LLM) agent with a web search agent for improved accuracy in misinformation detection, leveraging Retrieval-Augmented Generation (RAG) techniques. Various datasets are used to evaluate this methodology across several models, demonstrating substantial improvements in detection performance.",
    "type": {
      "value": "empirical",
      "justification": "The study involves experiments across multiple models and datasets to assess the effectiveness of combining LLMs with web search for misinformation detection, which is typical of empirical research.",
      "quote": "We evaluate the performance of this approach across a wide range of models..."
    },
    "primary_research_field": {
      "name": {
        "value": "Misinformation Detection",
        "justification": "The research is explicitly focused on developing methods for detecting misinformation using automated tools, which is a specific application within the field of Information Retrieval and NLP.",
        "quote": "This paper develops an agent-based automated fact-checking approach for detecting misinformation."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Retrieval-Augmented Generation (RAG)",
          "justification": "The methodology leverages RAG techniques to enhance misinformation detection by combining LLMs with web search capabilities.",
          "quote": "Surprisingly, there are relatively few tools for Retrieval-Augmented Generation (RAG) that combine external data sources with recent LLMs in the domain of misinformation detection."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Vicuna",
          "justification": "Vicuna is explicitly mentioned as one of the models tested for performance in the misinformation detection experiments.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Vicuna is mentioned as an existing model tested in the research, not as a new contribution.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed as part of the experimental evaluation in the study.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_compared": {
          "value": true,
          "justification": "Vicuna's performance is compared to other models like GPT-4 and Claude in terms of misinformation detection efficiency.",
          "quote": "We find that web-retrieval techniques improve the performance of all models except Vicuna..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The model is not linked to a specific reference paper within this document.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        }
      },
      {
        "name": {
          "value": "Mixtral",
          "justification": "Mixtral is another model tested for its performance in misinformation detection as part of the proposed approach evaluation.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Mixtral is used in the study but not introduced as a new model.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_executed": {
          "value": true,
          "justification": "It was used in the performance assessment of the misinformation detection approach.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_compared": {
          "value": true,
          "justification": "Mixtral is compared to other models such as GPT-4 and Claude with respect to misinformation detection.",
          "quote": "We find that web-retrieval techniques improve the performance of all models except Vicuna..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The model is not directly linked to a reference paper in the provided text.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        }
      },
      {
        "name": {
          "value": "Claude",
          "justification": "Claude is mentioned as one of the models evaluated for misinformation detection effectiveness, highlighting its use in the study.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Claude is used within the study's framework but is not a new contribution.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_executed": {
          "value": true,
          "justification": "It was included in the experimental setup for misinformation detection.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_compared": {
          "value": true,
          "justification": "Performance comparisons include Claude, which is juxtaposed with other models like GPT-3.5 and GPT-4.",
          "quote": "We find that web-retrieval techniques improve the performance of all models except Vicuna..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no specific reference paper title provided for Claude in the document.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        }
      },
      {
        "name": {
          "value": "GPT-3.5",
          "justification": "GPT-3.5 is a prominent model used to evaluate the performance of the misinformation detection approach, underscoring its direct involvement in the study.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GPT-3.5 is an existing model utilized in the experiments and not a new development in this research.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was part of the experiments conducted for evaluating misinformation detection improvements.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_compared": {
          "value": true,
          "justification": "The research compares the performance of GPT-3.5 alongside other models like GPT-4, demonstrating comparative analyses.",
          "quote": "We find that web-retrieval techniques improve the performance of all models except Vicuna..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The document doesn't specify any associated reference paper for GPT-3.5.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        }
      },
      {
        "name": {
          "value": "GPT-4",
          "justification": "Two versions of GPT-4 were tested as part of the study, illustrating its central role in the research focused on enhancing misinformation detection with retrieval techniques.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "aliases": [
          "GPT-4-0125",
          "GPT-4-0613"
        ],
        "is_contributed": {
          "value": false,
          "justification": "GPT-4 is included in the set of models evaluated for the research but is not presented as a new contribution.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_executed": {
          "value": true,
          "justification": "The research study involved executing GPT-4 to test its performance in misinformation detection applications.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of GPT-4 is compared with other models such as GPT-3.5, Vicuna, and Claude.",
          "quote": "We find that web-retrieval techniques improve the performance of all models except Vicuna..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The document does not specify a particular publication reference for the GPT-4 model.",
          "quote": "We evaluate the performance of this approach across a wide range of models: Vicuna, Mixtral, Claude, GPT-3.5, and two versions of GPT-4."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "LIAR-New",
          "justification": "The LIAR-New dataset is specifically mentioned as the main dataset used in the research to evaluate misinformation detection, highlighting its central importance in the experimental setup.",
          "quote": "We test this pipeline across several recent models (GPT-4, GPT-3.5, Cohere Coral, Mixtral, Claude 3, and Vicuna-1.5), focusing particularly on the LIAR-New dataset (Pelrine et al., 2023)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Towards reliable misinformation mitigation: Generalization, uncertainty, and gpt-4",
          "justification": "The reference to Pelrine et al., 2023, provides the title of the paper detailing the dataset used here for misinformation detection.",
          "quote": "...focusing particularly on the LIAR-New dataset (Pelrine et al., 2023)."
        }
      },
      {
        "name": {
          "value": "FEVER-v2",
          "justification": "FEVER-v2 is another dataset mentioned as part of the evaluation to investigate limitations and test the misinformation detection framework.",
          "quote": "Besides LIAR-New, we also test performance on FEVER-v2 (Thorne et al., 2019)..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The FEVER2.0 shared task",
          "justification": "The reference section attributes Thorne et al., 2019, for the FEVER-v2 dataset, aligning with its use case in this study.",
          "quote": "Besides LIAR-New, we also test performance on FEVER-v2 (Thorne et al., 2019)..."
        }
      },
      {
        "name": {
          "value": "FEVER",
          "justification": "The FEVER dataset is utilized as a part of the study to evaluate variations in model performance concerning misinformation detection.",
          "quote": "Additionally, we also evaluate performance on the FEVER, FaVIQ (Park et al., 2022) and X-FACT (Gupta & Srikumar, 2021) fact-checking datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "FEVER: a large-scale dataset for fact extraction and VERification",
          "justification": "FEVER is referenced through the Thorne et al., 2018 publication, which is noted in the text along with its acronym description.",
          "quote": "The FEVER dataset (Thorne et al., 2018) is utilized as another benchmark."
        }
      },
      {
        "name": {
          "value": "FaVIQ",
          "justification": "The FaVIQ dataset is included among others in assessing misinformation detection frameworks, hence identified as a used dataset for performance evaluation.",
          "quote": "Additionally, we also evaluate performance on the FEVER, FaVIQ (Park et al., 2022) and X-FACT (Gupta & Srikumar, 2021) fact-checking datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Fact verification from information seeking questions",
          "justification": "The reference in the text to Park et al., 2022, associates efforts with the FaVIQ dataset.",
          "quote": "Additionally, we also evaluate performance on the FEVER, FaVIQ (Park et al., 2022)..."
        }
      },
      {
        "name": {
          "value": "X-FACT",
          "justification": "X-FACT is listed among datasets to test the misinformation detection models, serving to illustrate the multilingual aspect of such evaluation.",
          "quote": "Additionally, we also evaluate performance on the FEVER, FaVIQ (Park et al., 2022) and X-FACT (Gupta & Srikumar, 2021) fact-checking datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "X-FACT: A New Benchmark Dataset for Multilingual Fact Checking",
          "justification": "Gupta & Srikumar, 2021, is cited in association with X-FACT, aligning with its usage as a reference dataset in this study.",
          "quote": "Additionally, we also evaluate performance on the FEVER, FaVIQ (Park et al., 2022) and X-FACT (Gupta & Srikumar, 2021) fact-checking datasets."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "FAISS",
          "justification": "The FAISS library is mentioned in the context of implementing a retrieval system, specifically for indexing embeddings related to Wikipedia pages.",
          "quote": "The embeddings of all chunks are indexed with the FAISS (Douze et al., 2024) library."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The faiss library",
          "justification": "The reference is to Douze et al., 2024, which corresponds to the FAISS library mentioned in context of dense retrieval.",
          "quote": "The embeddings of all chunks are indexed with the FAISS (Douze et al., 2024) library."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3025,
    "prompt_tokens": 20635,
    "total_tokens": 23660,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}