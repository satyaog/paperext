{
  "paper": "8f20f73c5784affdd9852233ebaab8b0.txt",
  "words": 12860,
  "extractions": {
    "title": {
      "value": "Memory Efficient Neural Processes via Constant Memory Attention Block",
      "justification": "This is mentioned as the main title of the research paper.",
      "quote": "Memory Efficient Neural Processes via Constant Memory Attention Block"
    },
    "description": "This paper introduces Constant Memory Attentive Neural Processes (CMANPs), a variant of Neural Processes (NPs) that are designed to use constant memory. The main contributions include an efficient update operation for Cross Attention, a novel Constant Memory Attention Block (CMAB), and an autoregressive not-diagonal extension (CMANP-AND). The proposed models achieve state-of-the-art results on popular NP benchmarks while significantly improving memory efficiency.",
    "type": {
      "value": "empirical",
      "justification": "The paper empirically evaluates the proposed models (CMANP and CMANP-AND) against state-of-the-art baselines on multiple NP benchmarks, including image completion and meta-regression tasks.",
      "quote": "Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Meta-Learning",
        "justification": "The paper focuses on improving Neural Processes, a family of meta-learning models, for better memory efficiency.",
        "quote": "Neural Processes (NPs) are a popular family of meta-learning models for uncertainty estimation."
      },
      "aliases": [
        "Meta-Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Attention Mechanisms",
          "justification": "The paper proposes a novel attention mechanism (CMAB) to improve memory efficiency.",
          "quote": "To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Uncertainty Estimation",
          "justification": "The paper deals with modeling predictive uncertainty, a key aspect of Neural Processes.",
          "quote": "Neural Processes (NPs) are a popular family of meta-learning models for uncertainty estimation."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Constant Memory Attentive Neural Processes (CMANPs)",
          "justification": "This is the main model proposed in the paper.",
          "quote": "To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes."
        },
        "aliases": [
          "CMANPs"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is introduced as a novel contribution in the paper.",
          "quote": "Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper mentions empirical evaluation and experiments which typically require execution.",
          "quote": "In the experiments, CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior state-of-the-art methods."
        },
        "is_compared": {
          "value": true,
          "justification": "The model is compared numerically to other models in terms of performance on benchmarks.",
          "quote": "In Table 1 and 2, we compare the memory complexities of state-of-the-art Neural Processes, showing the efficiency gains of CMANPs."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This model is a novel contribution and not directly referring to any previous paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Constant Memory Attention Block (CMAB)",
          "justification": "This is a key component of CMANPs proposed by the authors.",
          "quote": "To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates."
        },
        "aliases": [
          "CMAB"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is introduced as a novel contribution in the paper.",
          "quote": "Leveraging the update operation, we propose Constant Memory Attention Block (CMAB)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper mentions empirical evaluation and experiments which typically require execution.",
          "quote": "In the experiments, CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior state-of-the-art methods."
        },
        "is_compared": {
          "value": false,
          "justification": "This is a component of CMANPs and not directly compared to baseline models.",
          "quote": "N/A"
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This model is a novel contribution and not directly referring to any previous paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Autoregressive Not-Diagonal extension (CMANP-AND)",
          "justification": "This is an extension mentioned as part of the CMANPs model for more flexible predictive distribution.",
          "quote": "Leveraging the efficient updates property, we further introduce an Autoregressive Not-Diagonal extension (Section 3.3.1) which only requires constant memory unlike the quadratic memory required by all prior Not-Diagonal extensions of NPs."
        },
        "aliases": [
          "CMANP-AND"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model extension is introduced as a novel contribution in the paper.",
          "quote": "Leveraging the efficient updates property, we further introduce an Autoregressive Not-Diagonal extension (Section 3.3.1)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper mentions empirical evaluation and experiments which typically require execution.",
          "quote": "Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods."
        },
        "is_compared": {
          "value": true,
          "justification": "The model is compared numerically to other models in terms of performance on benchmarks.",
          "quote": "The results show that CMANP-AND achieves clear state-of-the-art results on CelebA (32x32), CelebA (64x64), and CelebA (128x128) while being scalable to more data points than prior Not-Diagonal variants."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "This model is a novel contribution and not directly referring to any previous paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Image Completion",
          "justification": "The dataset is used for evaluating the proposed model on image completion tasks in different resolutions.",
          "quote": "Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods. In Table 3, all NP baselines were able to be evaluated on CelebA (32 x 32) and EMNIST."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no separate referenced paper for this dataset.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "EMNIST",
          "justification": "The EMNIST dataset is used as one of the benchmarks for evaluating the proposed model.",
          "quote": "Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods. In Table 3, all NP baselines were able to be evaluated on CelebA (32 x 32) and EMNIST."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "EMNIST: Extending MNIST to handwritten letters",
          "justification": "The EMNIST dataset is well-known and it has a specific referenced paper.",
          "quote": "EMNIST (Cohen et al., 2017) comprises black and white images of handwritten letters of 32 × 32 resolution."
        }
      },
      {
        "name": {
          "value": "Meta-Regression",
          "justification": "The dataset is used for evaluating the model on meta-regression tasks.",
          "quote": "In this experiment, the goal is to model an unknown function f and make predictions for a batch of M target data points given a batch of N context data points... During each training epoch, a batch of B = 16 functions are sampled from a GP prior with an RBF kernel fi ∼ GP (m, k) where m(x) = 0 and k(x, x′ ) and l2 are the hyperparameters."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no separate referenced paper for this dataset.",
          "quote": "N/A"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments and implementations in the paper are likely conducted using PyTorch, a popular deep learning library.",
          "quote": "The implementation follows closely the baselines from the official repository of TNPs (https://github.com/tung-nd/TNP-pytorch) and LBANPs (https://github.com/BorealisAI/latentbottlenecked-anp)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no separate referenced paper for this library.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Numpy",
          "justification": "Numpy is commonly used for numerical operations in deep learning research.",
          "quote": "We performed a grid search over the weight decay term {0.0, 0.00001, 0.0001, 0.001}. Consistent with prior work, we set the number of latents to the same fixed value without tuning. The embedding sizes for the learned latent values are set similar to prior works."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no separate referenced paper for this library.",
          "quote": "N/A"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2067,
    "prompt_tokens": 23243,
    "total_tokens": 25310
  }
}