{
  "paper": "9e64cb4b3eacd75af8c4ca612fff5ab7.txt",
  "words": 751,
  "extractions": {
    "title": {
      "value": "Better entity matching with transformers through ensembles",
      "justification": "The title of the paper is clearly mentioned at the top of the article.",
      "quote": "Better entity matching with transformers through ensembles"
    },
    "description": "This paper introduces AttendEM, a transformer-based framework for improving entity matching by leveraging ensemble techniques. AttendEM incorporates intra-transformer ensembling, text rearrangement, aggregator tokens, and additional self-attention layers to improve performance over existing state-of-the-art solutions on EM benchmark datasets.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results comparing AttendEM to existing solutions, demonstrating performance improvements, which indicates an empirical approach.",
      "quote": "Against state-of-the-art (SOTA) solutions on the ER-Magellan benchmark datasets, AttendEM achieved higher F1 scores in most cases."
    },
    "primary_research_field": {
      "name": {
        "value": "Entity Matching",
        "justification": "The primary focus of the paper is on improving entity matching using a new transformer-based framework.",
        "quote": "In this paper, we introduce AttendEM, a framework for entity matching (EM)..."
      },
      "aliases": [
        "EM"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Entity Resolution",
          "justification": "Entity Resolution is a broader field that includes Entity Matching, and the paper discusses the framework's application in this context.",
          "quote": "The combined EB and EM phases is called entity resolution (ER)."
        },
        "aliases": [
          "ER"
        ]
      },
      {
        "name": {
          "value": "Transformers",
          "justification": "The paper specifically focuses on utilizing transformer models for entity matching, making it a relevant sub-research field.",
          "quote": "AttendEM leverages intra-transformer ensembling... to enhance the base transformer architecture."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "AttendEM",
          "justification": "AttendEM is the primary model introduced and evaluated in this paper.",
          "quote": "In this paper, we present AttendEM, a classifier framework built to accommodate pre-trained transformer models."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "The paper introduces AttendEM as a novel framework.",
          "quote": "In this paper, we introduce AttendEM..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper presents results of experiments conducted using AttendEM.",
          "quote": "AttendEM achieved higher F1 scores in most cases."
        },
        "is_compared": {
          "value": true,
          "justification": "AttendEM is compared against several state-of-the-art models in the experiments.",
          "quote": "Against state-of-the-art (SOTA) solutions on the ER-Magellan benchmark datasets, AttendEM achieved higher F1 scores in most cases."
        },
        "referenced_paper_title": {
          "value": "Better entity matching with transformers through ensembles",
          "justification": "The paper itself is about AttendEM, thus it serves as its own reference for this model.",
          "quote": "In this paper, we present AttendEM, a classifier framework built to accommodate pre-trained transformer models."
        }
      },
      {
        "name": {
          "value": "Ditto",
          "justification": "Ditto is mentioned as one of the state-of-the-art models against which AttendEM is compared.",
          "quote": "These SOTA solutions are Ditto (mean improvement of 0.21% with Ditto’s own reported results, 3.93% with DAEM’s Ditto replication, 2.99% with HierGAT’s Ditto replication)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Ditto is referenced as an existing solution, not a new contribution by this paper.",
          "quote": "These SOTA solutions are Ditto..."
        },
        "is_executed": {
          "value": true,
          "justification": "Ditto is part of the comparison experiments conducted.",
          "quote": "These SOTA solutions are Ditto (mean improvement of 0.21% with Ditto’s own reported results, 3.93% with DAEM’s Ditto replication, 2.99% with HierGAT’s Ditto replication)..."
        },
        "is_compared": {
          "value": true,
          "justification": "Ditto is explicitly compared to AttendEM in the results section.",
          "quote": "Against state-of-the-art (SOTA) solutions on the ER-Magellan benchmark datasets, AttendEM achieved higher F1 scores in most cases."
        },
        "referenced_paper_title": {
          "value": "Ditto: A Simple and Efficient Entity Matching Framework",
          "justification": "The referenced paper provides the Ditto results as a baseline for comparison.",
          "quote": "These SOTA solutions are Ditto..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ER-Magellan benchmark datasets",
          "justification": "The paper explicitly mentions using ER-Magellan datasets for testing the AttendEM model.",
          "quote": "Against state-of-the-art (SOTA) solutions on the ER-Magellan benchmark datasets, AttendEM achieved higher F1 scores in most cases."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ERMagellan: Scalable Entity Matching Benchmarking Framework",
          "justification": "The dataset is referenced as a standard benchmark for evaluating entity matching solutions.",
          "quote": "Against state-of-the-art (SOTA) solutions on the ER-Magellan benchmark datasets..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "DeepMatcher",
          "justification": "DeepMatcher is mentioned in the context of accessing datasets, implying its use in this research.",
          "quote": "Dataset link: https://github.com/anhaidgroup/deepmatcher/blob/master/Datasets.md"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepMatcher: A Deep Learning-Based Approach to Entity Matching",
          "justification": "DeepMatcher is referenced as a source for datasets, indicating its role in the research.",
          "quote": "Dataset link: https://github.com/anhaidgroup/deepmatcher/blob/master/Datasets.md"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1152,
    "prompt_tokens": 2378,
    "total_tokens": 3530,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}