{
  "paper": "2405.01684.txt",
  "words": 8883,
  "extractions": {
    "title": {
      "value": "Intelligent Switching in Reset-Free RL",
      "justification": "The title of the paper is explicitly mentioned in the text.",
      "quote": "I NTELLIGENT S WITCHING IN R ESET-F REE RL"
    },
    "description": "The paper proposes a method named Reset Free RL with Intelligently Switching Controller (RISC), which intelligently switches between two agents (a forward agent and a reset agent) based on the agent’s confidence in achieving its current goal. The method aims to improve state-of-the-art performance on challenging reset-free reinforcement learning environments by reducing time spent in already explored regions of the state space and reducing the average distance to goal states.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results comparing the proposed method (RISC) against existing baselines across several environments. It reports performance metrics and conducts an ablation study, which indicates it is empirical in nature.",
      "quote": "We evaluate our algorithm’s performance on the recently proposed EARL benchmark (Sharma et al., 2021b)."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary research field is focused on reinforcement learning, particularly in scenarios without environment resets, known as reset-free or autonomous RL.",
        "quote": "Recent works have started to explore learning in environments where automatic resets are not available in a setting known as reset-free or autonomous RL."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Robotics",
          "justification": "The paper's methods and evaluations include real-world applications such as robotics, indicating Robotics as a subfield.",
          "quote": "Environment resetting, while simple to do in a simulator, becomes much more expensive, time consuming, and difficult to scale for real world applications such as robotics."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Autonomous Systems",
          "justification": "The paper focuses on intelligent, autonomous control mechanisms, making Autonomous Systems a relevant sub-research field.",
          "quote": "The resetting assumption limits the potential of reinforcement learning in the real world, as providing resets to an agent usually requires the creation of additional handcrafted mechanisms or human interventions."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "RISC (Reset Free RL with Intelligently Switching Controller)",
          "justification": "The model introduced in the paper is called Reset Free RL with Intelligently Switching Controller (RISC).",
          "quote": "To that end, we introduce Reset Free RL with Intelligently Switching Controller (RISC)."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "RISC is the novel contribution of the paper.",
          "quote": "With this in mind, we create a new algorithm, Reset Free RL with Intelligently Switching Controller (RISC)..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses the implementation and experimental validation of RISC, indicating it was executed as part of the study.",
          "quote": "Algorithm 1: Reset Free RL with Intelligently Switching Controller (RISC)"
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared against existing baselines like Forward Backward RL, R3L, VapRL, and MEDAL.",
          "quote": "Figures 4 and 5 show the results of RISC on the EARL benchmark environments compared to previously published results."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no reference paper for RISC since it is the original contribution of this paper.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "EARL benchmark",
          "justification": "The EARL benchmark dataset was used to evaluate the proposed RISC algorithm.",
          "quote": "We evaluate our algorithm’s performance on the recently proposed EARL benchmark (Sharma et al., 2021b)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Autonomous Reinforcement Learning: Formalism and Benchmarking",
          "justification": "The referenced paper for the EARL benchmark dataset.",
          "quote": "We evaluate our algorithm’s performance on the recently proposed EARL benchmark (Sharma et al., 2021b)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyBullet",
          "justification": "The PyBullet library is referenced in the context of the Minitaur environment.",
          "quote": "The Minitaur environment (Coumans & Bai, 2016) is a locomotion task where a minitaur robot learns to navigate to a set of goal locations."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "PyBullet, a Python module for physics simulation for games, robotics and machine learning",
          "justification": "The PyBullet library is referenced in this context.",
          "quote": "The Minitaur environment (Coumans & Bai, 2016) is a locomotion task where a minitaur robot learns to navigate to a set of goal locations."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 987,
    "prompt_tokens": 15837,
    "total_tokens": 16824
  }
}