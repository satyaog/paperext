{
  "paper": "2401.02297.txt",
  "words": 6627,
  "extractions": {
    "title": {
      "value": "Are LLMs Robust for Spoken Dialogues?",
      "justification": "Exact title provided in the given paper.",
      "quote": "Are LLMs Robust for Spoken Dialogues?"
    },
    "description": "This paper evaluates the robustness of Large Language Models (LLMs) for spoken task-oriented dialogues, specifically using DSTC11 test sets. Due to the lack of a proper spoken dialogue dataset, the authors transcribed a set of spoken dialogues with an ASR engine, characterized ASR-error types, and simulated these errors in a larger dataset. They report the intrinsic and extrinsic performance of fine-tuned GPT-2 and T5 models in response generation and dialogue state tracking tasks, respectively.",
    "type": {
      "value": "empirical",
      "justification": "The study involves experiments to evaluate the performance of LLMs on spoken task-oriented dialogues by transcribing spoken dialogues and fine-tuning models on the data.",
      "quote": "In this work, we have evaluated the performance of LLMs for spoken task-oriented dialogues on the DSTC11 test sets."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on evaluating LLMs for spoken task-oriented dialogues, which is a key area in Natural Language Processing.",
        "quote": "Large Pre-Trained Language Models have demonstrated state-of-the-art performance in different downstream tasks, including dialogue state tracking and end-to-end response generation."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Dialogue Systems",
          "justification": "The study evaluates LLMs for dialogue state tracking and response generation, which are crucial tasks in dialogue systems.",
          "quote": "We evaluated the performance of LLMs for spoken task-oriented dialogues on the DSTC11 test sets."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Speech Recognition",
          "justification": "The study involves transcribing spoken dialogues using an ASR engine, which falls under the domain of speech recognition.",
          "quote": "We transcribed a development set of spoken dialogues with a state-of-the-art ASR engine."
        },
        "aliases": [
          "ASR"
        ]
      },
      {
        "name": {
          "value": "Model Robustness",
          "justification": "The research investigates how robust LLMs are to errors introduced by ASR systems when dealing with spoken dialogues.",
          "quote": "The robustness of the developed models to spoken interactions is unknown."
        },
        "aliases": [
          "Robustness"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-2",
          "justification": "GPT-2 is one of the primary models fine-tuned and evaluated for response generation in the experiments.",
          "quote": "We report the intrinsic (perplexity) and extrinsic (human evaluation) performance of fine-tuned GPT-2"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GPT-2 is not a new model contribution but is used and fine-tuned for this study.",
          "quote": "We report the intrinsic (perplexity) and extrinsic (human evaluation) performance of fine-tuned GPT-2"
        },
        "is_executed": {
          "value": true,
          "justification": "The model was fine-tuned and evaluated in the study, indicating its execution.",
          "quote": "The fine-tuning was performed using AdamW optimizer"
        },
        "is_compared": {
          "value": true,
          "justification": "GPT-2's performance was measured and compared in various conditions, including clean and noisy dialogues.",
          "quote": "We evaluated the models’ performance on the two subtasks of response generation and DST when fine-tuned on noisy data compared to the clean dialogues."
        },
        "referenced_paper_title": {
          "value": "Language models are unsupervised multitask learners",
          "justification": "This is the original GPT-2 paper by Radford et al., as GPT-2 is a well-known model introduced in this paper.",
          "quote": "The LLMs used for this task are GPT-2 small (12 layers of decoder blocks, 117M parameters), and GPT-2 medium (24 layers of decoder blocks, 345M parameters) [19], as decoder-only models pre-trained on WebText dataset with a 40GB web crawled documents."
        }
      },
      {
        "name": {
          "value": "T5",
          "justification": "T5 is another primary model fine-tuned and evaluated for dialogue state tracking in the experiments.",
          "quote": "We report the intrinsic (perplexity) and extrinsic (human evaluation) performance of fine-tuned...T5 models"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "T5 is not a new model contribution but is used and fine-tuned for this study.",
          "quote": "The LLM fine-tuned for this task is T5 Small [21], (12 layers, 60M parameters), a transformer-based encoder-decoder model."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was fine-tuned and evaluated in the study, indicating its execution.",
          "quote": "The fine-tuning was performed using AdamW optimizer"
        },
        "is_compared": {
          "value": true,
          "justification": "T5's performance was measured and compared in various conditions, including clean and noisy dialogues.",
          "quote": "We evaluated the models’ performance on the two subtasks of response generation and DST when fine-tuned on noisy data compared to the clean dialogues."
        },
        "referenced_paper_title": {
          "value": "Exploring the limits of transfer learning with a unified text-to-text transformer",
          "justification": "This is the original T5 paper by Colin Raffel et al., as T5 is a well-known model introduced in this paper.",
          "quote": "The LLM fine-tuned for this task is T5 Small [21], (12 layers, 60M parameters), a transformer-based encoder-decoder model, pre-trained on the Common Crawl dataset with 750GB of web page text."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MultiWOZ 2.1",
          "justification": "The dataset used for training and evaluation is explicitly specified as MultiWOZ 2.1.",
          "quote": "The dataset used in this work is MultiWOZ 2.1 [3], a large dataset of written multi-domain TODs."
        },
        "aliases": [
          "MultiWOZ"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling",
          "justification": "This is the reference paper for the MultiWOZ dataset.",
          "quote": "The dataset used in this work is MultiWOZ 2.1 [3], a large dataset of written multi-domain TODs."
        }
      },
      {
        "name": {
          "value": "DSTC11",
          "justification": "The test sets used for evaluation are specified as being from DSTC11.",
          "quote": "In this work, we have evaluated the performance of LLMs for spoken task-oriented dialogues on the DSTC11 test sets."
        },
        "aliases": [
          "DSTC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Speech Aware Dialog System Technology Challenge (DSTC11)",
          "justification": "The reference paper for DSTC11, as it is the speech aware dialogue system technology challenge that hosts these test sets.",
          "quote": "In another work, Soltau et al. [26] collected a spoken version of MultiWOZ 2.1 for the task of spoken DST and response generation. The authors shared with the community the speech audio files and transcriptions in 3 spoken scenarios...This data was then used as a benchmark in DSTC 11."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "SpeechBrain toolkit was used for transcribing dialogues in the study.",
          "quote": "we transcribed the dialogues via the SpeechBrain toolkit [23] using Whisper tiny model [20]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A General-Purpose Speech Toolkit",
          "justification": "This is the reference paper for the SpeechBrain toolkit.",
          "quote": "we transcribed the dialogues via the SpeechBrain toolkit [23] using Whisper tiny model [20]"
        }
      },
      {
        "name": {
          "value": "Whisper",
          "justification": "Whisper tiny model was used for automatic speech recognition in this study.",
          "quote": "The transcription was followed by automatic alignment of the predictions and the ground truths, via NIST scoring toolkit1, by minimising the Levenshtein distance. Using NIST toolkit, we defined three main categories for the errors: a) Insertions: one or more additional words can be found in the transcription, compared to ground truth; b) Deletions: one or more words are missing from the transcription, compared to ground truth; c) Substitutions: one or more words in the transcription are different from what is reported in the ground truth. Table 1 presents the percentage of each error category observed in the transcriptions of the HV development set, as well as the observed Word Error Rate (WER), and Sentence Error Rate (SER)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Robust speech recognition via large-scale weak supervision",
          "justification": "This is the reference paper for the Whisper model by Radford et.al",
          "quote": "we transcribed the dialogues via the SpeechBrain toolkit [23] using Whisper tiny model [20]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1852,
    "prompt_tokens": 12036,
    "total_tokens": 13888
  }
}