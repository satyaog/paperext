{
  "paper": "bc029ce5913deb133a69c635a878bdbe.txt",
  "words": 15270,
  "extractions": {
    "title": {
      "value": "Neural computations in prosopagnosia",
      "justification": "The title clearly states the focus of the paper on neural computations in relation to prosopagnosia.",
      "quote": "Neural computations in prosopagnosia"
    },
    "description": "The paper investigates the neural computations underlying the face identification deficits in patient PS, a case of acquired prosopagnosia. Through EEG, RSA, and deep neural network models, the study characterizes PS's impaired visual and semantic processes compared to neurotypical controls.",
    "type": {
      "value": "empirical",
      "justification": "The study employs EEG recordings, Representational Similarity Analysis (RSA), and machine learning to empirically investigate the neural computations in a prosopagnosic patient.",
      "quote": "We aimed to identify neural computations underlying the loss of face identification ability by modelling the brain activity of brain-lesioned patient PS..."
    },
    "primary_research_field": {
      "name": {
        "value": "Cognitive Neuroscience",
        "justification": "The paper explores neural computations and brain dynamics in the context of cognitive impairments in prosopagnosia.",
        "quote": "We aimed to identify neural computations underlying the loss of face identification ability by modelling the brain activity of brain-lesioned patient PS..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Visual Neuroscience",
          "justification": "The study extensively covers the visual processing deficits in a prosopagnosic patient using EEG and neural network models.",
          "quote": "We used Representational Similarity Analysis (RSA) to correlate human EEG representations with those of deep neural network (DNN) models of vision."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Neuropsychology",
          "justification": "The research relates to neural deficits and psychological implications in a patient with prosopagnosia.",
          "quote": "Patient PS is a very well-documented and described case of acquired prosopagnosia."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "AlexNet",
          "justification": "AlexNet is used as a model for visual computations in the study to represent complex neural visual processing.",
          "quote": "We used a pre-trained AlexNet (Krizhevsky et al., 2012) as one model of the visual computations along the ventral stream."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "AlexNet is a well-established model referenced from existing literature, not a new contribution of this paper.",
          "quote": "We used a pre-trained AlexNet (Krizhevsky et al., 2012)..."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not specify executing AlexNet for training or inference, but rather uses it as a theoretical model for comparison.",
          "quote": "We used a pre-trained AlexNet..."
        },
        "is_compared": {
          "value": true,
          "justification": "AlexNet's representational similarity is empirically compared with EEG data in terms of visual processing stages.",
          "quote": "We compared our participants’ brain representations to those from visual and caption deep neural networks using Representational Similarity Analysis..."
        },
        "referenced_paper_title": {
          "value": "ImageNet Classification with Deep Convolutional Neural Networks",
          "justification": "This is the reference paper for AlexNet, a foundational paper in deep learning for image classification.",
          "quote": "We used a pre-trained AlexNet (Krizhevsky et al., 2012)..."
        }
      },
      {
        "name": {
          "value": "Google's universal sentence encoder (GUSE)",
          "justification": "GUSE is used for deriving semantic embeddings from captions, assessing higher-level cognition in participants.",
          "quote": "The sentence captions were fed as input in Google’s universal sentence encoder (GUSE; Cer et al., 2018) resulting in 512 dimensional sentence embeddings."
        },
        "aliases": [
          "GUSE"
        ],
        "is_contributed": {
          "value": false,
          "justification": "GUSE is sourced from existing literature and tools, not a new contribution by the paper.",
          "quote": "Google’s universal sentence encoder (GUSE; Cer et al., 2018)..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper explicitly uses GUSE to process semantic embeddings from stimuli captions.",
          "quote": "The sentence captions were fed as input in Google’s universal sentence encoder (GUSE; Cer et al., 2018)..."
        },
        "is_compared": {
          "value": true,
          "justification": "GUSE's embeddings are directly compared to participants’ neural responses to examine semantic processing.",
          "quote": "We compared our participants’ brain representations to those from visual and caption deep neural networks using Representational Similarity Analysis..."
        },
        "referenced_paper_title": {
          "value": "Universal Sentence Encoder",
          "justification": "The referenced paper details the development and purpose of the Universal Sentence Encoder used in the study.",
          "quote": "Google’s universal sentence encoder (GUSE; Cer et al., 2018)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Radboud Face Dataset",
          "justification": "The Radboud Face Dataset is explicitly mentioned as the source for facial images used in experiments.",
          "quote": "The 24 faces (13 identities, 8 males, and 8 neutral, 8 happy, 8 fearful expressions) were taken from the Radboud Face dataset (Langner et al., 2010)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Presentation and validation of the Radboud Faces Database",
          "justification": "The Radboud Face Dataset reference is provided to give context and validation for the stimuli used.",
          "quote": "The 24 faces (13 identities, 8 males, and 8 neutral, 8 happy, 8 fearful expressions) were taken from the Radboud Face dataset (Langner et al., 2010)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1148,
    "prompt_tokens": 30437,
    "total_tokens": 31585,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}