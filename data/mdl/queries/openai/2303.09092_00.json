{
  "paper": "2303.09092.txt",
  "words": 10783,
  "extractions": {
    "title": {
      "value": "Challenges to Evaluating the Generalization of Coreference Resolution Models: A Measurement Modeling Perspective",
      "justification": "Paper title extracted directly from the provided text",
      "quote": "Challenges to Evaluating the Generalization of Coreference Resolution Models: A Measurement Modeling Perspective"
    },
    "description": "This paper discusses the complexities and limitations of evaluating the generalization capabilities of coreference resolution (CR) models using multiple datasets. It introduces the measurement modeling framework from the social sciences to highlight how differing definitions and operationalizations of coreference across datasets can confound evaluation results. Through a multi-dataset evaluation of CR models, the study shows that inconsistencies in annotations and definitions across datasets can limit the validity of conclusions drawn from such evaluations.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts a multi-dataset evaluation of coreference resolution models to draw conclusions, which is characteristic of empirical research.",
      "quote": "In this work, we conduct a multi-dataset evaluation across seven English-language datasets of coreference annotations and analyze the results through the lens of measurement modeling."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on evaluating coreference resolution models, which is a key task in Natural Language Processing (NLP).",
        "quote": "Coreference resolution (CR) is then the task of identifying those expressions that are coreferring (Winograd, 1972; Kantor, 1977; Hirst, 1981, i.a.). This task has been studied extensively (e.g., Sukthanker et al., 2020; Poesio et al., 2023) and is considered a core part of representing the semantics of natural language (Hobbs, 1978)."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Coreference Resolution",
          "justification": "The paper specifically mentions that it deals with coreference resolution, evaluating various CR models and datasets.",
          "quote": "Coreference resolution (CR) is then the task of identifying those expressions that are coreferring (Winograd, 1972; Kantor, 1977; Hirst, 1981, i.a.). This task has been studied extensively (e.g., Sukthanker et al., 2020; Poesio et al., 2023) and is considered a core part of representing the semantics of natural language (Hobbs, 1978)."
        },
        "aliases": [
          "CR"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "LinkAppend",
          "justification": "LinkAppend is explicitly mentioned as one of the coreference resolution models evaluated in the study.",
          "quote": "We focus our evaluation on four models: the publicly available LinkAppend model (Bohnet et al., 2023) and three versions of LingMess (Otmazgin et al., 2023), each trained on one of the training datasets."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "LinkAppend is used in the evaluation but is not introduced as a new model in this study.",
          "quote": "We focus our evaluation on four models: the publicly available LinkAppend model (Bohnet et al., 2023) and three versions of LingMess (Otmazgin et al., 2023), each trained on one of the training datasets."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was used in the empirical study to perform evaluations.",
          "quote": "We use the publicly released weights of the best performing trained model which we refer to as LinkAppend."
        },
        "is_compared": {
          "value": true,
          "justification": "LinkAppend's performance is compared against other models like LingMess in the study.",
          "quote": "We focus our evaluation on four models: the publicly available LinkAppend model (Bohnet et al., 2023) and three versions of LingMess (Otmazgin et al., 2023), each trained on one of the training datasets."
        },
        "referenced_paper_title": {
          "value": "Coreference resolution through a seq2seq transition-based system",
          "justification": "This is the reference paper for the LinkAppend model as mentioned in the text.",
          "quote": "LinkAppend model (Bohnet et al., 2023)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "OntoNotes",
          "justification": "OntoNotes is discussed extensively and is a key dataset used in the evaluation of coreference resolution models in the paper.",
          "quote": "OntoNotes 5.0 (Weischedel et al., 2013) consists of news, conversations, web data, and biblical text in which coreference was annotated by experts. We use the English CoNLL-2012 Shared Task version of this dataset (Pradhan et al., 2012)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ONTONOTES RELEASE 5.0",
          "justification": "This is the reference paper for the OntoNotes dataset, as mentioned in the text.",
          "quote": "OntoNotes 5.0 (Weischedel et al., 2013)"
        }
      },
      {
        "name": {
          "value": "PreCo",
          "justification": "PreCo is another key dataset used in the evaluation of coreference resolution models in the paper.",
          "quote": "PreCo (Chen et al., 2018) consists of English comprehensive exams annotated for coreference by trained university students."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PreCo: A large-scale dataset in preschool vocabulary for coreference resolution",
          "justification": "This is the reference paper for the PreCo dataset, as mentioned in the text.",
          "quote": "PreCo (Chen et al., 2018)"
        }
      },
      {
        "name": {
          "value": "Phrase Detectives 3.0",
          "justification": "Phrase Detectives is one of the datasets used for evaluating the model's coreference resolution capabilities in the paper.",
          "quote": "Phrase Detectives 3.0 (Yu et al., 2023b) consists of Wikipedia, fiction, and technical text."
        },
        "aliases": [
          "Phrase Detectives"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Aggregating crowdsourced and automatic judgments to scale up a corpus of anaphoric reference for fiction and Wikipedia texts",
          "justification": "This is the reference paper for the Phrase Detectives dataset, as mentioned in the text.",
          "quote": "Phrase Detectives 3.0 (Yu et al., 2023b)"
        }
      },
      {
        "name": {
          "value": "OntoGUM",
          "justification": "OntoGUM is used to examine differences in operationalization of coreference definitions in the study.",
          "quote": "OntoGUM (Zhu et al., 2021) is a reformatted version of the GUM corpus (Zeldes, 2017)."
        },
        "aliases": [
          "GUM"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "OntoGUM: Evaluating contextualized SOTA coreference resolution on 12 more genres",
          "justification": "This is the reference paper for the OntoGUM dataset, as mentioned in the text.",
          "quote": "OntoGUM (Zhu et al., 2021)"
        }
      },
      {
        "name": {
          "value": "LitBank",
          "justification": "LitBank is one of the test datasets used in evaluating coreference models in the paper.",
          "quote": "LitBank (Bamman et al., 2020) consists of coreference annotated in English literature by experts."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "An Annotated Dataset of Coreference in English Literature",
          "justification": "This is the reference paper for the LitBank dataset, as mentioned in the text.",
          "quote": "LitBank (Bamman et al., 2020)"
        }
      },
      {
        "name": {
          "value": "ARRAU 2.1",
          "justification": "ARRAU is another key dataset used in the multi-dataset evaluation of coreference resolution models in the paper.",
          "quote": "ARRAU 2.1 (Uryupina et al., 2020) is a dataset of written news and spoken conversations annotated for various anaphoric phenomenon by experts."
        },
        "aliases": [
          "ARRAU"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Annotating a broad range of anaphoric phenomena, in a variety of genresâ€”the ARRAU corpus",
          "justification": "This is the reference paper for the ARRAU dataset, as mentioned in the text.",
          "quote": "ARRAU 2.1 (Uryupina et al., 2020)"
        }
      },
      {
        "name": {
          "value": "Multilingual Coreference Resolution in Multiparty Dialogue (MMC)",
          "justification": "MMC is one of the datasets used to evaluate out-of-domain performance of coreference models in the paper.",
          "quote": "Multilingual Coreference Resolution in Multiparty Dialogue (MMC) (Zheng et al., 2023) is a dataset of television transcripts."
        },
        "aliases": [
          "MMC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Multilingual Coreference Resolution in Multiparty Dialogue",
          "justification": "This is the reference paper for the MMC dataset, as mentioned in the text.",
          "quote": "Multilingual Coreference Resolution in Multiparty Dialogue (MMC) (Zheng et al., 2023)"
        }
      },
      {
        "name": {
          "value": "KnowRef",
          "justification": "KnowRef is mentioned as a dataset used for evaluating CR models' ability to resolve gender-neutral pronouns.",
          "quote": ". . . models trained on OntoNotes are believed to generalize poorly to examples of pronominal coreference scraped from the web (Webster et al., 2018; Emami et al., 2019)."
        },
        "aliases": [
          "KnowRef"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The KnowRef coreference corpus: Removing gender and number cues for difficult pronominal anaphora resolution",
          "justification": "This is the reference paper for the KnowRef dataset as mentioned in the text.",
          "quote": "models trained on OntoNotes are believed to generalize poorly to examples of pronominal coreference scraped from the web (Webster et al., 2018; Emami et al., 2019)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stanza",
          "justification": "Stanza is mentioned as the library used for parsing included datasets.",
          "quote": "For PreCo, Phrase Detectives, and all other datasets, we parse the dataset using the Stanza parser (Qi et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Stanza: A Python natural language processing toolkit for many human languages",
          "justification": "This is the reference paper for the Stanza library as mentioned in the text.",
          "quote": "we parse the dataset using the Stanza parser (Qi et al., 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2194,
    "prompt_tokens": 19465,
    "total_tokens": 21659
  }
}