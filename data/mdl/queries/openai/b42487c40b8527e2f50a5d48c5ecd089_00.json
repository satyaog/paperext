{
  "paper": "b42487c40b8527e2f50a5d48c5ecd089.txt",
  "words": 8352,
  "extractions": {
    "title": {
      "value": "A self-attention-based CNN-Bi-LSTM model for accurate state-of-charge estimation of lithium-ion batteries",
      "justification": "The paper focuses on introducing and evaluating a deep learning model specifically designed for estimating the state of charge (SOC) of lithium-ion batteries.",
      "quote": "A self-attention-based CNN-Bi-LSTM model for accurate state-of-charge estimation of lithium-ion batteries"
    },
    "description": "This research paper presents a novel deep learning model, known as the CNN-Bi-LSTM-AM, which integrates convolutional neural networks (CNN), bidirectional long short-term memory (Bi-LSTM), and a self-attention mechanism to improve the accuracy of state-of-charge (SOC) estimation for lithium-ion batteries over various temperature conditions. The model is evaluated against other deep learning models and shows superior performance, particularly at lower temperatures.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments comparing the performance of different deep learning models on a dataset to evaluate SOC estimation.",
      "quote": "we propose a deep learning model, based on convolutional neural networks integrating bidirectional long short-term memory and self-attention mechanism...The proposed model demonstrates proficiency in capturing both spatial and temporal dependencies critical for lithium-ion battery SOC estimation."
    },
    "primary_research_field": {
      "name": {
        "value": "Energy Storage",
        "justification": "The focus and applications mentioned throughout the paper revolve around lithium-ion batteries, specifically their state-of-charge estimation which is central to energy storage.",
        "quote": "Accurate state-of-charge (SOC) estimation across a broad temperature range is essential for extending battery longevity, and enduring effective management of overcharge and over-discharge conditions."
      },
      "aliases": [
        "Lithium-Ion Battery Technology"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The paper employs deep learning techniques for the task of SOC estimation, which falls under the broader category of Machine Learning.",
          "quote": "we propose a deep learning model, based on convolutional neural networks integrating bidirectional long short-term memory and self-attention mechanism (CNN-Bi-LSTM-AM)."
        },
        "aliases": [
          "Deep Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "CNN-Bi-LSTM-AM",
          "justification": "The paper contributes this model specifically for lithium-ion battery SOC estimation, enhancing previous frameworks by integrating self-attention mechanisms.",
          "quote": "Hence, we propose a deep learning model, based on convolutional neural networks integrating bidirectional long short-term memory and self-attention mechanism (CNN-Bi-LSTM-AM)."
        },
        "aliases": [
          "Convolutional Neural Network with Bidirectional LSTM and Attention Mechanism"
        ],
        "is_contributed": {
          "value": true,
          "justification": "This is an original model proposed by the paper for SOC estimation.",
          "quote": "Hence, we propose a deep learning model, based on convolutional neural networks integrating bidirectional long short-term memory and self-attention mechanism (CNN-Bi-LSTM-AM)."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed and evaluated under different conditions, as evidenced by the performance metrics reported.",
          "quote": "The training process for all the models used in this study was performed on a Compute Canada cluster equipped with NVIDIA Tesla P100 and NVIDIA V100 GPUs."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared against several other models to assess its accuracy in SOC estimation.",
          "quote": "The study serves as a comprehensive study that compares the performance of 12 different deep learning models, with and with-out the attention mechanism, in terms of the state-of-charge estimation of lithium-ion batteries."
        },
        "referenced_paper_title": {
          "value": "Attention is All You Need",
          "justification": "The attention mechanism, a crucial part of the model, is inspired by prior work referenced as foundational to this aspect of the model.",
          "quote": "The attention mechanism has demonstrated its effectiveness in di-verse deep learning experiments... [60] Attention is all you need."
        }
      },
      {
        "name": {
          "value": "LSTM-AM",
          "justification": "This model is discussed as a baseline comparison, specifically improved by incorporating an attention mechanism.",
          "quote": "the self-attention-based single-layer LSTM (LSTM-AM) and the self-attention-based single-layer Bi-LSTM (Bi-LSTM-AM) exhibit more promising performance"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "LSTM-AM is an existing model leveraged in the paper for comparison purposes, not introduced as new work.",
          "quote": "the self-attention-based single-layer LSTM (LSTM-AM) and the self-attention-based single-layer Bi-LSTM (Bi-LSTM-AM) exhibit more promising performance"
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed and its performance evaluated as part of the comparative analysis.",
          "quote": "self-attention-based single-layer LSTM (LSTM-AM) with 80 hidden neurons has an RMSE of 1.04%"
        },
        "is_compared": {
          "value": true,
          "justification": "Performance metrics of LSTM-AM are compared with other models to benchmark effectiveness in SOC estimation.",
          "quote": "the self-attention-based single-layer LSTM (LSTM-AM) and the self-attention-based single-layer Bi-LSTM (Bi-LSTM-AM) exhibit more promising performance compared to more complex network architectures."
        },
        "referenced_paper_title": {
          "value": "Long short-term memory",
          "justification": "The foundational LSTM architecture is referenced for its role in time-series prediction tasks, contributing to the improvements discussed.",
          "quote": "Significance of Long Short-Term Memory (LSTM) models in capturing temporal dependencies has been highlighted [29]. [29] Long short-term memory."
        }
      },
      {
        "name": {
          "value": "CNN-LSTM",
          "justification": "CNN-LSTM is a hybrid model that serves as a comparison point to the CNN-Bi-LSTM-AM model.",
          "quote": "Song et al. [37] presented a hybrid model that harnesses convolutional layers to extract internal representations and LSTM layers to capture short-term and long-term dependencies in battery data."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The CNN-LSTM model is referenced from prior work, used for comparative analysis in this study.",
          "quote": "Song et al. [37] presented a hybrid model that harnesses convolutional layers to extract internal representations"
        },
        "is_executed": {
          "value": true,
          "justification": "Its performance is evaluated alongside the contributed model, indicating it was run and tested.",
          "quote": "The combination of the CNN and LSTM networks allows for capturing nonlinear battery's dynamics and for extracting both spatial and temporal features from the input data."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper reports on performance comparisons including the CNN-LSTM approach.",
          "quote": "The study serves as a comprehensive study that compares the performance of 12 different deep learning models, with and without the attention mechanism."
        },
        "referenced_paper_title": {
          "value": "Combined CNN-LSTM network for state-of-charge estimation of lithium-ion batteries",
          "justification": "CNN-LSTM model is compared as part of evaluating different architectures for SOC estimation.",
          "quote": "Combined CNN-LSTM network for state-of-charge estimation of lithium-ion batteries"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "McMaster University Dataset",
          "justification": "The dataset provides data crucial for testing the proposed model at various ambient temperatures.",
          "quote": "The McMaster dataset includes data of a series of tests carried out at six ambient temperatures"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "LG 18650HG2 li-ion battery data and example deep neural network xEV SOC estimator script",
          "justification": "The dataset, as described, is provided by McMaster University and is referenced in the context of state-of-charge estimation.",
          "quote": "The McMaster dataset includes data of a series of tests carried out at six ambient temperatures [58] [58] LG 18650HG2 li-ion battery data and example deep neural network xEV SOC estimator script."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Keras-self-attention",
          "justification": "This library is used for implementing the self-attention mechanism in the proposed model.",
          "quote": "In this study, the self-attention mechanism is implemented using the Keras-self-attention library [61]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Keras-Self-Attention: Attention Mechanism for Processing Sequential Data That Considers the Context for Each Timestamp",
          "justification": "The library is explicitly mentioned as part of the model's implementation.",
          "quote": "In this study, the self-attention mechanism is implemented using the Keras-self-attention library [61]. [61] Keras-Self-Attention: Attention Mechanism for Processing Sequential Data That Considers the Context for Each Timestamp, GitHub, Inc."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1772,
    "prompt_tokens": 15565,
    "total_tokens": 17337,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}