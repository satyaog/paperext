{
  "paper": "Euf7KofunK.txt",
  "words": 10552,
  "extractions": {
    "title": {
      "value": "Clustering units in neural networks: upstream vs downstream information",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Clustering units in neural networks: upstream vs downstream information Richard D. Lange"
    },
    "description": "This paper explores the concept of modularity in artificial neural networks by investigating how 'functional similarity' between hidden units can be quantified using upstream and downstream methods. The study evaluates these measures across various neural network configurations and regularization techniques, highlighting unexpected findings such as the impact of dropout on modularity.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental results from over 300 trained neural networks, making it an empirical study.",
      "quote": "Despite its theoretical motivations, this is an empirical study. We trained over 300 feedforward, fully-connected neural networks on MNIST."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper investigates neural network modularity, a core topic in deep learning, using deep learning models and methods.",
        "quote": "We conduct an empirical study quantifying modularity of hidden layer representations of a collection of feedforward networks trained on classification tasks."
      },
      "aliases": [
        "Machine Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Network Modularity",
          "justification": "The paper specifically focuses on the modularity of neural networks by clustering units based on functional similarity.",
          "quote": "In this paper, we ask this question at the level of pairs of hidden units."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Feedforward Network",
          "justification": "The paper conducts experiments using feedforward neural networks.",
          "quote": "We trained over 300 feedforward, fully-connected neural networks on MNIST."
        },
        "aliases": [
          "Feedforward Neural Network"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Feedforward networks are standard neural network architectures and not newly contributed by this paper.",
          "quote": "We trained over 300 feedforward, fully-connected neural networks on MNIST."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves training feedforward networks, which implies execution.",
          "quote": "We trained over 300 feedforward, fully-connected neural networks on MNIST."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares modularity scores across different network configurations and regularization methods.",
          "quote": "We quantify pairwise associations between hidden units in each layer using a variety of both upstream and downstream measures."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific reference paper title provided for feedforward networks as they are common architectures.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The paper uses the MNIST dataset for training the neural networks.",
          "quote": "A large collection of simple feedforward fully-connected networks trained on the MNIST dataset."
        },
        "aliases": [
          "MNIST Digits"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-Based Learning Applied to Document Recognition",
          "justification": "The MNIST dataset is commonly referenced to the paper titled 'Gradient-Based Learning Applied to Document Recognition'.",
          "quote": "LeCun et al., 1998"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The experiments were conducted using PyTorch as mentioned in the document.",
          "quote": "Models were written and trained using PyTorch (Paszke et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pytorch: An imperative style, high-performance deep learning library",
          "justification": "The PyTorch library is associated with the paper titled 'Pytorch: An imperative style, high-performance deep learning library.'",
          "quote": "Paszke et al., 2019"
        }
      },
      {
        "name": {
          "value": "PyTorch Lightning",
          "justification": "The document mentions PyTorch Lightning was used alongside PyTorch.",
          "quote": "Models were written and trained using PyTorch (Paszke et al., 2019) and PyTorch Lightning."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific reference paper title provided for PyTorch Lightning in the document.",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 847,
    "prompt_tokens": 17371,
    "total_tokens": 18218,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}