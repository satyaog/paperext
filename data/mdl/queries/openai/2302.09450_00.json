{
  "paper": "2302.09450.txt",
  "words": 14741,
  "extractions": {
    "title": {
      "value": "Robust and Versatile Bipedal Jumping Control through Reinforcement Learning",
      "justification": "Based on the provided text, the title accurately represents the content of the paper.",
      "quote": "Robust and Versatile Bipedal Jumping Control through Reinforcement Learning"
    },
    "description": "This work aims to improve bipedal robotic jumping abilities using a reinforcement learning framework. The developed system enables a bipedal robot, specifically Cassie, to perform robust and versatile dynamic jumps in real-world scenarios. The framework incorporates a multi-stage training scheme and a new policy structure that encodes the robot’s long-term and short-term input/output history. The policies trained in simulation are transferred directly to the hardware without further tuning, demonstrating capabilities such as standing long jumps, jumping onto elevated platforms, and multi-axes jumps.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves empirical experiments and evaluations of a reinforcement learning framework on a bipedal robot, Cassie. The developed techniques are tested in simulation and subsequently transferred to real-world scenarios.",
      "quote": "This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning in Robotics",
        "justification": "The primary research focus is on applying reinforcement learning to improve bipedal robotic jumping controls.",
        "quote": "We present a reinforcement learning framework for training a robot to accomplish a large variety of jumping tasks, such as jumping to different locations and directions."
      },
      "aliases": [
        "RL in Robotics",
        "Robotic Reinforcement Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Bipedal Robotics",
          "justification": "The paper specifically focuses on a bipedal robot, Cassie, which is used as the experimental platform.",
          "quote": "This work aims to push the limits of agility for bipedal robots by enabling a torque-controlled bipedal robot to perform robust and versatile dynamic jumps in the real world."
        },
        "aliases": [
          "Bipedal Robots",
          "Legged Robotics"
        ]
      },
      {
        "name": {
          "value": "Sim-to-Real Transfer",
          "justification": "The developed policies are trained in simulation and then transferred directly to the real-world robot.",
          "quote": "In order to successfully transfer the learned skill for such dynamic maneuvers from simulation to the real world, we utilize two new design decisions."
        },
        "aliases": [
          "Simulation to Real World Transfer",
          "Sim2Real"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Goal-Conditioned Policy",
          "justification": "This policy is central to enabling the bipedal robot to perform versatile jumping maneuvers by conditioning on specific goals related to jumping tasks.",
          "quote": "We use the term goal-conditioned to refer to a policy that can perform a variety of jumping tasks, such as jumping over various desired distances and/or directions, conditioned on the given goal."
        },
        "aliases": [
          "Goal-Conditioned Reinforcement Learning Policy"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The goal-conditioned policy structure is a new contribution of this paper.",
          "quote": "We present a new policy structure that encodes the robot’s long-term Input-Output (I/O) history while also providing direct access to a short-term I/O history."
        },
        "is_executed": {
          "value": true,
          "justification": "The policy is executed in simulation and then deployed on the real-world hardware (Cassie) without further tuning.",
          "quote": "The policies are trained in simulation and deployed on the hardware without further tuning."
        },
        "is_compared": {
          "value": true,
          "justification": "The policy is compared against other methods in both simulation and real-world experiments.",
          "quote": "Our method shows similar performance as the expert policy which is used to supervise RMAs and has access to the privileged environment parameters."
        },
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The goal-conditioned policy structure is an original contribution of the current paper.",
          "quote": "We present a new policy structure that encodes the robot’s long-term Input-Output (I/O) history while also providing direct access to a short-term I/O history."
        }
      },
      {
        "name": {
          "value": "Residual Policy",
          "justification": "This policy is used as a comparison baseline in the study to evaluate the performance of the proposed goal-conditioned policy.",
          "quote": "Residual (Fig. 4b): the policy shares the same structure as the proposed one, but the policy output is a residual term added to the reference motor position at the current timestep."
        },
        "aliases": [
          "Residual Reinforcement Learning Policy"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The residual policy is used as a baseline and is not a new contribution of this paper.",
          "quote": "Residual (Fig. 4b): the policy shares the same structure as the proposed one, but the policy output is a residual term added to the reference motor position at the current timestep."
        },
        "is_executed": {
          "value": true,
          "justification": "The residual policy is executed in simulation as part of the baseline comparisons.",
          "quote": "Residual (Fig. 4b): the policy shares the same structure as the proposed one, but the policy output is a residual term added to the reference motor position at the current timestep."
        },
        "is_compared": {
          "value": true,
          "justification": "The residual policy is compared numerically to other models, including the proposed goal-conditioned policy.",
          "quote": "Residual (Fig. 4b): the policy shares the same structure as the proposed one, but the policy output is a residual term added to the reference motor position at the current timestep."
        },
        "referenced_paper_title": {
          "value": "Adapting rapid motor adaptation for bipedal robots",
          "justification": "The concept of a residual policy is not unique to this paper but references prior work on adaptable reinforcement learning policies.",
          "quote": "Residual (Fig. 4b): the policy shares the same structure as the proposed one, but the policy output is a residual term added to the reference motor position at the current timestep."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "MuJoCo",
          "justification": "MuJoCo is used for simulating the training environment for the bipedal robot Cassie.",
          "quote": "The training environment is developed in a simulation of Cassie using MuJoCo."
        },
        "aliases": [
          "MuJoCo Physics Engine"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Mujoco: A platform for model-based control",
          "justification": "The MuJoCo library is referenced for its simulation capabilities used in this study.",
          "quote": "The training environment is developed in a simulation of Cassie using MuJoCo [13, 64]."
        }
      },
      {
        "name": {
          "value": "PPO (Proximal Policy Optimization)",
          "justification": "PPO is used as the reinforcement learning algorithm for training the control policies in this study.",
          "quote": "Proximal Policy Optimization (PPO) [56] is used to train all policies πθ in simulation."
        },
        "aliases": [
          "Proximal Policy Optimization",
          "PPO Algorithm"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "The PPO algorithm is referenced for its application in training policies within the study.",
          "quote": "Proximal Policy Optimization (PPO) [56] is used to train all policies πθ in simulation."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1788,
    "prompt_tokens": 24884,
    "total_tokens": 26672
  }
}