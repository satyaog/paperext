{
  "paper": "a42fc09d26efa378b367a5ac6fc9ed9f.txt",
  "words": 10360,
  "extractions": {
    "title": {
      "value": "CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark",
      "justification": "The title of the paper explicitly states the creation of the CVQA benchmark focusing on cultural diversity across different languages.",
      "quote": "We propose CVQA, a large-scale multilingual VQA benchmark, representing the cultures..."
    },
    "description": "The paper introduces CVQA, a culturally-diverse multilingual Visual Question Answering (VQA) benchmark that includes 10,000 samples across 30 countries and 31 languages. It is designed to assess the cultural capability and bias of multimodal models and encourages more research towards increasing cultural and linguistic diversity. The benchmark engages native speakers and cultural experts in data collection, providing a comprehensive tool for evaluating vision-language models.",
    "type": {
      "value": "empirical",
      "justification": "The research involves the construction and evaluation of a new dataset (CVQA) using empirical methods, such as benchmarking models like LLaVA, mBLIP, and others on this dataset.",
      "quote": "In this study, we benchmark CVQA across various MLLMs and find that it presents a significant challenge for open MLLMs..."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper addresses the task of Visual Question Answering, which is a sub-domain of Computer Vision.",
        "quote": "Visual Question Answering (VQA) is an important task in multimodal AI..."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The paper focuses on models that understand both visual and textual data for the task of Visual Question Answering, categorizing it under Multimodal Learning.",
          "quote": "Visual Question Answering (VQA) is an important task in multimodal AI, and it is often used to test the ability of vision-language models..."
        },
        "aliases": [
          "Multimodal AI"
        ]
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The inclusion of multiple languages in the benchmark dataset extends the study to Natural Language Processing, especially in multilingual settings.",
          "quote": "...most of the current VQA models use datasets that are primarily focused on English and a few major world languages..."
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "LLaVA-1.5-7B",
          "justification": "LLaVA-1.5-7B is explicitly mentioned as one of the evaluated models on the CVQA dataset.",
          "quote": "For multilingual models, we evaluate LLaVA-1.5 (7B)..."
        },
        "aliases": [
          "LLaVA"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is used for benchmarking but is not a contribution of this paper.",
          "quote": "For multilingual models, we evaluate LLaVA-1.5 (7B)..."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed for evaluation on the CVQA dataset as indicated in the experimental results.",
          "quote": "We benchmark CVQA across various MLLMs and find that it presents a significant challenge..."
        },
        "is_compared": {
          "value": true,
          "justification": "The model is compared numerically as part of the benchmark evaluations against other models.",
          "quote": "In this study, we benchmark CVQA across various MLLMs..."
        },
        "referenced_paper_title": {
          "value": "LLaVA-Med: Training a large language-and-vision assistant for biomedicine in one day",
          "justification": "The title of the cited paper mentions 'LLaVA,' referring to LLaVA models.",
          "quote": "AcCM [22]"
        }
      },
      {
        "name": {
          "value": "mBLIP",
          "justification": "mBLIP is a model that is evaluated on the CVQA dataset, as mentioned in the paper.",
          "quote": "we evaluate mBLIP [12] a BLIP-2 based model that covers 96 languages..."
        },
        "aliases": [
          "BLIP-2"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is used in the evaluation but not contributed by this research.",
          "quote": "we evaluate mBLIP [12]..."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed for benchmarking on the CVQA dataset as part of the study's assessment of MLLMs.",
          "quote": "we evaluate mBLIP [12]..."
        },
        "is_compared": {
          "value": true,
          "justification": "mBLIP's performance is compared to other models on the dataset, as shown in the results.",
          "quote": "We benchmark CVQA across various MLLMs..."
        },
        "referenced_paper_title": {
          "value": "mBLIP: Efficient bootstrapping of multilingual vision-LLMs",
          "justification": "The title is derived from the citation given for mBLIP.",
          "quote": "LLMs [12]"
        }
      },
      {
        "name": {
          "value": "Gemini-1.5-Flash",
          "justification": "Gemini-1.5-Flash is used as a closed-source model for evaluating CVQA, highlighting its use in the paper.",
          "quote": "We also evaluate the most advanced closed-source MLLMs, such as GPT-4o [36] and Gemini-1.5-Flash [45]."
        },
        "aliases": [
          "Gemini"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses this model for benchmarking purposes but does not contribute it.",
          "quote": "We also evaluate the most advanced closed-source MLLMs, such as... Gemini-1.5-Flash [45]."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed as part of the zero-shot evaluation experiments.",
          "quote": "We perform a zero-shot evaluation with two types of prompts..."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared to other open and closed-source models with respect to CVQA.",
          "quote": "The performance of various open and closed-source MLLMs are shown in..."
        },
        "referenced_paper_title": {
          "value": "Gemini: A family of highly capable multimodal models",
          "justification": "This is the cited title for the Gemini models mentioned in context.",
          "quote": "Gemini [45]"
        }
      },
      {
        "name": {
          "value": "InstructBLIP",
          "justification": "InstructBLIP is another model evaluated in the study, contributing context to the results shown.",
          "quote": "We also use InstructBLIP(4.1B)... trained with 13 held-in datasets covering different tasks in English."
        },
        "aliases": [
          "BLIP"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model was used for evaluation but not contributed by this paper.",
          "quote": "We also use InstructBLIP(4.1B)..."
        },
        "is_executed": {
          "value": true,
          "justification": "It was executed on the CVQA benchmark as part of the empirical evaluation.",
          "quote": "We also use InstructBLIP(4.1B)..."
        },
        "is_compared": {
          "value": true,
          "justification": "The results compare the performance of InstructBLIP against other models on the CVQA dataset.",
          "quote": "The performance of existing MLLMs on CVQA..."
        },
        "referenced_paper_title": {
          "value": "InstructBLIP: Towards general-purpose vision-language models with instruction tuning",
          "justification": "Referenced paper for InstructBLIP as given in the context of the study.",
          "quote": "Frameworks [8]"
        }
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is one of the vision-language models tested on the CVQA benchmark.",
          "quote": "we test CLIP [40] a contrastive-learning-based model..."
        },
        "aliases": [
          "Contrastive Languageâ€“Image Pretraining"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses CLIP for evaluation purposes.",
          "quote": "we test CLIP [40]..."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed during the performance evaluation on the CVQA dataset.",
          "quote": "We evaluate performance on our CVQA benchmark, we select...CLIP"
        },
        "is_compared": {
          "value": true,
          "justification": "CLIP's performance is numerically benchmarked against other models within the paper.",
          "quote": "We benchmark CVQA across various MLLMs..."
        },
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "The reference title provided corresponds to CLIP's work.",
          "quote": "CLIP [40]"
        }
      },
      {
        "name": {
          "value": "GPT-4o",
          "justification": "GPT-4o was used as one of the models for evaluation, highlighting its application in the context of CVQA.",
          "quote": "evaluate the most advanced closed-source MLLMs, such as GPT-4o [36]..."
        },
        "aliases": [
          "GPT-4"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is used to benchmark the dataset but was not contributed by this work.",
          "quote": "Evaluate the most advanced closed-source MLLMs, such as GPT-4o..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper mentions the execution of this model as part of the benchmarking process.",
          "quote": "evaluate the most advanced closed-source MLLMs..."
        },
        "is_compared": {
          "value": true,
          "justification": "GPT-4o's performance was compared with other models to analyze its results on CVQA.",
          "quote": "The experimental results also highlight a substantial performance gap..."
        },
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "The title is fitting with the OpenAI model cited in the references.",
          "quote": "GPT-4 [36]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CVQA",
          "justification": "The CVQA dataset is the primary subject of the paper, proposed as a new benchmark.",
          "quote": "We propose CVQA, a large-scale multilingual VQA benchmark, representing the cultures..."
        },
        "aliases": [
          "Culturally-diverse Visual Question Answering"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The CVQA dataset is the main contribution of the paper itself, hence there's no reference to another paper.",
          "quote": "We propose CVQA, a large-scale multilingual VQA benchmark, representing the cultures..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "JotForm",
          "justification": "JotForm is explicitly mentioned as the annotation platform used for data entry and validation.",
          "quote": "We use JotForm as our annotation platform."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "JotForm is mentioned as a tool used in the paper, and not necessarily referenced from another study.",
          "quote": "We use JotForm as our annotation platform."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2174,
    "prompt_tokens": 20916,
    "total_tokens": 23090,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}