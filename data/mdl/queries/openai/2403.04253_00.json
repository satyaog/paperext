{
  "paper": "2403.04253.txt",
  "words": 18061,
  "extractions": {
    "title": {
      "value": "Mastering Memory Tasks with World Models",
      "justification": "Title of the research paper",
      "quote": "Mastering Memory Tasks with World Models"
    },
    "description": "This paper introduces a new method, Recall to Imagine (R2I), which enhances long-term memory and credit assignment in model-based reinforcement learning (MBRL). It integrates state space models (SSMs) into world models of MBRL agents, showcasing significant improvements in challenging tasks involving memory and temporal dependencies, such as BSuite, POPGym, and Memory Maze.",
    "type": {
      "value": "empirical",
      "justification": "The study systematically demonstrates the effectiveness of R2I through various experiments and benchmarks.",
      "quote": "Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper primarily focuses on improving model-based reinforcement learning (MBRL) agents.",
        "quote": "In reinforcement learning (RL), world models (Kalweit & Boedecker, 2017; Ha & Schmidhuber, 2018; Hafner et al., 2019b), which capture the dynamics of the environment, have emerged as a powerful paradigm for integrating agents with the ability to perceive (Hafner et al., 2019a; 2020; 2023), simulate (Schrittwieser et al., 2020; Ye et al., 2021; Micheli et al., 2023), and plan (Schrittwieser et al., 2020) within the learned dynamics."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Credit Assignment",
          "justification": "The paper addresses the issue of long-horizon credit assignment in reinforcement learning tasks.",
          "quote": "This integration aims to enhance both long-term memory and long-horizon credit assignment."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Memory-Augmented Neural Networks",
          "justification": "The paper improves long-term memory capabilities in MBRL agents using a new family of state space models.",
          "quote": "To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Recall to Imagine (R2I)",
          "justification": "The main model introduced in the paper to enhance long-term memory and credit assignment in reinforcement learning.",
          "quote": "we introduce a novel method termed Recall to Imagine (R2I), which is the first MBRL approach utilizing a variant of S4 (which was previously employed in model-free RL (David et al., 2023; Lu et al., 2024))."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "R2I is a newly introduced method in this paper.",
          "quote": "we introduce a novel method termed Recall to Imagine (R2I), which is the first MBRL approach utilizing a variant of S4 (which was previously employed in model-free RL (David et al., 2023; Lu et al., 2024))."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments with R2I model were run with computational efficiency in mind, suggesting GPU execution.",
          "quote": "Through rigorous experiments, we demonstrate that R2I not only surpasses the best-performing baselines but also exceeds human performance in tasks requiring long-term memory or credit assignment, all while maintaining commendable performance across various other benchmarks. Our contributions can be summarized as follows: • We introduce R2I, a memory-enhanced MBRL agent built upon DreamerV3 (Hafner et al., 2023) that uses a modification of S4 to handle temporal dependencies. R2I inherits the generality of DreamerV3, operating with fixed world model hyperparameters on every domain, while also offering an improvement in computational speed of up to 9 times."
        },
        "is_compared": {
          "value": true,
          "justification": "The R2I model is compared against the state-of-the-art models in various benchmarks like BSuite, POPGym, Memory Maze, and others.",
          "quote": "Through rigorous experiments, we demonstrate that R2I not only surpasses the best-performing baselines but also exceeds human performance in tasks requiring long-term memory or credit assignment, all while maintaining commendable performance across various other benchmarks."
        },
        "referenced_paper_title": {
          "value": "Using state space models in reinforcement learning to enhance long-term memory",
          "justification": "The base methodology of using state space models in reinforcement learning originates from previous works.",
          "quote": "we introduce a novel method termed Recall to Imagine (R2I), which is the first MBRL approach utilizing a variant of S4 (which was previously employed in model-free RL (David et al., 2023; Lu et al., 2024))."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Behavior Suite (BSuite)",
          "justification": "BSuite is one of the benchmarks used in the paper to evaluate the performance of the R2I model.",
          "quote": "R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Behaviour Suite for reinforcement learning",
          "justification": "The Behavior Suite (BSuite) has been referenced for the evaluation of reinforcement learning tasks.",
          "quote": "Behavior Suite (BSuite; Osband et al., 2020))."
        }
      },
      {
        "name": {
          "value": "POPGym",
          "justification": "POPGym is one of the benchmarks used in the paper to evaluate the performance of the R2I model.",
          "quote": "R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "POPGym: Benchmarking partially observable reinforcement learning",
          "justification": "The POPGym dataset has been referenced to evaluate performance in partially observable reinforcement learning tasks.",
          "quote": "POPGym (Morad et al., 2023)"
        }
      },
      {
        "name": {
          "value": "Memory Maze",
          "justification": "Memory Maze is one of the benchmarks used in the paper to evaluate the performance of the R2I model.",
          "quote": "R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Evaluating Long-term Memory in 3D Mazes",
          "justification": "The Memory Maze dataset has been referenced for evaluating long-term memory tasks.",
          "quote": "Memory Maze (Pasukonis et al., 2022)"
        }
      },
      {
        "name": {
          "value": "Atari",
          "justification": "Atari is one of the benchmarks used in the paper to evaluate the generality and performance of the R2I model.",
          "quote": "We investigate R2I’s performance in established RL benchmarks, namely Atari (Bellemare et al., 2013) and DMC (Tassa et al., 2018)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Arcade Learning Environment: An Evaluation Platform for General Agents",
          "justification": "The Atari dataset has been referenced for the evaluation of general reinforcement learning tasks.",
          "quote": "(Atari; Bellemare et al., 2013)"
        }
      },
      {
        "name": {
          "value": "DMC - DeepMind Control Suite",
          "justification": "DMC is one of the benchmarks used in the paper to evaluate the generality and performance of the R2I model.",
          "quote": "We investigate R2I’s performance in established RL benchmarks, namely Atari (Bellemare et al., 2013) and DMC (Tassa et al., 2018)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepMind Control Suite",
          "justification": "The DeepMind Control Suite (DMC) has been referenced for the evaluation of reinforcement learning tasks.",
          "quote": "DMC (Tassa et al., 2018)"
        }
      },
      {
        "name": {
          "value": "Kinetics",
          "justification": "The Kinetics dataset is referenced to discuss the impressive empirical results on various established benchmarks.",
          "quote": "S4 has demonstrated impressive empirical results on various established SL and SSL benchmarks involving long dependencies, and it outperforms Transformers (Vaswani et al., 2017; Dao et al., 2022) in terms of inference speed and memory consumption due to its recurrent inference mode."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The Kinetics Human Action Video Dataset",
          "justification": "The S4 model's performance on Kinetics is discussed to highlight its empirical results.",
          "quote": "S4 has demonstrated impressive empirical results on various established SL and SSL benchmarks"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "DreamerV3",
          "justification": "R2I builds upon DreamerV3 and integrates state space models to enhance its long-term memory and credit assignment capabilities.",
          "quote": "We introduce R2I, a memory-enhanced MBRL agent built upon DreamerV3 (Hafner et al., 2023) that uses a modification of S4 to handle temporal dependencies."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Mastering Diverse Domains through World Models",
          "justification": "DreamerV3 is an essential library referred to as a backbone for R2I.",
          "quote": "We introduce R2I, a memory-enhanced MBRL agent built upon DreamerV3 (Hafner et al., 2023) that uses a modification of S4 to handle temporal dependencies."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2098,
    "prompt_tokens": 32480,
    "total_tokens": 34578
  }
}