{
  "paper": "89568f1bd267bc19ed5efdda98590949.txt",
  "words": 15995,
  "extractions": {
    "title": {
      "value": "VisMin: Visual Minimal-Change Understanding",
      "justification": "The title 'VisMin: Visual Minimal-Change Understanding' is clearly stated at the beginning of the document and aligns with the focus of the paper on Visual Minimal-Change Understanding benchmarks.",
      "quote": "Title: VisMin: Visual Minimal-Change Understanding"
    },
    "description": "The paper introduces VisMin, a new benchmark designed to evaluate the fine-grained understanding capability of Visual-Language Models (VLMs) by focusing on minimal changes in images while maintaining semantic consistency. The paper also provides a large-scale training dataset for fine-tuning these models to enhance their image-text alignment and understanding capabilities.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts empirical experiments to test the capabilities of existing Visual-Language Models on the new VisMin benchmark and reports on the improved performance when models are fine-tuned.",
      "quote": "Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary focus of the paper is on evaluating and improving the visual understanding of images, typically a concern of the Computer Vision field.",
        "quote": "Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs)."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The paper discusses the fine-grained understanding and alignment between visual and textual data, which falls under Multimodal Learning.",
          "quote": "To evaluate VLMs’ fine-grained understanding, existing benchmarks primarily focus on evaluating VLMs’ capability to distinguish between two very similar captions given an image."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP",
          "justification": "The paper mentions fine-tuning with CLIP, a foundational Visual-Language Model, to improve fine-grained understanding.",
          "quote": "We generate a large-scale training dataset, which we use to finetune CLIP (a foundational VLM) and Idefics2 (a multimodal large language model)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "CLIP is mentioned as a pre-existing model used within the experiments rather than a contribution of this paper.",
          "quote": "Fine-tuning CLIP (a foundational VLM) and Idefics2 (a multimodal large language model)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses fine-tuning CLIP on the described dataset, indicating it was executed for the research.",
          "quote": "Fine-tuning CLIP (a foundational VLM) and Idefics2 (a multimodal large language model)."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of CLIP is compared with other models to assess improvements in fine-grained understanding.",
          "quote": "Empirical experiments reveal that current VLMs exhibit notable deficiencies..."
        },
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "The referenced title of the paper that originally introduced CLIP is provided in the references.",
          "quote": "CLIP [30]"
        }
      },
      {
        "name": {
          "value": "Idefics2",
          "justification": "Idefics2, a multimodal large language model, is mentioned as being fine-tuned for the study.",
          "quote": "leveraging the automated nature of our data creation process, we generate a large-scale training dataset, which we use to finetune CLIP (a foundational VLM) and Idefics2 (a multimodal large language model)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Idefics2 is mentioned as an existing multimodal large language model used in the paper.",
          "quote": "Idefics2 (a multimodal large language model)."
        },
        "is_executed": {
          "value": true,
          "justification": "The execution of Idefics2 is implied by the context where it, alongside CLIP, benefits from fine-tuning on the dataset.",
          "quote": "Our findings show that both these models benefit significantly from fine-tuning on this data."
        },
        "is_compared": {
          "value": true,
          "justification": "The comparison between Idefics2 and other models is implied by the analysis of results mentioned in the paper.",
          "quote": "Our findings show that both these models benefit significantly from fine-tuning on this data, as evident by marked improvements..."
        },
        "referenced_paper_title": {
          "value": "Obelics: An open web-scale filtered dataset of interleaved image-text documents",
          "justification": "The paper discussing Idefics2 is cited among the references, indicating its prior existence and usage.",
          "quote": "Idefics2[14]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "COCO",
          "justification": "The paper mentions that the VisMin benchmark builds on top of images from the COCO dataset, implying its use.",
          "quote": "built on top of the images from the COCO [21] dataset that consists of complex everyday scene images."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Microsoft COCO: Common Objects in Context",
          "justification": "COCO is a well-known dataset referred to as a foundation for creating the VisMin benchmark.",
          "quote": "COCO [21]"
        }
      },
      {
        "name": {
          "value": "VisMin",
          "justification": "The VisMin dataset is introduced in this paper as a new contribution for evaluating fine-grained understanding in VLMs.",
          "quote": "To evaluate VLMs’ fine-grained understanding...we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin)."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "None",
          "justification": "As the paper is the original source introducing the VisMin benchmark, there is no reference to another paper for this dataset.",
          "quote": "To evaluate VLMs’ fine-grained understanding...we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1231,
    "prompt_tokens": 26480,
    "total_tokens": 27711,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}