{
  "paper": "d7c514b0cb5f463803d21059f88df00d.txt",
  "words": 15393,
  "extractions": {
    "title": {
      "value": "RedPajama: an Open Dataset for Training Large Language Models",
      "justification": "The title is explicitly mentioned in the first line of the paper.",
      "quote": "RedPajama: an Open Dataset for Training Large Language Models"
    },
    "description": "The paper introduces the RedPajama datasets, which are large, open datasets designed for training large language models (LLMs). These datasets aim to address challenges in transparency of model development, access to large quantities of high-quality data, and availability of artifacts and metadata for dataset curation. The datasets include RedPajama-V1, a reproduction of the LLaMA training dataset, and RedPajama-V2, a massive web-only dataset with quality signals and metadata. The datasets are used for training strong language models like Snowflake Arctic and AI2's OLMo.",
    "type": {
      "value": "empirical",
      "justification": "The paper discusses dataset creation, characteristics, and evaluations of models trained on the RedPajama datasets, indicating an empirical approach.",
      "quote": "To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The focus of the paper is on datasets for training large language models, which is a subfield of NLP.",
        "quote": "Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Language Model Training",
          "justification": "The paper specifically addresses issues around training datasets for large language models.",
          "quote": "we release RedPajama-V1, an open reproduction of the LLaMA training dataset."
        },
        "aliases": [
          "LLM Training"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "RedPajama-INCITE",
          "justification": "The paper details the RedPajama-INCITE models, which were trained as part of this research.",
          "quote": "we have developed the RedPajama-INCITE models, which include a base, instruction-tuned, and chat models at the 3B and 7B scales."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "The RedPajama-INCITE models are presented as a contribution of the research described in the paper.",
          "quote": "we have developed the RedPajama-INCITE models..."
        },
        "is_executed": {
          "value": true,
          "justification": "The execution of the model is discussed, specifically on the Summit supercomputer.",
          "quote": "To evaluate how well RedPajama-V1 matches the original LLaMA corpus, we train a family of LLMs of various sizes in collaboration with the Incite project on the Summit supercomputer."
        },
        "is_compared": {
          "value": true,
          "justification": "The RedPajama-INCITE models are compared with other models such as LLaMA in terms of performance.",
          "quote": "The RedPajama-INCITE-7B-Base model is 1.0 points behind Falcon-7B and 4.1 points behind Llama-7B on HELM-classic."
        },
        "referenced_paper_title": {
          "value": "Llama: Open and efficient foundation language models",
          "justification": "The comparison is made against the LLaMA model, which is referenced in the context of the RedPajama models.",
          "quote": "After the release of LLaMA [57], which used seven individual subsets and multiple domains, we published RedPajama-V1 as an open source replication of the LLaMA recipe."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "RedPajama-V1",
          "justification": "This dataset is a significant part of the paper, focused on as a reproduction of the LLaMA dataset.",
          "quote": "we release RedPajama-V1, an open reproduction of the LLaMA training dataset."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Llama: Open and efficient foundation language models",
          "justification": "The dataset is created as a reproduction of the one used in the LLaMA model training, thus directly referencing its origin.",
          "quote": "RedPajama-V1 is a publicly available, fully open, best-effort reproduction of the training data described in [57], used to train the first iteration of LLaMA family of models."
        }
      },
      {
        "name": {
          "value": "RedPajama-V2",
          "justification": "The paper contributes RedPajama-V2, a new dataset focusing on web data with quality signals.",
          "quote": "Based on the first set of learnings from these efforts, we have built the RedPajama-V2 dataset."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Llama: Open and efficient foundation language models",
          "justification": "While primarily focused on new data, the context still relates back to earlier efforts involving LLaMA data handling.",
          "quote": "Based on the first set of learnings from these efforts, we have built the RedPajama-V2 dataset."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1049,
    "prompt_tokens": 30185,
    "total_tokens": 31234,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}