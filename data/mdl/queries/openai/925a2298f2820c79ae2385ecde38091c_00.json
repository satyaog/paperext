{
  "paper": "925a2298f2820c79ae2385ecde38091c.txt",
  "words": 10924,
  "extractions": {
    "title": {
      "value": "Forgetting Enhances Episodic Control With Structured Memories",
      "justification": "The title of the paper is explicitly mentioned multiple times, including in the formal citation and header.",
      "quote": "Yalnizyan-Carson A and Richards BA (2022) Forgetting Enhances Episodic Control With Structured Memories."
    },
    "description": "The paper investigates the hypothesis that forgetting can enhance episodic control in artificial agents by reducing the influence of outdated information, using structured mnemonic representations. It explores the performance of reinforcement learning agents with limited memory capacity in navigating maze environments.",
    "type": {
      "value": "empirical",
      "justification": "The study conducts simulations of reinforcement learning agents in grid-world environments to test the effects of memory restricting and forgetting.",
      "quote": "Therefore, to determine the validity of the beneficial forgetting hypothesis, we used an episodic control agent trained to forage for rewards in a series of maze environments."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper centers around using reinforcement learning as a framework to investigate decision making with episodic memory and forgetting effects.",
        "quote": "Reinforcement learning offers a normative framework in which to test such hypotheses."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Episodic Memory",
          "justification": "Episodic memory is a core aspect of the paper as the authors investigate how forgetting affects the use of episodic memories.",
          "quote": "episodic memory"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The research falls under the broader category of machine learning, specifically using AI-based reinforcement learning agents.",
          "quote": "Reinforcement learning (RL) from artificial intelligence (AI) provides a normative framework"
        },
        "aliases": [
          "ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Episodic Control Model",
          "justification": "The primary model described in the paper is an episodic control model used for reinforcement learning tasks.",
          "quote": "Our episodic control model was designed to solve a reinforcement learning task."
        },
        "aliases": [
          "Episodic Controller"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper presents the episodic control model as a contribution to explore forgetting in episodic memory.",
          "quote": "Therefore, to determine the validity of the beneficial forgetting hypothesis, we used an episodic control agent."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented and tested in simulated grid-world environments as part of the research.",
          "quote": "Simulations of the foraging task were carried out in four different grid-world environments."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not compare the episodic control model numerically to other models.",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "Neural episodic control",
          "justification": "The concept of episodic control is discussed in relation to previously established methods.",
          "quote": "In particular, episodic control, an approach in RL that utilizes one-shot memories of past events to shape an agentâ€™s policy (Lengyel and Dayan, 2007; Blundell et al., 2016; Pritzel et al., 2017; Ritter et al., 2018)"
        }
      },
      {
        "name": {
          "value": "Monte-Carlo Methods",
          "justification": "Monte-Carlo methods are mentioned as a technique used for computing return information.",
          "quote": "...return information used here to generate policies was computed by Monte-Carlo sampling of rewards."
        },
        "aliases": [
          "Monte Carlo Sampling"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Monte-Carlo methods are a well-known established technique discussed in the context of the paper.",
          "quote": "...computed by Monte-Carlo sampling of rewards, which is not the only way animals compute relative value of events in a trajectory"
        },
        "is_executed": {
          "value": false,
          "justification": "The term is mainly discussed as a method rather than being central to execution in experiments.",
          "quote": "Monte-Carlo methods require the task structure to be episodic."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not directly perform a side-by-side comparison of Monte-Carlo methods with other methods.",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "Reinforcement Learning: An Introduction",
          "justification": "Monte-Carlo methods are key methods discussed within the reinforcement learning literature like this.",
          "quote": "Sutton, R. S., and Barto, A. G. (1998). Reinforcement Learning: An Introduction."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Grid-world Environments",
          "justification": "The experiments are conducted in multiple grid-world environments specifically designed for the study's purpose.",
          "quote": "Simulations of the foraging task were carried out in four different grid-world environments."
        },
        "aliases": [
          "Grid-world"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The grid-world environments are part of the experiment setup specific to this study; no external reference is indicated.",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Python (NumPy, SciPy)",
          "justification": "The paper mentions the use of Python along with NumPy and SciPy libraries for simulations.",
          "quote": "All simulations were run in Python 3.6.8 with functions from NumPy and SciPy libraries."
        },
        "aliases": [
          "Python"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "Although Python, NumPy, and SciPy are mentioned, they aren't explicitly associated with a reference paper in the document.",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1120,
    "prompt_tokens": 18160,
    "total_tokens": 19280,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}