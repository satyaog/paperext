{
  "paper": "85b331c203251771ba650e9e8eba1ba5.txt",
  "words": 17333,
  "extractions": {
    "title": {
      "value": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages",
      "justification": "The title clearly indicates the focus of the paper, which is on the IGLUE benchmark designed for transfer learning across different modalities, tasks, and languages.",
      "quote": "IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages"
    },
    "description": "This paper introduces the IGLUE benchmark, which is designed to evaluate multilingual multimodal models in transfer learning tasks involving vision and language. The benchmark is comprehensive, covering various tasks such as visual question answering, cross-modal retrieval, grounded reasoning, and entailment across 20 languages. It highlights the challenges and progress in the field, particularly in multilingual settings, and evaluates the performance of state-of-the-art models.",
    "type": {
      "value": "empirical",
      "justification": "The paper is empirical as it involves the creation and evaluation of a benchmark (IGLUE) to test multimodal models' performance and compare their results across different multilingual scenarios.",
      "quote": "Based on the evaluation of the available state-of-the-art models, we find... Moreover... We hope to encourage future research efforts in this area by releasing the benchmark to the community."
    },
    "primary_research_field": {
      "name": {
        "value": "Multimodal and Multilingual Learning",
        "justification": "The primary focus of the paper is on developing and evaluating multimodal models that can handle multiple languages and modalities, such as vision and language.",
        "quote": "Multilingual Multimodal Learning. Multilingual V&L research focuses on collecting resources, developing mod- els, and evaluating systems that need to jointly reason over multilingual text and multimodal inputs."
      },
      "aliases": [
        "Multimodal Learning",
        "Multilingual Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Vision and Language",
          "justification": "A significant portion of the paper deals with models and datasets that involve both vision and language.",
          "quote": "IGLUE by collating current research threads in this area and extending them... models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups."
        },
        "aliases": [
          "V&L"
        ]
      },
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The paper discusses transfer learning extensively, focusing on zero-shot and few-shot learning scenarios.",
          "quote": "We make the first leap into multimodal cross- lingual evaluation... IGLUE aims to extend sheer accuracy-driven evaluation towards other crucial aspects such as fine-tuning efficiency, sample efficiency and adaptation to low-data scenarios."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "Several NLP tasks are covered under the benchmark, such as natural language inference and question answering.",
          "quote": "Inspired by recent text-only multi-task benchmarks for natural language understanding: these benchmarks have been proven invaluable as key drivers of recent steep perfor- mance progress of NLP."
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "mUNITER",
          "justification": "The paper refers to it as one of the state-of-the-art multilingual V&L models being evaluated.",
          "quote": "mUNITER. Liu et al. (2021) extend the UNITER architecture (Chen et al., 2020) multilingual."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "mUNITER was referenced as an existing model rather than a contribution of this paper.",
          "quote": "...state-of-the-art multilingual V&L architectures (Ni et al., 2021; Zhou et al., 2021; Liu et al., 2021)"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves a systematic evaluation of models, including mUNITER.",
          "quote": "we also run the first systematic comparative evaluation of cutting- edge multilingually pretrained V&L architectures"
        },
        "is_compared": {
          "value": true,
          "justification": "mUNITER's performance is compared against other models in the benchmark study.",
          "quote": "...we provide an overview of our experimental setup. We evaluate all the existing multilingual V&L pretrained models released so far."
        },
        "referenced_paper_title": {
          "value": "UNITER: Universal image-text representation learning",
          "justification": "mUNITER is an extension of the UNITER architecture, which is described in the referenced paper.",
          "quote": "mUNITER from mBERT (Devlin et al., 2019)."
        }
      },
      {
        "name": {
          "value": "xUNITER",
          "justification": "The paper lists xUNITER as another model being evaluated for multilingual capabilities.",
          "quote": "xUNITER from XLM-R (Conneau et al., 2020)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "xUNITER is referenced as an existing model that is not introduced by this paper.",
          "quote": "state-of-the-art multilingual V&L architectures (Ni et al., 2021; Zhou et al., 2021; Liu et al., 2021)"
        },
        "is_executed": {
          "value": true,
          "justification": "xUNITER is executed as part of the benchmark evaluations.",
          "quote": "To further facilitate replika- ble research in this area, we re-implement the existing multi- lingual V&L pretrained encoders in a unified framework"
        },
        "is_compared": {
          "value": true,
          "justification": "xUNITER is compared to other models within the benchmark study for performance analysis.",
          "quote": "...we evaluate all the existing multilingual V&L pretrained models released so far..."
        },
        "referenced_paper_title": {
          "value": "XLM-R: Unsupervised Cross-lingual Representation Learning at Scale",
          "justification": "The referenced paper provides details on the XLM-R model, which xUNITER builds upon.",
          "quote": "...from XLM-R (Conneau et al., 2020)."
        }
      },
      {
        "name": {
          "value": "UC^2",
          "justification": "UC^2 is highlighted as a strong model within the benchmark evaluations for multilingual and multimodal tasks.",
          "quote": "UC 2 relies on (text-only) machine translation technologies to obtain CC data in five languages."
        },
        "aliases": [
          "UC 2"
        ],
        "is_contributed": {
          "value": false,
          "justification": "UC^2 is referenced as an existing model evaluated in the study.",
          "quote": "...state-of-the-art multilingual V&L architectures (Ni et al., 2021; Zhou et al., 2021; Liu et al., 2021)"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper evaluates UC^2 within its experimental framework.",
          "quote": "evaluate all the existing multilingual V&L pretrained models released so far..."
        },
        "is_compared": {
          "value": true,
          "justification": "UC^2 is compared to other models within the IGLUE benchmark study.",
          "quote": "...we provide an overview of our experimental setup. We evaluate all the existing multilingual V&L pretrained models released so far."
        },
        "referenced_paper_title": {
          "value": "UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pre-training",
          "justification": "UC^2 is a multimodal pre-training architecture discussed in the referenced paper.",
          "quote": "Zhou et al. (2021) rely on..."
        }
      },
      {
        "name": {
          "value": "M^3P",
          "justification": "The M^3P model is evaluated in the IGLUE benchmark as part of the discussion on multilingual multimodal architectures.",
          "quote": "M 3 P. Ni et al. (2021) further introduce multimodal code-switched training tasks."
        },
        "aliases": [
          "M 3 P"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model M^3P is evaluated but not newly introduced by the paper.",
          "quote": "Using the code-switched training tasks... a baseline for multilingual transfer in multimodal pretrained models."
        },
        "is_executed": {
          "value": true,
          "justification": "M^3P is executed and tested within the IGLUE benchmarking framework.",
          "quote": "we evaluate the parameter sets that yield the best validation performance."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of M^3P is compared to other models within the benchmark evaluation.",
          "quote": "...multilingual V&L encoders, we also benchmark representative English V&L encoders pretrained..."
        },
        "referenced_paper_title": {
          "value": "M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training",
          "justification": "The referred paper details the M^3P model's architecture and its multilingual capabilities.",
          "quote": "M 3 P. Ni et al. (2021) further introduce multimodal code-switched training tasks."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "XVNLI",
          "justification": "The paper introduces this new dataset/task as part of the IGLUE benchmark for visual entailment.",
          "quote": "We propose the new task of Cross-lingual Visual Natural Language Inference (XVNLI)."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "A large annotated corpus for learning natural language inference",
          "justification": "The referenced SNLI dataset is extended for the XVNLI task, as mentioned in the paper.",
          "quote": "We combine the text-only dataset SNLI (Bowman et al., 2015), with its multimodal (Xie et al., 2019) and cross-lingual counterparts."
        }
      },
      {
        "name": {
          "value": "xGQA",
          "justification": "xGQA is introduced in the paper as a dataset for evaluating cross-lingual visual question answering tasks.",
          "quote": "To solve the Cross-lingual Grounded Question Answering task (xGQA; Pfeiffer et al. 2022), a model must answer several types of structured questions."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "GQA: A new dataset for real-world visual reasoning and compositional question answering",
          "justification": "The xGQA extends the GQA dataset for cross-lingual setups, referencing its original creation.",
          "quote": "The evaluation data in 7 target languages are manually translated from the validation set of GQA (Hudson & Manning, 2019)."
        }
      },
      {
        "name": {
          "value": "MaRVL",
          "justification": "The paper introduces MaRVL as part of the benchmark, highlighting its unique focus on multicultural reasoning involving both vision and language.",
          "quote": "The Multicultural Reasoning over Vision and Language dataset (Liu et al., 2021) requires to determine whether a textual description is true or false about a pair of images."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Visually grounded reasoning across languages and cultures",
          "justification": "The paper builds on the dataset introduced by Liu et al., with MaRVL focusing on multicultural visual reasoning.",
          "quote": "The Multicultural Reasoning over Vision and Language dataset (Liu et al., 2021)."
        }
      },
      {
        "name": {
          "value": "xFlickr&CO",
          "justification": "As part of the IGLUE benchmark, xFlickr&CO is introduced for multilingual retrieval tasks, combining data from Flickr30K and COCO datasets.",
          "quote": "We create a new, multilingual evaluation set for retrieval by combining 1,000 images from the Flickr30K and 1,000 from the COCO test split."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
          "justification": "The Flickr30K part of xFlickr&CO is based on the dataset first introduced by Young et al., providing the foundation for the cross-lingual extensions made in this paper.",
          "quote": "1,000 images from the Flickr30K (Young et al., 2014) and 1,000 from the COCO (Lin et al., 2014) test split."
        }
      },
      {
        "name": {
          "value": "WIT",
          "justification": "WIT (Wikipedia-based Image Text) is cited as a diverse dataset within the benchmark, representing real-world entities for retrieval tasks.",
          "quote": "The Wikipedia-based Image Text dataset (Srinivasan et al., 2021) collected examples from Wikipedia in 108 languages."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning",
          "justification": "The WIT dataset referenced here provides the multilingual training data utilized in this benchmark.",
          "quote": "The Wikipedia-based Image Text dataset (Srinivasan et al., 2021) collected examples from Wikipedia in 108 languages."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "V OLTA",
          "justification": "V OLTA is mentioned as the framework used for re-implementing the multilingual V&L encoders to facilitate research.",
          "quote": "In order to further facilitate research and development in multilingual V&L mod elling, we re-implement them in a single framework based on V OLTA (Bugliarello et al., 2021)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Multimodal Pretraining Unmasked: A Meta-analysis and a Unified Framework of Vision-and-Language BERTs",
          "justification": "The referenced paper explains the purpose and design of VOLTA, aligning with its use in this study.",
          "quote": "a single framework based on V OLTA (Bugliarello et al., 2021)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2722,
    "prompt_tokens": 36561,
    "total_tokens": 39283,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}