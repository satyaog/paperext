{
  "paper": "956591498b609ef8a7663053a36e7491.txt",
  "words": 10211,
  "extractions": {
    "title": {
      "value": "Adaptive Accompaniment with ReaLchords",
      "justification": "The title is directly extracted from the provided text.",
      "quote": "Adaptive Accompaniment with ReaLchords"
    },
    "description": "The paper introduces ReaLchords, an online generative model designed for adaptive musical accompaniment. It uses reinforcement learning to finetune an online model for generating chord accompaniments to user melodies in real-time. The approach involves a novel reward model for evaluating musical coherence and a distillation method from a future-seeing teacher model. The study demonstrates the model's ability to adapt to unfamiliar inputs and produce fitting accompaniments, facilitating live jamming and simultaneous co-creation.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts quantitative experiments and listening tests to demonstrate the effectiveness of the ReaLchords model.",
      "quote": "Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment."
    },
    "primary_research_field": {
      "name": {
        "value": "Music Generation",
        "justification": "The paper focuses on generating chord accompaniments for melodies, which falls under Music Generation in the context of deep learning.",
        "quote": "This paper introduces ReaLchords, a generative model tailored for online adaptive musical accompaniment."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "Reinforcement learning is a key component of the model's training process.",
          "quote": "We propose ReaLchords, an online accompaniment generation model trained by RL finetuning."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Knowledge Distillation",
          "justification": "The paper utilizes a novel type of distillation from a teacher model during training.",
          "quote": "We leverage knowledge distillation to learn from a non-causal teacher that can see the future (§3.4)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ReaLchords",
          "justification": "ReaLchords is the main contribution of the paper as an online model for chord accompaniment.",
          "quote": "This paper introduces ReaLchords, a generative model tailored for online adaptive musical accompaniment."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "ReaLchords is introduced and evaluated in this paper, indicating it is a novel contribution.",
          "quote": "We propose ReaLchords, an online accompaniment generation model."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not specifically mention whether ReaLchords is executed on GPU or CPU.",
          "quote": "[No specific quote about execution context]"
        },
        "is_compared": {
          "value": true,
          "justification": "ReaLchords is compared to other models in terms of adaptation and perceptual quality.",
          "quote": "we show that ReaLchords accompanies with good harmony and synchronization, while effectively adapting to mistakes and perturbations."
        },
        "referenced_paper_title": {
          "value": "Training Music Transformers with Permutation Equivariant Attention",
          "justification": "The referenced paper is mentioned in relation to one of the models used in ReaLchords.",
          "quote": "The offline model φ ω is implemented as an 8-layer encoder-decoder transformer in the style of T5 (Raffel et al., 2020) with 6 heads and hidden dimension of 512."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Hooktheory dataset",
          "justification": "The models in the paper are trained on the Hooktheory dataset, which contains melody-chord pairs.",
          "quote": "We train our models on an updated version of the Hooktheory dataset (Donahue et al., 2022), which comprises crowd-sourced analyses of monophonic melodies and chords from recordings."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Melody transcription via generative pre-training",
          "justification": "The referenced paper is the source of the Hooktheory dataset used in the research.",
          "quote": "We train our models on an updated version of the Hooktheory dataset (Donahue et al., 2022)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 839,
    "prompt_tokens": 17604,
    "total_tokens": 18443,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 17536
    }
  }
}