{
  "paper": "2310.15372.txt",
  "words": 16629,
  "extractions": {
    "title": {
      "value": "EpiK-Eval: Evaluation for Language Models as Epistemic Models",
      "justification": "Title of the research paper clearly mentioned at the beginning.",
      "quote": "EpiK-Eval: Evaluation for Language Models as Epistemic Models"
    },
    "description": "In this study, the authors introduce EpiK-Eval, a novel question-answering benchmark designed to evaluate the proficiency of large language models (LLMs) in consolidating knowledge from segmented narratives. This evaluation aims to assess and improve their ability to form a coherent and consistent knowledge representation for more effective reasoning.",
    "type": {
      "value": "empirical",
      "justification": "The study conducts evaluations, introduces a benchmark (EpiK-Eval), and presents quantitative results comparing different models' performances.",
      "quote": "We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs’ proficiency in formulating a coherent and consistent knowledge representation from segmented narratives."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on evaluating and improving language models' ability to consolidate knowledge and reason effectively.",
        "quote": "Developing systems that can reason through language understanding has been a cornerstone in natural language processing research."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Question Answering",
          "justification": "The benchmark and evaluations are centered around the ability of LLMs to answer questions accurately by consolidating segmented information.",
          "quote": "We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs’ proficiency..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Knowledge Representation",
          "justification": "The study investigates how effectively LLMs can integrate and represent knowledge from different sources.",
          "quote": "We assess where LMs lie on the spectrum...based on their inferred knowledge state evaluated through aggregate performance on EpiK-Eval."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Evaluation Methods",
          "justification": "The paper introduces a new benchmark (EpiK-Eval) for assessing the performance of language models.",
          "quote": "We propose the novel Epistemic Knowledge Evaluation (EpiK-Eval) benchmark, to evaluate this ability to leverage such a consolidated knowledge state."
        },
        "aliases": [
          "Evaluation Techniques"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "T5",
          "justification": "T5 is one of the models evaluated using the EpiK-Eval benchmark in the study.",
          "quote": "Performance on our benchmark is shown in Figure 2. There is a noticeable decline in performance for models trained on segmented stories compared to unsegmented ones."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "T5 is not introduced in this paper but is used as one of the evaluated models.",
          "quote": "We experiment with three different LLMs: T5 (Raffel et al., 2020), its instruction-tuned variant Flan-T5 (Chung et al., 2022), and OPT (Zhang et al., 2022)."
        },
        "is_executed": {
          "value": 0,
          "justification": "The text specifies benchmarking and evaluating the model with the new dataset, but does not explicitly mention execution hardware.",
          "quote": "Performance on our benchmark is shown in Figure 2. There is a noticeable decline in performance for models trained on segmented stories compared to unsegmented ones."
        },
        "is_compared": {
          "value": 1,
          "justification": "T5's performance is compared against other models on the EpiK-Eval benchmark.",
          "quote": "We experiment with three different LLMs: T5 (Raffel et al., 2020), its instruction-tuned variant Flan-T5 (Chung et al., 2022), and OPT (Zhang et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "Exploring the limits of transfer learning with a unified text-to-text transformer",
          "justification": "The referenced paper for T5 is cited as (Raffel et al., 2020).",
          "quote": "We experiment with three different LLMs: T5 (Raffel et al., 2020), its instruction-tuned variant Flan-T5 (Chung et al., 2022), and OPT (Zhang et al., 2022)."
        }
      },
      {
        "name": {
          "value": "Flan-T5",
          "justification": "Flan-T5 is one of the models evaluated using the EpiK-Eval benchmark in the study.",
          "quote": "For the segmented-trained models, hallucination rates among different models are more similar and also decrease with scale."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Flan-T5 is not introduced in this paper but is used as one of the evaluated models.",
          "quote": "We experiment with three different LLMs: T5 (Raffel et al., 2020), its instruction-tuned variant Flan-T5 (Chung et al., 2022), and OPT (Zhang et al., 2022)."
        },
        "is_executed": {
          "value": 0,
          "justification": "The text specifies benchmarking and evaluating the model with the new dataset, but does not explicitly mention execution hardware.",
          "quote": "Performance on our benchmark is shown in Figure 2. There is a noticeable decline in performance for models trained on segmented stories compared to unsegmented ones."
        },
        "is_compared": {
          "value": 1,
          "justification": "Flan-T5's performance is compared against other models on the EpiK-Eval benchmark.",
          "quote": "For causal language models, they are simply expected to predict the next token in the given sequence, as is standard procedure."
        },
        "referenced_paper_title": {
          "value": "Scaling instruction-finetuned language models",
          "justification": "The referenced paper for Flan-T5 is cited as (Chung et al., 2022).",
          "quote": "We experiment with three different LLMs: T5 (Raffel et al., 2020), its instruction-tuned variant Flan-T5 (Chung et al., 2022), and OPT (Zhang et al., 2022)."
        }
      },
      {
        "name": {
          "value": "OPT",
          "justification": "OPT is one of the models evaluated using the EpiK-Eval benchmark in the study.",
          "quote": "Performance on our benchmark is shown in Figure 2. There is a noticeable decline in performance for models trained on segmented stories compared to unsegmented ones."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "OPT is not introduced in this paper but is used as one of the evaluated models.",
          "quote": "We experiment with three different LLMs: T5 (Raffel et al., 2020), its instruction-tuned variant Flan-T5 (Chung et al., 2022), and OPT (Zhang et al., 2022)."
        },
        "is_executed": {
          "value": 0,
          "justification": "The text specifies benchmarking and evaluating the model with the new dataset, but does not explicitly mention execution hardware.",
          "quote": "Performance on our benchmark is shown in Figure 2. There is a noticeable decline in performance for models trained on segmented stories compared to unsegmented ones."
        },
        "is_compared": {
          "value": 1,
          "justification": "OPT's performance is compared against other models on the EpiK-Eval benchmark.",
          "quote": "We experiment with three different LLMs: T5 (Raffel et al., 2020), its instruction-tuned variant Flan-T5 (Chung et al., 2022), and OPT (Zhang et al., 2022)."
        },
        "referenced_paper_title": {
          "value": "OPT: Open Pre-trained Transformer Language Models",
          "justification": "The referenced paper for OPT is cited as (Zhang et al., 2022).",
          "quote": "We experiment with three different LLMs: T5 (Raffel et al., 2020), its instruction-tuned variant Flan-T5 (Chung et al., 2022), and OPT (Zhang et al., 2022)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "EpiK-Eval",
          "justification": "EpiK-Eval is a novel benchmark dataset introduced in the paper to evaluate LLMs.",
          "quote": "We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs’ proficiency..."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no referenced paper title provided as EpiK-Eval is introduced in this paper.",
          "quote": ""
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2118,
    "prompt_tokens": 29148,
    "total_tokens": 31266
  }
}