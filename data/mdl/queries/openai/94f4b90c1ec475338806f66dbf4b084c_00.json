{
  "paper": "94f4b90c1ec475338806f66dbf4b084c.txt",
  "words": 17468,
  "extractions": {
    "title": {
      "value": "Feature learning as alignment: a structural property of gradient descent in non-linear neural networks",
      "justification": "The title is mentioned at the beginning of the document and is a clear identifier of the paper.",
      "quote": "Feature learning as alignment: a structural property of gradient descent in non-linear neural networks"
    },
    "description": "The paper investigates the structural properties of gradient descent in neural networks, focusing on feature learning as alignment. It explores the correlation between gram matrices of weights and average gradient outer products, known as the neural feature ansatz. The authors provide a theoretical explanation for this correlation, showing it as alignment between left singular structures of weight matrices and pre-activation tangent features. A new optimization rule is introduced to enhance feature learning through this alignment.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily focuses on providing a theoretical explanation for observed correlations in neural network training, rather than conducting empirical experiments.",
      "quote": "Through this centering, we show that the dynamics of the C-NFC can be understood analytically..."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning Theory",
        "justification": "The paper addresses unsolved problems in deep learning theory, focusing on feature learning and the neural feature ansatz.",
        "quote": "However, the specific mechanism through which features are learned is an important unsolved problem in deep learning theory."
      },
      "aliases": [
        "DL Theory"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization in Deep Learning",
          "justification": "The paper introduces a new optimization rule and discusses gradient descent dynamics, aligning with optimization in deep learning.",
          "quote": "Finally, we introduce a simple optimization rule motivated by our analysis of the centered correlation which dramatically increases the NFA correlations."
        },
        "aliases": [
          "DL Optimization"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "VGG",
          "justification": "VGG is mentioned as an example of a practical model to which the neural feature ansatz applies.",
          "quote": "In particular, the NFM and AGOP are highly correlated in all layers of trained neural networks of general architectures, including practical models such as VGG."
        },
        "aliases": [
          "VGGNet"
        ],
        "is_contributed": {
          "value": false,
          "justification": "VGG is referenced as a practical model rather than a contribution of this paper.",
          "quote": "...including practical models such as VGG..."
        },
        "is_executed": {
          "value": false,
          "justification": "The focus is on theoretical analysis rather than execution of specific models like VGG.",
          "quote": "...including practical models such as VGG..."
        },
        "is_compared": {
          "value": true,
          "justification": "VGG is compared in terms of how the neural feature ansatz applies to it.",
          "quote": "In particular, the NFM and AGOP are highly correlated in all layers of trained neural networks of general architectures, including practical models such as VGG."
        },
        "referenced_paper_title": {
          "value": "Very deep convolutional networks for large-scale image recognition",
          "justification": "The referenced paper title is provided, linking to the original VGG paper.",
          "quote": "Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014."
        }
      },
      {
        "name": {
          "value": "Vision Transformers",
          "justification": "Vision Transformers are also discussed as examples of architectures to which the neural feature ansatz is applicable.",
          "quote": "In particular, the NFM and AGOP are highly correlated in all layers of trained neural networks of general architectures, including practical models such as... vision transformers..."
        },
        "aliases": [
          "ViT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Vision Transformers are referenced, not introduced by this paper.",
          "quote": "...including practical models such as... vision transformers..."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not report execution of Vision Transformers but discusses them theoretically.",
          "quote": "In particular, the NFM and AGOP are highly correlated in all layers of trained neural networks of general architectures, including practical models such as... vision transformers..."
        },
        "is_compared": {
          "value": true,
          "justification": "Vision Transformers are mentioned as part of the architectures demonstrating neural feature ansatz correlations.",
          "quote": "In particular, the NFM and AGOP are highly correlated in all layers of trained neural networks of general architectures, including practical models such as... vision transformers..."
        },
        "referenced_paper_title": {
          "value": "An image is worth 16x16 words: Transformers for image recognition at scale",
          "justification": "The paper citing the architecture for Vision Transformers is noted here.",
          "quote": "An image is worth 16x16 words: Transformers for image recognition at scale, 2021."
        }
      },
      {
        "name": {
          "value": "GPT-family models",
          "justification": "GPT models are included in the discussion about models affected by neural feature ansatz.",
          "quote": "In particular, the NFM and AGOP are highly correlated in all layers of trained neural networks of general architectures, including practical models such as... GPT-family models..."
        },
        "aliases": [
          "Generative Pre-trained Transformer"
        ],
        "is_contributed": {
          "value": false,
          "justification": "GPT models are discussed as part of existing architectures, not as a new contribution.",
          "quote": "...including practical models such as... GPT-family models..."
        },
        "is_executed": {
          "value": false,
          "justification": "The focus is on theoretical application rather than execution of models like GPT.",
          "quote": "In particular, the NFM and AGOP are highly correlated in all layers of trained neural networks of general architectures, including practical models such as... GPT-family models..."
        },
        "is_compared": {
          "value": true,
          "justification": "GPT models are compared in terms of how neural feature ansatz applies to them.",
          "quote": "In particular, the NFM and AGOP are highly correlated in all layers of trained neural networks of general architectures, including practical models such as... GPT-family models..."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "This is the referenced work on GPT models cited in this paper.",
          "quote": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877â€“1901, 2020."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SVHN",
          "justification": "SVHN is used in experiments to verify trends in feature learning across networks.",
          "quote": "For (A,B) we trained a five layer MLP on the first 50,000 datapoints of Streetview House Numbers..."
        },
        "aliases": [
          "Street View House Numbers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Reading Digits in Natural Images with Unsupervised Feature Learning and Big Data",
          "justification": "SVHN dataset original reference should be included here for completeness.",
          "quote": "The SVHN (Street View House Numbers) dataset is referenced for experiments in (Netzer et al., 2011)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1402,
    "prompt_tokens": 30362,
    "total_tokens": 31764,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 30208
    }
  }
}