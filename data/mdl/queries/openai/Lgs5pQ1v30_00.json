{
  "paper": "Lgs5pQ1v30.txt",
  "words": 23994,
  "extractions": {
    "title": {
      "value": "FedShuffle: Recipes for Better Use of Local Work in Federated Learning",
      "justification": "The title is directly mentioned at the beginning of the paper.",
      "quote": "FedShuffle: Recipes for Better Use of Local Work in Federated Learning"
    },
    "description": "The paper introduces FedShuffle, a new method for Federated Learning that incorporates random reshuffling, data imbalance, and client sampling. It aims to solve the objective inconsistency problem in Federated Learning, improving upon FedAvg and FedNova by utilizing momentum variance reduction under a Hessian similarity assumption. The authors present a comprehensive theoretical analysis, showing FedShuffleâ€™s advantages both in theory and through experiments with synthetic and real datasets.",
    "type": {
      "value": "theoretical",
      "justification": "The paper presents a comprehensive theoretical analysis of FedShuffle, providing convergence guarantees and comparing it to other methods like FedAvg and FedNova.",
      "quote": "We present a comprehensive theoretical analysis of FedShuffle and show, both theoretically and empirically, that it does not suffer from the objective function mismatch..."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The main focus of the paper is on improving the approach of local updates in Federated Learning setups.",
        "quote": "In this work, we propose a general recipe, FedShuffle, that better utilizes the local updates in FL, especially in this regime encompassing random reshuffling and heterogeneity."
      },
      "aliases": [
        "FL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper addresses optimization challenges in Federated Learning such as objective inconsistency, random reshuffling, and data imbalance.",
          "quote": "We introduce and analyze FedShuffle to account for random reshuffling, client sampling, and address the objective inconsistency problem."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Variance Reduction",
          "justification": "The paper discusses momentum variance reduction techniques within the context of Federated Learning and compares them to traditional approaches.",
          "quote": "Similar to Mime, we show that FedShuffle with momentum variance reduction... improves upon non-local methods."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "FedShuffle",
          "justification": "FedShuffle is introduced in the paper as a new method for Federated Learning.",
          "quote": "In this paper, we propose a general recipe, FedShuffle, that better utilizes the local updates in FL."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "The paper introduces FedShuffle as a contribution to the field of Federated Learning to address objective inconsistencies and enhance performance.",
          "quote": "We present a comprehensive theoretical analysis of FedShuffle."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes experiments and results showing the performance of FedShuffle on datasets.",
          "quote": "Finally, our theoretical results and insights are corroborated by experiments, both in a controlled setting with synthetic data, and using commonly-used real datasets."
        },
        "is_compared": {
          "value": true,
          "justification": "FedShuffle is compared with other methods such as FedAvg and FedNova in terms of performance and convergence.",
          "quote": "By combining the ingredients above, FedShuffle improves upon FedNova..."
        },
        "referenced_paper_title": {
          "value": "Mime: Mimicking centralized stochastic algorithms in federated learning",
          "justification": "FedShuffle is compared to Mime in the context of using momentum variance reduction.",
          "quote": "Similar to Mime (Karimireddy et al., 2020), we show that FedShuffle with momentum variance reduction (Cutkosky & Orabona, 2019) improves upon non-local methods."
        }
      },
      {
        "name": {
          "value": "FedNova",
          "justification": "FedNova is mentioned as a method that FedShuffle aims to improve upon.",
          "quote": "In addition, by combining the ingredients above, FedShuffle improves upon FedNova (Wang et al., 2020), which was previously proposed to solve this mismatch."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "FedNova is not a contribution of this paper but is used as a benchmark for comparison.",
          "quote": "FedShuffle improves upon FedNova (Wang et al., 2020), which was previously proposed to solve this mismatch."
        },
        "is_executed": {
          "value": true,
          "justification": "FedNova is used for comparison and benchmarking against FedShuffle's performance.",
          "quote": "Our theoretical results and insights are corroborated by experiments... and comparison with other methods from the literature."
        },
        "is_compared": {
          "value": true,
          "justification": "FedNova is explicitly compared to FedShuffle in the context of solving objective inconsistencies and performance.",
          "quote": "FedShuffle improves upon FedNova (Wang et al., 2020)."
        },
        "referenced_paper_title": {
          "value": "Tackling the objective inconsistency problem in heterogeneous federated optimization.",
          "justification": "This is the reference paper title for FedNova as cited in the document.",
          "quote": "FedNova (Wang et al., 2020), which was previously proposed to solve this mismatch."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR100",
          "justification": "CIFAR100 is used in the experiments to benchmark and compare the performance of FedShuffle.",
          "quote": "In the next experiments, we evaluate the same methods on the CIFAR100 ... datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images.",
          "justification": "The CIFAR100 dataset is commonly accompanied by this reference paper by Krizhevsky et al.",
          "quote": "CIFAR100 Krizhevsky et al. (2009)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions using PyTorch for implementing their experiments, indicating its role in executing the models.",
          "quote": "Our implementation is in PyTorch (Paszke et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library.",
          "justification": "The associated reference for PyTorch is cited in the document.",
          "quote": "PyTorch: An Imperative Style, High-Performance Deep Learning Library (Paszke et al., 2019)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1245,
    "prompt_tokens": 43631,
    "total_tokens": 44876,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}