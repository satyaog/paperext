{
  "paper": "yIxtevizEA.txt",
  "words": 8664,
  "extractions": {
    "title": {
      "value": "Latent Bottlenecked Attentive Neural Processes",
      "justification": "TITLE ABBREVIATION: LBANPs",
      "quote": "L ATENT B OTTLENECKED ATTENTIVE N EURAL P RO CESSES Leo Feng Mila – Université de Montréal\n& Borealis AI leo.feng@mila.quebec Yoshua Bengio Mila – Université de Montréal yoshua.bengio@mila.quebec"
    },
    "description": "This paper introduces the Latent Bottlenecked Attentive Neural Processes (LBANPs) that enhance computational efficiency without sacrificing performance. The novel method encodes context datasets into a constant number of latent vectors and employs cross-attention mechanisms to retrieve higher-order information. The efficiency and effectiveness of LBANPs are validated through empirical evaluations on tasks like meta-regression, image completion, and contextual multi-armed bandits.",
    "type": {
      "value": "empirical",
      "justification": "The paper's primary focus is on empirical validation of the proposed LBANP model through various experiments and comparisons to show state-of-the-art performance across multiple tasks.",
      "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion,\nand contextual multi-armed bandits."
    },
    "primary_research_field": {
      "name": {
        "value": "Meta-learning",
        "justification": "The paper discusses enhancing Neural Processes within the meta-learning framework.",
        "quote": "Neural Processes (NPs) are popular methods in meta-learning that can estimate predictive uncertainty on target datapoints by conditioning on a context dataset."
      },
      "aliases": [
        "Meta-learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Attention Mechanisms",
          "justification": "The paper extensively discusses improvements to neural processes via attention mechanisms, including cross-attention and self-attention.",
          "quote": "Inspired by recent developments in efficient attention mechanisms, (1) we propose Latent Bottlenecked Attentive Neural Processes (LBANP), a computationally efficient NP variant that has a querying computational complexity independent of the number of context datapoints."
        },
        "aliases": [
          "Attention Mechanisms",
          "Attention"
        ]
      },
      {
        "name": {
          "value": "Image Completion",
          "justification": "One of the tasks for which the empirical results of LBANP were evaluated is image completion.",
          "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion,\nand contextual multi-armed bandits."
        },
        "aliases": [
          "Image Completion"
        ]
      },
      {
        "name": {
          "value": "Contextual Bandits",
          "justification": "One of the tasks used to evaluate LBANP is contextual multi-armed bandits.",
          "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion,\nand contextual multi-armed bandits."
        },
        "aliases": [
          "Contextual Bandits"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Latent Bottlenecked Attentive Neural Processes (LBANPs)",
          "justification": "The main model proposed in the paper is Latent Bottlenecked Attentive Neural Processes (LBANPs).",
          "quote": "Tackling this issue, we propose Latent Bottlenecked Attentive Neural Processes (LBANPs),\na new computationally efficient sub-quadratic NP variant, that has a querying computational complexity independent of the number of context datapoints."
        },
        "aliases": [
          "LBANPs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The primary contribution of the paper is the LBANPs model.",
          "quote": "Tackling this issue, we propose Latent Bottlenecked Attentive Neural Processes (LBANPs),\na new computationally efficient sub-quadratic NP variant, that has a querying computational complexity independent of the number of context datapoints."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes empirical evaluations for the proposed LBANPs model, indicating execution.",
          "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion,\nand contextual multi-armed bandits."
        },
        "is_compared": {
          "value": 1,
          "justification": "The LBANP model is compared numerically with other models in several experimental results tables.",
          "quote": "The aim of the experimental evaluation is to answer the following questions: (1) What is the performance of LBANP compared to the existing NP methods?"
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "This is a new model presented in this paper.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Transformer Neural Processes (TNPs)",
          "justification": "The Transformer Neural Processes (TNPs) model is compared against LBANPs in the paper.",
          "quote": "Previous state-of-the-art method Transformer Neural Processes (TNPs) achieve strong performance but require quadratic computation with respect to the number of context datapoints, significantly limiting its scalability."
        },
        "aliases": [
          "TNPs"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The TNPs model is referenced for comparison purposes and is not contributed by this paper.",
          "quote": "Previous state-of-the-art method Transformer Neural Processes (TNPs) achieve strong performance but require quadratic computation with respect to the number of context datapoints, significantly limiting its scalability."
        },
        "is_executed": {
          "value": 0,
          "justification": "The TNPs was used as a reference model for comparison.",
          "quote": "Previous state-of-the-art method Transformer Neural Processes (TNPs) achieve strong performance but require quadratic computation with respect to the number of context datapoints, significantly limiting its scalability."
        },
        "is_compared": {
          "value": 1,
          "justification": "The TNPs model's performance was compared with the LBANPs model.",
          "quote": "We empirically show that LBANPs achieve results competitive with the state-of-the-art on meta-regression, image completion,\nand contextual multi-armed bandits."
        },
        "referenced_paper_title": {
          "value": "Transformer Neural Processes: Uncertainty-Aware Meta-Learning via Sequence Modeling",
          "justification": "The TNPs model is referenced from this paper.",
          "quote": "Previous state-of-the-art method Transformer Neural Processes (TNPs) achieve strong performance but require quadratic computation with respect to the number of context datapoints, significantly limiting its scalability. ...  TNP-ND (Nguyen & Grover, 2022)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CelebA",
          "justification": "The CelebA dataset was used for empirical evaluation for image completion tasks.",
          "quote": "CelebA comprises of colored images of celebrity faces. We evaluate the models on settings of various resolutions."
        },
        "aliases": [
          "CelebA"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep Learning Face Attributes in the Wild",
          "justification": "The CelebA dataset is referenced from this paper.",
          "quote": "CelebA comprises of colored images of celebrity faces. We evaluate the models on settings of various resolutions. For CelebA32, the images are down-sampled to 32 × 32 and N ∼ U[3, 197) and M ∼ U[3, 200 − N ). For CelebA64, the images are down-sampled to 64 × 64 and N ∼ U[3, 797) and M ∼ U[3, 800 − N ). For CelebA128, the images are down-sampled to 128 × 128 and N ∼ U[3, 1597) and M ∼ U[3, 1600 − N )."
        }
      },
      {
        "name": {
          "value": "EMNIST",
          "justification": "The EMNIST dataset was used for empirical evaluation for image completion tasks.",
          "quote": "For these experiments, we consider two datasets:\nEMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015).\nEMNIST comprises of black and white images of handwritten letters of 32×32 resolution."
        },
        "aliases": [
          "EMNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "EMNIST: Extending MNIST to handwritten letters",
          "justification": "The EMNIST dataset is referenced from this paper.",
          "quote": "EMNIST comprises of black and white images of handwritten letters of 32×32 resolution."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1685,
    "prompt_tokens": 15565,
    "total_tokens": 17250
  }
}