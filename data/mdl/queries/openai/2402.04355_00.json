{
  "paper": "2402.04355.txt",
  "words": 9286,
  "extractions": {
    "title": {
      "value": "PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation",
      "justification": "The title is prominently listed at the top of the paper and on each page in the header.",
      "quote": "PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation"
    },
    "description": "The paper introduces a statistical framework called PQMass for assessing the quality of generative models. It uses probability mass estimation to evaluate how well generative models capture the fidelity, diversity, and novelty of the target distribution. PQMass is computationally efficient and does not rely on auxiliary models or dimensionality reduction, making it suitable for high-dimensional data. The framework is tested on various tasks including Gaussian mixtures, time series, and image datasets, showing its robustness and versatility for evaluating generative models.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results to demonstrate the effectiveness of PQMass on multiple datasets and tasks, indicating that it is an empirical study.",
      "quote": "We show an experiment using this Bayesian approach with a low number of regions in §B, but otherwise, given the numerical cost of this method, we will focus on the frequentist approach in this paper, and leave the Bayesian approach for future work."
    },
    "primary_research_field": {
      "name": {
        "value": "Generative Models",
        "justification": "The paper primarily focuses on evaluating generative models, which are used to approximate distributions and generate new data samples.",
        "quote": "When evaluating generative models, we are interested in three qualitative properties (Stein et al., 2023; Jiralerspong et al., 2023): Fidelity refers to the quality and realism of individual outputs generated by a model."
      },
      "aliases": [
        "Generative Modeling"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Probability and Statistics",
          "justification": "The methodology for assessing generative models relies heavily on probabilistic and statistical techniques, such as binomial and multinomial distributions.",
          "quote": "The number of data points lying in the region follows a binomial distribution with a parameter equal to the region’s true probability mass."
        },
        "aliases": [
          "Statistical Methods"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Variational Autoencoders (VAEs)",
          "justification": "VAEs are specifically mentioned as one of the methods for generative modeling that the paper discusses and tests.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        },
        "aliases": [
          "VAEs"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is referenced and used for experiments but is not introduced in this paper.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The VAE model was implemented and tested as part of the experiments in the paper, specifically trained on the MNIST dataset.",
          "quote": "In this section, we track the value of our metric as we train two generative models, a variational autoencoder (VAE; Kingma & Welling, 2013) and a denoising diffusion model (Ho et al., 2020a; Song et al., 2021) with hyperparameters detailed in §E."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the VAE is compared against other models using the PQMass metric.",
          "quote": "In §4.5, we track the value of our metric as we train two generative models."
        },
        "referenced_paper_title": {
          "value": "Auto-Encoding Variational Bayes",
          "justification": "The referenced paper where this model was initially introduced is 'Auto-Encoding Variational Bayes' by Kingma & Welling, 2013.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        }
      },
      {
        "name": {
          "value": "Denoising Diffusion Probabilistic Models (DDPM)",
          "justification": "DDPMs are another type of generative model discussed and tested in the experiments to demonstrate the effectiveness of PQMass.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        },
        "aliases": [
          "DDPM"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is referenced and used for experiments but is not introduced in this paper.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The DDPM model was implemented and tested as part of the experiments in the paper, specifically trained on the MNIST dataset.",
          "quote": "In this section, we track the value of our metric as we train two generative models, a variational autoencoder (VAE; Kingma & Welling, 2013) and a denoising diffusion model (Ho et al., 2020a; Song et al., 2021) with hyperparameters detailed in §E."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the DDPM is compared against other models using the PQMass metric.",
          "quote": "In §4.5, we track the value of our metric as we train two generative models."
        },
        "referenced_paper_title": {
          "value": "Denoising Diffusion Probabilistic Models",
          "justification": "The referenced paper where this model was initially introduced is 'Denoising Diffusion Probabilistic Models' by Ho et al., 2020.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        }
      },
      {
        "name": {
          "value": "Generative Adversarial Networks (GANs)",
          "justification": "GANs are mentioned as one of the commonly used generative models for distribution approximation that the paper evaluates.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        },
        "aliases": [
          "GANs"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is referenced in the paper but not introduced by it.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        },
        "is_executed": {
          "value": 0,
          "justification": "While GANs are mentioned in the paper, there is no indication that they were implemented or tested as part of the experiments.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        },
        "is_compared": {
          "value": 0,
          "justification": "GANs are not part of the empirical comparisons within the paper.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        },
        "referenced_paper_title": {
          "value": "Generative Adversarial Nets",
          "justification": "The referenced paper where this model was initially introduced is 'Generative Adversarial Nets' by Goodfellow et al., 2014.",
          "quote": "Generative machine learning has witnessed the development of a succession of methods for distribution approximation, including variational autoencoders (VAEs, Kingma & Welling, 2013), generative adversarial networks (GANs, Goodfellow et al., 2014), normalizing flows (Rezende & Mohamed, 2015), and score-based (diffusion) generative models (Ho et al., 2020b)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is explicitly used to test the generative models in the paper.",
          "quote": "We execute this experiment over the first 100 epochs to highlight the early stages of training for the models and repeat the experiment twice for robustness. We see in Fig. 8 that the samples from the MNIST dataset are visually consistent, aligning with the successful training metrics recorded."
        },
        "aliases": [
          "Modified National Institute of Standards and Technology database"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The MNIST Database",
          "justification": "The MNIST dataset was introduced by LeCun, Cortes, and Burges in the referenced paper 'The MNIST Database'.",
          "quote": "We track the quality of generated samples and compare them to the true distribution of the MNIST dataset to validate our assessments."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The CIFAR-10 dataset is explicitly used in the experiments to benchmark the generative models.",
          "quote": "We focus on the CIFAR-10 dataset3 . Following (Stein et al., 2023), we first use the human error rate as a measure of the fidelity and diversity of the generative models. We compare generated samples from each of the models, to the test data for CIFAR-10."
        },
        "aliases": [
          "Canadian Institute For Advanced Research"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The CIFAR-10 dataset",
          "justification": "The CIFAR-10 dataset was introduced by Krizhevsky and Hinton in the referenced paper 'The CIFAR-10 dataset'.",
          "quote": "We focus on the CIFAR-10 dataset3 . Following (Stein et al., 2023), we first use the human error rate as a measure of the fidelity and diversity of the generative models."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "emcee",
          "justification": "The 'emcee' library is used for Markov Chain Monte Carlo (MCMC) sampling in the paper's experiments.",
          "quote": "We use Markov Chain Monte Carlo (Metropolis et al., 1953), for which we use the emcee implementation (Foreman-Mackey et al., 2013)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "emcee: The MCMC Hammer",
          "justification": "The referenced paper where the 'emcee' library was initially introduced is 'emcee: The MCMC Hammer' by Foreman-Mackey et al., 2013.",
          "quote": "We use Markov Chain Monte Carlo (Metropolis et al., 1953), for which we use the emcee implementation (Foreman-Mackey et al., 2013)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2757,
    "prompt_tokens": 16570,
    "total_tokens": 19327
  }
}