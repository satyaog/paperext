{
  "paper": "2206.01251.txt",
  "words": 8509,
  "extractions": {
    "title": {
      "value": "Using Representation Expressiveness and Learnability to Evaluate Self-Supervised Learning Methods",
      "justification": "This is the complete title of the paper.",
      "quote": "Using Representation Expressiveness and Learnability\nto Evaluate Self-Supervised Learning Methods"
    },
    "description": "This paper proposes a framework to evaluate self-supervised learning (SSL) models by measuring the expressiveness and learnability of the representations. Specifically, intrinsic dimension (ID) is used to assess expressiveness, and Cluster Learnability (CL) is introduced to assess learnability. The combined predictor, CLID, is shown to correlate well with model performance across different SSL methods and tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes a large-scale empirical study with various SSL algorithms, architectures, and training epochs.",
      "quote": "Through a large-scale empirical study with a\ndiverse family of SSL algorithms, we find that CLID better correlates with in-distribution\nmodel performance than other competing recent evaluation schemes."
    },
    "primary_research_field": {
      "name": {
        "value": "Self-Supervised Learning",
        "justification": "The paper focuses on evaluating self-supervised learning (SSL) methods.",
        "quote": "Despite impressive recent progress in self-supervised learning (SSL) ... the problem of properly evaluating the quality of the learned representations without using labelled data has not been fully explored."
      },
      "aliases": [
        "SSL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper discusses evaluating the quality of learned representations in SSL.",
          "quote": "We argue that representations can be evaluated through the lens of expressiveness and learnability."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Evaluation Metrics",
          "justification": "The paper proposes new metrics (ID and CL) for evaluating SSL models.",
          "quote": "We propose to use the Intrinsic Dimension (ID) to assess expressiveness and introduce Cluster Learnability (CL) to assess learnability."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "DeepCluster-v2",
          "justification": "The paper mentions DeepCluster-v2 as one of the SSL methods evaluated.",
          "quote": "deepclusterv2"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "DeepCluster-v2 is referenced as part of the empirical study, not a novel contribution.",
          "quote": "deepclusterv2"
        },
        "is_executed": {
          "value": true,
          "justification": "The model is evaluated as part of the empirical study.",
          "quote": "In this section, we check the sensitivity of CLID to its hyperparameters ... We use the KNN evaluation on the validation data using the ground-truth labels to measure the performance of the model..."
        },
        "is_compared": {
          "value": true,
          "justification": "DeepCluster-v2 is compared to other SSL models.",
          "quote": "We perform both qualitative and a quantitative examination and show the results in Figure 1. On the left, we find that the self-supervised learning checkpoints with higher ImageNet accuracies tend to be both more learnable and more expressive."
        },
        "referenced_paper_title": {
          "value": "Unsupervised Learning of Visual Features by Contrasting Cluster Assignments",
          "justification": "This is the title of the reference paper for DeepCluster-v2.",
          "quote": "Caron et al., 2020"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is the primary dataset used for evaluating the models.",
          "quote": "We select in total 28 self-supervised learning checkpoints trained on ImageNet over different algorithms, architecture, and training epochs."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "This is the reference for the ImageNet dataset.",
          "quote": "ImageNet is referenced throughout the paper as the main evaluation dataset."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used for implementing and evaluating SSL models.",
          "quote": "All our experiments are computed on a single V100 GPU."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the reference paper for PyTorch.",
          "quote": "Paszke et al., 2019"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1105,
    "prompt_tokens": 16393,
    "total_tokens": 17498
  }
}