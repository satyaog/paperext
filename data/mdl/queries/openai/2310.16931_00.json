{
  "paper": "2310.16931.txt",
  "words": 12215,
  "extractions": {
    "title": {
      "value": "CL-MASR: A Continual Learning Benchmark for Multilingual ASR",
      "justification": "This is the title of the paper.",
      "quote": "CL-MASR: A Continual Learning Benchmark for Multilingual ASR"
    },
    "description": "The paper introduces CL-MASR, a benchmark designed for studying multilingual Automatic Speech Recognition (ASR) in a continual learning setting. It provides a diverse set of continual learning methods on large-scale pretrained ASR models and common metrics to assess the effectiveness of learning new languages without catastrophic forgetting.",
    "type": {
      "value": "empirical",
      "justification": "The paper is empirical in nature as it introduces a benchmark and performs experiments to evaluate various continual learning methods on multilingual ASR models.",
      "quote": "Based on the experiments conducted using CL-MASR, we conclude that experience replay [9] is on average one of the best performing CL approaches in the context of multilingual ASR."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper primarily focuses on Automatic Speech Recognition (ASR), which falls under the field of Natural Language Processing.",
        "quote": "Modern multilingual automatic speech recognition (ASR) systems like Whisper have made it possible to transcribe audio in multiple languages with a single model."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The paper specifically addresses the challenge of continually learning new languages in multilingual ASR.",
          "quote": "We propose CL-MASR, a benchmark designed for studying multilingual ASR in a continual learning setting."
        },
        "aliases": [
          "CL"
        ]
      },
      {
        "name": {
          "value": "Automatic Speech Recognition",
          "justification": "The paper deals with multilingual ASR systems and how to incrementally add new languages.",
          "quote": "Modern multilingual automatic speech recognition (ASR) systems like Whisper have made it possible to transcribe audio in multiple languages with a single model."
        },
        "aliases": [
          "ASR"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Whisper",
          "justification": "Whisper is one of the primary ASR models evaluated in the paper.",
          "quote": "We experiment with methods from all these categories. In particular, we implement... As a supervised pretrained ASR model, we employ the large-v2 version of Whisper."
        },
        "aliases": [
          "Whisper large-v2"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Whisper is not a contribution of this paper but used for evaluation.",
          "quote": "As a supervised pretrained ASR model, we employ the large-v2 version of Whisper."
        },
        "is_executed": {
          "value": true,
          "justification": "Whisper was executed as part of the benchmark experiments.",
          "quote": "We experiment with methods from all these categories... As a supervised pretrained ASR model, we employ the large-v2 version of Whisper."
        },
        "is_compared": {
          "value": true,
          "justification": "Whisper was compared against other models in the experiments.",
          "quote": "We experiment with methods from all these categories... As a supervised pretrained ASR model, we employ the large-v2 version of Whisper."
        },
        "referenced_paper_title": {
          "value": "Robust speech recognition via large-scale weak supervision",
          "justification": "The paper references Whisper's original research work.",
          "quote": "Notably, the emergence of models like M-CTC-T [2], Whisper [3], USM [4], and MMS [5] has enabled the automatic transcription of audio of hundreds of languages using a single shared model."
        }
      },
      {
        "name": {
          "value": "WavLM",
          "justification": "WavLM is another primary ASR model evaluated in the paper.",
          "quote": "As a self-supervised pretrained ASR model, we employ the large version of WavLM."
        },
        "aliases": [
          "WavLM large"
        ],
        "is_contributed": {
          "value": false,
          "justification": "WavLM is not a contribution of this paper but used for evaluation.",
          "quote": "As a self-supervised pretrained ASR model, we employ the large version of WavLM."
        },
        "is_executed": {
          "value": true,
          "justification": "WavLM was executed as part of the benchmark experiments.",
          "quote": "We experiment with methods from all these categories... As a self-supervised pretrained ASR model, we employ the large version of WavLM."
        },
        "is_compared": {
          "value": true,
          "justification": "WavLM was compared against other models in the experiments.",
          "quote": "We experiment with methods from all these categories... As a self-supervised pretrained ASR model, we employ the large version of WavLM."
        },
        "referenced_paper_title": {
          "value": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
          "justification": "The paper references WavLM's original research work.",
          "quote": "Notably, the emergence of models like M-CTC-T [2], Whisper [3], USM [4], and MMS [5] has enabled the automatic transcription of audio of hundreds of languages using a single shared model."
        }
      },
      {
        "name": {
          "value": "M-CTC-T",
          "justification": "M-CTC-T is mentioned as an important model in the context of multilingual ASR.",
          "quote": "One such model is M-CTC-T [2], a transformer encoder architecture with a connectionist temporal classification (CTC) [27] head trained using a combination of supervised and semi-supervised techniques on Common Voice 6 [25] and VoxPopuli [28] datasets."
        },
        "aliases": [
          "M-CTC-T"
        ],
        "is_contributed": {
          "value": false,
          "justification": "M-CTC-T is not a contribution of this paper but referenced.",
          "quote": "One such model is M-CTC-T [2], a transformer encoder architecture with a connectionist temporal classification (CTC) [27] head trained using a combination of supervised and semi-supervised techniques on Common Voice 6 [25] and VoxPopuli [28] datasets."
        },
        "is_executed": {
          "value": false,
          "justification": "The model is mentioned for its relevance but not executed in this paper.",
          "quote": "One such model is M-CTC-T [2], a transformer encoder architecture with a connectionist temporal classification (CTC) [27] head trained using a combination of supervised and semi-supervised techniques on Common Voice 6 [25] and VoxPopuli [28] datasets."
        },
        "is_compared": {
          "value": false,
          "justification": "M-CTC-T is mentioned but not directly compared in the experiments.",
          "quote": "One such model is M-CTC-T [2], a transformer encoder architecture with a connectionist temporal classification (CTC) [27] head trained using a combination of supervised and semi-supervised techniques on Common Voice 6 [25] and VoxPopuli [28] datasets."
        },
        "referenced_paper_title": {
          "value": "Pseudo-labeling for massively multilingual speech recognition",
          "justification": "The paper references M-CTC-T's original research work.",
          "quote": "One such model is M-CTC-T [2], a transformer encoder architecture with a connectionist temporal classification (CTC) [27] head trained using a combination of supervised and semi-supervised techniques on Common Voice 6 [25] and VoxPopuli [28] datasets."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Common Voice",
          "justification": "Common Voice is one of the datasets used for experiments in the paper.",
          "quote": "Our benchmark builds upon Common Voice 13 [25]."
        },
        "aliases": [
          "Common Voice 13"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Common Voice: A massively-multilingual speech corpus",
          "justification": "The dataset is referenced in the paper for its relevance and use in the experiments.",
          "quote": "Our benchmark builds upon Common Voice 13 [25]. This public dataset, obtained through crowd-sourcing, consists of short audio recordings and their corresponding transcriptions for 108 languages, comprising a total of 17,690 validated hours divided into training, validation, and test splits."
        }
      },
      {
        "name": {
          "value": "VoxPopuli",
          "justification": "VoxPopuli is mentioned as a dataset used to train models referenced in the paper.",
          "quote": "One such model is M-CTC-T [2], a transformer encoder architecture with a connectionist temporal classification (CTC) [27] head trained using a combination of supervised and semi-supervised techniques on Common Voice 6 [25] and VoxPopuli [28] datasets."
        },
        "aliases": [
          "VoxPopuli"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation",
          "justification": "The dataset is referenced in the paper for its role in training the M-CTC-T model.",
          "quote": "One such model is M-CTC-T [2], a transformer encoder architecture with a connectionist temporal classification (CTC) [27] head trained using a combination of supervised and semi-supervised techniques on Common Voice 6 [25] and VoxPopuli [28] datasets."
        }
      },
      {
        "name": {
          "value": "FLEURS",
          "justification": "FLEURS is another dataset used for additional experiments in the paper.",
          "quote": "To showcase the flexibility of our benchmark platform and to further validate the results of our analysis, we extend our comparative study to the Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS)7[72] dataset."
        },
        "aliases": [
          "FLEURS"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech",
          "justification": "The dataset is referenced in the paper for its use in experiments.",
          "quote": "To showcase the flexibility of our benchmark platform and to further validate the results of our analysis, we extend our comparative study to the Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS)7[72] dataset."
        }
      },
      {
        "name": {
          "value": "FLoRes-101",
          "justification": "FLoRes-101 is mentioned as the source for the FLEURS dataset.",
          "quote": "Derived from the machine translation FLoRes-101 [73] benchmark, it consists of n-way parallel natural human speech and text in 102 languages."
        },
        "aliases": [
          "FLoRes-101"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The FLoRes-101 evaluation benchmark for low-resource and multilingual machine translation",
          "justification": "The dataset is referenced in the paper as the source for the FLEURS dataset.",
          "quote": "Derived from the machine translation FLoRes-101 [73] benchmark, it consists of n-way parallel natural human speech and text in 102 languages."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "The paper uses the SpeechBrain toolkit for implementing their benchmark.",
          "quote": "We introduce CL-MASR, the first benchmark for CL in multilingual ASR. It includes the following four components... A modular platform based on the popular SpeechBrain [26] toolkit that enables the easy extension to other CL strategies and/or pretrained models."
        },
        "aliases": [
          "SpeechBrain"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A general-purpose speech toolkit",
          "justification": "The paper references the SpeechBrain toolkit for its role in implementing the benchmark.",
          "quote": "A modular platform based on the popular SpeechBrain [26] toolkit that enables the easy extension to other CL strategies and/or pretrained models."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2799,
    "prompt_tokens": 25252,
    "total_tokens": 28051
  }
}