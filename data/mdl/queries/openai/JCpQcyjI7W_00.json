{
  "paper": "JCpQcyjI7W.txt",
  "words": 46588,
  "extractions": {
    "title": {
      "value": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance",
      "justification": "This is the title of the provided research paper.",
      "quote": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance"
    },
    "description": "The paper addresses the high-probability convergence of stochastic optimization methods under unbounded variance conditions. It provides new high-probability bounds for various optimization problems and algorithms, extending the theoretical understanding and practical applicability of these methods.",
    "type": {
      "value": "theoretical",
      "justification": "The paper focuses on deriving high-probability convergence results and theoretical bounds for stochastic optimization methods under specific assumptions.",
      "quote": "In this paper, we propose several algorithms with high-probability convergence results under less restrictive assumptions and derive new high-probability results."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization",
        "justification": "The primary focus of the paper is on stochastic optimization methods and their high-probability convergence properties, making 'Optimization' the main research field.",
        "quote": "In this work, we contributed to the stochastic optimization literature via deriving new high-probability results under Assumption 1.1."
      },
      "aliases": [
        "Stochastic Optimization"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Learning Theory",
          "justification": "The methods and results are closely related to the theoretical aspects of machine learning optimization, thus the sub-research field is 'Machine Learning Theory'.",
          "quote": "Our contribution advances the theoretical understanding of stochastic optimization by providing new high-probability bounds."
        },
        "aliases": [
          "ML Theory"
        ]
      },
      {
        "name": {
          "value": "Convex Optimization",
          "justification": "Several sections of the paper discuss convergence results specifically for smooth convex minimization problems, categorizing 'Convex Optimization' as a sub-field.",
          "quote": "We obtain new high-probability convergence results under Assumption 1.1 for smooth convex minimization problems."
        },
        "aliases": [
          "Convex Opt"
        ]
      },
      {
        "name": {
          "value": "Non-Convex Optimization",
          "justification": "The paper extends its findings to non-convex optimization scenarios, focusing on the robustness of stochastic methods under heavy-tailed noise, indicating 'Non-Convex Optimization' as a relevant sub-field.",
          "quote": "When α = 2, the derived complexity bounds match the best-known ones for clipped-SGD in the setups where it was analyzed."
        },
        "aliases": [
          "Non-Convex Opt"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "clipped-SGD",
          "justification": "This model is studied extensively in the paper for high-probability convergence analysis under various conditions including non-convex settings.",
          "quote": "In contrast, there are no high-probability convergence results for SGD having logarithmic dependence on 1/β ."
        },
        "aliases": [
          "Stochastic Gradient Descent with Clipping"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper provides detailed analysis and results for clipped-SGD, extending existing theories.",
          "quote": "We derive the first high-probability result with logarithmic dependence on the confidence level for finding first-order stationary points of smooth (possibly, non-convex) functions without bounded gradients assumption."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention executing the model; it focuses on theoretical results.",
          "quote": "We derive the first high-probability result with logarithmic dependence on the confidence level."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of clipped-SGD is compared with standard SGD and other methods under specific assumptions.",
          "quote": "The result is derived for simple clipped-SGD. Moreover, we extend the analysis to the functions satisfying Polyak-Łojasiewicz condition; see Table 1 for the summary."
        },
        "referenced_paper_title": {
          "value": "A high probability analysis of adaptive sgd with momentum",
          "justification": "The study builds upon previous high-probability results for SGD, particularly those addressing heavy-tailed noise.",
          "quote": "Recently, Cutkosky & Mehta (2021) derived the first high-probability results for non-convex optimization under Assumption 1.1 for a version of SGD with gradient clipping and normalization of the momentum."
        }
      },
      {
        "name": {
          "value": "clipped-SSTM",
          "justification": "This is another model analyzed in the paper, especially for its high-probability convergence properties in convex settings.",
          "quote": "Next, we focus on the accelerated version of clipped-SGD called Clipped Stochastic Similar Triangles Method clippedSSTM (Gorbunov et al., 2020)."
        },
        "aliases": [
          "Clipped Stochastic Similar Triangles Method"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper contributes new results for the convergence of clipped-SSTM under specific assumptions.",
          "quote": "Using this lemma we prove the main convergence result for clipped-SSTM."
        },
        "is_executed": {
          "value": 0,
          "justification": "The focus is on theoretical analysis, without details on execution.",
          "quote": "The method constructs three sequences of points {xk }k≥0 , {y k }k≥0 , {z k }k≥0 satisfying the following update rules."
        },
        "is_compared": {
          "value": 1,
          "justification": "Clipped-SSTM is compared with other methods in terms of high-probability bounds and complexity results.",
          "quote": "The obtained complexity bound (see the proof in Appendix F.1) is the first optimal (up to logarithms) high-probability complexity bound under Assumption 1.1 for the smooth strongly convex problems."
        },
        "referenced_paper_title": {
          "value": "Stochastic optimization with heavy-tailed noise via accelerated gradient clipping",
          "justification": "This previous work is expanded upon in the current analysis of clipped-SSTM.",
          "quote": "The authors acknowledge prior works like (Gorbunov et al., 2020) in developing the clipped-SSTM method."
        }
      },
      {
        "name": {
          "value": "clipped-SEG",
          "justification": "The paper analyzes the clipped Stochastic Extragradient method in the context of variational inequalities and heavy-tailed noise.",
          "quote": "For this method we derive the following convergence guarantees."
        },
        "aliases": [
          "Clipped Stochastic Extragradient Method"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "New theoretical convergence results are provided for clipped-SEG, especially under unbounded conditions.",
          "quote": "Clipped Stochastic Extragradient method (clipped-SEG) for Lipschitz monotone and quasi-strongly monotone problems."
        },
        "is_executed": {
          "value": 0,
          "justification": "Focus on theoretical contributions with no experimental execution mentioned.",
          "quote": "We derive the first high-probability results for clipped-SEG under heavy-tailed noise."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance bounds for clipped-SEG are compared with those of other methods in the paper.",
          "quote": "Next, we derive high-probability results for clipped-SEG for smooth star-convex and quasi-strongly convex objectives under Assumption 1.1."
        },
        "referenced_paper_title": {
          "value": "Clipped Stochastic Methods for Variational Inequalities with Heavy-Tailed Noise",
          "justification": "The analysis in the paper builds on clipped stochastic methods for variational inequalities from this reference.",
          "quote": "Clipped Stochastic Extragradient method (clipped-SEG) (Gorbunov et al., 2022a)"
        }
      },
      {
        "name": {
          "value": "clipped-SGDA",
          "justification": "The paper discusses convergence guarantees for the clipped Stochastic Gradient Descent-Ascent method.",
          "quote": "For this method we derive the following convergence guarantees."
        },
        "aliases": [
          "Clipped Stochastic Gradient Descent-Ascent Method"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "The paper extends the existing understanding of convergence properties of the clipped-SGDA method.",
          "quote": "We derive the first high-probability results for Clipped Stochastic Gradient Descent-Ascent method (clipped-SGDA) for star-cocoercive and monotone/quasi-strongly monotone problems."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper focuses on theoretical contributions and does not include execution details.",
          "quote": "Theoretical analysis presented without experimental execution."
        },
        "is_compared": {
          "value": 1,
          "justification": "Theoretical comparison with other methods in terms of convergence bounds is provided.",
          "quote": "Clipped-Stochastic Gradient Descent-Ascent (clipped-SGDA) for star-cocoercive and monotone/quasi-strongly monotone problems."
        },
        "referenced_paper_title": {
          "value": "Clipped Stochastic Methods for Variational Inequalities with Heavy-Tailed Noise",
          "justification": "The analysis leverages previously known techniques and theoretical limits from clipped stochastic methods in variational inequalities.",
          "quote": "Clipped Stochastic Gradient Descent-Ascent (clipped-SGDA) method (Gorbunov et al., 2021)"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3571,
    "prompt_tokens": 190955,
    "total_tokens": 194526
  }
}