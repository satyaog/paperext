{
  "paper": "d7f12e5e68cb2014b98a4e74637b6118.txt",
  "words": 17299,
  "extractions": {
    "title": {
      "value": "REPLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content",
      "justification": "The title is provided at the beginning of the abstract and throughout the paper, indicating the introduction of a new dataset named REPLiQA.",
      "quote": "To foster sound evaluation of language models, we introduce a new test dataset named R EP L I QA, suited for question-answering and topic retrieval tasks."
    },
    "description": "The paper introduces REPLiQA, a novel dataset designed to evaluate language models in handling unseen reference content, focusing on question-answering and topic retrieval tasks. The dataset consists of five splits of test sets, with samples including a crafted reference document, a related question, and ground-truth answers extracted from the document. It aims to evaluate the models' true reading and comprehension skills, avoiding memorization of training data.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts large-scale benchmarks with several state-of-the-art LLMs using the introduced dataset (REPLiQA) to assess differences in performance.",
      "quote": "We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models of various types and sizes..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing (NLP)",
        "justification": "The research focuses on evaluating large language models' capabilities in question-answering, a key NLP task.",
        "quote": "R EP L I QA is designed to assess open-domain question answering based on reference documents and document topic retrieval."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Question Answering",
          "justification": "The dataset and evaluations are explicitly aimed at testing question-answering capabilities of language models.",
          "quote": "R EP L I QA is designed to assess open-domain question answering based on reference documents and document topic retrieval."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Benchmarking",
          "justification": "The paper introduces a new benchmark dataset for evaluating language models.",
          "quote": "We introduce a new test dataset named R EP L I QA, suited for question-answering and topic retrieval tasks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Dataset Creation",
          "justification": "The paper involves creating a new dataset specifically tailored for testing LLMs.",
          "quote": "We built R EP L I QA, a new dataset for testing LLMs on data concerning facts unseen during the training of any existing LLM."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-3.5",
          "justification": "GPT-3.5 was one of the models evaluated using the REPLiQA dataset.",
          "quote": "We select eighteen widely-used LLMs: GPT-3.5 and GPT-4 O by O PEN AI [Achiam et al., 2023], ..."
        },
        "aliases": [
          "GPT-3.5 Turbo"
        ],
        "is_contributed": {
          "value": false,
          "justification": "GPT-3.5 is a known model by OpenAI and was used in the study but not introduced by it.",
          "quote": "We select eighteen widely-used LLMs: GPT-3.5 and GPT-4 O by O PEN AI..."
        },
        "is_executed": {
          "value": true,
          "justification": "The model's performance on REPLiQA was assessed, indicating execution.",
          "quote": "Inference is carried out using O PEN R OUTER , which serves as a unified framework enabling access to multiple LLM providers through a single API."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of GPT-3.5 was compared to other models on the benchmark dataset.",
          "quote": "We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models..."
        },
        "referenced_paper_title": {
          "value": "GPT-4 technical report",
          "justification": "The reference title suggests that GPT-3.5 is part of the GPT series by OpenAI which includes GPT-4 as described in the reference.",
          "quote": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, ... GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023."
        }
      },
      {
        "name": {
          "value": "Mistral",
          "justification": "Mistral models are mentioned among the evaluated models on the REPLiQA dataset.",
          "quote": "M ISTRAL and M IXTRAL variants by M ISTRAL AI [Jiang et al., 2023]..."
        },
        "aliases": [
          "Mistral 7B Inst",
          "Mistral Large",
          "Mistral Small"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Mistral is a pre-existing model used in the research.",
          "quote": "Mistral models are part of the evaluation set and not newly introduced by this paper."
        },
        "is_executed": {
          "value": true,
          "justification": "The Mistral models were executed on the REPLiQA dataset for performance evaluation.",
          "quote": "We select eighteen widely-used LLMs: GPT-3.5 and GPT-4 O by O PEN AI [Achiam et al., 2023], L LAMA 3 by M ETA..."
        },
        "is_compared": {
          "value": true,
          "justification": "Mistral models are compared as part of the benchmarking effort against other LLMs.",
          "quote": "We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models..."
        },
        "referenced_paper_title": {
          "value": "Mistral 7b",
          "justification": "The reference title corresponds to the Mistral 7B mentioned in the models evaluated.",
          "quote": "Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, ... Mistral 7b. arXiv preprint arXiv:2310.06825, 2023."
        }
      },
      {
        "name": {
          "value": "Llama 3",
          "justification": "Llama 3 by META is one of the models evaluated using the REPLiQA dataset.",
          "quote": "L LAMA 3 by M ETA [Touvron et al., 2023, AI@Meta, 2024]..."
        },
        "aliases": [
          "Llama 3 70b Inst",
          "Llama 3 8b Inst"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Llama 3 is a pre-existing model by META used in the study.",
          "quote": "L LAMA 3 by M ETA ..."
        },
        "is_executed": {
          "value": true,
          "justification": "The Llama 3 model was executed during the benchmarking process.",
          "quote": "Inference is carried out using O PEN R OUTER , which serves as a unified framework enabling access to multiple LLM providers through a single API."
        },
        "is_compared": {
          "value": true,
          "justification": "Llama 3 was compared to other models as part of the benchmarking analysis.",
          "quote": "We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models..."
        },
        "referenced_paper_title": {
          "value": "Llama 3 model card",
          "justification": "The reference title aligns with META's iteration of the Llama series, specifically Llama 3, which was evaluated.",
          "quote": "AI@Meta. Llama 3 model card. 2024."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "REPLiQA",
          "justification": "REPLiQA is the primary dataset introduced and utilized in the paper for benchmarking LLMs.",
          "quote": "In this paper, we introduce R EP L I QA: Repository of Likely Question-Answer data, a novel test benchmark for evaluating language models using samples previously inaccessible on the Web."
        },
        "aliases": [
          "R EP L I QA"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "REPLiQA is a novel contribution of the paper and not based on a previously referenced work.",
          "quote": "In this paper, we introduce R EP L I QA..."
        }
      },
      {
        "name": {
          "value": "TriviaQA",
          "justification": "TriviaQA is used as a comparative dataset to assess performance against REPLiQA.",
          "quote": "For a point of reference, we additionally report question-answering results on T RIVIA QA [Joshi et al., 2017] , a well-known dataset..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
          "justification": "The reference title is the original publication for the TriviaQA dataset used in the research.",
          "quote": "Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "OpenRouter",
          "justification": "OpenRouter is used as a tool for interfacing with multiple LLM providers.",
          "quote": "Inference is carried out using O PEN R OUTER , which serves as a unified framework enabling access to multiple LLM providers through a single API."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "OpenRouter is a tool used in the methodology and not a focal point of existing research references.",
          "quote": "Inference is carried out using O PEN R OUTER , which serves as a unified framework enabling access to multiple LLM providers through a single API."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1916,
    "prompt_tokens": 29242,
    "total_tokens": 31158,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}