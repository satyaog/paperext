{
  "paper": "bUKyC0UiZcr.txt",
  "words": 9737,
  "extractions": {
    "title": {
      "value": "TEMPORAL ABSTRACTIONS - AUGMENTED TEMPORALLY CONTRASTIVE LEARNING: AN ALTERNATIVE TO THE LAPLACIAN IN RL",
      "justification": "The title is stated at the beginning of the paper and encapsulates the primary focus of the research, which is the proposed method TATC in reinforcement learning.",
      "quote": "TEMPORAL ABSTRACTIONS - AUGMENTED TEMPORALLY CONTRASTIVE LEARNING : AN ALTERNATIVE TO THE LAPLACIAN IN RL Anonymous authors Paper under double-blind review"
    },
    "description": "This paper proposes a new method called Temporal Abstractions-augmented Temporally-Contrastive learning (TATC) for reinforcement learning (RL) that integrates temporal abstractions into temporally-contrastive representation learning. TATC aims to improve exploration and representation quality in RL tasks, particularly in non-uniform prior settings, by leveraging learned skills to provide better coverage of the state space. The approach demonstrates superior performance in challenging environments and continuous navigation tasks with sparse rewards compared to standard skill discovery methods.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experiments conducted in both discrete and continuous environments, including gridworld domains and MuJoCo simulations, to demonstrate the efficacy of the proposed method.",
      "quote": "We empirically show our agent’s ability to progressively explore the state space...in a non-uniform prior setting."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on improving representation learning and exploration in reinforcement learning tasks by introducing a new method, TATC.",
        "quote": "In reinforcement learning (RL), the graph Laplacian has proved to be a valuable tool..."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper emphasizes learning representations that integrate temporal abstractions for enhanced RL tasks.",
          "quote": "representation learning has also become one of the main topics of interest in reinforcement learning (RL)."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Skill Discovery",
          "justification": "The proposed method involves learning skills to enhance exploration and representation in RL.",
          "quote": "Our approach leverages the learned representation to build a skill-based covering policy..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Laplace Representation (LAP-REP)",
          "justification": "LAP-REP is used as a baseline to compare with the proposed method TATC in the experiments.",
          "quote": "We show that our representation leads to better value predictions than LAP-REP, and that it recovers the representation quality expected from a uniform prior."
        },
        "aliases": [
          "LAP-REP"
        ],
        "is_contributed": {
          "value": false,
          "justification": "LAP-REP is not the model introduced by this paper but is used for comparison.",
          "quote": "Wu et al. (2019) recently proposed an efficient approximation of the Laplacian representation (LAP-REP)..."
        },
        "is_executed": {
          "value": true,
          "justification": "LAP-REP is executed in the experiments for comparison against TATC.",
          "quote": "We evaluate our representation in shaping rewards for goal-achieving tasks, and we show it outperforms LAP-REP..."
        },
        "is_compared": {
          "value": true,
          "justification": "LAP-REP is compared numerically in experiments concerning value predictions and control tasks.",
          "quote": "We show that our representation leads to better value predictions than LAP-REP..."
        },
        "referenced_paper_title": {
          "value": "The Laplacian in RL: Learning representations with efficient approximations",
          "justification": "The paper references work by Wu et al. (2019) concerning the Laplacian representation in RL.",
          "quote": "Wu et al. (2019) recently proposed an efficient approximation of the Laplacian representation (LAP-REP)..."
        }
      },
      {
        "name": {
          "value": "DIAYN (Diversity is All You Need)",
          "justification": "DIAYN is used as a baseline model for comparison with the skills learned through TATC.",
          "quote": "We evaluate the exploratory potential of TATC’s skills. Here, we compare the learned skills against 2 task-agnostic skill discovery methods, DIAYN (Eysenbach et al., 2019)..."
        },
        "aliases": [
          "DIAYN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "DIAYN is not introduced in this paper but used for comparison.",
          "quote": "Eysenbach et al. (2019)"
        },
        "is_executed": {
          "value": true,
          "justification": "DIAYN is executed in the evaluation of TATC’s skills.",
          "quote": "We evaluate the exploratory potential of TATC’s skills. Here, we compare..."
        },
        "is_compared": {
          "value": true,
          "justification": "DIAYN is numerically compared with the skills learned by TATC.",
          "quote": "We evaluate the exploratory potential of TATC’s skills. Here, we compare the learned skills against..."
        },
        "referenced_paper_title": {
          "value": "Diversity is All You Need: Learning Skills without a Reward Function",
          "justification": "The paper references the DIAYN method as part of its comparison.",
          "quote": "DIAYN (Eysenbach et al., 2019)"
        }
      },
      {
        "name": {
          "value": "DCO (Deep Covering Options)",
          "justification": "DCO is used as a baseline model in the evaluation of TATC’s skills.",
          "quote": "We evaluate the exploratory potential of TATC’s skills. Here, we compare the learned skills against 2 task-agnostic skill discovery methods, DIAYN ... and DCO (Jinnai et al., 2020)."
        },
        "aliases": [
          "DCO"
        ],
        "is_contributed": {
          "value": false,
          "justification": "DCO is not introduced in this paper but used for comparison.",
          "quote": "DCO (Jinnai et al., 2020)"
        },
        "is_executed": {
          "value": true,
          "justification": "DCO is executed in the evaluation of TATC’s skills.",
          "quote": "We evaluate the exploratory potential of TATC’s skills. Here, we compare the learned skills against..."
        },
        "is_compared": {
          "value": true,
          "justification": "DCO is numerically compared with the skills learned by TATC.",
          "quote": "We evaluate the exploratory potential of TATC’s skills. Here, we compare the learned skills against..."
        },
        "referenced_paper_title": {
          "value": "Exploration in Reinforcement Learning with Deep Covering Options",
          "justification": "The paper references the DCO method as part of its comparison.",
          "quote": "DCO (Jinnai et al., 2020)"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1361,
    "prompt_tokens": 16352,
    "total_tokens": 17713,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}