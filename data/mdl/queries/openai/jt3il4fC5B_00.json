{
  "paper": "jt3il4fC5B.txt",
  "words": 9477,
  "extractions": {
    "title": {
      "value": "The Unsolved Challenges of LLMs as Generalist Web Agents: A Case Study",
      "justification": "This is the title of the paper as provided.",
      "quote": "The Unsolved Challenges of LLMs as Generalist Web Agents: A Case Study"
    },
    "description": "This paper investigates the challenges of developing goal-driven AI agents that use large language models (LLMs) to perform novel tasks in web environments using zero-shot learning. The study evaluates different design considerations through extensive experiments using the MiniWoB benchmark.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves extensive experiments and empirical analysis to evaluate different design considerations for LLM-based agents.",
      "quote": "Our main contribution encompasses a set of extensive experiments where we compare and contrast various agent design considerations"
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on using large language models (LLMs) for creating goal-driven web agents, which is a sub-field of NLP.",
        "quote": "Our primary focus is on harnessing the capabilities of large language models (LLMs) as generalist web agents interacting with HTML-based user interfaces (UIs)."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper references the field of reinforcement learning in the context of developing goal-driven agents that can perform novel tasks.",
          "quote": "The field of reinforcement learning aims to achieve this using meta-learning and goal-conditioned RL"
        },
        "aliases": [
          "RL"
        ]
      },
      {
        "name": {
          "value": "Human-Computer Interaction",
          "justification": "The paper involves interactions with HTML-based user interfaces, making it relevant to the field of Human-Computer Interaction.",
          "quote": "Our primary focus is on harnessing the capabilities of large language models (LLMs) as generalist web agents interacting with HTML-based user interfaces (UIs)."
        },
        "aliases": [
          "HCI"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DOMNet",
          "justification": "DOMNet is mentioned as one of the agents evaluated for the MiniWoB benchmark in the paper.",
          "quote": "Liu et al. [2018] propose DOMNet, an attention-based neural network trained via reinforcement learning (RL) on MiniWoB."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "DOMNet was referenced in the paper but not contributed by this study.",
          "quote": "Liu et al. [2018] propose DOMNet, an attention-based neural network trained via reinforcement learning (RL) on MiniWoB."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention running the DOMNet model; it is only referenced.",
          "quote": "Liu et al. [2018] propose DOMNet, an attention-based neural network trained via reinforcement learning (RL) on MiniWoB."
        },
        "is_compared": {
          "value": 1,
          "justification": "DOMNet is mentioned in the comparative analysis of various agents for the MiniWoB benchmark.",
          "quote": "Liu et al. [2018] propose DOMNet, an attention-based neural network trained via reinforcement learning (RL) on MiniWoB."
        },
        "referenced_paper_title": {
          "value": "Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration",
          "justification": "This is the title of the referenced paper for DOMNet as mentioned in the citation.",
          "quote": "Liu et al. [2018] propose DOMNet, an attention-based neural network trained via reinforcement learning (RL) on MiniWoB."
        }
      },
      {
        "name": {
          "value": "CC-Net",
          "justification": "CC-Net is mentioned as one of the agents evaluated for the MiniWoB benchmark in the paper.",
          "quote": "Humphreys et al. [2022] propose CC-Net, a multimodal transformer trained via behavioural cloning (BC) and RL on MiniWoB."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "CC-Net was referenced in the paper but not contributed by this study.",
          "quote": "Humphreys et al. [2022] propose CC-Net, a multimodal transformer trained via behavioural cloning (BC) and RL on MiniWoB."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention running the CC-Net model; it is only referenced.",
          "quote": "Humphreys et al. [2022] propose CC-Net, a multimodal transformer trained via behavioural cloning (BC) and RL on MiniWoB."
        },
        "is_compared": {
          "value": 1,
          "justification": "CC-Net is mentioned in the comparative analysis of various agents for the MiniWoB benchmark.",
          "quote": "Humphreys et al. [2022] propose CC-Net, a multimodal transformer trained via behavioural cloning (BC) and RL on MiniWoB."
        },
        "referenced_paper_title": {
          "value": "A Data-Driven Approach for Learning to Control Computers",
          "justification": "This is the title of the referenced paper for CC-Net as mentioned in the citation.",
          "quote": "Humphreys et al. [2022] propose CC-Net, a multimodal transformer trained via behavioural cloning (BC) and RL on MiniWoB."
        }
      },
      {
        "name": {
          "value": "WebN-T5",
          "justification": "WebN-T5 is mentioned as one of the agents evaluated for the MiniWoB benchmark in the paper.",
          "quote": "Gur et al. [2023b] propose WebN-T5, a pre-trained T5 model fine-tuned via BC on MiniWoB."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "WebN-T5 was referenced in the paper but not contributed by this study.",
          "quote": "Gur et al. [2023b] propose WebN-T5, a pre-trained T5 model fine-tuned via BC on MiniWoB."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention running the WebN-T5 model; it is only referenced.",
          "quote": "Gur et al. [2023b] propose WebN-T5, a pre-trained T5 model fine-tuned via BC on MiniWoB."
        },
        "is_compared": {
          "value": 1,
          "justification": "WebN-T5 is mentioned in the comparative analysis of various agents for the MiniWoB benchmark.",
          "quote": "Gur et al. [2023b] propose WebN-T5, a pre-trained T5 model fine-tuned via BC on MiniWoB."
        },
        "referenced_paper_title": {
          "value": "Understanding HTML with Large Language Models",
          "justification": "This is the title of the referenced paper for WebN-T5 as mentioned in the citation.",
          "quote": "Gur et al. [2023b] propose WebN-T5, a pre-trained T5 model fine-tuned via BC on MiniWoB."
        }
      },
      {
        "name": {
          "value": "RCI",
          "justification": "RCI is mentioned as one of the pre-trained models evaluated for the MiniWoB benchmark in the paper.",
          "quote": "Kim et al. [2023] propose to Recursively Criticize and Improve (RCI), a self-correcting prompting scheme for LLMs, and report the performance of a web agent built upon GPT-3.5 on MiniWoB."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "RCI was referenced in the paper but not contributed by this study.",
          "quote": "Kim et al. [2023] propose to Recursively Criticize and Improve (RCI), a self-correcting prompting scheme for LLMs, and report the performance of a web agent built upon GPT-3.5 on MiniWoB."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention running the RCI model; it is only referenced.",
          "quote": "Kim et al. [2023] propose to Recursively Criticize and Improve (RCI), a self-correcting prompting scheme for LLMs, and report the performance of a web agent built upon GPT-3.5 on MiniWoB."
        },
        "is_compared": {
          "value": 1,
          "justification": "RCI is mentioned in the comparative analysis of various agents for the MiniWoB benchmark.",
          "quote": "Kim et al. [2023] propose to Recursively Criticize and Improve (RCI), a self-correcting prompting scheme for LLMs, and report the performance of a web agent built upon GPT-3.5 on MiniWoB."
        },
        "referenced_paper_title": {
          "value": "Language Models Can Solve Computer Tasks",
          "justification": "This is the title of the referenced paper for RCI as mentioned in the citation.",
          "quote": "Kim et al. [2023] propose to Recursively Criticize and Improve (RCI), a self-correcting prompting scheme for LLMs, and report the performance of a web agent built upon GPT-3.5 on MiniWoB."
        }
      },
      {
        "name": {
          "value": "SYNAPSE",
          "justification": "SYNAPSE is mentioned as one of the pre-trained models evaluated for the MiniWoB benchmark in the paper.",
          "quote": "Zheng et al. [2023] propose SYNAPSE, a few-shot prompting strategy for GPT-3.5, which recovers relevant demonstrations dynamically for each new task by querying a database of examples built using task embeddings."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "SYNAPSE was referenced in the paper but not contributed by this study.",
          "quote": "Zheng et al. [2023] propose SYNAPSE, a few-shot prompting strategy for GPT-3.5, which recovers relevant demonstrations dynamically for each new task by querying a database of examples built using task embeddings."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention running the SYNAPSE model; it is only referenced.",
          "quote": "Zheng et al. [2023] propose SYNAPSE, a few-shot prompting strategy for GPT-3.5, which recovers relevant demonstrations dynamically for each new task by querying a database of examples built using task embeddings."
        },
        "is_compared": {
          "value": 1,
          "justification": "SYNAPSE is mentioned in the comparative analysis of various agents for the MiniWoB benchmark.",
          "quote": "Zheng et al. [2023] propose SYNAPSE, a few-shot prompting strategy for GPT-3.5, which recovers relevant demonstrations dynamically for each new task by querying a database of examples built using task embeddings."
        },
        "referenced_paper_title": {
          "value": "SYNAPSE: Leveraging Few-Shot Exemplars for Human-Level Computer Control",
          "justification": "This is the title of the referenced paper for SYNAPSE as mentioned in the citation.",
          "quote": "Zheng et al. [2023] propose SYNAPSE, a few-shot prompting strategy for GPT-3.5, which recovers relevant demonstrations dynamically for each new task by querying a database of examples built using task embeddings."
        }
      },
      {
        "name": {
          "value": "WebGUM",
          "justification": "WebGUM is mentioned as one of the agents evaluated for the MiniWoB benchmark in the paper.",
          "quote": "Furuta et al. [2023] propose WebGUM, a multimodal transformer that combines a pre-trained Flan-T5 with a ViT vision model and is fine-tuned via BC on MiniWoB."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "WebGUM was referenced in the paper but not contributed by this study.",
          "quote": "Furuta et al. [2023] propose WebGUM, a multimodal transformer that combines a pre-trained Flan-T5 with a ViT vision model and is fine-tuned via BC on MiniWoB."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention running the WebGUM model; it is only referenced.",
          "quote": "Furuta et al. [2023] propose WebGUM, a multimodal transformer that combines a pre-trained Flan-T5 with a ViT vision model and is fine-tuned via BC on MiniWoB."
        },
        "is_compared": {
          "value": 1,
          "justification": "WebGUM is mentioned in the comparative analysis of various agents for the MiniWoB benchmark.",
          "quote": "Furuta et al. [2023] propose WebGUM, a multimodal transformer that combines a pre-trained Flan-T5 with a ViT vision model and is fine-tuned via BC on MiniWoB."
        },
        "referenced_paper_title": {
          "value": "Multimodal Web Navigation with Instruction-Finetuned Foundation Models",
          "justification": "This is the title of the referenced paper for WebGUM as mentioned in the citation.",
          "quote": "Furuta et al. [2023] propose WebGUM, a multimodal transformer that combines a pre-trained Flan-T5 with a ViT vision model and is fine-tuned via BC on MiniWoB."
        }
      },
      {
        "name": {
          "value": "Pix2Act",
          "justification": "Pix2Act is mentioned as one of the agents evaluated for the MiniWoB benchmark in the paper.",
          "quote": "Shaw et al. [2023] propose Pix2Act, an image-to-text architecture that combines a pre-trained ViT vision model and a T5 transformer, fine-tuned via BC and RL (using Monte-Carlo tree search) on MiniWoB"
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "Pix2Act was referenced in the paper but not contributed by this study.",
          "quote": "Shaw et al. [2023] propose Pix2Act, an image-to-text architecture that combines a pre-trained ViT vision model and a T5 transformer, fine-tuned via BC and RL (using Monte-Carlo tree search) on MiniWoB"
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper does not mention running the Pix2Act model; it is only referenced.",
          "quote": "Shaw et al. [2023] propose Pix2Act, an image-to-text architecture that combines a pre-trained ViT vision model and a T5 transformer, fine-tuned via BC and RL (using Monte-Carlo tree search) on MiniWoB"
        },
        "is_compared": {
          "value": 1,
          "justification": "Pix2Act is mentioned in the comparative analysis of various agents for the MiniWoB benchmark.",
          "quote": "Shaw et al. [2023] propose Pix2Act, an image-to-text architecture that combines a pre-trained ViT vision model and a T5 transformer, fine-tuned via BC and RL (using Monte-Carlo tree search) on MiniWoB"
        },
        "referenced_paper_title": {
          "value": "From Pixels to UI Actions: Learning to Follow Instructions via Graphical User Interfaces",
          "justification": "This is the title of the referenced paper for Pix2Act as mentioned in the citation.",
          "quote": "Shaw et al. [2023] propose Pix2Act, an image-to-text architecture that combines a pre-trained ViT vision model and a T5 transformer, fine-tuned via BC and RL (using Monte-Carlo tree search) on MiniWoB"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MiniWoB",
          "justification": "The paper evaluates the MiniWoB benchmark for assessing the performance of web agents.",
          "quote": "We evaluate the MiniWoB benchmark and show that it is a suitable yet challenging platform for assessing an agent’s ability to comprehend and solve tasks without prior human demonstrations."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "World of Bits: An Open-Domain Platform for Web-Based Agents",
          "justification": "This is the title of the referenced paper for the MiniWoB benchmark as mentioned in the citation.",
          "quote": "While benchmarks like MiniWoB [Shi et al., 2017, Liu et al., 2018] offer a critical evaluation platform, current top-performing algorithms often rely on exhaustive human demonstrations or numerous offline task interactions, underscoring a crucial gap in achieving novel tasks accomplishment in web environments"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Selenium",
          "justification": "Selenium is mentioned as a Python API for automated browser interaction used in the study.",
          "quote": "Selenium is a Python API for automated browser interaction, see https://www.selenium.dev/."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "No specific referenced paper for Selenium is mentioned; it is a well-known library.",
          "quote": "Selenium is a Python API for automated browser interaction, see https://www.selenium.dev/."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3391,
    "prompt_tokens": 16767,
    "total_tokens": 20158
  }
}