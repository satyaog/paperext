{
  "paper": "9901aa2adb0ee8756efb3c1b291d7ee1.txt",
  "words": 9861,
  "extractions": {
    "title": {
      "value": "Interpreting Learned Feedback Patterns in Large Language Models",
      "justification": "The title of the paper is clearly stated at the beginning of the document, making it easy to identify.",
      "quote": "Interpreting Learned Feedback Patterns in Large Language Models"
    },
    "description": "This paper investigates the Learned Feedback Patterns (LFPs) in Large Language Models (LLMs) that are fine-tuned using reinforcement learning from human feedback (RLHF). It explores whether these LFPs accurately reflect the preferences inherent in human feedback and introduces probes to estimate feedback signals. The study uses synthetic datasets, sparse autoencoders, and GPT-4 to validate the correlations between activation features and feedback signals, aiming to enhance model safety and alignment with human objectives.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experiments, training models, and validating probes, indicating that it is empirical.",
      "quote": "To test this, we train probes to estimate the feedback signal implicit in the activations of a fine-tuned LLM."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The study focuses on understanding patterns learned by Large Language Models, which are a cornerstone of Natural Language Processing.",
        "quote": "Reinforcement learning from human feedback (RLHF) is widely used to train large language models (LLMs)."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Interpretability",
          "justification": "The paper discusses understanding features and patterns learned by LLMs during RLHF, falling under model interpretability.",
          "quote": "A promising approach to disentangling superposed features in neural networks is to train autoencoders on neuron activations from those networks."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper investigates reinforcement learning from human feedback (RLHF) in LLMs, thus involving reinforcement learning.",
          "quote": "Large language models (LLMs) are often fine-tuned using reinforcement learning from human feedback (RLHF)."
        },
        "aliases": [
          "RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-4",
          "justification": "The model is mentioned as being used for validating feedback patterns and feature descriptions related to LLMs.",
          "quote": "We validate our probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features GPT-4 describes."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GPT-4 is referenced for validation but was not developed in this paper.",
          "quote": "We validate our probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features GPT-4 describes."
        },
        "is_executed": {
          "value": false,
          "justification": "GPT-4 is not executed in experiments but referred to for validation purposes.",
          "quote": "We validate our probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features GPT-4 describes."
        },
        "is_compared": {
          "value": false,
          "justification": "The model is used for feature validation, not for numerical comparison with other models.",
          "quote": "We validate our probes by comparing the features they identify as active in activations with implicit positive feedback signals against the features GPT-4 describes."
        },
        "referenced_paper_title": {
          "value": "Language models can explain neurons in language models",
          "justification": "The reference to GPT-4 aligns with the paper 'Language models can explain neurons in language models' as noted in the citations.",
          "quote": "Bills et al. [7] provide GPT-4 with a set of activations discretized and normalized to a range of 0 and 10 for a set of tokens passed to the model as a prompt."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "IMDB",
          "justification": "The IMDB dataset is used for sentiment tasks within the study.",
          "quote": "prefixes from the IMDB dataset [22]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning word vectors for sentiment analysis",
          "justification": "The reference to the IMDB dataset corresponds to the reference provided for sentiment analysis, establishing the connection to its use.",
          "quote": "A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis."
        }
      },
      {
        "name": {
          "value": "Anthropic HH-RLHF",
          "justification": "The paper mentions the use of the Anthropic HH-RLHF dataset for evaluating better responses in LLM fine-tuning.",
          "quote": "The more helpful and harmless response is designated the preferred response, and the less helpful and harmless response dispreferred."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Anthropic hh-rlhf dataset",
          "justification": "The dataset reference is directly mentioned, aligning with its usage in evaluating LLM responses.",
          "quote": "Anthropic. Anthropic hh-rlhf dataset, 2023."
        }
      },
      {
        "name": {
          "value": "toxic-dpo",
          "justification": "The dataset is used for fine-tuning models towards generating more toxic or less toxic responses for specific tasks.",
          "quote": "The second task uses DPO to optimize M RLHF for toxicity using the toxic-dpo dataset [34]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Toxic-dpo dataset v0.2",
          "justification": "This title corresponds with the paper mentioned, providing consistency with the dataset used in the study.",
          "quote": "Unalignment. Toxic-dpo dataset v0.2, 2023."
        }
      },
      {
        "name": {
          "value": "VADER",
          "justification": "The VADER lexicon is used for assigning sentiment rewards during RLHF fine-tuning tasks.",
          "quote": "Our reward function for this task comprises of sentiment assignments from the VADER lexicon [19]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "VADER: A parsimonious rule-based model for sentiment analysis of social media text",
          "justification": "This title accurately describes the VADER lexicon mentioned for sentiment analysis tasks in the paper.",
          "quote": "C. Hutto and E. Gilbert. Vader: A parsimonious rule-based model for sentiment analysis of social media text."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Spacy",
          "justification": "Spacy is explicitly used for tokenizing text within the paper's RLHF methodology.",
          "quote": "Given a prefix and completion, we tokenize the concatenated text using the Spacy [17] tokenizer for their en_core_web_md model."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "spacy: Industrial-strength natural language processing in python",
          "justification": "The reference to Spacy aligns with the usage in text processing as mentioned in the paper.",
          "quote": "M. Honnibal, I. Montani, S. Van Landeghem, and A. Boyd. spacy: Industrial-strength natural language processing in python, 2020."
        }
      },
      {
        "name": {
          "value": "TRL (Transformer Reinforcement Learning)",
          "justification": "The library is used for implementation of the reinforcement learning approach mentioned in the paper's experiments.",
          "quote": "We use the Transformer Reinforcement Learning (TRL) framework [37]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TRL: Transformer Reinforcement Learning",
          "justification": "The referenced library corresponds directly with its intended purpose and use as described in the research paper.",
          "quote": "L. von Werra, Y. Belkada, L. Tunstall, E. Beeching, T. Thrush, and N. Lambert. TRL: Transformer Reinforcement Learning, 2023."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1565,
    "prompt_tokens": 17418,
    "total_tokens": 18983,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}