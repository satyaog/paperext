{
  "paper": "c1e697542af41805a6445a0e11208169.txt",
  "words": 7863,
  "extractions": {
    "title": {
      "value": "Iterative Graph Self-distillation",
      "justification": "The title of the paper is clearly stated on the first page, in the header, and consistently throughout the document, including the ACM Reference Format.",
      "quote": "Iterative Graph Self-distillation"
    },
    "description": "This paper introduces the Iterative Graph Self-Distillation (IGSD) method for graph representation learning. IGSD leverages self-supervised and semi-supervised learning through a teacher-student distillation framework, employing graph diffusion augmentations to improve graph-level representations. The method is evaluated on various graph datasets, demonstrating superior performance in both unsupervised and semi-supervised settings.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental results and comparisons with state-of-the-art methods on various datasets, indicating empirical validation of the proposed IGSD method.",
      "quote": "We experiment with real-world datasets in various scales and compare the performance of IGSD with state-of-the-art graph representation learning methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Graph Representation Learning",
        "justification": "The paper focuses on developing methods for learning vector representations of graphs, which is a central theme in the field of Graph Representation Learning.",
        "quote": "graph representation learning, self-supervised learning, contrastive learning"
      },
      "aliases": [
        "Graph Representation Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-supervised Learning",
          "justification": "The paper uses self-supervised contrastive learning methods for graph representation learning.",
          "quote": "we propose IGSD, a novel graph-level representation learning framework via self-distillation. Our framework iteratively performs teacher-student distillation by instance discrimination on augmented views of graph instances using graph diffusion."
        },
        "aliases": [
          "Self-supervised Learning"
        ]
      },
      {
        "name": {
          "value": "Semi-supervised Learning",
          "justification": "IGSD extends to semi-supervised scenarios by incorporating labeled data and self-training approaches.",
          "quote": "We further extend IGSD to the semi-supervised scenarios, where the labeled data are utilized effectively with the supervised contrastive loss and self-training."
        },
        "aliases": [
          "Semi-supervised Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "InfoGraph",
          "justification": "InfoGraph is listed among the representative baseline models for self-supervised graph classification tasks.",
          "quote": "In the self-suppervised graph classification, we compare with the following representative baselines: InfoGraph [45]"
        },
        "aliases": [
          "InfoGraph"
        ],
        "is_contributed": {
          "value": false,
          "justification": "InfoGraph is used as a baseline for comparison, not introduced as a new model in this paper.",
          "quote": "In the self-suppervised graph classification, we compare with the following representative baselines: InfoGraph [45]"
        },
        "is_executed": {
          "value": true,
          "justification": "The results from experiments using InfoGraph are compared with those of the proposed method.",
          "quote": "Results on self-suppervised graph classification. We first present the results of the self-suppervised setting in Table 1."
        },
        "is_compared": {
          "value": true,
          "justification": "Performance of IGSD is compared with InfoGraph among other models.",
          "quote": "Results on self-suppervised graph classification. We first present the results of the self-suppervised setting in Table 1. InfoGraph ... IGSD outperforms state-of-the-art baselines like InfoGraph"
        },
        "referenced_paper_title": {
          "value": "InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization",
          "justification": "The reference title is included in citation [45] associated with InfoGraph.",
          "quote": "InfoGraph [45]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MUTAG",
          "justification": "MUTAG is one of the datasets used for evaluation in this study.",
          "quote": "we employ several widely-used graph kernel datasets [25] for learning and evaluation: 3 bioinformatics datasets (MUTAG, PTC, NCI1)"
        },
        "aliases": [
          "MUTAG"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Benchmark Data Sets for Graph Kernels",
          "justification": "The reference for datasets such as MUTAG is indicated in citation [25].",
          "quote": "datasets [25]"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 845,
    "prompt_tokens": 15443,
    "total_tokens": 16288,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}