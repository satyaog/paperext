{
  "paper": "2402.09299.txt",
  "words": 16612,
  "extractions": {
    "title": {
      "value": "Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code",
      "justification": "The title explicitly states the focus of the research on detecting code inclusion in language models.",
      "quote": "Trained Without My Consent: Detecting Code Inclusion In Language Models Trained on Code."
    },
    "description": "This paper presents TraWiC, a model-agnostic and interpretable method based on membership inference for detecting code inclusion in the training datasets of Large Language Models. The approach aims to address copyright infringement issues by detecting whether specific code projects were part of a language model's training data.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experiments, dataset construction, and comparison of detection methods to validate TraWiC's performance.",
      "quote": "To evaluate our approach, we constructed a large dataset of projects which were used to train an LLM... Our results indicate that TraWiC achieves an accuracy of 83.87% for dataset inclusion detection."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The study focuses on Large Language Models, which are a part of Natural Language Processing research.",
        "quote": "One of the highly active fields of research in ML is Natural Language Processing (NLP)."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Software Engineering",
          "justification": "The paper addresses code licensing, dataset inclusion detection, and other software development aspects.",
          "quote": "Large Language Models (LLMs) as coding assistants in the software development process pose new challenges for code auditing."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Machine Learning Security",
          "justification": "The paper discusses intellectual property infringement and membership inference attacks, which fall under Machine Learning Security.",
          "quote": "Membership Inference Attacks (MIA) have shown to be effective at both extracting information from ML models and inferring the presence of specific instances in a model’s training dataset."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "TraWiC",
          "justification": "The paper introduces TraWiC as a model-agnostic approach for code inclusion detection.",
          "quote": "We propose a new approach, TraWiC; a model-agnostic and interpretable method based on membership inference for detecting code inclusion in an LLM’s training dataset."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "TraWiC is proposed and developed as part of this research.",
          "quote": "...we propose a new approach, TraWiC..."
        },
        "is_executed": {
          "value": true,
          "justification": "TraWiC was implemented and used in experiments to detect code inclusion.",
          "quote": "In our experiments, we observe that TraWiC is capable of detecting 83.87% of codes that were used to train an LLM."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of TraWiC is compared against the NiCad tool.",
          "quote": "Our approach outperforms the widely used code clone detection tool NiCad."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "There is no specific reference paper for TraWiC as it is introduced in this paper.",
          "quote": "We propose a new approach, TraWiC..."
        }
      },
      {
        "name": {
          "value": "NiCad",
          "justification": "NiCad is used in the paper as a baseline for comparison with TraWiC.",
          "quote": "We compare TraWiC against one of the widely used open-source code clone detectors, NiCad."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "NiCad is an existing tool used for comparison, not developed in this paper.",
          "quote": "...one of the widely used open-source code clone detectors, NiCad."
        },
        "is_executed": {
          "value": true,
          "justification": "NiCad was used in the experiments to benchmark against TraWiC.",
          "quote": "NiCad is only capable of achieving an accuracy of 47.64% for detecting dataset inclusion..."
        },
        "is_compared": {
          "value": true,
          "justification": "NiCad is directly compared with TraWiC to evaluate performance.",
          "quote": "We compare TraWiC against one of the widely used open-source code clone detectors, NiCad."
        },
        "referenced_paper_title": {
          "value": "NICAD: Accurate detection of near-miss intentional clones using flexible pretty-printing and code normalization",
          "justification": "NiCad's methodology is referenced for understanding its clone detection approach.",
          "quote": "we compare TraWiC against one of the widely used open-source code clone detectors, NiCad [52]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "TheStack",
          "justification": "TheStack is used as the dataset for validating TraWiC's performance.",
          "quote": "To do so, we select TheStack 1. TheStack is a large dataset (3 terabytes) of code collected from GitHub by HuggingFace."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Stack: 3 TB of permissively licensed source code",
          "justification": "TheStack's characteristics and usage are discussed in relation to TraWiC's experiments.",
          "quote": "TheStack 1... This dataset contains codes for 30 different programming languages and 137.36 million repositories [35]."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1063,
    "prompt_tokens": 28670,
    "total_tokens": 29733,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}