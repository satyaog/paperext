{
  "paper": "a41db80e904c8c3a7a1e6074f5b7c5b9.txt",
  "words": 8618,
  "extractions": {
    "title": {
      "value": "Towards Painless Policy Optimization for Constrained MDPs",
      "justification": "The exact title of the paper as provided at the beginning of the document.",
      "quote": "Towards Painless Policy Optimization for Constrained MDPs"
    },
    "description": "This paper proposes a novel primal-dual framework for policy optimization in constrained Markov decision processes (CMDPs). It introduces the Coin Betting Politex (CBP) algorithm, which aims to optimize policies while minimally violating constraints in an infinite horizon CMDP setup. The framework and algorithm are designed to be robust to hyperparameter variations, contrasting with traditional methods that are sensitive to hyperparameters and often lead to large errors. Through synthetic experiments and real-world scenarios like the Cartpole environment, the authors demonstrate the effectiveness of CBP in achieving higher reward and consistent performance while managing constraint violations.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental evaluations and demonstrations of the proposed algorithm CBP on synthetic and real-world environments, supporting it as empirical research.",
      "quote": "Via experiments on synthetic and Cartpole environments, we demonstrate the effectiveness and robustness of CBP."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on policy optimization in Markov Decision Processes, which is a central topic in reinforcement learning.",
        "quote": "Popular reinforcement learning (RL) algorithms focus on optimizing an unconstrained objective..."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Constrained Markov Decision Processes",
          "justification": "The paper specifically addresses policy optimization within the context of CMDPs, making it a key subfield of research discussed in the work.",
          "quote": "We study policy optimization in an infinite horizon, Î³-discounted constrained Markov decision process (CMDP)."
        },
        "aliases": [
          "CMDP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Coin Betting Politex",
          "justification": "The Coin Betting Politex (CBP) algorithm is explicitly introduced and discussed as the main contribution of this paper.",
          "quote": "...and propose the Coin Betting Politex (CBP) algorithm."
        },
        "aliases": [
          "CBP"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The CBP algorithm is introduced as a new methodology within the paper, aiming to optimize policies under constraints with minimal hyperparameter tuning.",
          "quote": "...we instantiate the framework using coin-betting algorithms and propose the Coin Betting Politex (CBP) algorithm."
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments were conducted using this model to demonstrate its effectiveness and robustness in policy optimization.",
          "quote": "Via experiments on synthetic and Cartpole environments, we demonstrate the effectiveness and robustness of CBP."
        },
        "is_compared": {
          "value": true,
          "justification": "The CBP algorithm is compared to other algorithms such as GDA and CRPO in terms of performance, as shown in figures and discussion in the paper.",
          "quote": "Figure 1: Hyperparameter sensitivity: Optimality gap and constraint violation ... for GDA, CRPO and the proposed algorithm CBP..."
        },
        "referenced_paper_title": {
          "value": "Politex: Regret bounds for policy iteration using expert prediction",
          "justification": "The paper uses previous findings from the Politex algorithm as part of its foundational discussion, especially in policy optimization frameworks.",
          "quote": "In the unconstrained setting, reducing the policy optimization problem to that of online linear optimization has been previously explored in the Politex algorithm (Abbasi-Yadkori et al., 2019), and we build upon this work."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Cartpole",
          "justification": "The Cartpole environment is used in the paper's experiments to evaluate the performance and robustness of the CBP algorithm.",
          "quote": "Our experiments on synthetic tabular environment and the Cartpole environment with linear function approximation demonstrate the consistent effectiveness and robustness of CBP."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "OpenAI gym",
          "justification": "The Cartpole environment is a well-known benchmark from the OpenAI Gym, which is referenced in the paper.",
          "quote": "We use the Cartpole environment from the OpenAI gym (Brockman et al., 2016), and modify it to include multiple constraints."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 861,
    "prompt_tokens": 16428,
    "total_tokens": 17289,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}