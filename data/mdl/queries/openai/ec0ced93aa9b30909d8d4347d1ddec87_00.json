{
  "paper": "ec0ced93aa9b30909d8d4347d1ddec87.txt",
  "words": 15911,
  "extractions": {
    "title": {
      "value": "Learning Action and Reasoning-Centric Image Editing from Videos and Simulations",
      "justification": "The title is extracted directly from the beginning of the paper, signifying the main focus of the research, which is on learning image editing with an emphasis on action and reasoning from videos and simulations.",
      "quote": "Learning Action and Reasoning-Centric Image Editing from Videos and Simulations"
    },
    "description": "This paper addresses the challenge of creating an image editing model capable of performing diverse and complex edits, particularly focusing on action and reasoning-centric edits. To tackle the scarcity of high-quality data for such tasks, the authors curate the AURORA Dataset, which includes training data derived from videos and simulation engines. The paper evaluates an AURORA-finetuned model using a new benchmark and proposes a novel metric for evaluation, significantly outperforming previous editing models on action and reasoning-centric editing tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical research by curating datasets, training models, and evaluating their performance on specific tasks related to image editing.",
      "quote": "To this end, we meticulously curate the AURORA Dataset...To demonstrate the value of our dataset, we evaluate an AURORA-finetuned model..."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The research is centered around image editing, a fundamental task within the field of computer vision, particularly focusing on action and reasoning-centric image edits.",
        "quote": "An image editing model should be able to perform diverse edits..."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Editing",
          "justification": "The paper directly addresses challenges and advancements in image editing models, particularly those requiring action and reasoning-centric capabilities.",
          "quote": "...action and reasoning-centric edits."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The development of datasets and models for action and reasoning-centric image editing involves core machine learning tasks such as training and evaluation of models.",
          "quote": "...evaluating an AURORA-finetuned model..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper involves instruction-guided image editing, which necessitates the understanding of prompts, a task associated with natural language processing.",
          "quote": "Current general instruction-guided editing models have significant shortcomings with action and reasoning-centric edits."
        },
        "aliases": [
          "NLP"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "InstructPix2Pix",
          "justification": "InstructPix2Pix is mentioned as a baseline model for comparison within the paper's experiments on image editing capabilities.",
          "quote": "InstructPix2Pix introduced the first large-scale instruction-guided image editing dataset"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model InstructPix2Pix is not a contribution of this paper but rather an existing model used as a baseline for comparison.",
          "quote": "InstructPix2Pix introduced the first large-scale instruction-guided image editing dataset"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses experiments involving the execution and comparison of the InstructPix2Pix model's performance on editing tasks.",
          "quote": "In Sec. 5.3 we finetune InstructPix2Pix on our new dataset and thoroughly evaluate its performance"
        },
        "is_compared": {
          "value": true,
          "justification": "InstructPix2Pix is used as a baseline for comparison against the newly proposed model in the experiments conducted in the paper.",
          "quote": "Our baselines are InstructPix2Pix, GenHowTo, MGIE and MagicBrush..."
        },
        "referenced_paper_title": {
          "value": "Instructpix2pix: Learning to follow image editing instructions",
          "justification": "This is the reference paper for InstructPix2Pix, providing the foundational work and details on how the model operates for guided image editing tasks.",
          "quote": "InstructPix2Pix... In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392–18402, 2023."
        }
      },
      {
        "name": {
          "value": "AURORA",
          "justification": "AURORA appears to be the model evaluated and benchmarked in the paper, specifically trained on the curated dataset.",
          "quote": "We train our own AURORA model with the InstructPix2Pix architecture on the AURORA dataset"
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "The AURORA model is a contribution of the paper, showcasing its novel capabilities in action and reasoning-centric image editing.",
          "quote": "We train our own AURORA model with the InstructPix2Pix architecture on the AURORA dataset"
        },
        "is_executed": {
          "value": true,
          "justification": "The AURORA model is executed and tested as part of the paper's experimental setup.",
          "quote": "...evaluate its performance across all types of edits."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of the AURORA model is compared against other baseline models such as InstructPix2Pix and MagicBrush in the paper's experiments.",
          "quote": "We present a state-of-the-art instruction-tuned image editing model, finetuned on AURORA..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The AURORA model does not have a separate referenced paper; it is introduced as part of the current research.",
          "quote": "We train our own AURORA model..."
        }
      },
      {
        "name": {
          "value": "MagicBrush",
          "justification": "MagicBrush is used as a baseline for comparison against the paper's proposed AURORA model.",
          "quote": "...we compare to strong baselines in a set of experiments in Sec. 5.3."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "MagicBrush is not a contribution of this paper but an existing model used for experimental comparison.",
          "quote": "Our baselines are InstructPix2Pix, GenHowTo, MGIE and MagicBrush..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves executing MagicBrush to compare its output with the AURORA model on various tasks.",
          "quote": "Against baselines like MagicBrush..."
        },
        "is_compared": {
          "value": true,
          "justification": "MagicBrush is directly compared with the AURORA model as part of the experimental evaluation.",
          "quote": "We compare to strong baselines in a set of experiments in Sec. 5.3."
        },
        "referenced_paper_title": {
          "value": "Magicbrush: A manually annotated dataset for instruction-guided image editing",
          "justification": "MagicBrush's reference paper provides context and groundwork for using it as a comparative baseline in image editing.",
          "quote": "MagicBrush [Zhang et al., 2024] addresses some of InstructPix2Pix’s shortcomings..."
        }
      },
      {
        "name": {
          "value": "MGIE (Multimodal Guided Image Editing)",
          "justification": "MGIE is mentioned as part of the baseline models used for comparison against the paper's contributions.",
          "quote": "Our baselines are InstructPix2Pix, GenHowTo, MGIE and MagicBrush..."
        },
        "aliases": [
          "MGIE"
        ],
        "is_contributed": {
          "value": false,
          "justification": "MGIE is not introduced as a novel contribution of this paper, but rather used for comparison purposes.",
          "quote": "...multimodal guided image editing via..."
        },
        "is_executed": {
          "value": true,
          "justification": "MGIE is executed as part of the experiments to establish its performance against the AURORA model.",
          "quote": "Our baselines are...MGIE..."
        },
        "is_compared": {
          "value": true,
          "justification": "MGIE's performance is compared with the AURORA model during the evaluation process.",
          "quote": "Our baselines are InstructPix2Pix, GenHowTo, MGIE and MagicBrush..."
        },
        "referenced_paper_title": {
          "value": "Guiding instruction-based image editing via multimodal large language models",
          "justification": "This is the reference paper for MGIE, which provides the context for its use as a baseline in editing tasks.",
          "quote": "Guiding instruction-based image editing via multimodal large language models."
        }
      },
      {
        "name": {
          "value": "GenHowTo",
          "justification": "GenHowTo is cited as one of the baseline models used to evaluate and compare against the new contributions made by the paper.",
          "quote": "Our baselines are InstructPix2Pix, GenHowTo, MGIE and MagicBrush..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GenHowTo is not a new contribution by this paper but is used as a baseline for comparison.",
          "quote": "...learn to generate actions and state transformations from instructional videos."
        },
        "is_executed": {
          "value": true,
          "justification": "GenHowTo is executed as part of the experiments to evaluate its editing capabilities against the paper's models.",
          "quote": "Our baselines are InstructPix2Pix, GenHowTo, MGIE and MagicBrush..."
        },
        "is_compared": {
          "value": true,
          "justification": "GenHowTo's editing performance is compared with the AURORA model to illustrate the advancements made.",
          "quote": "Our baselines are InstructPix2Pix, GenHowTo, MGIE and MagicBrush..."
        },
        "referenced_paper_title": {
          "value": "Genhowto: Learning to generate actions and state transformations from instructional videos",
          "justification": "The referenced paper for GenHowTo is acknowledged, providing basis for its inclusion as a baseline model.",
          "quote": "Genhowto: Learning to generate actions and state transformations from instructional videos."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AURORA Dataset",
          "justification": "The AURORA Dataset is a central contribution of the paper, curated to provide high-quality training data for action and reasoning-centric image editing tasks.",
          "quote": "To this end, we meticulously curate the AURORA Dataset (Action-Reasoning-Object-Attribute)"
        },
        "aliases": [
          "AURORA"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The AURORA Dataset is introduced in this paper as a novel contribution, thus, there is no external referenced paper.",
          "quote": "...our efforts: (1) curating a quality training dataset...of the state-of-the-art model..."
        }
      },
      {
        "name": {
          "value": "MagicBrush Dataset",
          "justification": "The MagicBrush Dataset is mentioned as a baseline dataset for comparison with the AURORA Dataset and is used for testing the editing capabilities stated in the paper.",
          "quote": "MagicBrush [Zhang et al., 2024] addresses some of InstructPix2Pix’s shortcomings."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A manually annotated dataset for instruction-guided image editing",
          "justification": "The referenced paper describes the creation and purpose of the MagicBrush Dataset, making it useful for comparison in this research.",
          "quote": "Magicbrush: A manually annotated dataset for instruction-guided image editing."
        }
      },
      {
        "name": {
          "value": "Something-Something Dataset",
          "justification": "The Something-Something Dataset is referenced as a part of the video sources used in curating the AURORA Dataset, providing data for realistic action changes.",
          "quote": "For Something-Something-Edit we started from the original Something Something dataset [Goyal et al., 2017] which consists of 221K short clips..."
        },
        "aliases": [
          "Something-Something"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The 'something something' video database for learning and evaluating visual common sense",
          "justification": "The referenced paper provides the context for the Something-Something Dataset, which is used to curate new data for action-centric edits.",
          "quote": "The 'something something' video database for learning and evaluating visual common sense."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Stable Diffusion",
          "justification": "Stable Diffusion is utilized as a component in synthesizing data for model training within the study.",
          "quote": "InstructPix2Pix [Brooks et al., 2023] introduced the first large-scale instruction-guided image editing dataset...Stable Diffusion [Rombach et al., 2022]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "High-resolution image synthesis with latent diffusion models",
          "justification": "The referenced paper outlines the foundation of Stable Diffusion, supporting its use in image synthesis for model training in the empirical study.",
          "quote": "Stable Diffusion [Rombach et al., 2022]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2525,
    "prompt_tokens": 26177,
    "total_tokens": 28702,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}