{
  "paper": "a9c139b4da042375e5d1e77767b08ba7.txt",
  "words": 13614,
  "extractions": {
    "title": {
      "value": "A Tensor Decomposition Perspective on Second-order RNNs",
      "justification": "The paper explicitly states the title at the beginning and in multiple sections, emphasizing its focus on tensor decomposition in second-order RNNs.",
      "quote": "A Tensor Decomposition Perspective on Second-order RNNs"
    },
    "description": "This paper analyzes second-order Recurrent Neural Networks (2RNNs) by exploring their parameterization using the CP decomposition, creating a new model termed CPRNN. The study focuses on how the rank and hidden size affect model expressivity and compares CPRNNs to other models like RNNs and MIRNNs through experiments on the Penn Treebank dataset.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments on real data, specifically the Penn Treebank dataset, to support its theoretical findings, indicating empirical research.",
      "quote": "We investigate to which extent theory holds in practice for models trained on real data by conduction a set of experiments on the Penn Tree Bank dataset."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research involves experiments on the Penn Treebank dataset, a well-known resource in the field of Natural Language Processing.",
        "quote": "We support these results empirically with experiments on the Penn Treebank dataset which demonstrate that CPRNNs outperforms..."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Recurrent Neural Networks",
          "justification": "The paper primarily discusses various types of RNNs, including 2RNNs, CPRNNs, MIRNNs, and their comparisons.",
          "quote": "Second-order Recurrent Neural Networks (2RNNs) extend RNNs by leveraging second-order interactions for sequence modelling."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "2RNN",
          "justification": "2RNNs are discussed as a more expressive extension of traditional RNNs by integrating second-order interactions.",
          "quote": "Second-order Recurrent Neural Networks (2RNNs) are a generalization of RNNs which integrates second-order interactions between hidden states and inputs."
        },
        "aliases": [
          "Second-order Recurrent Neural Network"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model 2RNN is not presented as a new contribution of this paper but as an existing model being analyzed.",
          "quote": "Second-order Recurrent Neural Networks (2RNNs) are a generalization of RNNs..."
        },
        "is_executed": {
          "value": true,
          "justification": "2RNNs are used in experiments and their performance is evaluated alongside other models.",
          "quote": "We support these results empirically with experiments on the Penn Treebank dataset..."
        },
        "is_compared": {
          "value": true,
          "justification": "2RNNs are numerically compared to other models such as RNNs, MIRNNs, and CPRNNs in terms of performance on tasks.",
          "quote": "We support these results empirically with experiments on the Penn Treebank dataset which demonstrate that, with a fixed parameter budget, CPRNNs outperforms RNNs, 2RNNs, and MIRNNs."
        },
        "referenced_paper_title": {
          "value": "Higher order recurrent networks and grammatical inference",
          "justification": "This reference supports the theoretical background on 2RNNs.",
          "quote": "(Giles et al., 1989; Goudreau et al., 1994)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Penn Treebank dataset",
          "justification": "The Penn Treebank dataset is explicitly mentioned as the dataset used for experiments in the paper.",
          "quote": "We support these results empirically with experiments on the Penn Treebank dataset which demonstrate..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Building a large annotated corpus of English: The Penn Treebank.",
          "justification": "The dataset reference is indicated in the context of its typical use in language modeling tasks.",
          "quote": "We perform experiments on the Penn Treebank dataset (Marcus et al., 1993)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 815,
    "prompt_tokens": 40841,
    "total_tokens": 41656,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}