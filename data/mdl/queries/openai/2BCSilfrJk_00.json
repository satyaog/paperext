{
  "paper": "2BCSilfrJk.txt",
  "words": 17222,
  "extractions": {
    "title": {
      "value": "Maximal Initial Learning Rates in Deep ReLU Networks",
      "justification": "The title is explicitly given at the beginning of the provided paper.",
      "quote": "Maximal Initial Learning Rates in Deep ReLU Networks"
    },
    "description": "This paper introduces the concept of the maximal initial learning rate (η*) for neural networks, specifically focusing on fully-connected ReLU networks. The authors propose a method to estimate η* and analyze its relationship with network architecture, specifically the product of depth and width. They also examine how η* correlates with sharpness (the largest eigenvalue of the training loss Hessian) and provide empirical and theoretical analyses to support their findings.",
    "type": {
      "value": "theoretical",
      "justification": "The paper involves substantial theoretical analysis, including mathematical derivations and theoretical formulations. It provides empirical results primarily to validate the proposed theoretical models.",
      "quote": "In this paper, we consider both empirically and theoretically how large the learning rate can be in early training. Our main contributions are as follows: • In §3.1, we introduce the maximal initial learning rate η∗ – the largest learning rate at which a randomly initialized neural network can successfully begin training – and show how it can be computed. • For fully-connected deep ReLU networks, we empirically identify a power law relating the maximal initial learning rate and the product of width and depth:"
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on neural network training dynamics, a core topic within the field of Deep Learning.",
        "quote": "Training a neural network requires choosing a suitable learning rate, which involves a trade-off between speed and effectiveness of convergence."
      },
      "aliases": [
        "Machine Learning",
        "Neural Network Training"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization Methods",
          "justification": "The primary focus of the paper is on exploring and optimizing the initial learning rate for training neural networks.",
          "quote": "The learning rate plays a crucial role in the training of deep neural networks. Unfortunately, tuning the learning rate is a tricky task – too large a learning rate can cause the training loss to diverge, while too small a learning rate can result in inefficient use of time and computational resources."
        },
        "aliases": [
          "Learning Rate Scheduling",
          "Neural Network Initialization"
        ]
      },
      {
        "name": {
          "value": "Theoretical Analysis",
          "justification": "The paper provides theoretical bounds and mathematical proofs related to maximal initial learning rates and sharpness.",
          "quote": "We further analyze the relationship between η∗ and the sharpness λ1 of the network at initialization, indicating they are closely though not inversely related. We formally prove bounds for λ1 in terms of (depth × width) that align with our empirical results."
        },
        "aliases": [
          "Mathematical Analysis",
          "Theoretical Deep Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Fully-Connected ReLU Network",
          "justification": "The paper repeatedly refers to experiments conducted on fully-connected ReLU networks and studies the relationships involving these networks.",
          "quote": "Using a simple approach to estimate η∗, we observe that in constant-width fully-connected ReLU networks, η∗ behaves differently from the maximum learning rate later in training."
        },
        "aliases": [
          "FC-ReLU"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The Fully-Connected ReLU Network is a well-known model and not a contribution of this paper.",
          "quote": "These results hold for Kaiming initialization. For LeCun initialization the same results hold, except K must be replaced by 2^-L/2 K."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is empirically tested in the paper.",
          "quote": "In Figure 3, we consider the value of 2/λ1 at initialization as a function of (depth × width)−1 , finding a power law relationship as with η∗."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper empirically compares results from the Fully-Connected ReLU Network using different initializations and learning rates.",
          "quote": "Note that the sharpness at initialization is the same regardless of whether the input layer is trained at the same or smaller learning rate."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper does not specifically reference another paper for Fully-Connected ReLU Network as this model is a common neural network architecture.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The paper mentions that CIFAR-10 is used for empirical validation.",
          "quote": "Maximal Initial Learning Rates in Deep ReLU Networks (a) CIFAR-10"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "CIFAR-10 is a well-known dataset attributed to this reference.",
          "quote": "(a) CIFAR-10"
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "The paper uses the MNIST dataset for experimental evaluations.",
          "quote": "Maximal Initial Learning Rates in Deep ReLU Networks (b) MNIST"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Y. LeCun et al., “Gradient-based learning applied to document recognition,” Proc. IEEE, 1998.",
          "justification": "MNIST dataset is commonly attributed to this reference.",
          "quote": "(b) MNIST"
        }
      },
      {
        "name": {
          "value": "Fashion-MNIST",
          "justification": "The paper includes experimental results using the Fashion-MNIST dataset.",
          "quote": "For experimental results on Fashion-MNIST, we point the reader"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
          "justification": "The Fashion-MNIST dataset is commonly attributed to this reference.",
          "quote": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1465,
    "prompt_tokens": 34905,
    "total_tokens": 36370
  }
}