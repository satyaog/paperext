{
  "paper": "9bd4fde790dffa96305cd6ac61e81b2a.txt",
  "words": 17438,
  "extractions": {
    "title": {
      "value": "SVRG meets AdaGrad: painless variance reduction",
      "justification": "The title clearly states the combination of SVRG and AdaGrad for variance reduction methods.",
      "quote": "Variance reduction (VR) methods for finite-sum minimization typically require the knowl- edge of problem-dependent constants that are often unknown and difficult to estimate. To address this, we use ideas from adaptive gradient methods to propose AdaSVRG, which is a more-robust variant of SVRG, a common VR method."
    },
    "description": "The paper introduces AdaSVRG, a variant of SVRG that incorporates AdaGrad in its inner loop, making it more robust and eliminating the need for problem-dependent constants. The study proves the convergence rates of AdaSVRG and compares its performance with standard variance reduction methods through experiments on synthetic and real datasets.",
    "type": {
      "value": "empirical",
      "justification": "The paper provides experimental evaluations on synthetic and real-world datasets to demonstrate the effectiveness of AdaSVRG, supporting the empirical nature of the study.",
      "quote": "Via experiments on syn- thetic and real-world datasets, we validate the robustness and effectiveness of AdaSVRG, demonstrating its superior performance over standard and other “tune-free” VR methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Stochastic Optimization",
        "justification": "The paper primarily focuses on variance reduction methods for stochastic optimization problems.",
        "quote": "Variance reduction (VR) methods (Schmidt et al., 2017; Konečnỳ & Richtárik, 2013; Mai- ral, 2013; Shalev-Shwartz & Zhang, 2013; Johnson & Zhang, 2013; Mahdavi & Jin, 2013; Konečnỳ & Richtárik, 2013; Defazio et al., 2014; Nguyen et al., 2017) have proven to be an important class of algorithms for stochastic optimization."
      },
      "aliases": [
        "VR methods"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Adaptive Gradient Methods",
          "justification": "The paper uses ideas from adaptive gradient methods, specifically AdaGrad, to enhance SVRG.",
          "quote": "To address this, we use ideas from adaptive gradient methods to propose AdaSVRG, which is a more-robust variant of SVRG, a common VR method."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Convex Optimization",
          "justification": "The study mainly targets convex optimization problems, using VR methods to improve convergence rates.",
          "quote": "Variance reduction methods were developed to overcome this slower convergence by exploiting the finite-sum structure of the objective."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "AdaSVRG",
          "justification": "AdaSVRG is introduced in this paper as a combination of SVRG and AdaGrad to improve variance reduction in optimization.",
          "quote": "To address this, we use ideas from adaptive gradient methods to propose AdaSVRG, which is a more-robust variant of SVRG, a common VR method."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "AdaSVRG is a contribution of this paper, combining SVRG with AdaGrad for improved performance.",
          "quote": "To address this, we use ideas from adaptive gradient methods to propose AdaSVRG, which is a more-robust variant of SVRG, a common VR method."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not explicitly mention executing AdaSVRG on any hardware.",
          "quote": "Via experiments on syn- thetic and real-world datasets, we validate the robustness and effectiveness of AdaSVRG, demonstrating its superior performance over standard and other “tune-free” VR methods."
        },
        "is_compared": {
          "value": true,
          "justification": "AdaSVRG is compared to other variance reduction methods experimentally.",
          "quote": "Via experiments on syn- thetic and real-world datasets, we validate the robustness and effectiveness of AdaSVRG, demonstrating its superior performance over standard and other “tune-free” VR methods."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "AdaSVRG is a novel contribution of this paper and hence does not have a previous reference.",
          "quote": "To address this, we use ideas from adaptive gradient methods to propose AdaSVRG, which is a more-robust variant of SVRG, a common VR method."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "LIBSVM datasets",
          "justification": "The paper mentions using standard real-world datasets for evaluation.",
          "quote": "We use standard real-world datasets to empirically verify the robustness and effectiveness of AdaSVRG."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "LIBSVM: A library for support vector machines",
          "justification": "The referenced paper provides the source of the datasets used in the experiments.",
          "quote": "Experiments were done on the publicly available LIBSVM data- sets (Chang & Lin, 2011)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "AdaGrad",
          "justification": "AdaGrad, an adaptive gradient method, is used in the inner loop of SVRG to create AdaSVRG.",
          "quote": "In Sect. 3 we use AdaGrad (Duchi et al., 2011; Levy et al., 2018), an adaptive gradient method, with stochastic variance reduction techniques."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Adaptive subgradient methods for online learning and stochastic optimization",
          "justification": "This is the foundational paper for the AdaGrad methodology used in the study.",
          "quote": "In Sect. 3 we use AdaGrad (Duchi et al., 2011; Levy et al., 2018), an adaptive gradient method, with stochastic variance reduction techniques."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1159,
    "prompt_tokens": 32943,
    "total_tokens": 34102,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}