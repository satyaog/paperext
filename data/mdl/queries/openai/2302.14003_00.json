{
  "paper": "2302.14003.txt",
  "words": 12616,
  "extractions": {
    "title": {
      "value": "SYSTEMATIC RECTIFICATION OF LANGUAGE MODELS VIA DEAD-END ANALYSIS",
      "justification": "The title 'SYSTEMATIC RECTIFICATION OF LANGUAGE MODELS VIA DEAD-END ANALYSIS' is explicitly stated at the beginning of the provided paper.",
      "quote": "SYSTEMATIC RECTIFICATION OF LANGUAGE MODELS VIA DEAD-END ANALYSIS"
    },
    "description": "This paper proposes a systematic approach to reduce toxicity in large language models (LLMs) using a method called rectification. By extending dead-end theory from reinforcement learning literature, the authors create a model that can detoxify generated discourses without altering the LLM's parameters. The rectification model is smaller and more computationally efficient and can be applied across various LLMs as long as they share the same vocabulary.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves empirical validation of the proposed rectification model through experimental results on various language models like GPT-2, GPT-2 XL, and GPT-3. Empirical analyses include both automatic and human evaluations to demonstrate the effectiveness of the approach.",
      "quote": "When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary focus of the paper is on improving the performance of large language models, a subfield of Natural Language Processing.",
        "quote": "With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper extends dead-end theory from the reinforcement learning (RL) literature to tackle the issue of toxic language generation in LLMs.",
          "quote": "To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes."
        },
        "aliases": [
          "RL"
        ]
      },
      {
        "name": {
          "value": "Language Model Detoxification",
          "justification": "The specific application of the proposed method is to detoxify language models to reduce harmful or toxic text generation.",
          "quote": "This work proposes a systematic approach, called rectification, to mitigate toxicity for LLMs."
        },
        "aliases": [
          "Detoxification"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-2",
          "justification": "GPT-2 is one of the language models used in the experiments to validate the effectiveness of the proposed rectification method.",
          "quote": "We conduct detoxification experiments with LMs of various sizes: GPT-2, GPT-2 XL and GPT-3."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The GPT-2 model is not a contribution of this paper but is used for the experiments.",
          "quote": "We conduct detoxification experiments with LMs of various sizes: GPT-2, GPT-2 XL and GPT-3."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper conducts numerous experiments executing the GPT-2 model to demonstrate the effectiveness of the proposed detoxification method.",
          "quote": "We conduct detoxification experiments with LMs of various sizes: GPT-2, GPT-2 XL and GPT-3."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of GPT-2 is compared with other language models and detoxification techniques, including the proposed rectification method.",
          "quote": "Compared with the regular GPT-2 XL, our method yields a relative reduction in toxicity probability by 78% (83.2% → 18.5%, as measured by PERSPECTIVE API), and it outperforms eight detoxification baselines."
        },
        "referenced_paper_title": {
          "value": "Language Models are Unsupervised Multitask Learners",
          "justification": "The referenced paper for GPT-2 discusses its architecture and capabilities, which is important for understanding its role and performance in the experiments.",
          "quote": "GPT-2 is a Transformer-based auto-regressive LM that contains 117M parameters."
        }
      },
      {
        "name": {
          "value": "GPT-2 XL",
          "justification": "GPT-2 XL is used in the experiments to validate the effectiveness of the rectification method.",
          "quote": "We conduct detoxification experiments with LMs of various sizes: GPT-2, GPT-2 XL and GPT-3."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The GPT-2 XL model is not a contribution of this paper but is used for the experiments.",
          "quote": "We conduct detoxification experiments with LMs of various sizes: GPT-2, GPT-2 XL and GPT-3."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper conducts numerous experiments executing the GPT-2 XL model to demonstrate the effectiveness of the proposed detoxification method.",
          "quote": "We conduct detoxification experiments with LMs of various sizes: GPT-2, GPT-2 XL and GPT-3."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of GPT-2 XL is compared with other language models and detoxification techniques, including the proposed rectification method.",
          "quote": "Compared with the regular GPT-2 XL, our method yields a relative reduction in toxicity probability by 78% (83.2% → 18.5%, as measured by PERSPECTIVE API), and it outperforms eight detoxification baselines."
        },
        "referenced_paper_title": {
          "value": "Language Models are Unsupervised Multitask Learners",
          "justification": "The referenced paper for GPT-2 XL discusses its architecture and capabilities, which is important for understanding its role and performance in the experiments.",
          "quote": "GPT-2 XL is a 1.5B parameter version of GPT-2 pretrained on the same corpus."
        }
      },
      {
        "name": {
          "value": "GPT-3",
          "justification": "GPT-3 is used in the experiments to validate the effectiveness of the rectification method.",
          "quote": "We conduct detoxification experiments with LMs of various sizes: GPT-2, GPT-2 XL and GPT-3."
        },
        "aliases": [
          ""
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The GPT-3 model is not a contribution of this paper but is used for the experiments.",
          "quote": "We conduct detoxification experiments with LMs of various sizes: GPT-2, GPT-2 XL and GPT-3."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper conducts numerous experiments executing the GPT-3 model to demonstrate the effectiveness of the proposed detoxification method.",
          "quote": "For GPT-3, we use the DA V INCI -002 model in the OpenAI API"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of GPT-3 is compared with other language models and detoxification techniques, including the proposed rectification method.",
          "quote": "Compared with GPT-2 XL, our method achieves better detoxification performance on GPT-3."
        },
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "The referenced paper for GPT-3 discusses its architecture and capabilities, which is crucial for understanding its role and performance in the experiments.",
          "quote": "GPT-3 (Brown et al., 2020a) is a 175B parameters pretrained on a mix of CommonCrawl 3 , an expanded version of the WebText dataset (Radford et al., 2019), books corpora, and English-language Wikipedia."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "R EALT OXICITY P ROMPTS",
          "justification": "The R EALT OXICITY P ROMPTS benchmark is explicitly mentioned as being used to evaluate the proposed method.",
          "quote": "We evaluate our method on the R EALT OXICITY P ROMPTS benchmark."
        },
        "aliases": [
          "RTP"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models",
          "justification": "The referenced paper for R EALT OXICITY P ROMPTS details the benchmark used for evaluating the effectiveness of the detoxification method proposed in this paper.",
          "quote": "We evaluate our method on the R EALT OXICITY P ROMPTS benchmark."
        }
      },
      {
        "name": {
          "value": "T OXIC C OMMENT C LASSIFICATION C HALLENGE",
          "justification": "The T OXIC C OMMENT C LASSIFICATION C HALLENGE dataset is used to train the BERT-based toxicity classifier, which serves as a reward model in the experiments.",
          "quote": "We follow Dathathri et al. (2020) and train a BERT-based toxicity classifier on the T OXIC C OMMENT C LASSIFICATION C HALLENGE dataset as our reward model."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TOXIC COMMENT CLASSIFICATION CHALLENGE",
          "justification": "The referenced paper provides details about the T OXIC C OMMENT C LASSIFICATION C HALLENGE dataset used in the reward model training.",
          "quote": "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Transformers",
          "justification": "The paper uses the Hugging Face Transformers library for conducting detoxification experiments with GPT-2 and GPT-2 XL.",
          "quote": "All GPT-2 and GPT-2 XL experiments are carried out with the Hugging Face Transformers library."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "NA",
          "justification": "The reference to the Transformers library is a well-known library used in the NLP community, and no specific paper is cited.",
          "quote": "All GPT-2 and GPT-2 XL experiments are carried out with the Hugging Face Transformers library."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2019,
    "prompt_tokens": 22954,
    "total_tokens": 24973
  }
}