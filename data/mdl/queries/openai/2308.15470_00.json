{
  "paper": "2308.15470.txt",
  "words": 7110,
  "extractions": {
    "title": {
      "value": "Policy composition in reinforcement learning via multi-objective policy optimization",
      "justification": "This is the title of the paper as provided by the user.",
      "quote": "Policy composition in reinforcement learning\nvia multi-objective policy optimization"
    },
    "description": "This paper discusses how reinforcement learning agents can utilize pre-existing teacher policies to improve learning efficiency and task performance. Teacher policies are introduced as additional objectives in a multi-objective policy optimization setting. The study uses the Multi-Objective Maximum a Posteriori Policy Optimization (MO-MPO) algorithm to empirically show that teacher policies can speed up learning, especially in the absence of shaping rewards.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments to show the effectiveness of the proposed method in utilizing teacher policies to speed up reinforcement learning.",
      "quote": "In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel, and are also able to further extend the policies of the teachers in order to solve the task."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper's main focus is on improving reinforcement learning through the use of pre-existing teacher policies.",
        "quote": "We enable reinforcement learning agents to learn successful behavior policies by utilizing relevant pre-existing teacher policies."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Policy Optimization",
          "justification": "The study uses the Multi-Objective Maximum a Posteriori Policy Optimization (MO-MPO) algorithm for compositing teacher policies.",
          "quote": "Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm (Abdolmaleki et al. 2020), we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Multi-Objective Learning",
          "justification": "The paper formulates the problem as a multi-objective optimization where multiple objectives are introduced corresponding to adherence to pre-existing policies and the primary task objective.",
          "quote": "We formulate the problem of leveraging pre-existing policies using multi-objective policy optimization."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Multi-Objective Maximum a Posteriori Policy Optimization (MO-MPO)",
          "justification": "The MO-MPO algorithm is used in the paper to incorporate multiple objectives, including adherence to teacher policies and the task objective.",
          "quote": "Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm (Abdolmaleki et al. 2020), we show that teacher policies can help speed up learning, particularly in the absence of shaping rewards."
        },
        "aliases": [
          "MO-MPO"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The MO-MPO algorithm was proposed in previous work and is used in this paper to demonstrate the effectiveness of policy composition.",
          "quote": "We formulate policy composition using pre-existing policies as multi-objective optimization problem using the MO-MPO algorithm of Abdolmaleki et al. (2020)."
        },
        "is_executed": {
          "value": true,
          "justification": "The MO-MPO algorithm is implemented and executed to demonstrate the effectiveness of the proposed method.",
          "quote": "Using the Multi-Objective Maximum a Posteriori Policy Optimization algorithm (Abdolmaleki et al. 2020), we show that teacher policies can help speed up learning."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not compare the MO-MPO algorithm to other models; it focuses on demonstrating the usefulness of multi-objective optimization for policy composition.",
          "quote": "We use the Multi-Objective Maximum a Posteriori Policy Optimization (MO-MPO) algorithm (Abdolmaleki et al. 2020), a multi-objective actor-critic algorithm, to compose skills."
        },
        "referenced_paper_title": {
          "value": "A Distributional View on Multi-Objective Policy Optimization",
          "justification": "This is the referenced paper for the MO-MPO algorithm as mentioned in the research paper.",
          "quote": "We use the Multi-Objective Maximum a Posteriori Policy Optimization (MO-MPO) algorithm (Abdolmaleki et al. 2020)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "DeepMind Control Suite",
          "justification": "The DeepMind Control Suite is used for the experimental validation of the proposed method.",
          "quote": "In the humanoid domain (Tassa et al. 2018), we also equip agents with the ability to control the selection of teachers."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "DeepMind Control Suite",
          "justification": "This is the referenced paper corresponding to the dataset used in the experiments.",
          "quote": "We empirically demonstrate the usefulness of this formulation on point mass and humanoid domain in the DeepMind control suite (Tassa et al. 2018)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Acme",
          "justification": "The Acme library is used for the implementation and experimentation of the proposed method.",
          "quote": "In this section, we describe experiments conducted using the method specified in Section 2 and implemented using the Acme framework (Hoffman et al. 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Acme: A research framework for distributed reinforcement learning",
          "justification": "This is the referenced paper for the Acme library as mentioned in the research paper.",
          "quote": "implemented using the Acme framework (Hoffman et al. 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1333,
    "prompt_tokens": 14338,
    "total_tokens": 15671
  }
}