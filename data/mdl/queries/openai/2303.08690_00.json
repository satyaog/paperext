{
  "paper": "2303.08690.txt",
  "words": 12854,
  "extractions": {
    "title": {
      "value": "Replay Buffer with Local Forgetting for Adapting to Local Environment Changes in Deep Model-Based Reinforcement Learning",
      "justification": "This is the exact title of the paper provided by the user",
      "quote": "Replay Buffer with Local Forgetting for Adapting to Local Environment Changes in Deep Model-Based Reinforcement Learning"
    },
    "description": "The paper proposes a variation of the traditional Replay Buffer to improve the adaptivity of Deep Model-Based Reinforcement Learning (MBRL) to local environment changes. The novel variation is called the Local Forgetting (LoFo) replay buffer, which removes the oldest samples in the local neighborhood of the new samples. This method is shown to mitigate interference from stale data and address catastrophic forgetting. The effectiveness of the approach is demonstrated on various domains, including modified versions of Dyna-Q, PlaNet, and DreamerV2 methods.",
    "type": {
      "value": "empirical",
      "justification": "The paper provides experimental results and evaluation of the proposed Local Forgetting (LoFo) replay buffer on several domains. This involves empirical analysis and testing of the new method.",
      "quote": "We demonstrate the effectiveness of the LoFo replay buffer by combining it with a deep version of the classical Dyna method and measuring its adaptivity on a variation of the MountainCar task as well as a mini-grid domain."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on improving model-based reinforcement learning methods.",
        "quote": "In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement learning (MBRL) methods adapt poorly to local environment changes."
      },
      "aliases": [
        "RL",
        "MBRL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Catastrophic Forgetting",
          "justification": "One of the main problems addressed by the paper is catastrophic forgetting in deep world models.",
          "quote": "This is challenging for deep-learning-based world models due to catastrophic forgetting."
        },
        "aliases": [
          "Interference"
        ]
      },
      {
        "name": {
          "value": "Deep Model-Based Reinforcement Learning",
          "justification": "The study expands upon model-based reinforcement learning by integrating deep learning techniques.",
          "quote": "In reinforcement learning, however, recent work has shown that modern deep model-based reinforcement learning (MBRL) methods adapt poorly to local environment changes."
        },
        "aliases": [
          "MBRL",
          "Deep MBRL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "PlaNet",
          "justification": "The paper evaluates the LoFo replay buffer on both the PlaNet and DreamerV2 methods.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet and DreamerV2 (Hafner et al., 2020)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "PlaNet is not introduced in this paper but is rather used as a baseline for evaluation.",
          "quote": "Follow-up work by Wan et al. (2022) showed that other popular methods such as PlaNet (Hafner et al., 2019b) and Dreamer (Hafner et al., 2019a; 2020) lack this core capability as well."
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments with these modified methods on variations of the MuJoCo Reacher domain demonstrate that a LoFo replay buffer can substantially improve adaptivity.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet and DreamerV2 (Hafner et al., 2020)"
        },
        "is_compared": {
          "value": true,
          "justification": "The effectiveness of the LoFo buffer is compared to standard methods including PlaNet.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet and DreamerV2 (Hafner et al., 2020)"
        },
        "referenced_paper_title": {
          "value": "Learning Latent Dynamics for Planning from Pixels",
          "justification": "This is the reference paper for PlaNet as mentioned in the text.",
          "quote": "Follow-up work by Wan et al. (2022) showed that other popular methods such as PlaNet (Hafner et al., 2019b) and Dreamer (Hafner et al., 2019a; 2020) lack this core capability as well."
        }
      },
      {
        "name": {
          "value": "DreamerV2",
          "justification": "The paper evaluates the LoFo replay buffer on both PlaNet and DreamerV2 methods.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet and DreamerV2 (Hafner et al., 2020)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "DreamerV2 is not introduced in this paper but is used as a baseline for evaluation.",
          "quote": "Follow-up work by Wan et al. (2022) showed that other popular methods such as PlaNet (Hafner et al., 2019b) and Dreamer (Hafner et al., 2019a; 2020) lack this core capability as well."
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments with these modified methods on variations of the MuJoCo Reacher domain demonstrate that a LoFo replay buffer can substantially improve adaptivity.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet and DreamerV2 (Hafner et al., 2020)"
        },
        "is_compared": {
          "value": true,
          "justification": "The effectiveness of the LoFo buffer is compared to standard methods including DreamerV2.",
          "quote": "We then test the limits of our approach by applying the same idea to both PlaNet and DreamerV2 (Hafner et al., 2020)"
        },
        "referenced_paper_title": {
          "value": "Mastering Atari with Discrete World Models",
          "justification": "This is the reference paper for DreamerV2 as mentioned in the text.",
          "quote": "Follow-up work by Wan et al. (2022) showed that other popular methods such as PlaNet (Hafner et al., 2019b) and Dreamer (Hafner et al., 2019a; 2020) lack this core capability as well."
        }
      },
      {
        "name": {
          "value": "MuZero",
          "justification": "MuZero is mentioned as an existing method evaluated in the context of local change adaptation.",
          "quote": "Interestingly, Van Seijen et al. (2020) showed that the deep MBRL method MuZero (Schrittwieser et al., 2019), which has been shown to get strong single-task sample-efficiency on Atari, actually lacks this core capability."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "MuZero is not introduced in this paper but is referenced for its limitations.",
          "quote": "Interestingly, Van Seijen et al. (2020) showed that the deep MBRL method MuZero (Schrittwieser et al., 2019), which has been shown to get strong single-task sample-efficiency on Atari, actually lacks this core capability."
        },
        "is_executed": {
          "value": false,
          "justification": "MuZero is not executed in this paper; it is discussed in terms of its existing limitations.",
          "quote": "Interestingly, Van Seijen et al. (2020) showed that the deep MBRL method MuZero (Schrittwieser et al., 2019), which has been shown to get strong single-task sample-efficiency on Atari, actually lacks this core capability."
        },
        "is_compared": {
          "value": false,
          "justification": "MuZero is not numerically compared to other models in this paper.",
          "quote": "Interestingly, Van Seijen et al. (2020) showed that the deep MBRL method MuZero (Schrittwieser et al., 2019), which has been shown to get strong single-task sample-efficiency on Atari, actually lacks this core capability."
        },
        "referenced_paper_title": {
          "value": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
          "justification": "This is the reference paper for MuZero as mentioned in the text.",
          "quote": "Interestingly, Van Seijen et al. (2020) showed that the deep MBRL method MuZero (Schrittwieser et al., 2019), which has been shown to get strong single-task sample-efficiency on Atari, actually lacks this core capability."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2050,
    "prompt_tokens": 20896,
    "total_tokens": 22946
  }
}