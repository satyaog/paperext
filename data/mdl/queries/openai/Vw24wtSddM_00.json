{
  "paper": "Vw24wtSddM.txt",
  "words": 9707,
  "extractions": {
    "title": {
      "value": "Tree Cross Attention",
      "justification": "The title explicitly mentioned in the paper header is 'Tree Cross Attention'.",
      "quote": "T REE C ROSS ATTENTION"
    },
    "description": "This paper introduces Tree Cross Attention (TCA), a module that enhances Cross Attention by organizing data in a tree structure and applying a tree search, allowing retrieval of information from a logarithmic number of tokens instead of linear scaling with the number of tokens. The authors also propose ReTreever, leveraging TCA to create a flexible architecture for token-efficient inference.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results comparing the proposed Tree Cross Attention with traditional Cross Attention and other models across various tasks, emphasizing empirical performance and efficiency.",
      "quote": "We show empirically that Tree Cross Attention (TCA) performs comparable to Cross Attention across various classification and uncertainty regression tasks while being significantly more token-efficient."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The core focus of the paper is the development and evaluation of a new machine learning method for attention mechanisms, which is a fundamental aspect of machine learning.",
        "quote": "With the rapid growth in applications of machine learning, an important objective is to make inference efficient both in terms of compute and memory."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Attention Mechanisms",
          "justification": "The paper specifically deals with enhancements to Cross Attention mechanisms, making it a primary focus on attention within machine learning.",
          "quote": "Cross Attention (CA) is a popular method at inference time for retrieving relevant information from a set of context tokens."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The paper utilizes reinforcement learning to learn good representations for the internal nodes of the tree structure within Tree Cross Attention.",
          "quote": "TCA leverages Reinforcement Learning (RL) to learn good representations for the internal nodes of the tree."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Tree Cross Attention (TCA)",
          "justification": "The entire paper is centered around the proposal and evaluation of the Tree Cross Attention model.",
          "quote": "We propose Tree Cross Attention (TCA), a replacement for Cross Attention that performs retrieval, scaling logarithmically O(log(N )) with the number of tokens."
        },
        "aliases": [
          "TCA"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is introduced and detailed as a novel contribution in the paper.",
          "quote": "We propose Tree Cross Attention (TCA)..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes empirical experiments where the model is executed to demonstrate its capabilities.",
          "quote": "We show empirically that Tree Cross Attention (TCA) performs comparable to Cross Attention..."
        },
        "is_compared": {
          "value": true,
          "justification": "Tree Cross Attention is numerically compared against other models like Cross Attention and Perceiver IO in various experiments.",
          "quote": "Compare Tree Cross Attention (TCA) with Cross Attention in terms of the number of tokens used and the performance."
        },
        "referenced_paper_title": {
          "value": "Published as a conference paper at ICLR 2024",
          "justification": "The paper cites itself as the primary reference for the Tree Cross Attention model.",
          "quote": "Published as a conference paper at ICLR 2024"
        }
      },
      {
        "name": {
          "value": "ReTreever",
          "justification": "ReTreever is introduced as an architecture built upon Tree Cross Attention for efficient inference.",
          "quote": "Building on TCA, we also propose (2) ReTreever, a flexible architecture that achieves token-efficient inference."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "ReTreever is introduced in the paper as an extension of TCA, contributing to the research field.",
          "quote": "Building on TCA, we also propose (2) ReTreever..."
        },
        "is_executed": {
          "value": true,
          "justification": "ReTreever is actively evaluated in experiments demonstrating its efficiency.",
          "quote": "ReTreever outperforms Perceiver IO on various classification and uncertainty estimation tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper includes experimental comparisons between ReTreever and models like Perceiver IO and Cross Attention.",
          "quote": "Compare the performances of ReTreever and Perceiver while using the same number of tokens."
        },
        "referenced_paper_title": {
          "value": "Published as a conference paper at ICLR 2024",
          "justification": "The paper itself is the reference for its introduction of ReTreever.",
          "quote": "Published as a conference paper at ICLR 2024"
        }
      },
      {
        "name": {
          "value": "Cross Attention",
          "justification": "Cross Attention is discussed as the baseline method against which TCA is compared.",
          "quote": "Cross Attention (CA) is a popular method at inference time for retrieving relevant information from a set of context tokens."
        },
        "aliases": [
          "CA"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Cross Attention is a well-known existing method, not contributed by this paper.",
          "quote": "Cross Attention (CA) is a popular method at inference time..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper carries out experiments using Cross Attention to compare with the newly proposed TCA.",
          "quote": "Compare Tree Cross Attention (TCA) with Cross Attention in terms of the number of tokens used and the performance."
        },
        "is_compared": {
          "value": true,
          "justification": "Cross Attention is used as a benchmark for comparison throughout the experiments.",
          "quote": "Compare Tree Cross Attention (TCA) with Cross Attention in terms of the number of tokens used and the performance."
        },
        "referenced_paper_title": {
          "value": "Published as a conference paper at ICLR 2024",
          "justification": "The document itself serves as the reference context where Cross Attention is used as a comparative baseline.",
          "quote": "Published as a conference paper at ICLR 2024"
        }
      },
      {
        "name": {
          "value": "Perceiver IO",
          "justification": "Perceiver IO is mentioned as a comparative model for token-efficient inference against ReTreever.",
          "quote": "General-purpose architectures such as Perceiver IO (Jaegle et al., 2021) perform inference cheaply by first distilling..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Perceiver IO is an existing model from prior research explicitly referenced in the paper.",
          "quote": "Perceiver IO (Jaegle et al., 2021) is a general attention-based neural network architecture applicable to various tasks."
        },
        "is_executed": {
          "value": true,
          "justification": "Perceiver IO is executed in experimental comparisons against TCA and ReTreever.",
          "quote": "ReTreever outperforms Perceiver IO on various classification and uncertainty estimation tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "Perceiver IO is numerically compared with ReTreever on several tasks.",
          "quote": "Compare the performances of ReTreever and Perceiver while using the same number of tokens."
        },
        "referenced_paper_title": {
          "value": "Perceiver IO: A general architecture for structured inputs & outputs",
          "justification": "The referenced paper for Perceiver IO model cited in-text is by Jaegle et al. 2021.",
          "quote": "Perceiver IO (Jaegle et al., 2021) is a general attention-based neural network architecture... the encoder√≠s stacked iterative attention blocks..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CelebA",
          "justification": "CelebA is used as a dataset for image completion experiments in the paper.",
          "quote": "The CelebA dataset comprises coloured images of celebrity faces."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Perceiver IO: A general architecture for structured inputs & outputs",
          "justification": "The paper references datasets used which include previously acknowledged datasets like CelebA, fitting the context of prior base references alongside Perceiver IO concept.",
          "quote": "Perceiver IO (Jaegle et al., 2021) is a general attention-based neural network architecture... applicable to various tasks."
        }
      },
      {
        "name": {
          "value": "EMNIST",
          "justification": "EMNIST is used for image completion experiments and is discussed in the experiment section.",
          "quote": "The EMNIST dataset comprises black and white images of handwritten letters..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Perceiver IO: A general architecture for structured inputs & outputs",
          "justification": "Like CelebA, EMNIST is noted amongst datasets typically examined with model structures akin to Perceiver IO.",
          "quote": "Perceiver IO (Jaegle et al., 2021) is a general attention-based neural network architecture... applicable to various tasks..."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1752,
    "prompt_tokens": 16810,
    "total_tokens": 18562,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}