{
  "paper": "9bc78cb3811d7d92a1ea54db0a3452a2.txt",
  "words": 9626,
  "extractions": {
    "title": {
      "value": "Chain of Targeted Verification Questions to Improve the Reliability of Code Generated by LLMs",
      "justification": "The title is explicitly mentioned at the beginning of the document, and it is consistent throughout the paper, including in the abstract and the body of the text.",
      "quote": "Chain of Targeted Verification Questions to Improve the Reliability of Code Generated by LLMs"
    },
    "description": "This paper proposes a self-refinement method aimed at improving the reliability of code generated by LLMs by minimizing bugs before execution, without human intervention, through a chain of targeted Verification Questions.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents an empirical study focusing on the development and evaluation of a self-refinement method for improving LLM-generated code using verification questions and provides experimental results.",
      "quote": "In this study, we propose a self-refinement method aimed at improving the reliability of code generated by LLMs... Our evaluation, based on programming tasks in the CoderEval dataset, demonstrates that our proposed method outperforms state-of-the-art methods by decreasing the number of targeted errors..."
    },
    "primary_research_field": {
      "name": {
        "value": "Software Engineering",
        "justification": "The primary research field is Software Engineering as the paper deals with improving the reliability of LLM-generated code, which falls under software development and testing.",
        "quote": "KEYWORDS\n\nLLM-based assistants, such as GitHub Copilot and ChatGPT, have\nthe potential to generate code that fulfills a programming task... the reliability of\ncode generated by LLMs..."
      },
      "aliases": [
        "SE"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Large Language Models",
          "justification": "The paper extensively discusses the use of Large Language Models (LLMs) such as GPT4, Codex, and others for generating code.",
          "quote": "Large Language Models (LLMs) like GPT4 [3], Codex [8], and Llama-2 [36] that can generate code..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ChatGPT",
          "justification": "ChatGPT is used in the methodology to improve the reliability of the initial code by prompting the model with the chain of VQs and repairing potential bugs.",
          "quote": "Our method makes use of ChatGPT (gpt3.5-turbo) to iteratively improve the reliability of the initial code..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "ChatGPT is used as an existing model and not contributed by this paper.",
          "quote": "Our method makes use of ChatGPT (gpt3.5-turbo)"
        },
        "is_executed": {
          "value": true,
          "justification": "The paper describes the execution of ChatGPT in the methodology for improving code reliability.",
          "quote": "Our method makes use of ChatGPT (gpt3.5-turbo) to iteratively improve..."
        },
        "is_compared": {
          "value": false,
          "justification": "ChatGPT is not compared against other models as a subject of comparison within this study.",
          "quote": "Our method makes use of ChatGPT (gpt3.5-turbo) to iteratively improve..."
        },
        "referenced_paper_title": {
          "value": "ChatGPT: Optimizing Language Models for Dialogue",
          "justification": "This is the associated technical report for ChatGPT mentioned in the reference list of the paper.",
          "quote": "John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. 2022. ChatGPT: Optimizing Language Models for Dialogue."
        }
      },
      {
        "name": {
          "value": "Codex",
          "justification": "Codex is mentioned as one of the Large Language Models that can generate code based on a given description.",
          "quote": "Large Language Models (LLMs) like GPT4 [3], Codex [8], and Llama-2 [36] that can generate code..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Codex is discussed as a pre-existing model and not contributed by the authors of this paper.",
          "quote": "Large Language Models (LLMs) like GPT4 [3], Codex [8], and Llama-2 [36]..."
        },
        "is_executed": {
          "value": false,
          "justification": "There is no indication in the text that the authors executed Codex within this study.",
          "quote": "Large Language Models (LLMs) like GPT4 [3], Codex [8], and Llama-2 [36]..."
        },
        "is_compared": {
          "value": false,
          "justification": "Codex is referred to as a model in the field but is not compared in terms of performance within this study.",
          "quote": "Large Language Models (LLMs) like GPT4 [3], Codex [8], and Llama-2 [36]..."
        },
        "referenced_paper_title": {
          "value": "Evaluating large language models trained on code",
          "justification": "This is the reference associated with Codex as provided in the research paper.",
          "quote": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al. 2021. Evaluating large language models trained on code."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CoderEval",
          "justification": "The CoderEval dataset is explicitly mentioned as being used in the evaluation of the proposed refinement method.",
          "quote": "Our evaluation, based on programming tasks in the CoderEval dataset, demonstrates that our proposed method outperforms state-of-the-art methods..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models",
          "justification": "This is the relevant paper for the CoderEval dataset, as listed in the references of the research paper.",
          "quote": "Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, and Qianxiang Wang. 2023. CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1277,
    "prompt_tokens": 16253,
    "total_tokens": 17530,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}