{
  "paper": "a6e7ff35ac1735d9966f062029f6efe5.txt",
  "words": 9230,
  "extractions": {
    "title": {
      "value": "Offline Retrieval Evaluation Without Evaluation Metrics",
      "justification": "The title is clearly stated at the beginning of the paper and in the ACM reference format.",
      "quote": "Fernando Diaz and Andres Ferraro. 2022. Offline Retrieval Evaluation Without Evaluation Metrics."
    },
    "description": "This paper proposes a new evaluation method called recall-paired preference (RPP) for assessing the performance of retrieval and recommendation systems without relying on traditional evaluation metrics. RPP focuses on modeling preferences across different user subpopulations, offering improved discriminative power and robustness.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical evaluations of the proposed RPP method across multiple information retrieval and recommendation tasks, comparing it with existing metrics.",
      "quote": "Our results across multiple search and recommendation tasks demonstrate that RPP substantially improves discriminative power while correlating well with existing metrics and being equally robust to incomplete data."
    },
    "primary_research_field": {
      "name": {
        "value": "Information Retrieval",
        "justification": "The paper focuses on evaluating retrieval systems and directly mentions information retrieval in its keywords.",
        "quote": "information retrieval; recommender systems; offline evaluation"
      },
      "aliases": [
        "IR"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Recommender Systems",
          "justification": "The paper discusses the application of RPP in recommender systems and is mentioned in the keywords.",
          "quote": "information retrieval; recommender systems; offline evaluation"
        },
        "aliases": []
      }
    ],
    "models": [],
    "datasets": [
      {
        "name": {
          "value": "TREC 2019 Deep Learning Track",
          "justification": "The paper references the TREC 2019 Deep Learning Track as part of its analysis of evaluation metrics and experiments.",
          "quote": "...the Deep Learning Document Ranking (2019, 2020)... TREC 2019 Deep Learning document ranking task [11]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Overview of the TREC 2019 Deep Learning Track",
          "justification": "The referenced paper provides an overview of the TREC 2019 Deep Learning Track, which is used in this study.",
          "quote": "Overview of the TREC 2019 Deep Learning Track."
        }
      },
      {
        "name": {
          "value": "MovieLens 1M",
          "justification": "The dataset is used for recommendation system experiments as described in the paper.",
          "quote": "Additionally, we used a variety of recommendation systems runs prepared by Valcarce et al. [28] for the MovieLens 1M..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Assessing ranking metrics in top-N recommendation",
          "justification": "Valcarce et al.'s paper is referenced as providing the recommendation systems runs for MovieLens 1M used in this study.",
          "quote": "Assessing ranking metrics in top-N recommendation."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 559,
    "prompt_tokens": 19711,
    "total_tokens": 20270,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}