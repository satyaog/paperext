{
  "paper": "2310.03419.txt",
  "words": 10200,
  "extractions": {
    "title": {
      "value": "Pre-Training and Fine-Tuning Generative Flow Networks",
      "justification": "The title clearly indicates the focus on both pre-training and fine-tuning GFlowNets.",
      "quote": "Pre-Training and Fine-Tuning Generative Flow Networks"
    },
    "description": "This paper proposes a novel approach for pre-training Generative Flow Networks (GFlowNets) in a reward-free, unsupervised manner. The authors introduce an outcome-conditioned GFlowNet (OC-GFN) which learns to reach targeted outcomes in a self-supervised way, enabling efficient adaptation to downstream tasks. The paper demonstrates significant improvements in sample diversity and efficiency through extensive experiments in domains like GridWorld and biological sequence design tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes extensive experimental results validating the efficacy of the proposed approach.",
      "quote": "Extensive experimental results validate the efficacy of our approach, demonstrating the effectiveness of pre-training the OC-GFN, and its ability to swiftly adapt to downstream tasks and discover modes more efficiently."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper is focused on a novel approach for pre-training and fine-tuning deep learning models, specifically Generative Flow Networks.",
        "quote": "In this paper, we propose a novel method for reward-free unsupervised pre-training of GFlowNets."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Generative Models",
          "justification": "The focus on Generative Flow Networks (GFlowNets) and improving their training and adaptation processes aligns this paper with the sub-field of Generative Models in deep learning.",
          "quote": "Generative Flow Networks (GFlowNets) are amortized samplers that learn stochastic policies to sequentially generate compositional objects from a given unnormalized reward distribution."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Outcome-Conditioned Generative Flow Network",
          "justification": "The primary model introduced and used in the study is the outcome-conditioned GFlowNet for pre-training.",
          "quote": "we propose an outcome-conditioned GFlowNet (OC-GFN) that learns to explore the candidate space"
        },
        "aliases": [
          "OC-GFN"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Role:['contributed', 'used', 'referenced']",
          "quote": "contributed"
        },
        "is_executed": {
          "value": true,
          "justification": "ModelMode:['trained', 'fine-tuned', 'inference']",
          "quote": "trained"
        },
        "is_compared": {
          "value": true,
          "justification": "",
          "quote": ""
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GridWorld",
          "justification": "The paper conducts experiments on the GridWorld domain to validate the proposed model.",
          "quote": "Through extensive experiments on the GridWorld domain, we empirically validate the efficacy of the proposed pre-training and fine-tuning paradigm."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "TF Bind",
          "justification": "Experiments include generating DNA sequences with high binding activity with targeted transcription factors, evaluated on multiple downstream tasks.",
          "quote": "We consider 30 different downstream tasks studied in (Barrera et al., 2016), which conducted biological experiments to determine the binding properties between a range of transcription factors and every conceivable DNA sequence."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "RNA Sequence",
          "justification": "The paper uses RNA sequence datasets for evaluating the pre-training and fine-tuning in generating RNA sequences that bind to a given target.",
          "quote": "We study the performance of the proposed method in a larger task of generating RNA sequences that bind to a given target."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Adam",
          "justification": "The paper mentions using the Adam optimizer in model training.",
          "quote": "We train all models with the Adam optimizer (Kingma and Ba, 2015) based on samples from a parallel of 16 rollouts in the environment."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 760,
    "prompt_tokens": 17928,
    "total_tokens": 18688
  }
}