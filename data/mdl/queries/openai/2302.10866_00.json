{
  "paper": "2302.10866.txt",
  "words": 15758,
  "extractions": {
    "title": {
      "value": "Hyena Hierarchy: Towards Larger Convolutional Language Models",
      "justification": "The literal title of the paper is 'Hyena Hierarchy: Towards Larger Convolutional Language Models'.",
      "quote": "Hyena Hierarchy: Towards Larger Convolutional Language Models"
    },
    "description": "This paper proposes Hyena, a subquadratic drop-in replacement for the attention mechanism in Transformers by using implicitly parameterized convolutions and data-controlled gating. The authors introduce the Hyena hierarchy and evaluate its performance across various language modeling tasks, demonstrating superior speed and efficiency compared to existing models, including Transformers.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves empirical evaluation of the proposed Hyena model on various tasks and presents experimental results to demonstrate its performance and efficiency.",
      "quote": "In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper primarily deals with language modeling, which is a core task in the field of Natural Language Processing (NLP).",
        "quote": "In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Language Modeling",
          "justification": "The paper focuses specifically on tasks related to language modeling, such as recall and reasoning tasks, and empirically evaluates the Hyena model on these tasks.",
          "quote": "We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Model Efficiency",
          "justification": "The paper places significant emphasis on the efficiency of the Hyena model, particularly in terms of computational cost and speed compared to Transformers.",
          "quote": "Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100× faster at sequence length 64K."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Mechanistic Interpretability",
          "justification": "The paper discusses how the design of the Hyena hierarchy is motivated by mechanistic interpretability and its correlation with performance on reasoning tasks.",
          "quote": "Our evaluation is grounded in recent work on mechanistic interpretability (Elhage et al., 2021; Power et al., 2022; Olsson et al., 2022; Zhang et al., 2022) such as recall and induction, to distill three properties of attention correlated with its performance and the quality gap with existing subquadratic approaches."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Empirical Benchmarking",
          "justification": "The paper extensively evaluates the Hyena model against existing models across multiple benchmarks, focusing on empirical results.",
          "quote": "We test Hyena on autoregressive language modeling at the sub-billion parameter scale, setting a new state-of-the-art for dense-attention-free architectures in standard datasets (WikiText103 and The Pile) and matching Transformer quality."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Hyena",
          "justification": "The paper introduces and evaluates the Hyena model as a subquadratic drop-in replacement for the attention mechanism in Transformers.",
          "quote": "In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "The Hyena model is introduced as a new approach in this paper.",
          "quote": "In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper presents empirical results based on the execution of the Hyena model.",
          "quote": "Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100× faster at sequence length 64K."
        },
        "is_compared": {
          "value": true,
          "justification": "The Hyena model is compared with other models, particularly Transformers, in terms of performance and efficiency.",
          "quote": "Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models."
        },
        "referenced_paper_title": {
          "value": "Attention Is All You Need",
          "justification": "The attention mechanism that Hyena aims to replace was first introduced in the paper 'Attention Is All You Need'.",
          "quote": "Large Transformers have enabled a number of breakthrough advances in modeling language, vision, audio, biology and numerous other domains (Vaswani et al., 2017), (Dosovitskiy et al., 2020), (Radford et al., 2022), (Cramer, 2021)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "WikiText103",
          "justification": "The dataset WikiText103 is explicitly mentioned and used to evaluate the Hyena model in the paper.",
          "quote": "We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "WikiText: A Large-Scale Languge Modeling Dataset",
          "justification": "WikiText103 is part of the WikiText dataset introduced in the referenced paper.",
          "quote": "We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K."
        }
      },
      {
        "name": {
          "value": "The Pile",
          "justification": "The dataset 'The Pile' is explicitly mentioned and used to evaluate the Hyena model in the paper.",
          "quote": "We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
          "justification": "The Pile is introduced and detailed in the referenced paper.",
          "quote": "We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20% reduction in training compute required at sequence length 2K."
        }
      },
      {
        "name": {
          "value": "ImageNet-1k",
          "justification": "The ImageNet-1k dataset is used for evaluating the Hyena model's performance in image classification tasks.",
          "quote": "In image classification, Hyena is able to match attention in accuracy when training on ImageNet-1k from scratch."
        },
        "aliases": [
          "ImageNet"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "The referenced paper introduces the ImageNet dataset, which includes ImageNet-1k.",
          "quote": "In image classification, Hyena is able to match attention in accuracy when training on ImageNet-1k from scratch."
        }
      },
      {
        "name": {
          "value": "PG-19",
          "justification": "The PG-19 dataset is mentioned and used to evaluate the Hyena model's performance.",
          "quote": "We provide results of additional training runs on other datasets. We train a Hyena 153M model on the standard PG-19 long-range corpus (Rae et al., 2019), with a context length of 16k tokens, reaching a test perplexity of 14.6 (using the standard GPT2 tokenizer) in 8 epochs."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Compressive Transformers for Long-Range Sequence Modelling",
          "justification": "The referenced paper provides details about the PG-19 dataset used for long-range sequence modeling tasks.",
          "quote": "We provide results of additional training runs on other datasets. We train a Hyena 153M model on the standard PG-19 long-range corpus (Rae et al., 2019), with a context length of 16k tokens, reaching a test perplexity of 14.6 (using the standard GPT2 tokenizer) in 8 epochs."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "The CIFAR-10 dataset is used to evaluate the performance of the Hyena model on sequential and 2D image classification tasks.",
          "quote": "We use CIFAR-10 in sequential and 2D experiments. For sequential, we use the Hyena operator defined in our language tasks and compare with an S4 model (Gu et al., 2021) of the same size by swapping layers in the residual blocks."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "The referenced paper introduces the CIFAR-10 dataset used for evaluating image classification models.",
          "quote": "We use CIFAR-10 in sequential and 2D experiments. For sequential, we use the Hyena operator defined in our language tasks and compare with an S4 model (Gu et al., 2021) of the same size by swapping layers in the residual blocks."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions the use of PyTorch for implementing the models and running the experiments.",
          "quote": "We measure 5x speedups over dense self-attention at length 8192 – 2x over highly optimized FlashAttention2 (Dao et al., 2022b) – and 100x speedup over FlashAttention at sequence lengths of 64k, where standard attention implementation in PyTorch runs out of memory."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The referenced paper provides details about PyTorch, which is used in the implementation of the models and experiments.",
          "quote": "We measure 5x speedups over dense self-attention at length 8192 – 2x over highly optimized FlashAttention2 (Dao et al., 2022b) – and 100x speedup over FlashAttention at sequence lengths of 64k, where standard attention implementation in PyTorch runs out of memory."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2214,
    "prompt_tokens": 27318,
    "total_tokens": 29532
  }
}