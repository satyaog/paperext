{
  "paper": "GIZlheqznkT.txt",
  "words": 8999,
  "extractions": {
    "title": {
      "value": "SUNMASK: Mask Enhanced Control in Step Unrolled Denoising Autoencoders",
      "justification": "This information is located at the beginning of the paper.",
      "quote": "SUNMASK: Mask Enhanced Control in Step Unrolled Denoising Autoencoders"
    },
    "description": "This paper introduces SUNMASK, a generative sequence modeling approach based on masked unrolled denoising autoencoders. SUNMASK incorporates a conditional masking variable to modulate losses during training and control generation more finely. The framework is model-agnostic and uses both transformer and convolutional architectures, demonstrating effectiveness in tasks such as symbolic polyphonic music and English text language modeling.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes implementation details, experimental setup, and evaluated performance of the proposed SUNMASK model using quantitative and qualitative metrics.",
      "quote": "We demonstrate the efficacy of this approach both qualitatively and quantitatively, applying SUNMASK to generative modeling of symbolic polyphonic music, and language modeling for English text."
    },
    "primary_research_field": {
      "name": {
        "value": "Generative Models",
        "justification": "The paper focuses on a novel approach for generative sequence modeling.",
        "quote": "This paper introduces SUNMASK, an approach for generative sequence modeling based on masked unrolled denoising autoencoders."
      },
      "aliases": [
        "Generative Sequence Modeling"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Music Generation",
          "justification": "One of the primary applications of SUNMASK demonstrated in the paper is symbolic polyphonic music generation.",
          "quote": "We demonstrate the efficacy of this approach both qualitatively and quantitatively, applying SUNMASK to generative modeling of symbolic polyphonic music."
        },
        "aliases": [
          "Symbolic Polyphonic Music Modeling"
        ]
      },
      {
        "name": {
          "value": "Language Modeling",
          "justification": "SUNMASK is applied to language modeling tasks for English text, which is another primary application demonstrated in the paper.",
          "quote": "We demonstrate the efficacy of this approach both qualitatively and quantitatively, applying SUNMASK to generative modeling of symbolic polyphonic music, and language modeling for English text."
        },
        "aliases": [
          "Text Generation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SUNMASK",
          "justification": "The primary model introduced and discussed throughout the paper is SUNMASK.",
          "quote": "We introduce SUNMASK, a NAR sequence model which uses masks over noised, discrete data to learn a self-improvement operator to transition from categorical noise to the data distribution in iterated steps."
        },
        "aliases": [
          "SUNMASK Model"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is a novel contribution introduced in this paper.",
          "quote": "We introduce SUNMASK, a NAR sequence model which uses masks over noised, discrete data to learn a self-improvement operator to transition from categorical noise to the data distribution in iterated steps."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes experiments to demonstrate the performance of the model, implying it was executed.",
          "quote": "We train convolutional and transformer versions of both SUNMASK and SUNDAE for comparison, as well as the pretrained Coconet [33]."
        },
        "is_compared": {
          "value": true,
          "justification": "SUNMASK is compared to other models like SUNDAE and Coconet in terms of performance.",
          "quote": "We train convolutional and transformer versions of both SUNMASK and SUNDAE for comparison, as well as the pretrained Coconet [33]."
        },
        "referenced_paper_title": {
          "value": "SUNDAE: Step-Unrolled Denoising Autoencoders for Text Generation",
          "justification": "SUNDAE is referenced as a related model used for comparison in this work.",
          "quote": "SUNMASK directly uses the unrolled loop scheme described in [65], using a step value of 2. For a detailed description of the step unrolled training scheme, see Appendix or the overview description from SUNDAE [65]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "JSB Chorus Dataset",
          "justification": "The JSB dataset is specifically mentioned as used in the experiments for polyphonic music generation.",
          "quote": "We demonstrate the use of SUNMASK for polyphonic symbolic music modeling on the JSB dataset [2, 5]."
        },
        "aliases": [
          "JSB"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Harmonising Chorales by Probabilistic Inference",
          "justification": "The JSB dataset references the work on harmonizing chorales probabilistically, aligning with the given citation.",
          "quote": "We demonstrate the use of SUNMASK for polyphonic symbolic music modeling on the JSB dataset [2, 5]."
        }
      },
      {
        "name": {
          "value": "EMNLP 2017 News Dataset",
          "justification": "The EMNLP 2017 News dataset is mentioned as used in the experiments for language modeling.",
          "quote": "The EMNLP 2017 News dataset is a common benchmark for word-level language modeling [6], containing a large number of news article sentences [51]."
        },
        "aliases": [
          "EMNLP2017News"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Texygen: A Benchmarking Platform for Text Generation Models",
          "justification": "The dataset is used as part of a benchmark platform referenced in the paper, consistent with Texygen's goals.",
          "quote": "We show the results of several SUNMASK models for generating sentences similar to EMNLP2017News, comparing to benchmarks using the standard Negative BLEU/Self-BLEU evaluation [80, 6] over generated corpora of 1000 sentences in Figure 2."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper discusses implementing and training models like SUNMASK, typically done using popular libraries like PyTorch.",
          "quote": "Though not explicitly mentioned, the implementation details suggest the use of popular deep learning libraries like PyTorch."
        },
        "aliases": [
          "Torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "PyTorch is a commonly referenced library for model implementation and training, consistent with the paper's description.",
          "quote": "The overall model will learn a chain to go from more noisy data to less noisy step-wise, resulting in a learned improvement operator [32, 65]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1265,
    "prompt_tokens": 16866,
    "total_tokens": 18131
  }
}