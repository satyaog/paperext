{
  "paper": "uXeEBgzILe5.txt",
  "words": 8675,
  "extractions": {
    "title": {
      "value": "B LOCKWISE S ELF -S UPERVISED L EARNING WITH BARLOW T WINS",
      "justification": "The title is directly avaialable at the beginning of the research paper.",
      "quote": "B LOCKWISE S ELF -S UPERVISED L EARNING WITH BARLOW T WINS"
    },
    "description": "This paper explores an alternative to full backpropagation by using blockwise learning rules in self-supervised learning. It focuses on training different blocks of a ResNet-50 independently using the Barlow Twins loss function, showing competitive performance on ImageNet compared to end-to-end backpropagation. The paper performs extensive experiments to understand various components and adaptations of this blockwise learning paradigm.",
    "type": {
      "value": "empirical",
      "justification": "The paper focuses on empirical results and performance evaluations on large-scale datasets using a ResNet-50 architecture. It conducts extensive experiments to show the efficacy of its blockwise self-supervised learning method.",
      "quote": "We perform extensive experiments to understand the impact of different components within our method and explore a variety of adaptations of self-supervised learning to the blockwise paradigm."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary focus of the paper is on using deep learning techniques, particularly ResNet-50 architecture, for image classification tasks on ImageNet.",
        "quote": "we explore alternatives to full backpropagation in the form of blockwise learning rules, leveraging the latest developments in self-supervised learning. ... This approach was recently illustrated at scale in the domain of video prediction, using a stack of VAEs trained sequentially in a greedy fashion"
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Classification",
          "justification": "The paper evaluates the performance of its proposed method using top-1 classification accuracy on ImageNet, which is a standard benchmark for image classification.",
          "quote": "a linear probe trained on top of our blockwise pretrained model obtains a top-1 classification accuracy of 70.48%, only 1.1% below the accuracy of an end-to-end pretrained network (71.57% accuracy)"
        },
        "aliases": [
          "Image Categorization"
        ]
      },
      {
        "name": {
          "value": "Self-supervised Learning",
          "justification": "The paper investigates the use of self-supervised learning methods, specifically Barlow Twins, as an alternative to backpropagation.",
          "quote": "we revisit the possibility of using local learning rules as a replacement for backpropagation using a recent self-supervised learning method, Barlow Twins"
        },
        "aliases": [
          "SSL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet-50",
          "justification": "The paper uses ResNet-50 architecture to implement its blockwise learning rules and evaluate their performance on ImageNet.",
          "quote": "We consider a ResNet-50 backbone and mostly adapt the Barlow Twins (Zbontar et al., 2021) codebase2 for our experiments."
        },
        "aliases": [
          "ResNet"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet-50 is not a new model contribution of this paper.",
          "quote": "We consider a ResNet-50 backbone and mostly adapt the Barlow Twins (Zbontar et al., 2021) codebase2 for our experiments."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper reports execution and experimental results using ResNet-50 on ImageNet.",
          "quote": "We consider a ResNet-50 backbone and mostly adapt the Barlow Twins (Zbontar et al., 2021) codebase2 for our experiments."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of the ResNet-50 with blockwise training is compared to its performance with end-to-end training.",
          "quote": "Notably, we show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins loss function at each block performs almost as well as end-to-end backpropagation on ImageNet."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "The reference paper for ResNet-50 is cited in the text.",
          "quote": "ResNet-50 architecture is comprised of 5 different feature spatial resolutions, followed by global average pooling and a linear layer for final classification."
        }
      },
      {
        "name": {
          "value": "Barlow Twins",
          "justification": "The paper uses the Barlow Twins model as a self-supervised learning technique for training the ResNet-50 blocks.",
          "quote": "we revisit the possibility of using local learning rules as a replacement for backpropagation using a recent self-supervised learning method, Barlow Twins"
        },
        "aliases": [
          "BarlowTwins"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "Barlow Twins is a previously developed self-supervised learning method and is not a contribution of this paper.",
          "quote": "we revisit the possibility of using local learning rules as a replacement for backpropagation using a recent self-supervised learning method, Barlow Twins"
        },
        "is_executed": {
          "value": 1,
          "justification": "Barlow Twins is executed as part of the experiments conducted in the paper.",
          "quote": "We consider a ResNet-50 backbone and mostly adapt the Barlow Twins (Zbontar et al., 2021) codebase2 for our experiments."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of Barlow Twins when used in blockwise training is compared with end-to-end backpropagation results.",
          "quote": "Notably, we show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins loss function at each block performs almost as well as end-to-end backpropagation on ImageNet."
        },
        "referenced_paper_title": {
          "value": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "justification": "The reference paper for Barlow Twins is cited in the text.",
          "quote": "Most of the experiments in the paper rely on the Barlow Twins objective function (Zbontar et al., 2021), which we briefly describe here."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet",
          "justification": "The paper uses the ImageNet dataset to evaluate the performance of its blockwise learning method.",
          "quote": "Notably, we show that a blockwise pretraining procedure consisting of training independently the 4 main blocks of layers of a ResNet-50 with Barlow Twins loss function at each block performs almost as well as end-to-end backpropagation on ImageNet"
        },
        "aliases": [
          "ILSVRC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "The reference paper for ImageNet is cited in the text.",
          "quote": "3 RELATED WORK This section provides a comparison between existing local learning paradigms in the literature and our method (Fig. 1). We compare these methods from two perspectives: biological plausibility and performance on large-scale datasets, such as ImageNet (Deng et al., 2009)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Barlow Twins Codebase",
          "justification": "The paper mentions adapting the Barlow Twins codebase for its experiments.",
          "quote": "We consider a ResNet-50 backbone and mostly adapt the Barlow Twins (Zbontar et al., 2021) codebase2 for our experiments."
        },
        "aliases": [
          "BarlowTwinsCode"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "justification": "The reference paper for Barlow Twins is cited in the text.",
          "quote": "Most of the experiments in the paper rely on the Barlow Twins objective function (Zbontar et al., 2021), which we briefly describe here."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1597,
    "prompt_tokens": 14904,
    "total_tokens": 16501
  }
}