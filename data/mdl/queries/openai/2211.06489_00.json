{
  "paper": "2211.06489.txt",
  "words": 12673,
  "extractions": {
    "title": {
      "value": "Equivariance with Learned Canonicalization Functions",
      "justification": "Title is extracted directly from the provided text.",
      "quote": "Equivariance with Learned Canonicalization Functions"
    },
    "description": "This paper presents a novel framework for achieving equivariance in neural networks by learning canonical representations of data. This approach can be plugged into non-equivariant backbone architectures, enabling them to achieve equivariance without architectural constraints. The paper provides theoretical guarantees, efficient implementations for specific domains, and empirical evidence that the proposed method performs well on tasks like image classification, N-body dynamics prediction, and point cloud classification and segmentation.",
    "type": {
      "value": "theoretical",
      "justification": "The paper presents a novel theoretical framework for achieving equivariance by learning canonical representations instead of enforcing architectural constraints. It provides theoretical guarantees and efficient implementation strategies, backed by experimental results.",
      "quote": "In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data... Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics."
    },
    "primary_research_field": {
      "name": {
        "value": "Equivariance in Neural Networks",
        "justification": "The primary focus of the paper is on achieving equivariance in neural networks through learned canonical representations.",
        "quote": "In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data... We hypothesize that among all valid canonicalization functions, some will lead to better downstream performance than others."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Classification",
          "justification": "The paper includes experiments and empirical results on image classification tasks, specifically on the Rotated MNIST dataset.",
          "quote": "Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "N-body Dynamics Prediction",
          "justification": "The paper extends the discussion on the framework to physical dynamics, specifically N-body dynamics prediction, and provides experimental results on this task.",
          "quote": "Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including... N-body dynamics prediction..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Point Cloud Classification and Part Segmentation",
          "justification": "The paper presents experimental results on point cloud classification and part segmentation, using datasets like ModelNet40 and ShapeNet.",
          "quote": "Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including... point cloud classification and segmentation..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Canonicalization Network (CN)",
          "justification": "The paper introduces Canonicalization Network (CN) as a model that learns to output canonical representations, making non-equivariant architectures equivariant.",
          "quote": "In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data...We hypothesize that among all valid canonicalization functions, some will lead to better downstream performance than others."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The Canonicalization Network is introduced and empirically evaluated in this paper.",
          "quote": "In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data..."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed in the experiments provided in the paper, specifically in image classification, N-body dynamics prediction, and point cloud classification.",
          "quote": "We perform experiments that show that the proposed method achieves excellent results on images, physical dynamical systems and point clouds."
        },
        "is_compared": {
          "value": 1,
          "justification": "The Canonicalization Network is compared to other existing techniques and models across various tasks like image classification and dynamics prediction.",
          "quote": "Our experiments show that the proposed method achieves excellent results on images, physical dynamical systems and point clouds."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "This is the primary model contributed by this paper and is not referenced from another paper.",
          "quote": "In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Rotated MNIST",
          "justification": "The dataset Rotated MNIST is used in the image classification experiments to evaluate the proposed method.",
          "quote": "We first perform an empirical analysis of the proposed framework in the image domain. We selected the Rotated MNIST dataset (Larochelle et al., 2007), often used as a benchmark for equivariant architectures."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "An empirical evaluation of deep architectures on problems with many factors of variation",
          "justification": "The dataset Rotated MNIST was referenced to the paper by Larochelle et al., 2007.",
          "quote": "We selected the Rotated MNIST dataset (Larochelle et al., 2007), often used as a benchmark for equivariant architectures."
        }
      },
      {
        "name": {
          "value": "ModelNet40",
          "justification": "The dataset ModelNet40 is used for point cloud classification tasks in this paper.",
          "quote": "The ModelNet40 dataset consists of 40 classes of 3D models, with a total of 12,311 models. 9,843 models were used for training, and the remaining models were used for testing in the classification task."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "3D ShapeNet: A deep representation for volumetric shapes",
          "justification": "The dataset ModelNet40 was referenced to the paper by Wu et al., 2015.",
          "quote": "The ModelNet40 dataset consists of 40 classes of 3D models, with a total of 12,311 models. 9,843 models were used for training, and the remaining models were used for testing in the classification task."
        }
      },
      {
        "name": {
          "value": "ShapeNet",
          "justification": "The dataset ShapeNet is used for part segmentation tasks in this paper.",
          "quote": "The ShapeNet dataset was used for part segmentation with the ShapeNet-part subset, which includes 16 categories of objects and more than 30,000 models."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ShapeNet: An Information-Rich 3D Model Repository",
          "justification": "The dataset ShapeNet was referenced to the paper by Chang et al., 2015.",
          "quote": "The ShapeNet dataset was used for part segmentation with the ShapeNet-part subset, which includes 16 categories of objects and more than 30,000 models."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is the primary deep learning library used in the implementations and experiments.",
          "quote": "A P Y T ORCH code snippet to perform the canonicalization function of images in a differentiable way using a G-CNN."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "PyTorch is explicitly mentioned in the implementation section without referencing a specific paper.",
          "quote": "A P Y T ORCH code snippet to perform the canonicalization function of images in a differentiable way using a G-CNN."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1477,
    "prompt_tokens": 23153,
    "total_tokens": 24630
  }
}