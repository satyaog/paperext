{
  "paper": "sTYuRVrdK3.txt",
  "words": 19537,
  "extractions": {
    "title": {
      "value": "Evaluating Representation Learning on the Protein Structure Universe",
      "justification": "The title clearly indicates the focus of the paper on evaluating representation learning, specifically in the context of protein structures.",
      "quote": "E VALUATING R EPRESENTATION L EARNING ON THE P ROTEIN S TRUCTURE U NIVERSE Arian R. Jamasb∗,1,† , Alex Morehead∗,2 , Chaitanya K. Joshi∗,1 , Zuobai Zhang∗,3 , Kieran Didi1 , Simon Mathis1 , Charles Harris1 , Jian Tang3 , Jianlin Cheng2 , Pietro Liò1 , Tom L. Blundell1 1"
    },
    "description": "The paper introduces ProteinWorkshop, a benchmark suite for evaluating representation learning on protein structures using Geometric Graph Neural Networks (GNNs). It involves large-scale pre-training on experimental and predicted structures to assess the capability of models to learn informative representations of protein structures. The study benchmarks different GNN versions and pretraining setups to establish a common evaluation ground for the field, providing tools for efficient dataset handling.",
    "type": {
      "value": "empirical",
      "justification": "The study involves iterative evaluation and benchmarking of various models and setups, emphasizing quantitative analysis and performance metrics.",
      "quote": "Our contributions are as follows: • We curate numerous structure-based pretraining and fine-tuning datasets from the literature with a focus on tasks that can enable structural annotation of predicted structures. We compile a highly-modular benchmark, enabling the community to rapidly evaluate protein representation learning methods across tasks, models, and pretraining setups."
    },
    "primary_research_field": {
      "name": {
        "value": "Computational Biology",
        "justification": "The paper is centered around representation learning and protein structure, thus falling squarely in the domain of computational biology.",
        "quote": "We aim to establish a common ground for the machine learning and computational biology communities to rigorously compare and advance protein structure representation learning."
      },
      "aliases": [
        "Computational Biology"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper heavily focuses on learning representations of protein structures, employing various GNN models.",
          "quote": "A BSTRACT We introduce ProteinWorkshop, a comprehensive benchmark suite for representation learning on protein structures with Geometric Graph Neural Networks."
        },
        "aliases": [
          "Representation Learning"
        ]
      },
      {
        "name": {
          "value": "Protein Structure Prediction",
          "justification": "The research prominently includes evaluation and utilization of protein structures, including pre-trained models on AlphaFold structures.",
          "quote": "large-scale pretraining on AlphaFold structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant GNNs"
        },
        "aliases": [
          "Protein Structure Prediction"
        ]
      },
      {
        "name": {
          "value": "Graph Neural Networks",
          "justification": "The core technology used in this paper revolves around Geometric Graph Neural Networks (GNNs).",
          "quote": "Several deep learning methods have been developed for protein structures. In particular, Geometric Graph Neural Networks (GNNs) have emerged as the architecture of choice for learning structural representations of biomolecules."
        },
        "aliases": [
          "GNNs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "AlphaFold",
          "justification": "AlphaFold is used as a reference for structures in pretraining tasks, indicating its role as a foundational model in the research.",
          "quote": "large-scale pretraining on AlphaFold structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant GNNs"
        },
        "aliases": [
          "AlphaFold"
        ],
        "is_contributed": {
          "value": false,
          "justification": "AlphaFold is utilized for pretraining, but it is not a contribution of the paper itself.",
          "quote": "large-scale pretraining on AlphaFold structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant GNNs"
        },
        "is_executed": {
          "value": false,
          "justification": "The paper refers to AlphaFold as a source of structures rather than a model executed in their experiments.",
          "quote": "large-scale pretraining on AlphaFold structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant GNNs"
        },
        "is_compared": {
          "value": false,
          "justification": "AlphaFold is referenced for its structural data, not directly compared numerically in the paper's experiments.",
          "quote": "large-scale pretraining on AlphaFold structures and auxiliary tasks consistently improve the performance of both rotation-invariant and equivariant GNNs"
        },
        "referenced_paper_title": {
          "value": "Highly accurate protein structure prediction with AlphaFold",
          "justification": "This paper is the reference for the foundational model AlphaFold, which provides the structural data used in this research.",
          "quote": "(Jumper et al., 2021) Highly accurate protein structure prediction with AlphaFold."
        }
      },
      {
        "name": {
          "value": "ESM-2",
          "justification": "ESM-2 is mentioned as a sequence-based pretrained language model used in benchmarking.",
          "quote": "sequence-based pretrained ESM-2-650M augmented with our structural featurisation matches state-of-the-art GNNs on (super)family fold and gene ontology prediction."
        },
        "aliases": [
          "ESM-2-650M"
        ],
        "is_contributed": {
          "value": false,
          "justification": "ESM-2 is integrated into the experiments but originally developed outside this paper.",
          "quote": "sequence-based pretrained ESM-2-650M augmented with our structural featurisation matches state-of-the-art GNNs on (super)family fold and gene ontology prediction."
        },
        "is_executed": {
          "value": true,
          "justification": "ESM-2-650M is executed as part of the benchmarking tasks in this study.",
          "quote": "sequence-based pretrained ESM-2-650M augmented with our structural featurisation matches state-of-the-art GNNs on (super)family fold and gene ontology prediction."
        },
        "is_compared": {
          "value": true,
          "justification": "Numerical comparisons are made with state-of-the-art GNNs to assess ESM-2-650M's performance.",
          "quote": "sequence-based pretrained ESM-2-650M augmented with our structural featurisation matches state-of-the-art GNNs on (super)family fold and gene ontology prediction."
        },
        "referenced_paper_title": {
          "value": "Language models of protein sequences at the scale of evolution enable accurate structure prediction",
          "justification": "The ESM-2 model refers to this paper which details its development and applications.",
          "quote": "(Lin et al., 2022) Language models of protein sequences at the scale of evolution enable accurate structure prediction."
        }
      },
      {
        "name": {
          "value": "SchNet",
          "justification": "SchNet is recognized for using scalar feature updates and radial filters in the geometric context.",
          "quote": "SchNet is one of the most popular and simplest instantiation of E(3) invariant message passing GNNs."
        },
        "aliases": [
          "SchNet"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The SchNet model is utilized in the paper but not developed or contributed by it.",
          "quote": "SchNet is one of the most popular and simplest instantiation of E(3) invariant message passing GNNs."
        },
        "is_executed": {
          "value": true,
          "justification": "SchNet is benchmarked and executed as part of the experiments.",
          "quote": "SchNet is one of the most popular and simplest instantiation of E(3) invariant message passing GNNs."
        },
        "is_compared": {
          "value": true,
          "justification": "SchNet's performance is compared with other models during benchmarking.",
          "quote": "SchNet is one of the most popular and simplest instantiation of E(3) invariant message passing GNNs."
        },
        "referenced_paper_title": {
          "value": "SchNet: A deep learning architecture for molecules and materials",
          "justification": "This title references the development and introduction of the SchNet model.",
          "quote": "(Schütt et al., 2018) SchNet: A deep learning architecture for molecules and materials."
        }
      },
      {
        "name": {
          "value": "EGNN",
          "justification": "EGNN is mentioned as an E(3) equivariant GNN model used in the benchmarking.",
          "quote": "EGNN (Satorras et al., 2021). We consider E(3) equivariant GNN layers proposed by Satorras et al."
        },
        "aliases": [
          "EGNN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "EGNN, although used in the paper, is not a novel contribution of this study.",
          "quote": "EGNN (Satorras et al., 2021). We consider E(3) equivariant GNN layers proposed by Satorras et al."
        },
        "is_executed": {
          "value": true,
          "justification": "EGNN is run and tested within the benchmark setups.",
          "quote": "EGNN (Satorras et al., 2021). We consider E(3) equivariant GNN layers proposed by Satorras et al."
        },
        "is_compared": {
          "value": true,
          "justification": "Once executed, EGNN's performance is compared numerically with other benchmarks.",
          "quote": "EGNN (Satorras et al., 2021). We consider E(3) equivariant GNN layers proposed by Satorras et al."
        },
        "referenced_paper_title": {
          "value": "E(n) Equivariant Graph Neural Networks",
          "justification": "This title from the references introduces EGNN, providing context to its use in this paper.",
          "quote": "(Satorras et al., 2021) E(n) Equivariant Graph Neural Networks."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CATH - Inverse Folding",
          "justification": "The CATH dataset is used to evaluate inverse folding as a downstream task.",
          "quote": "Inverse folding is a generic task that can be applied to any dataset in the benchmark. In the literature, it is commonly evaluated on the CATH dataset compiled by Ingraham et al. (2019)."
        },
        "aliases": [
          "CATH"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Generative models for graph-based protein design",
          "justification": "This title is linked to the inverse folding benchmark involving the CATH dataset.",
          "quote": "(Ingraham et al., 2019) Generative models for graph-based protein design."
        }
      },
      {
        "name": {
          "value": "AlphaFoldDB",
          "justification": "This large structural dataset is utilized for model pretraining, demonstrating its role in the methodology.",
          "quote": "We pretrain and evaluate models on, to our knowledge, the largest non-redundant protein structure corpus containing 2.27 million structures from AlphaFoldDB."
        },
        "aliases": [
          "AlphaFoldDB"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "AlphaFold Protein Structure Database: Massively expanding the structural coverage of protein-sequence space with high-accuracy models",
          "justification": "The AlphaFoldDB is directly referenced in this context, detailing its comprehensive dataset of structures.",
          "quote": "(Varadi et al., 2021) AlphaFold Protein Structure Database: Massively expanding the structural coverage of protein-sequence space with high-accuracy models."
        }
      },
      {
        "name": {
          "value": "ESM Atlas",
          "justification": "ESM Atlas is used as a pretraining dataset, specifically highlighted in the paper for its utility.",
          "quote": "storage-efficient dataloaders for large-scale structural databases including AlphaFoldDB and ESM Atlas"
        },
        "aliases": [
          "ESM Atlas"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Language models of protein sequences at the scale of evolution enable accurate structure prediction",
          "justification": "This title from the references section mentions the dataset 'ESM Atlas', linking it to extensive scale applications.",
          "quote": "(Lin et al., 2022) Language models of protein sequences at the scale of evolution enable accurate structure prediction."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is identified as the primary deep learning library utilized for experiments.",
          "quote": "The benchmark is developed using PyTorch (Paszke et al., 2019)"
        },
        "aliases": [
          "PyTorch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "This reference details the main library supporting the performance of deep learning tasks in the paper.",
          "quote": "(Paszke et al., 2019) PyTorch: An imperative style, high-performance deep learning library."
        }
      },
      {
        "name": {
          "value": "PyTorch Geometric",
          "justification": "This library supports geometric deep learning tasks, reinforcing GNNs' versatility in the study.",
          "quote": "The benchmark is developed using...PyTorch Geometric (Fey & Lenssen, 2019)"
        },
        "aliases": [
          "PyTorch Geometric"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Fast Graph Representation Learning with PyTorch Geometric",
          "justification": "The reference specifically deals with PyTorch Geometric, solidifying its contribution to graph learning tasks in this study.",
          "quote": "(Fey & Lenssen, 2019) Fast Graph Representation Learning with PyTorch Geometric."
        }
      },
      {
        "name": {
          "value": "Graphein",
          "justification": "Used for geometric deep learning and network analysis, aiding in processing biomolecular structures.",
          "quote": "The benchmark is developed using...Graphein (Jamasb et al., 2022)."
        },
        "aliases": [
          "Graphein"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Graphein - a Python library for geometric deep learning and network analysis on biomolecular structures and interaction networks",
          "justification": "The use of Graphein is aligned with its capability to model geometric and network properties, crucial for this study.",
          "quote": "(Jamasb et al., 2022) Graphein - a Python library for geometric deep learning and network analysis on biomolecular structures and interaction networks."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2768,
    "prompt_tokens": 37394,
    "total_tokens": 40162,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}