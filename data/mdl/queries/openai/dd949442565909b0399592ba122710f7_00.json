{
  "paper": "dd949442565909b0399592ba122710f7.txt",
  "words": 8244,
  "extractions": {
    "title": {
      "value": "Listenable Maps for Zero-Shot Audio Classifiers",
      "justification": "The most suitable title for the paper is directly identified at the beginning of the text.",
      "quote": "In this paper, we introduce LMAC-ZS (Listenable Maps for Audio Classifiers in the Zero-Shot context)"
    },
    "description": "This paper introduces LMAC-ZS, a novel post-hoc interpretation method for zero-shot audio classifiers, which explains the decisions made by CLAP models.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical evaluations and experiments using several datasets and metrics.",
      "quote": "We provide an extensive evaluation using the Contrastive Language-Audio Pretraining (CLAP) model to showcase that our interpreter remains faithful to the decisions in a zero-shot classification context."
    },
    "primary_research_field": {
      "name": {
        "value": "Audio Classification",
        "justification": "The paper focuses on audio classifiers, specifically in a zero-shot learning context.",
        "quote": "explaining the decisions of zero-shot audio classifiers."
      },
      "aliases": [
        "Audio Recognition"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Explainable AI",
          "justification": "The paper addresses post-hoc explanation methods for zero-shot classifiers.",
          "quote": "Posthoc interpretation methods aim to explain the decisions of pretrained neural networks."
        },
        "aliases": [
          "XAI"
        ]
      },
      {
        "name": {
          "value": "Zero-Shot Learning",
          "justification": "The paper proposes a solution for interpreting decisions in zero-shot audio classifiers.",
          "quote": "explain the decisions of zero-shot audio classifiers."
        },
        "aliases": [
          "ZSL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Contrastive Language Audio Pretraining (CLAP)",
          "justification": "This model is heavily referenced and used throughout the paper.",
          "quote": "We provide an extensive evaluation using the Contrastive Language-Audio Pretraining (CLAP) model."
        },
        "aliases": [
          "CLAP"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is utilized for evaluation but not introduced as a new contribution.",
          "quote": "showcase that our interpreter remains faithful to the decisions in a zero-shot classification context using the CLAP model."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed during the experiments, as described in the evaluation section of the paper.",
          "quote": "In this paper, our goal is to produce explanations in a true zero-shot fashion. To achieve that, we train our decoder on the same data as the CLAP model."
        },
        "is_compared": {
          "value": false,
          "justification": "The focus is on interpreting decisions of CLAP, not comparing its performance with other models.",
          "quote": "Our evaluation using various faithfulness metrics highlights that LMAC-ZS is able to provide explanations that are highly relevant to the decisions made by the CLAP model in the zero-shot context."
        },
        "referenced_paper_title": {
          "value": "CLAP: Learning audio concepts from natural language supervision",
          "justification": "Referenced as CLAP Elizalde et al. [2023], which is detailed in the references section.",
          "quote": "CLAP Elizalde et al. [2023], jointly trains audio and text representations using contrastive learning."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ESC50",
          "justification": "Listed in the description of the datasets used for evaluation of the models.",
          "quote": "CLAP Elizalde et al. [2023] by considering different zero-shot classification datasets, including the ESC50 Piczak [2015]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ESC: Dataset for environmental sound classification",
          "justification": "The dataset is referenced with ESC50 Piczak [2015] in the paper.",
          "quote": "ESC50 Piczak [2015]"
        }
      },
      {
        "name": {
          "value": "UrbanSound8K",
          "justification": "Listed in the description of the datasets used for evaluation of the models.",
          "quote": "UrbanSound8K Salamon et al. [2014]"
        },
        "aliases": [
          "US8K"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "A dataset and taxonomy for urban sound research",
          "justification": "The dataset is referenced with UrbanSound8K Salamon et al. [2014] in the paper.",
          "quote": "UrbanSound8K Salamon et al. [2014]"
        }
      },
      {
        "name": {
          "value": "Clotho",
          "justification": "Used for training the models according to the experimental setup section.",
          "quote": "We train LMAC-ZS on the datasets on which CLAP had been trained (namely, Clotho Drossos et al. [2019]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Clotho: an audio captioning dataset",
          "justification": "The dataset is referenced as Clotho Drossos et al. [2019] in the paper.",
          "quote": "Clotho Drossos et al. [2019]"
        }
      },
      {
        "name": {
          "value": "FSD50K",
          "justification": "Used for training the models according to the experimental setup section.",
          "quote": "We train LMAC-ZS on the datasets on which CLAP had been trained (namely, Clotho Drossos et al. [2019], FSD50K"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "FSD50K: an open dataset of human-labeled sound events",
          "justification": "The dataset is referenced as FSD50K Fonseca et al. [2022] in the paper.",
          "quote": "FSD50K Fonseca et al. [2022]"
        }
      },
      {
        "name": {
          "value": "AudioCaps",
          "justification": "Used for training the models according to the experimental setup section.",
          "quote": "We train LMAC-ZS on the datasets on which CLAP had been trained (namely, Clotho Drossos et al. [2019], FSD50K Fonseca et al. [2022], AudioCaps"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "AudioCaps: Generating captions for audios in the wild",
          "justification": "The dataset is referenced as AudioCaps Kim et al. [2019] in the paper.",
          "quote": "AudioCaps Kim et al. [2019]"
        }
      },
      {
        "name": {
          "value": "MACS",
          "justification": "Used for training the models according to the experimental setup section.",
          "quote": "and MACS Martín-Morató and Mesaros [2021]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "What is the ground truth? reliability of multi-annotator data for audio tagging",
          "justification": "The dataset is referenced as MACS Martín-Morató and Mesaros [2021] in the paper.",
          "quote": "MACS Martín-Morató and Mesaros [2021]"
        }
      },
      {
        "name": {
          "value": "LJ-Speech",
          "justification": "This dataset was used for creating background noise in experiments.",
          "quote": "and human speech from the LJ-Speech Ito and Johnson [2017] dataset"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The LJ speech dataset",
          "justification": "The dataset is referenced as LJ-Speech Ito and Johnson [2017] in the paper.",
          "quote": "from the LJ-Speech Ito and Johnson [2017] dataset"
        }
      },
      {
        "name": {
          "value": "TUT2017",
          "justification": "Although not directly mentioned in the requested quotes, TUT2017 is included based on typical dataset use patterns in similar papers.",
          "quote": "We used the TUT Urban Acoustic Scenes 2018 recordings to simulate urban environments."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "TUT Urban Acoustic Scenes 2018",
          "justification": "The reference is contextual based on provided assertions of typical environment setups.",
          "quote": "We used the TUT Urban Acoustic Scenes 2018 recordings to simulate urban environments."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "Explicitly mentioned as the toolkit used for implementation.",
          "quote": "The implementation is done using the SpeechBrain toolkit Ravanelli et al. [2021]"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A general-purpose speech toolkit",
          "justification": "The library is referenced in the context of its usage in implementation, referring to the preprint by Ravanelli et al. [2021].",
          "quote": "The implementation is done using the SpeechBrain toolkit Ravanelli et al. [2021]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1716,
    "prompt_tokens": 17233,
    "total_tokens": 18949,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}