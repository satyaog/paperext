{
  "paper": "e1e2ff771089ed195cfe4b29c5e38caa.txt",
  "words": 8210,
  "extractions": {
    "title": {
      "value": "Findings of the 1st Shared Task on Multi-lingual Multi-task Information Retrieval at MRL 2023",
      "justification": "The title is clearly stated at the beginning of the document.",
      "quote": "Findings of the 1st Shared Task on Multi-lingual Multi-task Information Retrieval at MRL 2023"
    },
    "description": "This paper presents the findings from the first shared task focused on multi-lingual multi-task information retrieval conducted at the MRL 2023, emphasizing the capability and fairness of leading large language models (LLMs) in information retrieval across seven low-resource languages.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves evaluation and analysis of large language models across multiple languages, providing empirical data on model performance.",
      "quote": "Our evaluation of leading LLMs reveals that, despite their competitive performance, they still have notable weaknesses..."
    },
    "primary_research_field": {
      "name": {
        "value": "Information Retrieval",
        "justification": "The main focus of the paper is on information retrieval using large language models.",
        "quote": "By organizing the 1st Shared Task on Multi-lingual Multi-task Information Retrieval (MMIR), we aim to provide a common means where multi-lingual LLMs can be evaluated..."
      },
      "aliases": [
        "MMIR"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Named Entity Recognition",
          "justification": "Named Entity Recognition (NER) is explicitly mentioned as one of the key tasks evaluated in the shared task.",
          "quote": "We pick 7 languages...and produce annotations in two tasks crucial for IR: named entity recognition (NER)..."
        },
        "aliases": [
          "NER"
        ]
      },
      {
        "name": {
          "value": "Reading Comprehension",
          "justification": "Reading Comprehension (RC) is the other key task evaluated in the shared task.",
          "quote": "...named entity recognition (NER) and reading comprehension (RC)."
        },
        "aliases": [
          "RC"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "MT-0",
          "justification": "MT-0 is mentioned as one of the prominent large language models evaluated in the study.",
          "quote": "...including prominent LLMs trained on multi-lingual multi-task settings: MT-0 (Muennighoff et al., 2022) and GPT-4 (OpenAI, 2023a), in addition to the system submissions."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is referred to but not introduced as a novel contribution of this paper.",
          "quote": "MT-0 is the open-source multi-lingual multi-task model developed by Big Science (Muennighoff et al., 2022)."
        },
        "is_executed": {
          "value": true,
          "justification": "MT-0 was executed as part of the evaluation process in the shared task.",
          "quote": "We use the mT0-large version of the model..."
        },
        "is_compared": {
          "value": true,
          "justification": "MT-0 was compared with other models in the performance evaluation.",
          "quote": "...in addition to the system submissions."
        },
        "referenced_paper_title": {
          "value": "Crosslingual generalization through multitask finetuning",
          "justification": "The referenced paper that initially introduced the MT0 model is mentioned.",
          "quote": "MT-0 is the open-source multi-lingual multi-task model developed by Big Science (Muennighoff et al., 2022)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "XTREME-UP",
          "justification": "XTREME-UP is explicitly mentioned as a dataset used for evaluation in the paper.",
          "quote": "Recently, initiatives for creating standardized benchmarks for evaluating natural language processing (NLP) systems...had been proposed by corpora like XTREME (Hu et al., 2020) and XTREME-UP (Ruder et al., 2023)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages",
          "justification": "The referenced paper for XTREME-UP is acknowledged as the source defining the dataset.",
          "quote": "XTREME-UP (Ruder et al., 2023)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 849,
    "prompt_tokens": 17792,
    "total_tokens": 18641,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}