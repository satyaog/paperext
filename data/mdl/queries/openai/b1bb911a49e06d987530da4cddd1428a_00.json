{
  "paper": "b1bb911a49e06d987530da4cddd1428a.txt",
  "words": 23909,
  "extractions": {
    "title": {
      "value": "When is an Embedder More Promising than Another?",
      "justification": "The title directly mentions the key focus of the paper, which is to evaluate when one embedding model is more promising than another.",
      "quote": "When is an Embedder More Promising than Another?"
    },
    "description": "This paper presents a unified approach to evaluate embedder models, establishing theoretical foundations and introducing a task-agnostic and self-supervised ranking procedure based on the concept of information sufficiency. The research aims to overcome the limitations of embedding model evaluation by formulating mathematical tools for comparison, applicable in fields like natural language processing and molecular biology.",
    "type": {
      "value": "theoretical",
      "justification": "The paper establishes theoretical foundations for comparing embedding models and formalizes the problem using concepts from information theory.",
      "quote": "First, we establish theoretical foundations for comparing embedding models, drawing upon the concepts of sufficiency and informativeness."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper focuses on embedding models, which are a central technique in machine learning applied to various fields.",
        "quote": "Embeddings are a prominent tool in machine learning and are used in multiple fields, such as natural language processing, computer vision or bioinformatics."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "NLP is explicitly mentioned as one of the fields where embeddings and their evaluation are applied.",
          "quote": "We demonstrate experimentally that our approach aligns closely with the capability of embedding models to facilitate various downstream tasks in both natural language processing and molecular biology."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Molecular Biology",
          "justification": "The paper discusses the application of their embedder evaluation approach in molecular biology.",
          "quote": "We demonstrate experimentally that our approach aligns closely with the capability of embedding models to facilitate various downstream tasks in both natural language processing and molecular biology."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "LLaMA",
          "justification": "LLaMA is mentioned as one of the modern language models from which embedders are derived, highlighting its relevance in the context of the paper.",
          "quote": "We included embedders derived from modern LLM such as LLaMA, Mistral, Gemma, Croissant and T5 encoders."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "LLaMA is mentioned as an existing model used for embedding but not as a contribution of this paper.",
          "quote": "We included embedders derived from modern LLM such as LLaMA, Mistral, Gemma, Croissant and T5 encoders."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not specify whether LLaMA was executed; it only mentions the use of its derived embedders.",
          "quote": "Our experimental protocol is divided into three main steps."
        },
        "is_compared": {
          "value": true,
          "justification": "LLaMA-derived embedders are compared in ranking their performance in terms of the new evaluation method proposed in the paper.",
          "quote": "Embedding evaluation is mainly performed based on a limited set of downstream tasks for which the embeddings are used as inputs to smaller models."
        },
        "referenced_paper_title": {
          "value": "Llama: Open foundation and fine-tuned chat models",
          "justification": "The paper mentions using embedders from this known large language model.",
          "quote": "LLaMA 2: Open foundation and fine-tuned chat models"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 709,
    "prompt_tokens": 49432,
    "total_tokens": 50141,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}