{
  "paper": "d02ddf7dce7e2c5cfbfb3778b7a4a866.txt",
  "words": 26625,
  "extractions": {
    "title": {
      "value": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
      "justification": "The title captures the essence of the research, which focuses on improving the efficiency of pre-training large language models through continual learning strategies.",
      "quote": "Simple and Scalable Strategies to Continually Pre-train Large Language Models"
    },
    "description": "This paper presents methods for efficiently continuing the pre-training of large language models (LLMs) without starting from scratch, by using strategies such as learning rate re-warming, re-decaying, and replay of previous data. These strategies are shown to match the performance of full re-training while using significantly less compute, across experiments involving English and German datasets.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts empirical studies and experiments to measure the performance of various continual learning strategies on large language models, making it an empirical research paper.",
      "quote": "To answer this question, we conduct a large-scale empirical study of continual learning techniques for LLM pre-training."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The research focuses on optimizing the pre-training of large language models, which is a subfield of machine learning.",
        "quote": "Large language models (LLMs) are at the center of all these improvements, providing an intuitive means for humans to interface with machine learning algorithms through language."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The main contribution of the paper is to provide scalable continual learning strategies for large language models.",
          "quote": "To avoid complete re-training, we explore simple and scalable continual learning strategies for continuing to pre-train LLMs."
        },
        "aliases": [
          "CL"
        ]
      },
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper revolves around improving the pre-training of large language models, which is a key area within natural language processing.",
          "quote": "Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Neural Network Optimization",
          "justification": "The paper discusses optimization techniques like learning rate schedules for better pre-training of neural networks.",
          "quote": "...alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-NeoX",
          "justification": "GPT-NeoX was specifically mentioned as a model used for experiments in the paper.",
          "quote": "Using GPT-NeoX (Andonian et al., 2021) based on Megatron-DeepSpeed."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "While GPT-NeoX was used in the research, it wasn't contributed as a new model.",
          "quote": "Using GPT-NeoX (Andonian et al., 2021) based on Megatron-DeepSpeed."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was actively used in experiments carried out in the research.",
          "quote": "Using GPT-NeoX (Andonian et al., 2021) based on Megatron-DeepSpeed."
        },
        "is_compared": {
          "value": false,
          "justification": "There is no indication that GPT-NeoX was specifically compared to other models in the context of the research's findings.",
          "quote": "Using GPT-NeoX (Andonian et al., 2021) based on Megatron-DeepSpeed."
        },
        "referenced_paper_title": {
          "value": "GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch",
          "justification": "The referenced paper is the original publication of the GPT-NeoX model.",
          "quote": "Using GPT-NeoX (Andonian et al., 2021) based on Megatron-DeepSpeed."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SlimPajama",
          "justification": "The SlimPajama dataset was used for continual pre-training experiments in the research as a major dataset for experiments.",
          "quote": "We use three datasets for training and validation: SlimPajama (Soboleva et al., 2023)..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SlimPajama: A 627B token cleaned and deduplicated version of RedPajama",
          "justification": "The referenced paper title describes the dataset used in the experiments.",
          "quote": "We use three datasets for training and validation: SlimPajama (Soboleva et al., 2023)..."
        }
      },
      {
        "name": {
          "value": "German Common Crawl",
          "justification": "It was used as a dataset to show the performance of continual learning strategies across a stronger distribution shift.",
          "quote": "We select the German Common Crawl for a stronger distribution shift in our experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Towards better structured and less noisy web data: Oscar with register annotations",
          "justification": "The referenced paper details aspects of the German Common Crawl dataset.",
          "quote": "We select the German Common Crawl for a stronger distribution shift in our experiments."
        }
      },
      {
        "name": {
          "value": "Pile",
          "justification": "The Pile dataset was extensively used for pre-training and comparison in the experiments.",
          "quote": "The Pile dataset comes pre-shuffled and mixed."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Pile: An 800GB Dataset of Diverse Text for Language Modeling",
          "justification": "The Pile is referenced as a major dataset used for pre-training in the study.",
          "quote": "The Pile dataset comes pre-shuffled and mixed."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "HuggingFace",
          "justification": "HuggingFace is mentioned as a source for getting pre-trained models for continual learning.",
          "quote": "We note that this setting assumes that a pre-trained model is available (e.g., via HuggingFace hub or an in-house model designed to be continually pre-trained)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Hugging Face Datasets: A Community Library for Natural Language Processing",
          "justification": "The referenced paper is centered around the tools developed by HuggingFace for NLP applications.",
          "quote": "We note that this setting assumes that a pre-trained model is available (e.g., via HuggingFace hub or an in-house model designed to be continually pre-trained)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1294,
    "prompt_tokens": 48551,
    "total_tokens": 49845,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}