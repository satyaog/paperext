{
  "paper": "c91548055100e4b35c44076f8edd96ae.txt",
  "words": 11038,
  "extractions": {
    "title": {
      "value": "Using Unity to Help Solve Reinforcement Learning",
      "justification": "The title is clearly mentioned at the beginning of the document as 'Using Unity to Help Solve Reinforcement Learning'. The paper focuses on the development of the United Unity Universe (U3) using Unity for reinforcement learning environments.",
      "quote": "38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks."
    },
    "description": "The paper proposes the United Unity Universe (U3), an open-source ecosystem based on the Unity game engine, aimed at developing reinforcement learning environments. It supports procedural generation of 3D environments for reinforcement learning and provides a pipeline for integration with Python. The paper presents U3 as a tool for creating diverse and scalable environments for training adaptive agents, with the inclusion of datasets to facilitate reinforcement learning research.",
    "type": {
      "value": "empirical",
      "justification": "The paper focuses on the development and empirical evaluation of the United Unity Universe (U3), providing datasets and discussing the practical implementation and usage of procedural generation in reinforcement learning environments.",
      "quote": "We explore the scalability of U3 across 3 dimensions. The x-axis explores the effect of increasing the number of Unity instances running in parallel."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper is centered around the development of environments for reinforcement learning, aiming to enhance the capabilities of agents through U3.",
        "quote": "Leveraging the depth and flexibility of XLand as well as the rapid prototyping features of the Unity engine, we present the United Unity Universe, an open-source toolkit designed to accelerate the creation of innovative reinforcement learning environments."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Meta-Reinforcement Learning",
          "justification": "The paper discusses meta-reinforcement learning as part of the U3's capability in generating environments that support meta-RL tasks.",
          "quote": "Reinforcement learning (RL) and meta-reinforcement learning (meta-RL) foundation models have shown potential as adaptive agents that quickly adapt to diverse tasks in open-ended settings."
        },
        "aliases": [
          "meta-RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Soft Actor-Critic (SAC)",
          "justification": "Soft Actor-Critic is used within the paper to validate and evaluate the learning process in the U3 generated environments.",
          "quote": "We use the U3 ecosystem to implement OpenXLand, an open-source, open-ended RL framework based on the XLand line on work... with clean implementations of RL algorithms such as Soft Actor-Critic (SAC)."
        },
        "aliases": [
          "SAC"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Soft Actor-Critic is used as an existing reinforcement learning method within the paper, and not introduced as a novel contribution.",
          "quote": "We also include a pipeline between U3 and Python based on the Unity ML-Agents Toolkit [14], with clean implementations of RL algorithms such as Soft Actor-Critic (SAC) [11]."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed as part of the experiments to demonstrate U3's capability in reinforcement learning tasks.",
          "quote": "We tested our environment using the Soft Actor-Critic (SAC) implementation available in CleanRL[12]."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper does not focus on comparing Soft Actor-Critic numerically with other models; rather, it uses SAC for experimentation within U3 environments.",
          "quote": "...using SAC on the whole dataset with 8 trials per episode and changing the task each episode."
        },
        "referenced_paper_title": {
          "value": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
          "justification": "This is the reference given for the Soft Actor-Critic model, indicating its prior introduction and use.",
          "quote": "...Soft Actor-Critic (SAC) [11]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "OpenXLand Datasets",
          "justification": "The OpenXLand datasets are supplied as part of U3 to facilitate experimentation and evaluation in the newly defined RL environments.",
          "quote": "We provide 6 datasets of one million pre-generated worlds and 6 datasets of one million pre-generated production rules for a total of 36 trillion possible environment combinations."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Human-timescale adaptation in an open-ended task space",
          "justification": "The datasets are part of the U3 system designed based on principles found in the work around XLand, referenced in the paper.",
          "quote": "...that introduces XLand 2.0, which leverages both diverse world and compositional task generation to create rich environment dynamics."
        }
      },
      {
        "name": {
          "value": "MemoryMaze",
          "justification": "MemoryMaze is mentioned in the discussion on implementing and testing environments in U3 to showcase its flexibility by recreating varying tasks.",
          "quote": "We reimplement MemoryMaze with U3 to demonstrate the frameworkâ€™s extendability in Section 4.4."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Evaluating long-term memory in 3d mazes",
          "justification": "MemoryMaze is referenced regarding its original introduction for testing agent's memory capabilities, which U3 expands on.",
          "quote": "MemoryMaze [18], a framework which generates random 3D mazes to evaluate the long-term memory capabilities of agents."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Unity",
          "justification": "The entire U3 system is based on leveraging Unity for creating RL environments, highlighting its use throughout the paper.",
          "quote": "Inspired by the use of the Unity Game Engine for reinforcement learning development [27, 24, 23], we propose the United Unity Universe (U3), an open-source environment development ecosystem."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Using unity to help solve intelligence",
          "justification": "The referenced paper discusses Unity's application in reinforcement learning, which supports its extensive role in U3.",
          "quote": "Using Unity to Help Solve Intelligence. arXiv preprint arXiv:2011.09294, 2020."
        }
      },
      {
        "name": {
          "value": "Unity ML-Agents Toolkit",
          "justification": "The Unity ML-Agents Toolkit is used to enable communication between Unity environments and Python, supporting RL model integration.",
          "quote": "We also include a pipeline between U3 and Python based on the Unity ML-Agents Toolkit [14], with clean implementations of RL algorithms."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Unity: A general platform for intelligent agents",
          "justification": "The toolkit's reference provides context on its capacity to serve as a bridge for machine learning experiments with Unity.",
          "quote": "Unity ML-Agents Toolkit [14]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1376,
    "prompt_tokens": 17365,
    "total_tokens": 18741,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}