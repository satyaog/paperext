{
  "paper": "d5eca6fa39d93375e6ea34d996085010.txt",
  "words": 6285,
  "extractions": {
    "title": {
      "value": "Knowledge Distillation for Federated Learning: A Practical Guide",
      "justification": "The title explicitly mentioned in the paper header.",
      "quote": "Knowledge Distillation for Federated Learning: a Practical Guide"
    },
    "description": "This paper reviews Knowledge Distillation (KD) methods applied to Federated Learning (FL) to address issues like model and data heterogeneity, communication costs, and privacy concerns. The paper categorizes existing KD-based approaches for FL and provides guidelines for their adoption based on different objectives.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily provides a theoretical review and comparison of existing methods without introducing new empirical results or experiments.",
      "quote": "In this article, we provide a review of the current literature about KD-based approach in FL, with the help of tabular comparisons, following an issue-solutions structure."
    },
    "primary_research_field": {
      "name": {
        "value": "Federated Learning",
        "justification": "The paper focuses on Federated Learning and its integration with Knowledge Distillation techniques.",
        "quote": "Federated Learning (FL) enables the training of Deep Learning models without centrally collecting possibly sensitive raw data."
      },
      "aliases": [
        "FL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Knowledge Distillation",
          "justification": "The paper explores how Knowledge Distillation can be adapted for Federated Learning.",
          "quote": "This article focuses on reviewing federated adaptations of regular Knowledge Distillation (KD)..."
        },
        "aliases": [
          "KD"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "FedAvg",
          "justification": "FedAvg is discussed as a baseline algorithm for Federated Learning.",
          "quote": "Federated Averaging (FedAvg) represents the baseline algorithm for Federated Learning (FL)."
        },
        "aliases": [
          "Federated Averaging"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper discusses FedAvg as an existing method, not as a contribution of this paper.",
          "quote": "Federated Averaging (FedAvg) represents the baseline algorithm for Federated Learning (FL)."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not mention executing FedAvg; it discusses its algorithmic concept.",
          "quote": "Federated Averaging (FedAvg) represents the baseline algorithm for Federated Learning (FL)."
        },
        "is_compared": {
          "value": true,
          "justification": "FedAvg is used as a baseline for comparisons with other KD-based FL algorithms.",
          "quote": "In regards of strategies to mitigate the degradation introduced by non-IIDness, we differentiate..."
        },
        "referenced_paper_title": {
          "value": "Communication-Efficient Learning of Deep Networks from Decentralized Data",
          "justification": "The paper refers to FedAvg as the baseline algorithm in FL literature without introducing it as new work.",
          "quote": "Federated Averaging (FedAvg) represents the baseline algorithm for Federated Learning (FL) [34]."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 574,
    "prompt_tokens": 11845,
    "total_tokens": 12419,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}