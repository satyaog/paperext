{
  "paper": "aea5613d425b027ea82b5b3d6854cd86.txt",
  "words": 4842,
  "extractions": {
    "title": {
      "value": "Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers",
      "justification": "The title is provided at the beginning of the document and describes the main focus of the research.",
      "quote": "Monocular Robot Navigation with Self-Supervised Pretrained Vision Transformers"
    },
    "description": "This paper addresses the challenge of learning a perception model for monocular robot navigation using a minimal number of annotated images. It leverages Vision Transformers (ViT) that are pretrained using a self-supervised method to perform coarse image segmentation in a robot navigation setting. The study highlights the deployment of the model for vision-based navigation tasks like lane following and obstacle avoidance in the Duckietown environment.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experimental validation and quantitative assessments, indicating an empirical study.",
      "quote": "We perform experimental validation by using the resulting segmentations for visual servoing of a real robot in the Duckietown environment."
    },
    "primary_research_field": {
      "name": {
        "value": "Robotics",
        "justification": "The paper's focus is primarily on using deep learning for robotics navigation tasks.",
        "quote": "In this work, we consider the problem of learning an instance segmentation model for monocular robot navigation using few annotated images."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The study employs image segmentation using Vision Transformers, a key area within Computer Vision.",
          "quote": "Powered by architectures adapted to the image domain, like Convolutional Neural Networks (CNN) [1] or Vision Transformers (ViT) [2], deep networks can successfully tackle tasks ranging from classification to dense semantic segmentation."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Vision Transformer (ViT)",
          "justification": "The paper discusses the implementation of Vision Transformers for image segmentation in the task of robot navigation.",
          "quote": "Using a Vision Transformer (ViT) pretrained with a label-free self-supervised method, we successfully train a coarse image segmentation model."
        },
        "aliases": [
          "ViT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses Vision Transformers, a known model architecture, rather than contributing a new architecture.",
          "quote": "Using a Vision Transformer (ViT) pretrained with a label-free self-supervised method, we successfully train a coarse image segmentation model."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed on hardware as part of the experimentation, including GPU and CPU.",
          "quote": "Encouragingly, we find that using only a few transformer blocks is sufficient to achieve good performance."
        },
        "is_compared": {
          "value": true,
          "justification": "The Vision Transformer model is compared to CNN models in the experiments section.",
          "quote": "In Table I, we study the inference speed and quality of the finetuned 1-block and 3-block ViT segmentation models from the previous section. For comparison, we also study backbones built with the first layers of a DINO-pretrained ResNet-50 CNN architecture to perform the same task."
        },
        "referenced_paper_title": {
          "value": "An image is worth 16x16 words: Transformers for image recognition at scale",
          "justification": "This is the foundation paper for Vision Transformers, referenced as [2] in the document.",
          "quote": "ViTs [2] have recently emerged as a competitor to CNNs [1] for computer vision tasks [4]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Duckietown",
          "justification": "The dataset used for the experiments is collected from the Duckietown platform.",
          "quote": "Our dataset is composed of RGB images gathered using the on-board camera of our Duckiebot."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Duckietown: an open, inexpensive and flexible platform for autonomy education and research",
          "justification": "Duckietown is referenced for its platform and usage in the research.",
          "quote": "To assess the performance of our visual-servoing agent, we use the Duckietown platform [24]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "DINO",
          "justification": "DINO is used as a pretrained model for applying Vision Transformers.",
          "quote": "A followup to BYOL is DINO [3], which employs a similar architecture with a different loss."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Emerging properties in self-supervised vision transformers",
          "justification": "This paper is referenced as the source of the DINO method used for pretraining.",
          "quote": "A followup to BYOL is DINO [3], which employs a similar architecture with a different loss."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 932,
    "prompt_tokens": 9277,
    "total_tokens": 10209,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}