{
  "paper": "cb6ddd5473e02e853653f687840bf16f.txt",
  "words": 22445,
  "extractions": {
    "title": {
      "value": "Length Independent PAC-Bayes Bounds for Simple RNNs",
      "justification": "The title is clearly stated at multiple points throughout the document, including the top of the first page and reiterated in the conclusion section.",
      "quote": "Length Independent PAC-Bayes Bounds for Simple RNNs"
    },
    "description": "The paper investigates the conditions required to establish PAC-Bayes bounds for non-linear Recurrent Neural Networks (RNNs) that are independent of the input sequence length. The authors contribute theoretical bounds for different classes of RNNs, including β-saturated and DS β-saturated Simple RNNs (SRNs), using perturbation analysis and PAC-Bayes theory.",
    "type": {
      "value": "theoretical",
      "justification": "The paper extensively discusses the derivation of theoretical results and PAC-Bayes bounds for RNNs rather than conducting empirical evaluations or experiments.",
      "quote": "In this paper, we provide an extensive study on the conditions leading to PAC-Bayes bounds for non-linear RNNs that are independent of the length of the data."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning (Theoretical Foundations)",
        "justification": "The focus is on theoretical aspects and PAC-Bayes bounds relevant to Recurrent Neural Networks, which are foundational topics in deep learning.",
        "quote": "A powerful framework to tackle this question is the one of PAC-Bayes theory, which allows one to derive bounds providing guarantees on the expected performance of learning models on unseen data."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Recurrent Neural Networks",
          "justification": "The paper specifically addresses PAC-Bayes bounds in the context of RNNs and introduces classes like β-saturated and DS β-saturated SRNs.",
          "quote": "In this paper, we provide an extensive study on the conditions leading to PAC-Bayes bounds for non-linear RNNs that are independent of the length of the data."
        },
        "aliases": [
          "RNNs",
          "Simple RNNs",
          "SRNs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Simple Recurrent Neural Networks (SRNs)",
          "justification": "The paper discusses PAC-Bayes bounds for Simple RNNs and defines them rigorously.",
          "quote": "An input sequence of length T , is denoted X ^{T} := {x k } Tk=1 with x k ∈ R ^{u} . Let h 0 ∈ R ^{d} be the initial hidden state. The computation of a SRN is defined recursively by the following equations:"
        },
        "aliases": [
          "SRNs"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Simple RNNs are foundational models in the field and are studied rather than introduced in this paper.",
          "quote": "An input sequence of length T , is denoted X ^{T} := {x k } Tk=1 with x k ∈ R ^{u} . Let h 0 ∈ R ^{d} be the initial hidden state. The computation of a SRN is defined recursively by the following equations:"
        },
        "is_executed": {
          "value": false,
          "justification": "The paper focuses on theoretical insights and derivations rather than executing models experimentally.",
          "quote": "In this paper, we provide an extensive study on the conditions leading to PAC-Bayes bounds for non-linear RNNs that are independent of the length of the data."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper is heavily centered on theoretical derivations and does not perform numerical comparisons with other models.",
          "quote": "We pioneer the question of non stable RNNs by showing that the non-contractiveness of the update map can be compensated by the saturating effect of the squashing recurrent activation function."
        },
        "referenced_paper_title": {
          "value": "Finding structure in time",
          "justification": "This is the original paper where Simple Recurrent Neural Networks (SRNs) were first introduced.",
          "quote": "Simple Recurrent Networks (SRNs) are a class of models designed to process sequential data (Elman, 1990)."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 816,
    "prompt_tokens": 39761,
    "total_tokens": 40577,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}