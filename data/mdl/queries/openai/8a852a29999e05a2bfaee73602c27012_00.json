{
  "paper": "8a852a29999e05a2bfaee73602c27012.txt",
  "words": 22840,
  "extractions": {
    "title": {
      "value": "Post-hoc Interpretability for Neural NLP: A Survey",
      "justification": "The title of the paper is clearly stated on the first page and header of the document.",
      "quote": "Post-hoc Interpretability for Neural NLP: A Survey"
    },
    "description": "This paper surveys methods for interpreting neural networks used in natural language processing, specifically focusing on post-hoc interpretability, which provides explanations after a model has been trained and is generally model-agnostic.",
    "type": {
      "value": "theoretical",
      "justification": "The paper is a survey, which focuses on categorizing and discussing various interpretability methods rather than performing empirical experiments.",
      "quote": "This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper explicitly mentions that it discusses interpretability methods specifically in the context of NLP models.",
        "quote": "Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Interpretability",
          "justification": "The focus of the paper is to survey methods for interpretability in neural NLP models, which is a specific subfield within NLP and machine learning.",
          "quote": "Providing these explanations is often a core motivation for interpretability."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Model-Agnostic Interpretability",
          "justification": "The paper emphasizes post-hoc interpretability which is often model-agnostic, meaning it can be applied to various models after they have been trained.",
          "quote": "Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic."
        },
        "aliases": [
          "Post-hoc Interpretability"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "The paper mentions BERT-like models as prevalent in neural NLP and a target for post-hoc interpretability methods.",
          "quote": "Large neural NLP models, most notably BERT-like models [20, 36, 70], have become highly widespread, both in research and industry applications [133]."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BERT is referenced as an existing model used in the context of the survey and not something newly developed or contributed by the paper.",
          "quote": "Large neural NLP models, most notably BERT-like models..."
        },
        "is_executed": {
          "value": false,
          "justification": "Being a survey, the paper does not execute any models itself.",
          "quote": "This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans."
        },
        "is_compared": {
          "value": false,
          "justification": "The paper discusses BERT in a context setting and not in a quantitative comparison with other models.",
          "quote": "Large neural NLP models, most notably BERT-like models..."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of deep bidirectional transformers for language understanding",
          "justification": "The paper references BERT in several contexts; this is a recognizable key reference for BERT.",
          "quote": "Large neural NLP models, most notably BERT-like models [20, 36, 70], have become highly widespread."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Stanford Sentiment Treebank (SST)",
          "justification": "The SST dataset is mentioned as a commonly used dataset for demonstrating interpretability methods in NLP models.",
          "quote": "The SST dataset has been modeled using LSTM [126], Self-Attention-based models [36], and so on, all of which are popular examples of neural networks."
        },
        "aliases": [
          "SST"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
          "justification": "The SST dataset originates from this reference work, known for introducing the dataset.",
          "quote": "The SST dataset has been modeled using LSTM [126], Self-Attention-based models [36], and so on."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 844,
    "prompt_tokens": 39636,
    "total_tokens": 40480,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}