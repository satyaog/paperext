{
  "paper": "2310.19737.txt",
  "words": 7054,
  "extractions": {
    "title": {
      "value": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "Adversarial Attacks and Defenses in Large Language\nModels: Old and New Threats"
    },
    "description": "The paper discusses the challenges and issues related to adversarial attacks and defenses in large language models (LLMs). It highlights the historical context of flawed defense evaluations in neural networks and applies these lessons to the emerging field of LLMs. Moreover, the paper introduces embedding space attacks as a new threat model and emphasizes the need for accurate defense evaluations specific to LLMs.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves empirical experiments and evaluations, especially in the context of embedding space attacks and defense mechanisms.",
      "quote": "Lastly, we illustrate the necessity of evaluation\nguidelines using a recently proposed defense as an example."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper primarily deals with large language models (LLMs) and discusses attacks and defenses in the context of natural language processing.",
        "quote": "substantial challenges associated with an impending adversarial arms race in natural\nlanguage processing, specifically with closed-source Large Language Models (LLMs)"
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Adversarial Machine Learning",
          "justification": "The paper is deeply focused on adversarial attacks, defense mechanisms, and robustness evaluations, which are key aspects of adversarial machine learning.",
          "quote": "one major impediment has been the overestimation of the robustness of new defense\napproaches due to faulty defense evaluations ... endpoints presented as viable threat models for generating malicious content in open-sourced models."
        },
        "aliases": [
          "Adversarial ML"
        ]
      },
      {
        "name": {
          "value": "Machine Learning Security",
          "justification": "The paper discusses the security implications of adversarial attacks and the necessity of robust defenses in machine learning models.",
          "quote": "Moreover, overconfidence in the robustness of deployed LLMs could have severe\nreal-world consequences, when faulty defenses are deployed in real-world applications."
        },
        "aliases": [
          "ML Security"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Llama2-7b-chat",
          "justification": "The model is explicitly mentioned as a target of embedding space attacks in the experiments.",
          "quote": "with 8.8 forward/backward passes on average, we achieve 100% trigger response rate against Llama2-7b-chat"
        },
        "aliases": [
          "Llama2"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not the authors' contribution but is used for conducting experiments on embedding space attacks.",
          "quote": "with 8.8 forward/backward passes on average, we achieve 100% trigger response rate against Llama2-7b-chat"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the adversarial attack experiments.",
          "quote": "with 8.8 forward/backward passes on average, we achieve 100% trigger response rate against Llama2-7b-chat"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance and robustness of the model are compared in the context of attacks and defenses.",
          "quote": "we demonstrate that open-source LLMs can be triggered into making any\ntype of affirmative response with little computational effort ..."
        },
        "referenced_paper_title": {
          "value": "Llama 2: Open foundation and fine-tuned chat models",
          "justification": "The referenced paper provides the foundational information on the model used in the experiments.",
          "quote": "Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023."
        }
      },
      {
        "name": {
          "value": "ChatGPT",
          "justification": "The paper discusses the security implications and robustness concerns of ChatGPT.",
          "quote": "We provide a first\nset of prerequisites to improve the robustness assessment of new approaches and\nreduce the amount of faulty evaluations. Additionally, we identify embedding space\nattacks on LLMs as another viable threat model for the purposes of generating\nmalicious content in open-sourced models."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model is not the authors' contribution but is used as an example of LLM.",
          "quote": "Specifically, LLM (Large Language Model)\nassistants, such as ChatGPT, are used by millions of users"
        },
        "is_executed": {
          "value": 0,
          "justification": "The model is not executed in the scope of the paper; it is used as an example for discussion.",
          "quote": "Specifically, LLM (Large Language Model)\nassistants, such as ChatGPT, are used by millions of users"
        },
        "is_compared": {
          "value": 1,
          "justification": "The robustness of ChatGPT is discussed in relation to other LLMs and the general adversarial threats.",
          "quote": "Moreover, overconfidence in the robustness of deployed LLMs could have severe\nreal-world consequences, when faulty defenses are deployed in real-world applications."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The referenced paper provides the foundational information on ChatGPT used in the discussion.",
          "quote": "Language models are few-shot learners. In Advances in Neural Information Processing Systems, (NeurIPS), volume 33, 2020."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Harmful Behavior dataset",
          "justification": "The paper mentions this dataset in the context of benchmark evaluations in adversarial attacks.",
          "quote": "Zou et al.\n[2023] propose two narrow benchmark datasets. The Harmful String dataset, where the attacker is\nsupposed to trigger an exact toxic target response string and the Harmful Behavior dataset"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Universal and transferable adversarial attacks on aligned language models",
          "justification": "The referenced paper provides the foundational information for the datasets used.",
          "quote": "Zou et al. [2023] propose two narrow benchmark datasets."
        }
      },
      {
        "name": {
          "value": "Harmful String dataset",
          "justification": "The paper mentions this dataset in the context of benchmark evaluations in adversarial attacks.",
          "quote": "Zou et al.\n[2023] propose two narrow benchmark datasets. The Harmful String dataset, where the attacker is\nsupposed to trigger an exact toxic target response string and the Harmful Behavior dataset"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Universal and transferable adversarial attacks on aligned language models",
          "justification": "The referenced paper provides the foundational information for the datasets used.",
          "quote": "Zou et al. [2023] propose two narrow benchmark datasets."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Metasploit",
          "justification": "The library is mentioned within the context of example code provided by the LLM under adversarial attack.",
          "quote": "The exploit module used\nis \"meterpreter/meterpreter\", which is a payload that allows an attacker\nto execute arbitrary code on the target system."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The referenced paper provides the foundational information for the models discussed which mention Metasploit.",
          "quote": "Language models are few-shot learners. In Advances in Neural Information Processing Systems, (NeurIPS), volume 33, 2020."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1523,
    "prompt_tokens": 11837,
    "total_tokens": 13360
  }
}