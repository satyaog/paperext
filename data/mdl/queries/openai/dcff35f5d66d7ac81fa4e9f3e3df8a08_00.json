{
  "paper": "dcff35f5d66d7ac81fa4e9f3e3df8a08.txt",
  "words": 11955,
  "extractions": {
    "title": {
      "value": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
      "justification": "The title is clearly presented at the beginning of the paper and reflects the focus on evaluating debiasing techniques for language models.",
      "quote": "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models"
    },
    "description": "This paper evaluates the effectiveness of five debiasing techniques applied to pre-trained language models. The study quantifies their effectiveness using three intrinsic bias benchmarks and examines the impact on language modeling abilities and downstream NLU tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experimental evaluation of different debiasing techniques, examining their effectiveness through empirical testing and measurement.",
      "quote": "In this paper, we perform an empirical survey of the effectiveness of five recently proposed debiasing techniques for pre-trained language models."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on pre-trained language models and debiasing techniques, which are central topics in Natural Language Processing.",
        "quote": "Large pre-trained language models have proven effective across a variety of tasks in natural language processing, often obtaining state of the art performance."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Bias Mitigation",
          "justification": "The primary focus of the paper is on the effectiveness of bias mitigation techniques for pre-trained language models.",
          "quote": "Recent work has shown pre-trained language models capture social biases... This has attracted attention to developing techniques that mitigate such biases."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Language Modeling",
          "justification": "The paper evaluates the impact of debiasing techniques on language modeling ability.",
          "quote": "We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model’s language modeling ability."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is one of the pre-trained language models investigated for its bias and mitigation effectiveness in the paper.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses existing models like BERT for evaluation, rather than contributing new models.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper conducts experiments using BERT to evaluate debiasing techniques.",
          "quote": "We evaluate debiased BERT... models against SEAT, StereoSet, and CrowS-Pairs."
        },
        "is_compared": {
          "value": true,
          "justification": "BERT is compared with other models (ALBERT, RoBERTa, GPT-2) on bias benchmarks.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2)."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "BERT as used in this paper is based on the work of Devlin et al., 2019, which is most commonly associated with the original BERT paper.",
          "quote": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota."
        }
      },
      {
        "name": {
          "value": "ALBERT",
          "justification": "ALBERT is identified as one of the key models whose debiasing effectiveness is evaluated in the paper.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses existing models like ALBERT for evaluation, rather than contributing new models.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper conducts experiments using ALBERT to evaluate debiasing techniques.",
          "quote": "We evaluate debiased... ALBERT... models against SEAT, StereoSet, and CrowS-Pairs."
        },
        "is_compared": {
          "value": true,
          "justification": "ALBERT is compared with other models (BERT, RoBERTa, GPT-2) on bias benchmarks.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2)."
        },
        "referenced_paper_title": {
          "value": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
          "justification": "ALBERT is typically credited to Lan et al., 2019, as outlined in the paper's references.",
          "quote": "An ALBERT model is a lighter version of BERT intended to train more efficiently (Turc et al., 2019), evaluated for bias."
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is another key model whose performance on debiasing tasks is empirically evaluated in the paper.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa)"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses existing models like RoBERTa for evaluation, rather than contributing new models.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper conducts experiments using RoBERTa to evaluate debiasing techniques.",
          "quote": "We evaluate debiased... RoBERTa... models against SEAT, StereoSet, and CrowS-Pairs."
        },
        "is_compared": {
          "value": true,
          "justification": "RoBERTa is compared with other models (BERT, ALBERT, GPT-2) on bias benchmarks.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2)."
        },
        "referenced_paper_title": {
          "value": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "justification": "RoBERTa as used in this paper is based on the work of Liu et al., 2019, specifically associated with RoBERTa.",
          "quote": "RoBERTa is another model we investigate, cited from Liu et al., 2019."
        }
      },
      {
        "name": {
          "value": "GPT-2",
          "justification": "GPT-2 is included as one of the models tested for debiasing efficacy in the study.",
          "quote": "We investigate mitigating gender, racial, and religious biases... in an autoregressive language model (GPT-2)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses existing models like GPT-2 for evaluation, rather than contributing new models.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper conducts experiments using GPT-2 to evaluate debiasing techniques.",
          "quote": "We evaluate debiased... GPT-2 models against SEAT, StereoSet, and CrowS-Pairs."
        },
        "is_compared": {
          "value": true,
          "justification": "GPT-2 is compared with other models (BERT, ALBERT, RoBERTa) on bias benchmarks.",
          "quote": "We investigate mitigating gender, racial, and religious biases in three masked language models (BERT, ALBERT, and RoBERTa) and an autoregressive language model (GPT-2)."
        },
        "referenced_paper_title": {
          "value": "Language Models are Unsupervised Multitask Learners",
          "justification": "GPT-2 is based on Radford et al., 2019, often associated with GPT-2.",
          "quote": "We investigate... an autoregressive language model (GPT-2)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Sentence Encoder Association Test (SEAT)",
          "justification": "SEAT is a primary benchmark used in the paper to test the bias in models.",
          "quote": "We evaluate debiased models against three intrinsic bias benchmarks: the Sentence Encoder Association Test (SEAT)..."
        },
        "aliases": [
          "SEAT"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "On Measuring Social Biases in Sentence Encoders",
          "justification": "The SEAT benchmark is based on the work by May et al., 2019.",
          "quote": "SEAT (May et al., 2019) as our first intrinsic bias benchmark."
        }
      },
      {
        "name": {
          "value": "StereoSet",
          "justification": "StereoSet is another benchmark used to evaluate bias in language models.",
          "quote": "...improvements on bias benchmarks such as StereoSet by using debiasing strategies are often..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "StereoSet: Measuring stereotypical bias in pre-trained language models",
          "justification": "The StereoSet dataset is based on work by Nadeem et al., 2021.",
          "quote": "StereoSet (Nadeem et al., 2021), a crowdsourced dataset for measuring four types of stereotypical bias in language models."
        }
      },
      {
        "name": {
          "value": "Crowdsourced Stereotype Pairs (CrowS-Pairs)",
          "justification": "CrowS-Pairs is the third benchmark used to test model biases in the paper.",
          "quote": "Crowdsourced Stereotype Pairs (CrowS-Pairs; Nangia et al. 2020) as our third intrinsic bias benchmark."
        },
        "aliases": [
          "CrowS-Pairs"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models",
          "justification": "The CrowS-Pairs dataset is sourced from the work by Nangia et al., 2020.",
          "quote": "CrowS-Pairs (Nangia et al., 2020) as our third intrinsic bias benchmark."
        }
      },
      {
        "name": {
          "value": "WikiText-2",
          "justification": "WikiText-2 is used to measure changes in language modeling ability after debiasing.",
          "quote": "we evaluate debiased models against WikiText-2 (Merity et al., 2017) and..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pointer Sentinel Mixture Models",
          "justification": "WikiText-2 is attributed to Merity et al., 2017, as indicated in its use in evaluations.",
          "quote": "we evaluate debiased models against WikiText-2 (Merity et al., 2017)"
        }
      },
      {
        "name": {
          "value": "General Language Understanding Evaluation (GLUE)",
          "justification": "GLUE is used to test the impact of debiasing techniques on downstream NLU task performance.",
          "quote": "we evaluate debiased models against... the General Language Understanding Evaluation (GLUE; Wang and Cho 2019) benchmark."
        },
        "aliases": [
          "GLUE"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
          "justification": "GLUE is associated with Wang and Cho, 2019, noted for its multi-task benchmarking capabilities.",
          "quote": "we evaluate debiased models against... the General Language Understanding Evaluation (GLUE; Wang and Cho 2019) benchmark."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Hugging Face Transformers",
          "justification": "The paper mentions using the Hugging Face Transformers library for implementing debiasing techniques.",
          "quote": "We make use of the Hugging Face Transformers... libraries in the implementations of our debiasing techniques."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-Art Natural Language Processing",
          "justification": "Hugging Face Transformers is linked to the work by Wolf et al., 2020.",
          "quote": "We make use of the Hugging Face Transformers (Wolf et al., 2020) and Datasets (Lhoest et al., 2021) libraries"
        }
      },
      {
        "name": {
          "value": "Hugging Face Datasets",
          "justification": "The paper utilizes the Hugging Face Datasets library as part of its experimental setup for debiasing.",
          "quote": "We make use of the Hugging Face Transformers... and Datasets libraries in the implementations of our debiasing techniques."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Datasets: A Community Library for Natural Language Processing",
          "justification": "Hugging Face Datasets is attributed to Lhoest et al., 2021, in the context of this study.",
          "quote": "We make use of the Hugging Face Transformers (Wolf et al., 2020) and Datasets (Lhoest et al., 2021) libraries"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2832,
    "prompt_tokens": 25887,
    "total_tokens": 28719,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}