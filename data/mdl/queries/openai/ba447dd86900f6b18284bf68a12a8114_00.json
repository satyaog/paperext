{
  "paper": "ba447dd86900f6b18284bf68a12a8114.txt",
  "words": 11401,
  "extractions": {
    "title": {
      "value": "Towards Geographic Inclusion in the Evaluation of Text-to-Image Models",
      "justification": "The title is explicitly mentioned at the beginning of the document and encapsulates the main theme of the paper, which is the geographic inclusivity in evaluating text-to-image models.",
      "quote": "Towards Geographic Inclusion in the Evaluation of Text-to-Image Models"
    },
    "description": "The paper explores the biases and disparities in text-to-image generative models by conducting a large, cross-cultural study involving annotators from different geographic regions. It evaluates the performance of generative models based on human and automated metrics, particularly focusing on how geographic representation, visual appeal, and consistency are perceived differently across regions. The study contrasts human annotations with automated metrics to reveal the diversity of human preferences and biases in generated images. Recommendations for improving evaluations of text-to-image models are also included.",
    "type": {
      "value": "empirical",
      "justification": "The paper is based on a cross-cultural study collecting annotations from humans and comparing these with existing automated metrics, focusing on empirical evidence.",
      "quote": "In this work, we conduct a large, cross-cultural study to study how much annotators in Africa, Europe, and Southeast Asia vary in their perception of geographic representation, visual appeal, and consistency in real and generated images..."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The study is focused on text-to-image generative models, which fall under the domain of Computer Vision as it involves image generation and evaluation.",
        "quote": "Computing methodologies â†’ Computer vision;"
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text-to-Image Generation",
          "justification": "The paper specifically discusses the evaluation of text-to-image generative models.",
          "quote": "Generative models for visual content creation, with works achieving impressively photorealistic image generations."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Performance Evaluation",
          "justification": "The paper includes a thorough evaluation of model performance based on human and automated metrics.",
          "quote": "In pursuit of models that generate images that are realistic, diverse, visually appealing, and consistent with the given prompt, researchers and practitioners often turn to automated metrics..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "LDM 2.1",
          "justification": "The paper mentions and uses LDM 2.1 as one of the state-of-the-art generative models evaluated in their study.",
          "quote": "We also use a latent diffusion model trained on a public dataset of approximately 5 billion images, excluding explicit material [46], which we refer to as 'LDM 2.1.'"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is referenced and used, not developed or contributed by the authors in this paper.",
          "quote": "We also use a latent diffusion model... referred to as 'LDM 2.1.'"
        },
        "is_executed": {
          "value": true,
          "justification": "The model is used to generate images for the evaluation conducted in the study.",
          "quote": "Generated images from DM w/ CLIP and LDM 2.1 using the prompt {object} in {region}."
        },
        "is_compared": {
          "value": true,
          "justification": "LDM 2.1 is compared with human annotations and other models to study geographic representation, visual appeal, and consistency.",
          "quote": "The study includes... images created by two state-of-the-art generative systems."
        },
        "referenced_paper_title": {
          "value": "High-Resolution Image Synthesis with Latent Diffusion Models",
          "justification": "This is the title of the reference [46] mentioned in context with LDM 2.1.",
          "quote": "...latent diffusion model trained on a public dataset of approximately 5 billion images, excluding explicit material [46]."
        }
      },
      {
        "name": {
          "value": "DM w/ CLIP",
          "justification": "The paper introduces and utilizes this model as a multi-modal implementation leveraging CLIP image embeddings for generating images.",
          "quote": "We also include generated images from a multi-modal implementation of a generative pre-trained transformer leveraging CLIP image embeddings. This model, which we call 'DM w/ CLIP'."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "The model is mentioned as being implemented for the study, suggesting some form of contribution.",
          "quote": "This model, which we call 'DM w/ CLIP.'"
        },
        "is_executed": {
          "value": true,
          "justification": "DM w/ CLIP is used to generate images that are evaluated in the study.",
          "quote": "Generated images from DM w/ CLIP and LDM 2.1 using the prompt {object} in {region}."
        },
        "is_compared": {
          "value": true,
          "justification": "DM w/ CLIP is evaluated against human annotations and compared to other models like LDM 2.1.",
          "quote": "The study includes... images created by two state-of-the-art generative systems."
        },
        "referenced_paper_title": {
          "value": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
          "justification": "This title corresponds to the paper [43] associated with CLIP-based image generation, as cited in the context.",
          "quote": "Hierarchical Text-Conditional Image Generation with CLIP Latents"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GeoDE",
          "justification": "GeoDE is referenced as a diverse, geographically representative dataset of images used in the study.",
          "quote": "Our tasks include real images from GeoDE [42], a diverse, geographically representative dataset of images taken across multiple regions."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Beyond web-scraping: Crowd-sourcing a geodiverse dataset",
          "justification": "The mentioned reference [42] provides the title of the paper related to GeoDE.",
          "quote": "real images from GeoDE [42], a diverse, geographically representative dataset."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is used as a modern feature extractor to correlate better with human judgment in the study.",
          "quote": "We also include generated images from a multi-modal implementation of a generative pre-trained transformer leveraging CLIP image embeddings."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "This corresponds to the CLIP reference [41] used for image embedding and comparison in the study.",
          "quote": "the older, more ubiquitous Inceptionv3 [55] compares to the more recent CLIP ViT-B/32 [41] and DINO ViT-L/14 [36] feature extractors."
        }
      },
      {
        "name": {
          "value": "DINOv2",
          "justification": "DINOv2 is used as a feature extractor for comparing image similarity based on human judgment, as discussed in the study.",
          "quote": "We study how the older, more ubiquitous Inceptionv3 [55] compares to the more recent CLIP ViT-B/32 [41] and DINO ViT-L/14 [36] feature extractors trained on larger data sources."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "DINOv2: Learning Robust Visual Features without Supervision",
          "justification": "The referenced paper [36] corresponds to the use of DINOv2 in the study.",
          "quote": "We study how the older, more ubiquitous Inceptionv3 [55] compares to the more recent CLIP ViT-B/32 [41] and DINO ViT-L/14 [36] feature extractors."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1525,
    "prompt_tokens": 19676,
    "total_tokens": 21201,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}