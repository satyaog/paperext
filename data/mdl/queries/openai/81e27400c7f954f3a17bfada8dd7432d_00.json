{
  "paper": "81e27400c7f954f3a17bfada8dd7432d.txt",
  "words": 11721,
  "extractions": {
    "title": {
      "value": "Feeding What You Need by Understanding What You Learned",
      "justification": "The title is mentioned at the very beginning of the paper right after the author list.",
      "quote": "Feeding What You Need by Understanding What You Learned"
    },
    "description": "This paper focuses on improving Machine Reading Comprehension (MRC) by assessing model capabilities and using the assessment in a curriculum learning framework to enhance training efficiency. The authors propose a multi-dimensional capability assessment framework and a curriculum strategy named Capability Boundary Breakthrough Curriculum (CBBC). They conduct extensive experiments demonstrating significant improvement over traditional methods.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments to demonstrate performance improvements, indicating it is empirical research.",
      "quote": "Extensive experiments demonstrate that our approach significantly improves performance, achieving up to an 11.22% / 8.71% improvement of EM / F 1 on MRC tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper is focused on Machine Reading Comprehension, a task within Natural Language Processing.",
        "quote": "Machine Reading Comprehension (MRC) is a core task in natural language processing (NLP) that aims to teach machines to understand human languages and answer questions."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Reading Comprehension",
          "justification": "The specific task addressed in the paper is Machine Reading Comprehension (MRC).",
          "quote": "Machine Reading Comprehension (MRC) reveals the ability to understand a given text passage and answer questions based on it."
        },
        "aliases": [
          "MRC"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is mentioned as a backbone model used for the experiments in the paper.",
          "quote": "We use BERT-base (Devlin et al., 2018) as our backbone model."
        },
        "aliases": [
          "BERT-base"
        ],
        "is_contributed": {
          "value": false,
          "justification": "BERT is referenced as a pre-existing model utilized in the study, not developed as part of this research.",
          "quote": "We use BERT-base (Devlin et al., 2018) as our backbone model."
        },
        "is_executed": {
          "value": true,
          "justification": "BERT is used as the backbone for experiments, indicating its execution in the paper's scope.",
          "quote": "The learning rate warms up over the first 10% steps and then decays linearly to 0 for all experiments with training batch size 16 and maximum iteration 40, 000."
        },
        "is_compared": {
          "value": true,
          "justification": "BERT's performance is compared numerically with other baselines as shown in various experimental results.",
          "quote": "Table 3: Quantitative results on four benchmark datasets. B and C represent the BERT backbone and our CBBC strategy, respectively."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "The referenced paper title for BERT is provided in the list of references.",
          "quote": "Devlin et al., 2018"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SQuADv1",
          "justification": "SQuADv1 is used as one of the datasets for evaluating the models in the paper.",
          "quote": "We employ two question styles to evaluate our CBBC: answer span extraction and multiple choice. The former consists of SQuADv1 (Rajpurkar et al., 2016),"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SQuAD: 100,000+ Questions for Machine Comprehension of Text",
          "justification": "The paper title of SQuADv1 is available in the references,",
          "quote": "Rajpurkar et al., 2016"
        }
      },
      {
        "name": {
          "value": "SQuADv2",
          "justification": "SQuADv2 is used to train and test the models as per the experimental setup.",
          "quote": "we also report the results of scaled F 1 (denoted as F logits ) by taking the model’s confidence to an answer span or candidate into account."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Know What You Don’t Know: Unanswerable Questions for SQuAD",
          "justification": "The referenced paper title for SQuADv2 is provided in the references.",
          "quote": "Rajpurkar et al., 2018"
        }
      },
      {
        "name": {
          "value": "HotpotQA",
          "justification": "HotpotQA is used in the experiments as one of the datasets for testing the models.",
          "quote": "Extensive experiments on four benchmark datasets demonstrate that our approach significantly improves the performance of existing MRC models."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering",
          "justification": "The referenced paper title for HotpotQA is listed in the paper's references.",
          "quote": "Yang et al., 2018"
        }
      },
      {
        "name": {
          "value": "RACE",
          "justification": "RACE is utilized in the paper as a benchmark dataset for evaluating the proposed models.",
          "quote": "the latter adopts RACE (Lai et al., 2017). For each dataset, we train and evaluate the model on official training and dev split, respectively."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "RACE: Large-scale ReAding Comprehension Dataset From Examinations",
          "justification": "The title of the reference paper for RACE is cited in the references section.",
          "quote": "Lai et al., 2017"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1153,
    "prompt_tokens": 22797,
    "total_tokens": 23950,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}