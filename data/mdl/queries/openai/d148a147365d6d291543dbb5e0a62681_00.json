{
  "paper": "d148a147365d6d291543dbb5e0a62681.txt",
  "words": 25862,
  "extractions": {
    "title": {
      "value": "Learning Successor Features the Simple Way",
      "justification": "The title is clearly stated at the beginning of the paper and in the metadata.",
      "quote": "Learning Successor Features the Simple Way"
    },
    "description": "This paper introduces a novel, simple method for learning Successor Features (SFs) directly from pixel observations using a combination of Temporal-difference (TD) loss and reward prediction loss. The authors demonstrate that their approach matches or outperforms existing SF learning techniques in various environments without pretraining.",
    "type": {
      "value": "empirical",
      "justification": "The study conducts experiments to validate the proposed method for learning Successor Features, including results from various RL environments showing the effectiveness of the approach.",
      "quote": "We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Miniworld) mazes and Mujoco, for both single and continual learning scenarios."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The research focuses on improving representation learning in Deep Reinforcement Learning by using Successor Features to handle non-stationary environments.",
        "quote": "In Deep Reinforcement Learning (RL), it is a challenge to learn representations that do not exhibit catastrophic forgetting or interference in non-stationary environments."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper addresses representation collapse in learning Successor Features, which is a topic within representation learning.",
          "quote": "Successor Features (SFs) offer a potential solution to this challenge. However, canonical techniques for learning SFs from pixel-level observations often lead to representation collapse, wherein representations degenerate and fail to capture meaningful variations in the data."
        },
        "aliases": [
          "Rep Learning"
        ]
      },
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The study evaluates the proposed method's effectiveness in continual learning scenarios where environments are dynamic.",
          "quote": "We show that our approach matches or outperforms existing SF learning techniques... for both single and continual learning scenarios."
        },
        "aliases": [
          "CL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Double Deep Q-Network (DQN)",
          "justification": "Double DQN is explicitly mentioned as a baseline model used for comparison in the experiments.",
          "quote": "In all experiments, we make comparisons with several baselines, namely, a Double Deep Q-Network (DQN) agent..."
        },
        "aliases": [
          "DQN"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses Double DQN as a baseline rather than contributing it as a novel model.",
          "quote": "In all experiments, we make comparisons with several baselines, namely, a Double Deep Q-Network (DQN) agent..."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed and used in the experiments for comparison purposes.",
          "quote": "In all experiments, we make comparisons with several baselines, namely, a Double Deep Q-Network (DQN) agent..."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared with the proposed method and other approaches in the results.",
          "quote": "In all experiments, we make comparisons with several baselines, namely, a Double Deep Q-Network (DQN) agent..."
        },
        "referenced_paper_title": {
          "value": "Deep reinforcement learning with double q-learning.",
          "justification": "Double DQN was introduced in a paper titled 'Deep reinforcement learning with double q-learning.'",
          "quote": "Deep reinforcement learning with double q-learning."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Minigrid",
          "justification": "Minigrid is explicitly mentioned as a 2D environment used to validate the proposed method.",
          "quote": "We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid)..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks.",
          "justification": "The paper discusses the use of Minigrid, which is part of the Minigrid & Miniworld environments.",
          "quote": "Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks."
        }
      },
      {
        "name": {
          "value": "Miniworld",
          "justification": "Miniworld is explicitly mentioned as a 3D environment used to validate the proposed method.",
          "quote": "We show that our approach matches or outperforms existing SF learning techniques in both 2D (Minigrid), 3D (Miniworld)..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks.",
          "justification": "The paper discusses the use of Miniworld, which is part of the Minigrid & Miniworld environments.",
          "quote": "Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented tasks."
        }
      },
      {
        "name": {
          "value": "Mujoco",
          "justification": "Mujoco is explicitly mentioned as a complex environment used to test the proposed model's effectiveness in dynamic settings.",
          "quote": "As well, we show that our technique improves continual reinforcement learning in dynamic environments, in both 2D grid worlds and 3D mazes."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Mastering visual continuous control: Improved data-augmented reinforcement learning.",
          "justification": "Mujoco was discussed in the context of learning control from pixels, as referenced in 'Mastering visual continuous control: Improved data-augmented reinforcement learning.'",
          "quote": "Mastering visual continuous control: Improved data-augmented reinforcement learning."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper uses PyTorch for implementing neural network components, specifically mentioned in relation to the URL Benchmark repository.",
          "quote": "In our study, we take inspiration from the neural network architecture from Liu and Abbeel [2021] from the Unsupervised Reinforcement Learning (URL) Benchmark repository[Laskin et al., 2021], which utilizes PyTorch [Paszke et al., 2019]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library.",
          "justification": "PyTorch is acknowledged in its implementation context as described in its reference paper.",
          "quote": "PyTorch: An imperative style, high-performance deep learning library."
        }
      },
      {
        "name": {
          "value": "JAX",
          "justification": "JAX is used in the implementation of the proposed method, as mentioned in the appendix related to software details.",
          "quote": "The agent creation and computational components were developed using Jax [Bradbury et al., 2018, Godwin* et al., 2020]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "JAX: composable transformations of Python+NumPy programs.",
          "justification": "JAX's usage in this paper is based on its functional purpose as indicated in its reference title.",
          "quote": "JAX: composable transformations of Python+NumPy programs."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1447,
    "prompt_tokens": 57898,
    "total_tokens": 59345,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}