{
  "paper": "b7baf4a547588f8d1ca68c33cb3445c4.txt",
  "words": 6229,
  "extractions": {
    "title": {
      "value": "Systematic Generalization by Finetuning? Analyzing Pretrained Language Models Using Constituency Tests",
      "justification": "This is the title of the paper, which discusses the analysis of pretrained language models using constituency tests.",
      "quote": "Systematic Generalization by Finetuning? Analyzing Pretrained Language Models Using Constituency Tests"
    },
    "description": "This paper investigates how different finetuning setups affect the ability of pretrained sequence-to-sequence language models, such as BART and T5, to replicate constituency tests, which involve manipulating sentence constituents. The authors propose multiple evaluation settings and show that while models can replicate linguistic transformations on sentences seen during finetuning, their performance degrades on unseen sentence types, indicating a lack of systematic generalization. The results suggest that models often learn surface-level cues rather than constituent-level syntactic structures.",
    "type": {
      "value": "empirical",
      "justification": "The study involves conducting experiments with pretrained language models and evaluating their performance on various syntactic transformation tasks.",
      "quote": "In our experiments, we systematically vary two dimensions of generalization: i) whether input sentences represent novel syntactic constructions and ii) whether the main verb of the sentence is novel."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper deals with analyzing pretrained language models using linguistic tests, which falls under the domain of Natural Language Processing.",
        "quote": "We design multiple evaluation settings by varying the combinations of constituency tests and sentence types that a model is exposed to during finetuning."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Syntax",
          "justification": "The study involves understanding and testing the syntactic competence of language models using constituency tests.",
          "quote": "We focus on the phenomenon of constituency because of its core role in supporting semantic understanding and natural language inference."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BART",
          "justification": "The paper mentions the use of the BART model for conducting constituency tests and analyzing its performance.",
          "quote": "We evaluate two pretrained sequence-to-sequence transformer models BART (Lewis et al., 2019) and T5 (Raffel et al., 2020)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The BART model itself is not contributed by this paper; rather, it is used for evaluation purposes.",
          "quote": "We evaluate two pretrained sequence-to-sequence transformer models BART (Lewis et al., 2019) and T5 (Raffel et al., 2020)."
        },
        "is_executed": {
          "value": true,
          "justification": "The BART model is executed as part of the experiments conducted in the paper.",
          "quote": "The model is trained using a standard next-word cross-entropy loss function."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper evaluates and compares the performance of BART with other models like T5.",
          "quote": "We evaluate two pretrained sequence-to-sequence transformer models BART (Lewis et al., 2019) and T5 (Raffel et al., 2020)."
        },
        "referenced_paper_title": {
          "value": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "justification": "This is the title of the referenced paper which introduces the BART model used in this study.",
          "quote": "We evaluate two pretrained sequence-to-sequence transformer models BART (Lewis et al., 2019) and T5 (Raffel et al., 2020)."
        }
      },
      {
        "name": {
          "value": "T5",
          "justification": "The paper mentions the use of the T5 model for conducting constituency tests and analyzing its performance.",
          "quote": "We evaluate two pretrained sequence-to-sequence transformer models BART (Lewis et al., 2019) and T5 (Raffel et al., 2020)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The T5 model itself is not contributed by this paper; rather, it is used for evaluation purposes.",
          "quote": "We evaluate two pretrained sequence-to-sequence transformer models BART (Lewis et al., 2019) and T5 (Raffel et al., 2020)."
        },
        "is_executed": {
          "value": true,
          "justification": "The T5 model is executed as part of the experiments conducted in the paper.",
          "quote": "The model is trained using a standard next-word cross-entropy loss function."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper evaluates and compares the performance of T5 with other models like BART.",
          "quote": "We evaluate two pretrained sequence-to-sequence transformer models BART (Lewis et al., 2019) and T5 (Raffel et al., 2020)."
        },
        "referenced_paper_title": {
          "value": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
          "justification": "This is the title of the referenced paper which introduces the T5 model used in this study.",
          "quote": "We evaluate two pretrained sequence-to-sequence transformer models BART (Lewis et al., 2019) and T5 (Raffel et al., 2020)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SCAN",
          "justification": "The SCAN dataset is mentioned as a reference dataset used to demonstrate the generalization capabilities of models.",
          "quote": "Lake and Baroni (2018) show the zero-shot compositional generalization capabilities of several sequence-to-sequence models on a specialized dataset called SCAN."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
          "justification": "This is the title of the referenced paper which discusses the use of the SCAN dataset.",
          "quote": "Lake and Baroni (2018) show the zero-shot compositional generalization capabilities of several sequence-to-sequence models on a specialized dataset called SCAN."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Adam",
          "justification": "The Adam optimization algorithm is used for training the models in the paper.",
          "quote": "Optimization We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 3e − 5 to train all non-head word oracle models and 3e − 6 for the training of HWO models."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A Method for Stochastic Optimization",
          "justification": "This is the title of the referenced paper which introduces the Adam optimization algorithm used in this study.",
          "quote": "Optimization We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 3e − 5 to train all non-head word oracle models and 3e − 6 for the training of HWO models."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1407,
    "prompt_tokens": 11191,
    "total_tokens": 12598
  }
}