{
  "paper": "9c19941f3e97612266f96e56e35da672.txt",
  "words": 11041,
  "extractions": {
    "title": {
      "value": "On the consistency of hyper-parameter selection in value-based deep reinforcement learning",
      "justification": "The title is directly mentioned at the beginning of the paper, identifying the main focus of the research.",
      "quote": "On the consistency of hyper-parameter selection in value-based deep reinforcement learning"
    },
    "description": "This paper evaluates the reliability of hyper-parameter selection in value-based deep reinforcement learning agents. It introduces a new score to assess the consistency and reliability of various hyper-parameters across different training regimes.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts extensive empirical studies on hyper-parameter selection and introduces a new scoring method.",
      "quote": "This paper conducts an extensive empirical study focusing on the reliability of hyper-parameter selection for value-based deep reinforcement learning agents."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Reinforcement Learning",
        "justification": "The paper discusses value-based deep reinforcement learning agents and hyper-parameter selection within this domain.",
        "quote": "Deep reinforcement learning (deep RL) has achieved tremendous success on various domains through a combination of algorithmic design and careful selection of hyper-parameters."
      },
      "aliases": [
        "deep RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Hyper-Parameter Optimization",
          "justification": "The paper is concerned with the optimization of hyper-parameters in deep reinforcement learning models, evaluating their selection process.",
          "quote": "This paper conducts an extensive empirical study focusing on the reliability of hyper-parameter selection for value-based deep reinforcement learning agents."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Data Efficient Rainbow (DER)",
          "justification": "The paper discusses DER as a model developed to outperform SiMPLe, a model mentioned for providing state-of-the-art performance with proper hyper-parameter tuning.",
          "quote": "as an example of this dichotomy, we examine how DER (van Hasselt et al., 2019), a method that has become a common baseline for the Atari 100k benchmark (Kaiser et al., 2019), came to be."
        },
        "aliases": [
          "DER"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model was developed by van Hasselt et al. (2019) and is extensively evaluated in this paper.",
          "quote": "van Hasselt et al. (2019) introduced Data Efficient Rainbow (DER)"
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed on a Tesla P100 GPU for experiments.",
          "quote": "All experiments were run on a Tesla P100 GPU."
        },
        "is_compared": {
          "value": true,
          "justification": "DER is compared to other models like DrQ(ε) in different data regimes and environments.",
          "quote": "optimal hyper-parameters for DrQ(ε) agree quite often with DER..."
        },
        "referenced_paper_title": {
          "value": "When to use parametric models in reinforcement learning",
          "justification": "The model is referenced to van Hasselt et al. (2019), which corresponds to the cited work by the authors in the context.",
          "quote": "as an example of this dichotomy, we examine how DER (van Hasselt et al., 2019)"
        }
      },
      {
        "name": {
          "value": "DrQ(ε)",
          "justification": "The paper evaluates DrQ(ε) alongside DER, emphasizing its optimization for the 100k suite.",
          "quote": "We focus our attention on two value-based agents developed for the Atari 100k suite: DER mentioned above, and DrQ(ε), a variant of DQN that was optimized for the 100k suite."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "DrQ(ε) is a variant optimized based on existing work but not introduced in this paper itself.",
          "quote": "DrQ(ε), a variant of DQN that was optimized for the 100k suite."
        },
        "is_executed": {
          "value": true,
          "justification": "DrQ(ε) is executed similarly to DER for hyper-parameter evaluation and experiments.",
          "quote": "We focus our attention on two value-based agents... DrQ(ε), a variant of DQN that was optimized for the 100k suite."
        },
        "is_compared": {
          "value": true,
          "justification": "The model is compared with DER across data regimes and environments for hyper-parameter selection.",
          "quote": "We find that optimal hyper-parameters for DrQ(ε) agree quite often with DER..."
        },
        "referenced_paper_title": {
          "value": "Deep reinforcement learning at the edge of the statistical precipice",
          "justification": "DrQ(ε) refers back to work by Yarats et al. (2021) and is cited as part of ongoing research improvements.",
          "quote": "(introduced by Agarwal et al. (2021) as an improvement over the DrQ of Yarats et al. (2021))"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari 100k",
          "justification": "The study evaluates models using the Atari 100k benchmark, a common environment to assess algorithms' sample-efficiency.",
          "quote": "DrQ(ε), a variant of DQN that was optimized for the 100k suite."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Model based reinforcement learning for atari",
          "justification": "The dataset's evaluation standard is derived from works by Kaiser et al. (2019) on the SiMPLe algorithm and its competitiveness in sample-efficient environments.",
          "quote": "Kaiser et al. (2019) introduced the SiMPLe algorithm as a sample-efficient method, they argued for evaluating it only on 100k agent actions."
        }
      },
      {
        "name": {
          "value": "Arcade Learning Environment (ALE)",
          "justification": "The ALE is extensively referenced to demonstrate the learning performance of various DRL approaches in structured environments.",
          "quote": "DQN, considered to be the start of the field of DRL research, was introduced by showcasing its superhuman performance on the ALE (Bellemare et al., 2012)."
        },
        "aliases": [
          "ALE"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "The arcade learning environment: An evaluation platform for general agents",
          "justification": "The ALE is frequently cited as a foundational framework for assessing agent performance in DRL benchmarks, referenced as Bellemare et al. (2012).",
          "quote": "DQN, considered to be the start of the field of DRL research, was introduced by showcasing its superhuman performance on the ALE (Bellemare et al., 2012)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Dopamine",
          "justification": "The Dopamine library is utilized to implement the evaluated algorithms DER and DrQ(ε).",
          "quote": "Both algorithms are implemented in the Dopamine library (Castro et al., 2018)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Dopamine: A Research Framework for Deep Reinforcement Learning",
          "justification": "The Dopamine library's reference matches the cited implementation framework for the experiments in the study.",
          "quote": "Both algorithms are implemented in the Dopamine library (Castro et al., 2018)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1432,
    "prompt_tokens": 20833,
    "total_tokens": 22265,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}