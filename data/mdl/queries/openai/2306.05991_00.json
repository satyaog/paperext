{
  "paper": "2306.05991.txt",
  "words": 13867,
  "extractions": {
    "title": {
      "value": "Approximate information state based convergence analysis of recurrent Q-learning",
      "justification": "The title is taken directly from the provided research paper.",
      "quote": "Approximate information state based convergence\nanalysis of recurrent Q-learning"
    },
    "description": "This paper investigates the convergence of recurrent Q-learning (RQL) in the tabular setting for partially observable Markov decision processes (POMDPs). The study introduces and utilizes the concept of approximate information state (AIS) to quantify the quality of the representation and how it affects the convergence quality. The paper proposes a variant of RQL called RQL-AIS, which incorporates AIS losses and performs better than state-of-the-art baselines in numerical experiments on the MiniGrid benchmark.",
    "type": {
      "value": "theoretical",
      "justification": "The paper predominantly focuses on theoretical analysis, including establishing convergence proofs and approximation bounds.",
      "quote": "we establish the convergence of recurrent Q-learning (RQL) in the tabular setting for\nPOMDPs using a representation of the history called an approximate information state (AIS)."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper deals specifically with the convergence analysis of reinforcement learning algorithms, specifically recurrent Q-learning.",
        "quote": "In recent years, Reinforcement Learning (RL) has witnessed many successes such as achieving\nhuman-level performance in Go [Sil+16], learning to play Atari [Mni+13; Mni+15], as well as\nsolving many control problems arising in engineering and robotics [Sch+15b; Sch+17; Haa+18;\nTas+18]."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Partially Observable Markov Decision Processes",
          "justification": "The paper specifically addresses RL in the context of partially observable Markov decision processes (POMDPs).",
          "quote": "In various applications such as finance, healthcare, and robotics, the agent does not observe the full\nstate of the environment. Such partially observed systems are mathematically modeled as partially\nobservable Markov decision processes (POMDPs)."
        },
        "aliases": [
          "POMDP"
        ]
      },
      {
        "name": {
          "value": "Recurrent Neural Networks",
          "justification": "The paper uses recurrent neural networks (RNNs) as function approximators for the Q-function in recurrent Q-learning.",
          "quote": "In this paper, we investigate one of the most popular RL algorithms for POMDPs: Recurrent Qlearning (RQL), which uses a recurrent neural network (RNN) for approximating a history-based\nQ-function."
        },
        "aliases": [
          "RNN"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Recurrent Q-Learning",
          "justification": "Recurrent Q-learning (RQL) is the primary model analyzed in the paper for its convergence properties in partially observable environments.",
          "quote": "One of the most popular RL algorithms for POMDPs: Recurrent Q-learning (RQL)"
        },
        "aliases": [
          "RQL"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "RQL itself is not a new contribution of this paper; it is a well-known algorithm in the literature.",
          "quote": "RQL was initially proposed by [Sch91; HS92] with a substantial follow up literature [Bak02; Wie+07; Wie+10; DSH13]."
        },
        "is_executed": {
          "value": 1,
          "justification": "RQL was executed during the numerical experiments to evaluate its performance.",
          "quote": "using ideas from approximate information state (AIS) [Sub+22], we quantify\nthe quality of the converged limit of RQL in terms of error in representation."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of RQL was compared against state-of-the-art baselines.",
          "quote": "We illustrate via detailed numerical experiments that RQL-AIS learns better than R2D2 [Kap+19], which is the state-of-the-art RQL algorithm for POMDPs."
        },
        "referenced_paper_title": {
          "value": "Reinforcement learning through recurrent connectionist models",
          "justification": "This paper is cited as an initial proposal for RQL.",
          "quote": "RQL was initially proposed by [Sch91; HS92] with a substantial follow up literature"
        }
      },
      {
        "name": {
          "value": "Recurrent Q-Learning with AIS",
          "justification": "The paper proposes a new variant of RQL called RQL-AIS which incorporates AIS losses.",
          "quote": "we propose a variant of RQL called RQL-AIS which incorporates AIS losses."
        },
        "aliases": [
          "RQL-AIS"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "RQL-AIS is a new variant of RQL proposed in this paper.",
          "quote": "we propose a variant of RQL called RQL-AIS which incorporates AIS losses."
        },
        "is_executed": {
          "value": 1,
          "justification": "RQL-AIS was executed during the numerical experiments to evaluate its performance.",
          "quote": "we illustrate via detailed numerical experiments that RQL-AIS learns better than R2D2 [Kap+19], which is the state-of-the-art RQL algorithm for POMDPs."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of RQL-AIS was compared against state-of-the-art baselines.",
          "quote": "we illustrate via detailed numerical experiments that RQL-AIS learns better than R2D2 [Kap+19], which is the state-of-the-art RQL algorithm for POMDPs."
        },
        "referenced_paper_title": {
          "value": "Approximate Information State for Approximate Planning and Reinforcement Learning in Partially Observed Systems",
          "justification": "This paper forms the basis for the AIS methodology used in RQL-AIS.",
          "quote": "we propose a variant of RQL called RQL-AIS which incorporates AIS losses."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MiniGrid benchmark",
          "justification": "Models were empirically evaluated on the MiniGrid benchmark.",
          "quote": "we propose a variant of RQL called RQL-AIS which incorporates AIS losses. We illustrate via detailed numerical experiments that RQL-AIS learns better than R2D2 [Kap+19], which is the state-of-the-art RQL algorithm for POMDPs. We also empirically demonstrate that there is a strong correlation between the performance of RQL over time and the loss associated with the AIS representation.\n\n4.2 Empirical evaluation... We evaluate the two algorithms on 22 environments from the MiniGrid benchmark."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Minimalistic Gridworld Environment for Gymnasium",
          "justification": "The MiniGrid benchmark is developed by this paper and is used for evaluating the proposed models.",
          "quote": "We evaluate the two algorithms on 22 environments from the MiniGrid benchmark [CWP18]"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1670,
    "prompt_tokens": 26017,
    "total_tokens": 27687
  }
}