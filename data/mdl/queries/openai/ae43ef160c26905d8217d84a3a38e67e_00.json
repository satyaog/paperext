{
  "paper": "ae43ef160c26905d8217d84a3a38e67e.txt",
  "words": 5136,
  "extractions": {
    "title": {
      "value": "Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge",
      "justification": "The title is provided at the beginning of the research paper.",
      "quote": "Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge"
    },
    "description": "The paper investigates whether masked language models like BERT acquire commonsense knowledge through systematic inference or from surface-level patterns. By injecting verbalized knowledge into BERT during pre-training, the researchers evaluate if BERT generalizes supported inferences. The study finds that generalization does not improve significantly during pre-training, suggesting that commonsense is learned from patterns rather than systematic reasoning.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes experiments conducted by injecting verbalized knowledge into the pre-training minibatches of BERT to evaluate its reasoning capabilities, which indicates that it is an empirical study.",
      "quote": "We find generalization does not improve over the majority of pre-training which supports the hypothesis that the type of commonsense knowledge studied is not acquired by systematic inference."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The research focuses on transformers like BERT, which are tools commonly used in the field of NLP, to explore how these models acquire commonsense knowledge.",
        "quote": "Commonsense knowledge acquisition is a long-standing challenge in natural language processing."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Commonsense Knowledge Acquisition",
          "justification": "The paper specifically deals with the acquisition of commonsense knowledge within the context of language model pre-training.",
          "quote": "We find generalization does not improve over the course of pre-training BERT from scratch, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Pre-trained Language Models",
          "justification": "The study is centered around pre-trained language models like BERT and their ability to encode knowledge.",
          "quote": "In this work, we investigate whether such knowledge is acquired during pre-training through systematic inference over the semantics of the pre-training corpora."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "The paper focuses on BERT to investigate if it can acquire commonsense knowledge through systematic inference.",
          "quote": "Transformer models pre-trained with a masked- language-modeling objective (e.g., BERT) encode commonsense knowledge as evidenced by behavioral probes."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BERT is a well-established model in the field of NLP and is not newly introduced in this paper.",
          "quote": "Pre-trained Transformers, such as BERT, encode knowledge about the world."
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments were conducted using BERT to evaluate how well it generalizes to supported inferences after pre-training.",
          "quote": "We inject verbalized knowledge into the minibatches of BERT and evaluate how well the model generalizes to supported inferences."
        },
        "is_compared": {
          "value": true,
          "justification": "BERT's performance was measured and analyzed, likely in comparison to control predicates and other points in pre-training iterations.",
          "quote": "We consider PMI for evaluating generalization. When BERT is updated on a pre-training minibatch containing a super-statement, this unsurprisingly increases the probability of the super-statement predicates."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "The referenced paper detailing BERT is cited within the context of explaining the model used for the experiments.",
          "quote": "We consider the training dynamics of a BERT-base model from random initialization to fully pre- trained, replicating details of the original BERT implementation (Devlin et al., 2019)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Leap-of-Thought",
          "justification": "The dataset is used for evaluating the reasoning abilities of BERT as part of the experiments conducted in the study.",
          "quote": "We evaluate on the Leap-of-Thought dataset presented by Talmor et al. (2020b)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Leap-of-Thought: Teaching Pre-trained Models to Systematically Reason Over Implicit Knowledge",
          "justification": "The Leap-of-Thought dataset's reference is included in relation to its application in testing the model's reasoning abilities.",
          "quote": "We evaluate on the Leap-of-Thought dataset presented by Talmor et al. (2020b)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Huggingface Transformers",
          "justification": "The Huggingface Transformers library was used to build the BERT model for experimentation in this study.",
          "quote": "Our code builds on the Huggingface Transformers (Wolf et al., 2020) and MegatronLM (Shoeybi et al., 2019) implementations of BERT."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-Art Natural Language Processing",
          "justification": "The paper by Wolf et al. is referenced as the source of the implementation of the Huggingface Transformers library used.",
          "quote": "Our code builds on the Huggingface Transformers (Wolf et al., 2020) and MegatronLM (Shoeybi et al., 2019) implementations of BERT."
        }
      },
      {
        "name": {
          "value": "MegatronLM",
          "justification": "The MegatronLM library was used alongside Huggingface Transformers for implementing BERT in the experiments.",
          "quote": "Our code builds on the Huggingface Transformers (Wolf et al., 2020) and MegatronLM (Shoeybi et al., 2019) implementations of BERT."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism",
          "justification": "The paper by Shoeybi et al. is cited as the source of the MegatronLM implementation used in the research.",
          "quote": "Our code builds on the Huggingface Transformers (Wolf et al., 2020) and MegatronLM (Shoeybi et al., 2019) implementations of BERT."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1273,
    "prompt_tokens": 10589,
    "total_tokens": 11862,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}