{
  "paper": "82944eb09743ec0a34d854bd3ec318b1.txt",
  "words": 13076,
  "extractions": {
    "title": {
      "value": "Fortuitous Forgetting in Connectionist Networks",
      "justification": "The title is a concise representation of the paper's central theme and is found at the beginning of the paper.",
      "quote": "F ORTUITOUS F ORGETTING IN C ONNECTIONIST N ETWORKS"
    },
    "description": "The paper explores the concept of forget-and-relearn in artificial neural networks, proposing that forgetting can benefit learning by selectively removing undesirable information and reinforcing useful features. It unifies existing training algorithms under this framework and suggests ways to improve generalization through targeted forgetting.",
    "type": {
      "value": "theoretical",
      "justification": "The paper proposes a new paradigm (forget-and-relearn) and provides a theoretical framework for understanding the dynamics of neural network training, rather than focusing solely on empirical results.",
      "quote": "In this work, we study a general learning paradigm that we refer to as forget-and-relearn, and show that forgetting can also benefit learning in artificial neural networks."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper focuses on improving learning algorithms and neural network performance, which are core aspects of Machine Learning.",
        "quote": "we propose that forgetting can in fact be favorable to learning. We introduce forget-and-relearn as a powerful paradigm for shaping the learning trajectories of artificial neural networks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Classification",
          "justification": "The paper discusses iterative training algorithms that have been applied successfully in image classification tasks.",
          "quote": "a number of training algorithms have been proposed to improve generalization by iteratively refining the learned solution. Knowledge evolution... Iterative magnitude pruning..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Language Emergence",
          "justification": "The paper applies the forget-and-relearn framework to language emergence, showing its utility in improving compositionality in emergent languages.",
          "quote": "Several existing algorithms in the language emergence subfield can also be seen as forget-and-relearn algorithms."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Iterative Magnitude Pruning",
          "justification": "Iterative Magnitude Pruning is identified as an example of the forget-and-relearn paradigm.",
          "quote": "Iterative magnitude pruning (Frankle & Carbin, 2019; Frankle et al., 2019) removes weights through an iterative pruning-retraining process, and outperforms unpruned models in certain settings."
        },
        "aliases": [
          "IMP"
        ],
        "is_contributed": {
          "value": false,
          "justification": "IMP was not introduced in this paper but referenced as existing work.",
          "quote": "Iterative magnitude pruning (Frankle & Carbin, 2019; Frankle et al., 2019) removes weights through an iterative pruning-retraining process."
        },
        "is_executed": {
          "value": false,
          "justification": "There is no explicit mention of execution or implementation within this paper.",
          "quote": "Iterative magnitude pruning (Frankle & Carbin, 2019; Frankle et al., 2019) removes weights through an iterative pruning-retraining process."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of IMP is discussed in comparison to other training schemes within the context of forget-and-relearn.",
          "quote": "Iterative magnitude pruning...removes weights through an iterative pruning-retraining process, and outperforms unpruned models in certain settings."
        },
        "referenced_paper_title": {
          "value": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
          "justification": "The referenced paper is about Iterative Magnitude Pruning and its approach.",
          "quote": "Iterative magnitude pruning (Frankle & Carbin, 2019) removes weights through an iterative pruning-retraining process, and outperforms unpruned models in certain settings."
        }
      },
      {
        "name": {
          "value": "Iterated Learning",
          "justification": "The paper uses iterated learning as an example of forget-and-relearn in language emergence.",
          "quote": "Iterated learning (Kirby, 2001) has been shown to improve compositionality in emergent languages (Ren et al., 2020; Vani et al., 2021) and prevent language drift."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Iterated Learning is referenced as a known concept rather than being newly introduced in this paper.",
          "quote": "Iterated learning (Kirby, 2001) has been shown to improve compositionality in emergent languages (Ren et al., 2020; Vani et al., 2021) and prevent language drift."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper discusses the paradigm conceptually without demonstrating its practical execution.",
          "quote": "Iterated learning (Kirby, 2001) has been shown to improve compositionality in emergent languages."
        },
        "is_compared": {
          "value": true,
          "justification": "Iterated learning is compared to other methods of language emergence and its benefits are evaluated.",
          "quote": "Iterated learning (Kirby, 2001) has been shown to improve compositionality in emergent languages (Ren et al., 2020; Vani et al., 2021) and prevent language drift."
        },
        "referenced_paper_title": {
          "value": "Spontaneous evolution of linguistic structure-an iterated learning model of the emergence of regularity and irregularity",
          "justification": "Kirby's paper is referenced for the Iterated Learning model as foundational work in this area.",
          "quote": "Iterated learning (Kirby, 2001) has been shown to improve compositionality in emergent languages."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is used as one of the datasets to analyze the effects of the proposed methods on image classification tasks.",
          "quote": "We perform these experiments using a two-layer MLP trained on MNIST."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "Standard reference to MNIST dataset as it is commonly used in literature without a specific paper title for citation.",
          "quote": "We perform these experiments using a two-layer MLP trained on MNIST."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used as a benchmark dataset to test and compare the proposed methods.",
          "quote": "We further extend to a 4-layer convolution model and a ResNet18 on CIFAR-10."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "CIFAR-10 is a standard dataset and typically cited this way.",
          "quote": "We further extend to a 4-layer convolution model and a ResNet18 on CIFAR-10."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is utilized in experiments to test the scalability and effectiveness of the proposed approaches in a large-scale setting.",
          "quote": "and to ResNet50 on ImageNet in Appendix A1.2."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Imagenet: A large-scale hierarchical image database",
          "justification": "The citation is standard for the ImageNet dataset in research literature.",
          "quote": "ImageNet (Deng et al., 2009) on ResNet50."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SGD",
          "justification": "Stochastic Gradient Descent (SGD) is used as an optimization algorithm within the experiments conducted.",
          "quote": "We use an MLP with hidden layers 300-100-10, trained using SGD with a constant learning rate of 0.1."
        },
        "aliases": [
          "Stochastic Gradient Descent"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Understanding the difficulty of training deep feedforward neural networks",
          "justification": "SGD is widely known and used without typically referencing a singular paper.",
          "quote": "trained using SGD with a constant learning rate of 0.1."
        }
      },
      {
        "name": {
          "value": "Adam",
          "justification": "Adam is used as an optimizer in several of the training experiments conducted.",
          "quote": "Conv4 model is trained using the Adam optimizer."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A method for stochastic optimization",
          "justification": "Adam optimizer is a well-established method typically cited by its original paper.",
          "quote": "Conv4 model is trained using the Adam optimizer."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1701,
    "prompt_tokens": 23173,
    "total_tokens": 24874,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}