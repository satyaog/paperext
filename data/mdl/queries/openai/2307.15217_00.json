{
  "paper": "2307.15217.txt",
  "words": 20024,
  "extractions": {
    "title": {
      "value": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
      "justification": "This is the exact title of the paper.",
      "quote": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback"
    },
    "description": "This paper surveys the open problems and fundamental limitations of Reinforcement Learning from Human Feedback (RLHF) and related methods. It discusses techniques to understand, improve, and complement RLHF in practice, and proposes standards for auditing and disclosure to improve societal oversight of RLHF systems. The paper emphasizes the limitations of RLHF and highlights the importance of a multi-layered approach to developing safer AI systems.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily discusses open problems, limitations, and theoretical aspects of RLHF rather than presenting empirical experiments or results.",
      "quote": "Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper is explicitly focused on Reinforcement Learning from Human Feedback (RLHF).",
        "quote": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Learning in AI Alignment",
          "justification": "The paper discusses various challenges and limitations specific to aligning AI systems with human goals using reinforcement learning techniques.",
          "quote": "Despite this, deployed models finetuned with RLHF have revealed sensitive private information (Li et al., 2023a; El-Mhamdi et al., 2022), hallucinated untrue content (Ji et al., 2023; OpenAI, 2023; Zhang et al., 2023), spread biases that favor specific political ideologies (Santurkar et al., 2023; Perez et al., 2022b), exhibited sycophantic responses (Perez et al., 2022b), and expressed undesirable preferences (e.g., not wanting to be shut down) (Perez et al., 2022b)."
        },
        "aliases": [
          "AI Safety"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-4",
          "justification": "The paper explicitly mentions GPT-4 as an example of a state-of-the-art large language model trained using RLHF.",
          "quote": "RLHF is a key component of training state-of-the-art large language models (LLMs), such as OpenAI’s GPT-4 (OpenAI, 2023)..."
        },
        "aliases": [
          "GPT"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-4 is mentioned as an example of a model that uses RLHF but is not contributed by this paper.",
          "quote": "In particular, RLHF is a key component of training state-of-the-art large language models (LLMs), such as OpenAI’s GPT-4 (OpenAI, 2023), Anthropic’s Claude (Anthropic, 2023), Google’s Bard (Google, 2023), and Meta’s Llama 2-Chat (Touvron et al., 2023)."
        },
        "is_executed": {
          "value": 1,
          "justification": "GPT-4 and similar large language models typically require GPU execution due to their high computational demands.",
          "quote": "In particular, RLHF is a key component of training state-of-the-art large language models (LLMs), such as OpenAI’s GPT-4 (OpenAI, 2023), Anthropic’s Claude (Anthropic, 2023), Google’s Bard (Google, 2023), and Meta’s Llama 2-Chat (Touvron et al., 2023)."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not compare GPT-4 numerically to other models; it is mentioned only as an example.",
          "quote": "In particular, RLHF is a key component of training state-of-the-art large language models (LLMs), such as OpenAI’s GPT-4 (OpenAI, 2023)..."
        },
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "This is the reference where GPT-4 is originally described.",
          "quote": "OpenAI, 2023"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Internet text",
          "justification": "The datasets used in pretraining large language models like GPT-4 often involve large collections of internet text.",
          "quote": "RLHF begins with an initial base model πθ with parameters θ which generates a distribution of examples. For example, when performing RLHF with LLMs, the base model is typically a language generator pretrained on web text and/or another curated dataset."
        },
        "aliases": [
          "Web text"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "The referenced paper for GPT-4 likely discusses the dataset used for pretraining, which includes internet text.",
          "quote": "RLHF begins with an initial base model πθ with parameters θ which generates a distribution of examples. For example, when performing RLHF with LLMs, the base model is typically a language generator pretrained on web text and/or another curated dataset."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1111,
    "prompt_tokens": 36382,
    "total_tokens": 37493
  }
}