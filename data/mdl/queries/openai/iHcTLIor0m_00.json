{
  "paper": "iHcTLIor0m.txt",
  "words": 22376,
  "extractions": {
    "title": {
      "value": "Poly-View Contrastive Learning",
      "justification": "The title of the paper is prominently displayed on the first page of the document, highlighted as \"P OLY-V IEW C ONTRASTIVE L EARNING\" in the introduction section of the paper.",
      "quote": "P OLY-V IEW C ONTRASTIVE L EARNING"
    },
    "description": "This paper investigates poly-view tasks in contrastive learning, where there are more than two related views. It introduces new representation learning objectives using information theory and sufficient statistics. The study finds that maximizing the number of related views can enhance learning and challenges the notion that large batch sizes are necessary, with empirical evaluations on ImageNet1k outperforming SimCLR.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts empirical investigations into poly-view contrastive tasks, providing experimental results on models trained with ImageNet1k to demonstrate the effectiveness of the proposed methods.",
      "quote": "We show theoretically and empirically that this has a positive impact on learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Contrastive Learning",
        "justification": "The paper focuses on contrastive learning as it explores poly-view tasks and their impact on learning, expanding existing contrastive learning methodologies.",
        "quote": "Contrastive learning typically matches pairs of related views among a number of unrelated negative views."
      },
      "aliases": [
        "Contrastive Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The study builds on concepts in Self-Supervised Learning (SSL) by designing tasks to exploit unlabeled data structures.",
          "quote": "Self-Supervised Learning (SSL) trains models to solve tasks designed take advantage of the structure and relationships within unlabeled data."
        },
        "aliases": [
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Information Theory",
          "justification": "Information theory principles are used to derive the new learning objectives in the context of poly-view contrastive models, emphasizing mutual information maximization.",
          "quote": "One principle behind contrastive learning is Mutual Information (MI) maximization."
        },
        "aliases": [
          "Information Theory"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper conducts experiments specifically on ImageNet1k, a benchmark dataset in the field of computer vision, to evaluate their proposed methods.",
          "quote": "poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "SimCLR is referenced as a baseline model to compare against poly-view contrastive models in terms of performance on ImageNet1k.",
          "quote": "poly-view contrastive models trained for 128 epochs with batch size 256 outperform SimCLR trained for 1024 epochs at batch size 4096 on ImageNet1k."
        },
        "aliases": [
          "SimCLR"
        ],
        "is_contributed": {
          "value": false,
          "justification": "SimCLR is used as a baseline model for comparison, not as a contribution of the paper.",
          "quote": "poly-view contrastive models trained... outperform SimCLR trained for 1024 epochs."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper compares the empirical results of poly-view models and SimCLR by executing experiments on ImageNet1k.",
          "quote": "empirically... poly-view contrastive models trained for 128 epochs... outperform SimCLR."
        },
        "is_compared": {
          "value": true,
          "justification": "SimCLR is used as a comparison baseline, and the results highlight performance differences.",
          "quote": "outperform SimCLR trained for 1024 epochs."
        },
        "referenced_paper_title": {
          "value": "A simple framework for contrastive learning of visual representations",
          "justification": "The referenced paper title \"A simple framework for contrastive learning of visual representations\" by Chen et al. outlines SimCLR, which is cited in the article.",
          "quote": "(Chen et al., 2020a) A simple framework for contrastive learning of visual representations"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet1k",
          "justification": "The ImageNet1k dataset is used for empirical evaluation of the poly-view contrastive models and compared against SimCLR results.",
          "quote": "we demonstrate poly-view contrastive learning is useful for image representation learning...outperform SimCLR...on ImageNet1k."
        },
        "aliases": [
          "ImageNet1k"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet large scale visual recognition challenge",
          "justification": "The reference to ImageNet1k relates to the paper by Russakovsky et al., describing the dataset and its challenges.",
          "quote": "ImageNet large scale visual recognition challenge"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 944,
    "prompt_tokens": 42255,
    "total_tokens": 43199,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}