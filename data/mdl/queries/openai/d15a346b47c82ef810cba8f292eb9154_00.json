{
  "paper": "d15a346b47c82ef810cba8f292eb9154.txt",
  "words": 4366,
  "extractions": {
    "title": {
      "value": "RESOURCE-EFFICIENT SEPARATION TRANSFORMER",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "RESOURCE-EFFICIENT SEPARATION TRANSFORMER"
    },
    "description": "This paper presents the Resource-Efficient Separation Transformer (RE-SepFormer), a self-attention-based architecture for efficient speech separation with reduced computational cost, achieving competitive performance on the WSJ0-2Mix and WHAM! datasets.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments to validate the performance of the RE-SepFormer on specific datasets, which is an empirical approach.",
      "quote": "We conducted the experimental validation on the popular WSJ0-2Mix dataset... In addition to the non-causal offline scenario we provide experimental evidence in a real-time low-latency scenario by considering causal versions of the RE-SepFormer."
    },
    "primary_research_field": {
      "name": {
        "value": "Speech Processing",
        "justification": "The research focuses on speech separation and processing, which are subsets of speech processing.",
        "quote": "Transformers have recently achieved state-of-the-art performance in speech separation."
      },
      "aliases": [
        "Speech Separation",
        "Speech Enhancement"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Efficient Deep Learning",
          "justification": "The emphasis on reducing computational cost and efficient processing indicates a focus on efficient deep learning.",
          "quote": "This paper explores Transformer-based speech separation with a reduced computational cost."
        },
        "aliases": [
          "Resource-Efficient Deep Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "RE-SepFormer",
          "justification": "The paper introduces and elaborates on the Resource-Efficient Separation Transformer (RE-SepFormer) as its main contribution.",
          "quote": "Our main contribution is the development of the Resource-Efficient Separation Transformer (RE-SepFormer)."
        },
        "aliases": [
          "Resource-Efficient Separation Transformer"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The RE-SepFormer is presented as a new model that the authors claim as their contribution.",
          "quote": "Our main contribution is the development of the Resource-Efficient Separation Transformer (RE-SepFormer)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes experimental results with the RE-SepFormer on specific datasets.",
          "quote": "We conducted the experimental validation on the popular WSJ0-2Mix dataset..."
        },
        "is_compared": {
          "value": true,
          "justification": "The RE-SepFormer is compared against several other models in the study.",
          "quote": "We compare the RE-SepFormer against popular speech separation methods (e.g., TasNet, Conv-TasNet, Dual-Path RNN, etc.)."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "The RE-SepFormer is introduced in the current paper without reference to an earlier publication.",
          "quote": "Our main contribution is the development of the Resource-Efficient Separation Transformer (RE-SepFormer)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "WSJ0-2Mix",
          "justification": "The WSJ0-2Mix dataset is explicitly mentioned as one of the datasets used for testing the models in the paper.",
          "quote": "The RE-SepFormer reaches a competitive performance on the popular WSJ0-2Mix and WHAM! datasets..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deep clustering: Discriminative embeddings for segmentation and separation",
          "justification": "The dataset WSJ0-2Mix is referenced by the paper using it for the model's evaluation.",
          "quote": "We provide experimental evidence on the popular WSJ0-2Mix dataset [31], which is a standard benchmark largely adopted in speech separation."
        }
      },
      {
        "name": {
          "value": "WHAM!",
          "justification": "The WHAM! dataset is mentioned as another dataset used for the validation of the RE-SepFormer.",
          "quote": "To assess our model in more realistic conditions, we also considered the WHAM! [29] dataset..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "WHAM!: Extending speech separation to noisy environments",
          "justification": "The WHAM! dataset is discussed in the context of its use for evaluating models under noisy conditions, citing the original reference.",
          "quote": "We also assess our models on the WHAM! corpus [29], which adds non-stationary noises..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "SpeechBrain is mentioned as the platform where the implementation of RE-SepFormer is available.",
          "quote": "The implementation of RE-SepFormer is available in the SpeechBrain [33] GitHub repository."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A general-purpose speech toolkit",
          "justification": "The paper refers to SpeechBrain as a toolkit used for neural network implementations.",
          "quote": "M. Ravanelli et al., “SpeechBrain: A general-purpose speech toolkit,” arXiv preprint arXiv:2106.04624, 2021."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1012,
    "prompt_tokens": 9451,
    "total_tokens": 10463,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}