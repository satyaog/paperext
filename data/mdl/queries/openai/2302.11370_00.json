{
  "paper": "2302.11370.txt",
  "words": 23307,
  "extractions": {
    "title": {
      "value": "Recall, Robustness, and Lexicographic Evaluation",
      "justification": "The title is clearly stated on the first page of the paper.",
      "quote": "Recall, Robustness, and Lexicographic Evaluation"
    },
    "description": "This paper analyzes the use and definition of recall in rankings, proposing a new metric called lexirecall that is grounded in robustness and fairness. Through extensive empirical analysis across multiple tasks, the paper establishes the validity and sensitivity of lexirecall compared to existing recall metrics.",
    "type": {
      "value": "theoretical",
      "justification": "The paper provides a formal analysis and definition of recall in rankings, introduces a new metric, and connects theoretical constructs to practical evaluation.",
      "quote": "In light of this debate, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation."
    },
    "primary_research_field": {
      "name": {
        "value": "Information Retrieval",
        "justification": "The paper focuses on the evaluation of rankings, a core topic within Information Retrieval.",
        "quote": "Recall is often used to evaluate rankings of items, including those produced by recommender, retrieval, and other machine learning systems."
      },
      "aliases": [
        "IR"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Recommender Systems",
          "justification": "The paper explicitly mentions recommendation tasks as part of its empirical analysis.",
          "quote": "Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation. First, we formally define ‘recall-orientation’ as the sensitivity of a metric to a user interested in finding every relevant item. Second, we analyze recall-orientation from the perspective of robustness with respect to possible content consumers and providers, connecting recall to recent conversations about fair ranking. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across three recommendation tasks..."
        },
        "aliases": [
          "RecSys"
        ]
      },
      {
        "name": {
          "value": "Metric Evaluation",
          "justification": "The study revolves around evaluating different metrics for recall in rankings.",
          "quote": "Through extensive empirical analysis across 17 TREC tracks and three recommendation tasks, we establish that our new evaluation method, lexirecall, has convergent validity..."
        },
        "aliases": [
          "Evaluation Metrics"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Lexirecall",
          "justification": "The model 'lexirecall' is proposed as a new metric within the paper.",
          "quote": "Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across 17 TREC tracks and three recommendation tasks, we establish that our new evaluation method, lexirecall, has convergent validity..."
        },
        "aliases": [
          "lexicographic recall"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "Lexirecall is introduced and extensively analyzed within the paper.",
          "quote": "Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison."
        },
        "is_executed": {
          "value": 0,
          "justification": "The paper focuses on the theoretical and empirical validation of the metric, and doesn't specify execution on hardware.",
          "quote": "Our conceptual, theoretical, and empirical analysis substantially deepens our understanding of recall and motivates its adoption through connections to robustness and fairness."
        },
        "is_compared": {
          "value": 1,
          "justification": "Lexirecall is compared with existing recall metrics through empirical analysis.",
          "quote": "Through extensive empirical analysis across 17 TREC tracks and three recommendation tasks, we establish that our new evaluation method, lexirecall, has convergent validity..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Lexirecall is a new metric proposed by this paper, not based on prior models.",
          "quote": "Through extensive empirical analysis across 17 TREC tracks and three recommendation tasks, we establish that our new evaluation method, lexirecall, has convergent validity, and exhibits substantially higher sensitivity in terms of discriminative power and stability in the presence of missing labels."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "TREC tracks",
          "justification": "The datasets used in the empirical analysis are referred to as TREC tracks.",
          "quote": "Through extensive empirical analysis across 17 TREC tracks and three recommendation tasks, we establish that our new evaluation method, lexirecall, has convergent validity."
        },
        "aliases": [
          "Text REtrieval Conference"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The 'TREC tracks' refer to a series of datasets standardized across various TREC conferences, not a single reference paper.",
          "quote": "Through extensive empirical analysis across 17 TREC tracks and three recommendation tasks, we establish that our new evaluation method, lexirecall, has convergent validity."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 977,
    "prompt_tokens": 51412,
    "total_tokens": 52389
  }
}