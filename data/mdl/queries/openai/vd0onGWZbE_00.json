{
  "paper": "vd0onGWZbE.txt",
  "words": 12256,
  "extractions": {
    "title": {
      "value": "Identifiable Deep Generative Models via Sparse Decoding",
      "justification": "The title directly reflects the core contribution of the paper, which is about making deep generative models identifiable through the use of sparse decoding methods.",
      "quote": "Identifiable Deep Generative Models via Sparse Decoding"
    },
    "description": "This paper introduces the sparse variational autoencoder (VAE) for unsupervised representation learning in high-dimensional data. It emphasizes learning identifiable and meaningful latent factors that reflect true underlying patterns in data by leveraging sparsity. Through theoretical proofs and empirical studies, the paper demonstrates the model's ability to recover meaningful latent representations and compares its effectiveness against other generative models.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily focuses on developing theoretical concepts regarding identifiability in deep generative models, along with theoretical proofs supporting their approach.",
      "quote": "We prove such sparse deep generative models are identifiable: with infinite data, the true model parameters can be learned."
    },
    "primary_research_field": {
      "name": {
        "value": "Generative Models",
        "justification": "The paper focuses on improving deep generative models, specifically around the concept of identifiability and sparse representation learning.",
        "quote": "To learn such factors, many researchers fit deep generative models (DGMs) (Kingma and Welling, 2014; Rezende et al., 2014)."
      },
      "aliases": [
        "DGMs"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Variational Autoencoders",
          "justification": "The paper proposes a specific model called the sparse VAE, prominently discussing its advantages and modifications in comparison to conventional VAEs.",
          "quote": "In a standard DGM, the vector zi is a latent variable that is fed through a neural network to reconstruct the distribution of xi. The sparse DGM introduces an additional parameter."
        },
        "aliases": [
          "VAEs"
        ]
      },
      {
        "name": {
          "value": "Unsupervised Representation Learning",
          "justification": "The paper addresses learning latent representations without supervision, using sparse VAEs to capture underlying patterns in data.",
          "quote": "We develop the sparse VAE for unsupervised representation learning on high-dimensional data."
        },
        "aliases": [
          ""
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Sparse VAE",
          "justification": "The Sparse VAE is the proposed model in the paper, central to their experiments and theoretical contributions.",
          "quote": "We call this procedure the sparse VAE."
        },
        "aliases": [
          "Sparse VAE"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The Sparse VAE is introduced as a novel contribution in the paper.",
          "quote": "We develop the sparse VAE for unsupervised representation learning on high-dimensional data."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses the empirical studies involving the Sparse VAE, indicating its implementation and execution.",
          "quote": "We study the sparse VAE empirically."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the Sparse VAE against other methods like VAE, β-VAE, Variational Sparse Coding, and OI-VAE, on both synthetic and real datasets.",
          "quote": "Empirically, we compare the sparse VAE to existing algorithms for fitting DGMs."
        },
        "referenced_paper_title": {
          "value": "Auto-Encoding Variational Bayes",
          "justification": "The foundational concept of VAEs, which the Sparse VAE builds upon, is introduced in \"Auto-Encoding Variational Bayes\" by Kingma and Welling.",
          "quote": "In a standard DGM, the vector zi is a latent variable that is fed through a neural network to reconstruct the distribution of xi"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MovieLens",
          "justification": "The MovieLens dataset is used to test the Sparse VAE's performance in an empirical study.",
          "quote": "We compare the sparse VAE... on text and movie-ratings data..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The MovieLens Datasets: History and Context",
          "justification": "The paper by Harper and Konstan is the primary source describing the MovieLens dataset.",
          "quote": "Harper, F. M. and Konstan, J. A. (2015). The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems."
        }
      },
      {
        "name": {
          "value": "PeerRead",
          "justification": "The PeerRead dataset is utilized for benchmarking the model's performance on text data.",
          "quote": "PeerRead (Kang et al., 2018). Dataset of word counts for paper abstracts..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A dataset of peer reviews (PeerRead): Collection, insights and NLP applications",
          "justification": "The paper by Kang et al. discusses the creation and application of the PeerRead dataset.",
          "quote": "Kang, D., Ammar, W., Dalvi, B., van Zuylen, M., Kohlmeier, S., Hovy, E., and Schwartz, R. (2018). A dataset of peer reviews (PeerRead): Collection, insights and NLP applications."
        }
      },
      {
        "name": {
          "value": "Zeisel",
          "justification": "The Zeisel dataset is used for testing the model in the context of genomics data.",
          "quote": "Zeisel (Zeisel et al., 2015). Dataset of RNA molecule counts in mouse cortex cells..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Cell types in the mouse cortex and hippocampus revealed by single-cell RNA-seq",
          "justification": "The Zeisel dataset's details are provided in the referenced study by Zeisel et al., which investigates cell types using RNA-seq.",
          "quote": "Zeisel, A., Muñoz-Manchado, A. B., Codeluppi, S., et al. (2015). Cell types in the mouse cortex... revealed by single-cell RNA-seq. Science."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1189,
    "prompt_tokens": 21929,
    "total_tokens": 23118,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}