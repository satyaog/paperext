{
  "paper": "d174480f9b2cfe5d2e75095856ac3a59.txt",
  "words": 18579,
  "extractions": {
    "title": {
      "value": "A general class of surrogate functions for stable and efficient reinforcement learning",
      "justification": "The title directly reflects the focus on developing a class of surrogate functions for reinforcement learning.",
      "quote": "A general class of surrogate functions for stable and efficient reinforcement learning"
    },
    "description": "This paper proposes a framework called Functional Mirror Ascent for Policy Gradient (FMA-PG) that generates a broad class of surrogate functions for reinforcement learning. The presented FMA-PG framework provides theoretical guarantees for policy improvement, leading to a family of surrogate functions that are stable and efficient compared to existing methods like TRPO, PPO, and MPO.",
    "type": {
      "value": "theoretical",
      "justification": "The focus is on developing a framework with theoretical guarantees for surrogate functions in reinforcement learning.",
      "quote": "We address these issues through the following contributions. Functional mirror ascent for policy gradient...a generic policy optimization algorithm that relies on approximately maximizing a sequence of surrogate functions."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper discusses policy gradient methods and surrogate functions specifically in the context of reinforcement learning.",
        "quote": "In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Policy Gradient Methods",
          "justification": "The paper makes use of policy gradient methods as a major component of its framework.",
          "quote": "Policy gradient (PG) methods (Williams, 1992; Sutton et al., 2000; Konda and Tsitsiklis, 2000; Kakade, 2002) are an important class of model-free methods in reinforcement learning."
        },
        "aliases": [
          "PG Methods"
        ]
      },
      {
        "name": {
          "value": "Functional Mirror Ascent",
          "justification": "The paper introduces the concept of Functional Mirror Ascent as a novel approach.",
          "quote": "In Section 3, we construct surrogate functions using mirror ascent on a functional representation of the policy itself, rather than on its parameters."
        },
        "aliases": [
          "FMA"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "PPO",
          "justification": "PPO is used as a benchmark to demonstrate the proposed framework's improvements.",
          "quote": "FMA-PG also suggests an improved variant of PPO, whose robustness and efficiency we empirically demonstrate on the MuJoCo suite."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "PPO is an existing method that is used for comparison, not a contribution of this paper.",
          "quote": "FMA-PG also suggests an improved variant of PPO, whose robustness and efficiency we empirically demonstrate on the MuJoCo suite."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes experiments using PPO to demonstrate empirical results.",
          "quote": "Experimental evaluation: Finally, in Section 6, we evaluate the performance of surrogate functions instantiated by FMA-PG on simple bandit and reinforcement learning settings."
        },
        "is_compared": {
          "value": true,
          "justification": "PPO is compared with proposed methods in the experiments.",
          "quote": "The proposed framework also suggests an improved variant of PPO, whose robustness and efficiency we empirically demonstrate on the MuJoCo suite."
        },
        "referenced_paper_title": {
          "value": "Proximal Policy Optimization Algorithms",
          "justification": "This is the original paper introducing PPO, as cited in the document.",
          "quote": "Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O. (2017). Proximal policy optimization algorithms."
        }
      },
      {
        "name": {
          "value": "TRPO",
          "justification": "TRPO is used as a reference and benchmark for the FMA-PG approach.",
          "quote": "Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "TRPO is an existing method referenced for comparison.",
          "quote": "Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiment section utilizes TRPO for performance comparison.",
          "quote": "In Appendix A, we show that FMA-PG can handle stochastic value gradients (Heess et al., 2015)."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared with the proposed FMA-PG methods in experiments.",
          "quote": "For the softmax functional representation, FMA-PG results in a surrogate function that is a more stable variant of TRPO."
        },
        "referenced_paper_title": {
          "value": "Trust Region Policy Optimization",
          "justification": "This is the original paper introducing TRPO, as cited in the document.",
          "quote": "Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. (2015). Trust Region Policy Optimization."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MuJoCo",
          "justification": "The MuJoCo suite is used as an environment for evaluating the algorithms.",
          "quote": "The proposed framework also suggests an improved variant of PPO, whose robustness and efficiency we empirically demonstrate on the MuJoCo suite."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MuJoCo: A physics engine for model-based control",
          "justification": "This is the paper where the MuJoCo environment is introduced.",
          "quote": "Todorov, E., Erez, T., and Tassa, Y. (2012). MuJoCo: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026â€“5033. IEEE."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1198,
    "prompt_tokens": 34410,
    "total_tokens": 35608,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}