{
  "paper": "8347206bebbcbb10be6412b1f20f316f.txt",
  "words": 15296,
  "extractions": {
    "title": {
      "value": "Compositional Attention: Disentangling Search and Retrieval",
      "justification": "The title encapsulates the main idea introduced in the paper, which is the 'Compositional Attention' mechanism aimed at separating search and retrieval processes.",
      "quote": "We propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure."
    },
    "description": "The paper introduces Compositional Attention, a new mechanism for attention in machine learning models that separates the search and retrieval functions from the standard multi-head attention framework used in Transformers. This method allows for more flexible and dynamic combinations of searches and retrievals, potentially improving model performance in various tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes numerical experiments and empirical validations of the proposed model against existing models on various tasks, showing practical improvements.",
      "quote": "Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Machine Learning",
        "justification": "The paper addresses improvements in fundamental mechanisms of deep learning models, specifically the attention mechanism in machine learning.",
        "quote": "Attention mechanisms have become integral parts of machine learning models across a variety of domains."
      },
      "aliases": [
        "ML"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The paper discusses applications and experiments that relate to NLP, such as language modeling tasks.",
          "quote": "Attention is all you need. CoRR, abs/1706.03762, 2017."
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper extends attention mechanisms to computer vision tasks, showing the general applicability and enhancement potential.",
          "quote": "Dosovitskiy et al., 2020"
        },
        "aliases": [
          "CV"
        ]
      },
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The paper discusses improvements to deep learning models and architectures, particularly involving attention mechanisms.",
          "quote": "The advent of transformer-like models have led to advancements on various flavours of attention based models."
        },
        "aliases": [
          "DL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Compositional Attention",
          "justification": "The paper introduces the Compositional Attention mechanism as its primary contribution, differentiating it from standard multi-head attention.",
          "quote": "We propose Compositional Attention, where the search and retrieval operations can be flexibly composed."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Compositional Attention is the main contribution of the paper and is proposed as a novel advancement over existing attention mechanisms.",
          "quote": "We propose Compositional Attention, where the search and retrieval operations can be flexibly composed."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper describes implementation and empirical evaluation, indicating the model was executed to obtain results.",
          "quote": "Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "Compositional Attention is directly compared to standard multi-head attention in various tasks to showcase its improvements.",
          "quote": "We show that it outperforms standard multi-head attention on a variety of tasks."
        },
        "referenced_paper_title": {
          "value": "Attention is All You Need",
          "justification": "The paper references the foundational work on attention mechanisms within the Transformer model while introducing its improvements.",
          "quote": "Attention is all you need. CoRR, abs/1706.03762, 2017."
        }
      },
      {
        "name": {
          "value": "Multi-Head Attention",
          "justification": "The paper discusses Multi-Head Attention in the context of its limitations and uses it as a baseline for comparison.",
          "quote": "This attention mechanism uses multiple parallel key-value attention blocks (called heads)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Multi-Head Attention is an existing model used for comparison, not a contribution of this paper.",
          "quote": "This attention mechanism uses multiple parallel key-value attention blocks (called heads)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper evaluates Compositional Attention against Multi-Head Attention, indicating execution of this model for empirical comparison.",
          "quote": "Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "Multi-Head Attention is used as a baseline for comparison with the newly proposed Compositional Attention.",
          "quote": "We show that it outperforms standard multi-head attention on a variety of tasks."
        },
        "referenced_paper_title": {
          "value": "Attention is All You Need",
          "justification": "The seminal work on attention mechanisms in Transformers is repeatedly referred to for context about Multi-Head Attention.",
          "quote": "Attention is all you need. CoRR, abs/1706.03762, 2017."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Sort-of-CLEVR",
          "justification": "The dataset is used to evaluate relational reasoning capabilities of the proposed attention mechanism.",
          "quote": "Sort-of-CLEVR is a Visual Question-Answering (VQA) task."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A simple neural network module for relational reasoning",
          "justification": "The dataset is frequently utilized in machine learning papers to evaluate relational reasoning.",
          "quote": "Sort-of-CLEVR (Santoro et al., 2017) is a Visual Question-Answering (VQA) task."
        }
      },
      {
        "name": {
          "value": "SCAN",
          "justification": "SCAN is a benchmark used to assess the compositional generalization capabilities of models.",
          "quote": "SCAN (Lake & Baroni, 2018a) is a synthetic sequence to sequence task."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks",
          "justification": "The referenced paper originally introduced SCAN as a method for testing generalization in sequence models.",
          "quote": "SCAN (Lake & Baroni, 2018a) is a synthetic sequence to sequence task."
        }
      },
      {
        "name": {
          "value": "WikiText-103",
          "justification": "Used for language modeling experiments to evaluate performance improvements of the proposed model.",
          "quote": "We perform experiments on the WikiText-103 data corpus for the language modeling task."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pointer Sentinel Mixture Models",
          "justification": "The dataset is a widely used language modeling benchmark introduced in the referenced paper.",
          "quote": "We perform experiments on the WikiText-103 data corpus (Merity et al., 2016) for the language modeling task."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used to evaluate the model on image classification tasks.",
          "quote": "We pose the problem of image classification across four different datasets – CIFAR10, FashionMNIST, SVHN and Equilateral Triangle Detection as a multi-task learning setup."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "CIFAR-10 is a common benchmark dataset introduced in the mentioned study.",
          "quote": "We pose the problem of image classification across four different datasets – CIFAR10, FashionMNIST, SVHN and Equilateral Triangle Detection as a multi-task learning setup."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1503,
    "prompt_tokens": 26324,
    "total_tokens": 27827,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}