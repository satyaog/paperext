{
  "paper": "b4691b0de84e189487988f9fa600cdfd.txt",
  "words": 12021,
  "extractions": {
    "title": {
      "value": "In value-based deep reinforcement learning, a pruned network is a good network",
      "justification": "The title is stated at the beginning of the document and sets the main theme of the research, which is about deep reinforcement learning and the use of pruned networks.",
      "quote": "In value-based deep reinforcement learning,\na pruned network is a good network"
    },
    "description": "This paper investigates the effectiveness of gradual magnitude pruning in deep reinforcement learning (RL) to improve network performance while using a smaller fraction of parameters. The study shows that pruned networks enhance parameter efficiency and continue to perform well as the network size scales.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results to demonstrate the effectiveness of gradual magnitude pruning in different settings and with various agents.",
      "quote": "We investigate the general usefulness of gradual magnitude\npruning in deep RL agents in both online and offline settings."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on improving agent performance in deep reinforcement learning settings through pruning techniques.",
        "quote": "In value-based deep reinforcement learning,\na pruned network is a good network"
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Reinforcement Learning",
          "justification": "The research specifically targets deep reinforcement learning by using neural network architectures and agents like DQN and Rainbow.",
          "quote": "Recent work has shown that deep reinforcement\nlearning agents have difficulty in effectively using\ntheir network parameters."
        },
        "aliases": [
          "DRL"
        ]
      },
      {
        "name": {
          "value": "Neural Network Pruning",
          "justification": "The study employs pruning techniques to the neural networks used in reinforcement learning settings.",
          "quote": "gradual magnitude pruning enables value-based agents to maximize\nparameter effectiveness."
        },
        "aliases": [
          "Pruning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DQN",
          "justification": "The paper uses DQN as a baseline model to apply pruning techniques and compare performance.",
          "quote": "One of the most surprising findings of this last work is that applying the gradual magnitude pruning technique proposed by Zhu & Gupta (2017) on DQN (Mnih et al., 2015) with a ResNet backbone (as introduced in Impala (Espeholt et al., 2018)), results in a 50% performance improvement."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "DQN is a pre-existing model utilized in the study rather than a newly developed one.",
          "quote": "The base DQN and Rainbow agents we use the Jax implementations of the Dopamine library."
        },
        "is_executed": {
          "value": true,
          "justification": "DQN is executed to analyze the effects of pruning on performance.",
          "quote": "gradual magnitude pruning on DQN with the Impala architecture, where we have scaled the convolutional layers by a factor of 3."
        },
        "is_compared": {
          "value": true,
          "justification": "DQN is compared with other models like Rainbow in the context of pruning effectiveness.",
          "quote": "we evaluated pruning on DQN and Rainbow over all 60 Atari 2600 games, confirming our findings are not specific to the 15 games initially selected."
        },
        "referenced_paper_title": {
          "value": "Human-level control through deep reinforcement learning",
          "justification": "This is the foundational paper where DQN was introduced by Mnih et al. which is being referred to and used for implementing pruning techniques.",
          "quote": "The idea was introduced by Mnih et al. (2015) with their DQN agent."
        }
      },
      {
        "name": {
          "value": "Rainbow",
          "justification": "Rainbow is one of the RL agents used and tested under the gradual pruning technique.",
          "quote": "One of the most surprising findings of this last work is that applying the gradual magnitude pruning technique proposed by Zhu & Gupta (2017) on DQN (Mnih et al., 2015) with a ResNet backbone (as introduced in Impala (Espeholt et al., 2018)), results in a 50% performance improvement over"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Rainbow is an existing model used to test the pruning methodology rather than contributed by this research.",
          "quote": "The base DQN and Rainbow agents we use the Jax implementations of the Dopamine library."
        },
        "is_executed": {
          "value": true,
          "justification": "Rainbow is executed in experiments to evaluate pruning techniques.",
          "quote": "we evaluated pruning on DQN and Rainbow over all 60 Atari 2600 games."
        },
        "is_compared": {
          "value": true,
          "justification": "Rainbow is compared with DQN to measure the impact of pruning on RL performance.",
          "quote": "we evaluated pruning on DQN and Rainbow over all 60 Atari 2600 games, confirming our findings are not specific to the 15 games initially selected."
        },
        "referenced_paper_title": {
          "value": "Rainbow: Combining improvements in deep reinforcement learning",
          "justification": "This is the primary work by Hessel et al. introducing Rainbow that this study has utilized and compared.",
          "quote": "Rainbow (Hessel et al., 2018) extended, and improved, the"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Atari 2600 suite",
          "justification": "The paper conducts experiments on the Atari 2600 suite to evaluate the performance of pruned networks.",
          "quote": "we evaluated pruning on DQN and Rainbow over all 60 Atari 2600 games, confirming our findings are not specific to the 15 games initially selected."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The arcade learning environment: An evaluation platform for general agents.",
          "justification": "The paper by Bellemare et al. (2013) introduced the Atari 2600 suite as a benchmark for RL agents.",
          "quote": "Most of our experiments were run with 15 games from the ALE suite (Bellemare et al., 2013), as suggested by Graesser et al. (2022)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Dopamine",
          "justification": "Dopamine is explicitly mentioned as the framework used for implementing DQN and Rainbow agents in this study.",
          "quote": "Dopamine code available at https://github.com/\ngoogle/dopamine."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Dopamine: A research framework for deep reinforcement learning.",
          "justification": "The library is mentioned alongside Castro et al., and its usage is clear in the experiments from the paper.",
          "quote": "For the base DQN and Rainbow agents we use the Jax implementations of the Dopamine library."
        }
      },
      {
        "name": {
          "value": "JaxPruner",
          "justification": "JaxPruner is used in the experiments of this paper for gradual magnitude pruning.",
          "quote": "We use the JaxPruner 2 (Lee et al., 2024) library for gradual magnitude pruning, as it already provides integration with Dopamine."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Jax: composable transformations of python+ numpy programs.",
          "justification": "JAX is a foundational library upon which JaxPruner builds, as shown in the paper.",
          "quote": "JaxPruner code available at https://github.com/\ngoogle-research/jaxpruner."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1468,
    "prompt_tokens": 23252,
    "total_tokens": 24720,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}