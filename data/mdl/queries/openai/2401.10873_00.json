{
  "paper": "2401.10873.txt",
  "words": 16825,
  "extractions": {
    "title": {
      "value": "An AI-Resilient Text Rendering Technique for Reading and Skimming Documents",
      "justification": "The title clearly describes the main focus of the paper, which is introducing and evaluating a technique for rendering text in a way that is resilient to AI errors.",
      "quote": "An AI-Resilient Text Rendering Technique for Reading and Skimming Documents"
    },
    "description": "The paper introduces Grammar-Preserving Text Saliency Modulation (GP-TSM), a novel text rendering method that aims to enhance reading and skimming efficiency by modulating text saliency while preserving grammatical correctness. The method leverages recursive sentence compression to identify and de-emphasize non-crucial details in text documents. The GP-TSM algorithm is empirically evaluated through user studies, highlighting its benefits over existing text rendering approaches.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical studies to evaluate the effectiveness of the GP-TSM method compared to other text rendering methods.",
      "quote": "A within-subjects user study (N=18) demonstrates that the final design of GP-TSM not only helps readers complete non-trivial (GRE) reading comprehension tasks more efficiently..."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The study focuses on text rendering and summarization, which are key areas in Natural Language Processing.",
        "quote": "GP-TSM is an extension of a range of prior work on text summarization and text rendering intended for reading, skimming, and information retrieval support."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Human-Computer Interaction",
          "justification": "The study includes human-centered computing aspects such as empirical user studies and the evaluation of interaction designs.",
          "quote": "CCS Concepts: • Human-centered computing → Empirical studies in HCI."
        },
        "aliases": [
          "HCI"
        ]
      },
      {
        "name": {
          "value": "Text Summarization",
          "justification": "The paper discusses and evaluates techniques related to text summarization as a part of its methodology.",
          "quote": "GP-TSM uses a recursive sentence compression method to identify successive levels of detail beyond the core meaning of a passage."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Grammar-Preserving Text Saliency Modulation (GP-TSM)",
          "justification": "GP-TSM is the main model proposed in the paper, targeting enhanced reading and skimming through grammar-preserving text saliency modulation.",
          "quote": "We introduce Grammar-Preserving Text Saliency Modulation (GP-TSM), a text rendering method with a novel means of identifying what to de-emphasize."
        },
        "aliases": [
          "GP-TSM"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is introduced as a new contribution in the paper.",
          "quote": "...we contribute: • The design and implementation of GP-TSM, a recursive sentence-compression-based text rendering method..."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not specify if the model was executed on any hardware during the study.",
          "quote": "...evaluated GP-TSM in two studies—a preliminary user study of the effectiveness of the visualization given a partially automated backend..."
        },
        "is_compared": {
          "value": true,
          "justification": "GP-TSM is directly compared to WF-TSM and other text rendering methods in user studies.",
          "quote": "...the final design of GP-TSM not only helps readers complete non-trivial (GRE) reading comprehension tasks more efficiently, it is also strongly preferred over font opacity modulated by unigram frequency (WF-TSM)."
        },
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "GP-TSM is a new model introduced by this paper itself, so there are no prior reference titles.",
          "quote": "We introduce Grammar-Preserving Text Saliency Modulation (GP-TSM)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GRE reading comprehension test",
          "justification": "The GRE reading comprehension test is used as a benchmark to evaluate the effectiveness of GP-TSM in improving reading efficiency and comprehension.",
          "quote": "...applied to two paragraphs from a passage from the GRE reading comprehension test."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "The GRE test is a standard examination and not reliant on a specific paper reference.",
          "quote": "We chose Graduate Record Examinations (GRE) passages and reading comprehension questions as our tasks..."
        }
      },
      {
        "name": {
          "value": "Web Content Accessibility Guidelines (WCAG)",
          "justification": "WCAG guidelines are referenced for ensuring legibility of the text at lower opacity levels.",
          "quote": "...consistent with guidelines on contrast ratios provided by WCAG (Web Content Accessibility Guidelines)"
        },
        "aliases": [
          "WCAG"
        ],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Web Content Accessibility Guidelines (WCAG)",
          "justification": "WCAG is well-known and does not require a specific paper citation in this context.",
          "quote": "consistent with guidelines on contrast ratios provided by WCAG (Web Content Accessibility Guidelines)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Flask",
          "justification": "Flask is mentioned as the framework used to implement the GP-TSM interface.",
          "quote": "The interface is a Flask application that is configured to make API calls to OpenAI’s GPT4 model."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "Flask is a widely used web framework and does not require a scientific paper citation.",
          "quote": "The interface is a Flask application..."
        }
      },
      {
        "name": {
          "value": "OpenAI's GPT-4",
          "justification": "GPT-4 is used within the recursive sentence compression process as the backend LLM for extractive summarization.",
          "quote": "Our approach is powered by a large language model (LLM). Specifically, we prompt OpenAI’s GPT4..."
        },
        "aliases": [
          "GPT-4"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "GPT-4 is a recognized model by OpenAI and does not necessitate a specific reference in this instance.",
          "quote": "...configured to make API calls to OpenAI’s GPT4 model."
        }
      },
      {
        "name": {
          "value": "Difflib SequenceMatcher",
          "justification": "SequenceMatcher is utilized in identifying word changes introduced by GPT-4 in the summarization process.",
          "quote": "We use a SequenceMatcher to identify words that the LLM has added or changed."
        },
        "aliases": [
          "SequenceMatcher"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Not applicable",
          "justification": "SequenceMatcher is a part of the standard Python library and does not have a specific paper reference.",
          "quote": "We use a SequenceMatcher to identify words that the LLM has added or changed."
        }
      },
      {
        "name": {
          "value": "Sentence Transformers",
          "justification": "Used for calculating semantic fidelity by generating embeddings for similarity comparison.",
          "quote": "...calculated using the cosine similarity of their respective embeddings produced by Sentence Transformers"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
          "justification": "Sentence Transformers are based on Sentence-BERT as depicted by Reimers and Gurevych.",
          "quote": "The semantic fidelity score is the similarity between the original (pre-summarization) paragraph and the shortened paragraph, calculated using the cosine similarity of their respective embeddings produced by Sentence Transformers [76]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1515,
    "prompt_tokens": 27010,
    "total_tokens": 28525,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}