{
  "paper": "a7nvXxNmdV.txt",
  "words": 12323,
  "extractions": {
    "title": {
      "value": "Improved baselines for vision-language pre-training",
      "justification": "The title is present at the start of the research paper",
      "quote": "Improved baselines for vision-language pre-training"
    },
    "description": "This paper investigates the state of the art for Vision-Language Pre-training (VLP) by proposing, implementing, and evaluating several baselines that combine contrastive learning with recent advances in self-supervised learning, resulting in substantial improvements in the performance of the CLIP model using an improved training recipe that incorporates advanced augmentation techniques, label smoothing, and architectural improvements.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes experimental investigations including evaluations of multiple baselines and improvements in training recipes, along with numerical performance comparisons.",
      "quote": "In this paper, we first propose, implement and evaluate several baselines obtained by combining contrastive learning with recent advances in self-supervised learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The primary research field is Vision-Language Pre-training, which involves learning from both images and text, a task related to Natural Language Processing.",
        "quote": "Vision-language pre-training (VLP) is a recent learning paradigm that enables neural networks to learn multimodal representations from images and text."
      },
      "aliases": [
        "Vision-Language Pre-training"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Contrastive Learning",
          "justification": "The research extends the CLIP model, which is based on a contrastive learning framework.",
          "quote": "CLIP (Radford et al., 2021) is a de facto standard for VLP which leverages cross-modal contrastive learning."
        },
        "aliases": [
          "Contrastive Learning for VLP"
        ]
      },
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The research combines contrastive learning with advances in self-supervised learning to improve VLP baselines.",
          "quote": "combine contrastive learning with recent advances in self-supervised learning."
        },
        "aliases": [
          "SSL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is the primary baseline model evaluated and improved upon in the paper.",
          "quote": "Arguably, the most prominent approach to VLP is CLIP (Radford et al., 2021), which leverages a contrastive loss to learn aligned image and text representations from large-scale image-text datasets scraped from the internet."
        },
        "aliases": [
          "CLIP"
        ],
        "is_contributed": {
          "value": 0,
          "justification": "CLIP is not contributed by this paper. Itâ€™s used as a baseline for comparisons.",
          "quote": "Arguably, the most prominent approach to VLP is CLIP (Radford et al., 2021)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper includes implementation and evaluation of CLIP, thus it is executed.",
          "quote": "we first propose, implement and evaluate several baselines obtained by combining contrastive learning with recent advances in self-supervised learning."
        },
        "is_compared": {
          "value": 1,
          "justification": "CLIP is compared with the authors' improved models and other baselines.",
          "quote": "We test these methods in the zero-shot image classification task and we find that they cannot learn strong aligned multimodal features on their own, but can be used in composition with the CLIP loss to boost the performance compared to a standard CLIP baseline."
        },
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "The referenced paper title for CLIP from Radford et al. 2021 is mentioned in the paper.",
          "quote": "CLIP (Radford et al., 2021) is a de facto standard for VLP which leverages cross-modal contrastive learning."
        }
      },
      {
        "name": {
          "value": "CLIP ðŸš€ (CLIP Rocket)",
          "justification": "CLIP ðŸš€ is an improved version of the base CLIP model, incorporating advanced training techniques proposed by the paper.",
          "quote": "we propose a well-tuned improved recipe, CLIP ðŸš€, that dramatically boosts its performance up to 11% on ImageNet (relative boost âˆ¼25%)"
        },
        "aliases": [
          "CLIP-Rocket"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "CLIP ðŸš€ is proposed in this paper as an improved version of the baselines.",
          "quote": "we propose a well-tuned improved recipe, CLIP ðŸš€, that dramatically boosts its performance up to 11% on ImageNet (relative boost âˆ¼25%)"
        },
        "is_executed": {
          "value": 1,
          "justification": "Implemented and evaluated extensively to show improvements over baseline CLIP and other models.",
          "quote": "with our improved training recipe, CLIP ðŸš€, we obtain state-of-the-art performance on four standard datasets."
        },
        "is_compared": {
          "value": 1,
          "justification": "CLIP ðŸš€ is proposed as an improvement over baseline CLIP and is compared against it and other models for performance evaluation.",
          "quote": "our improved training recipe, CLIP ðŸš€, that dramatically boosts its performance up to 11% on ImageNet (relative boost âˆ¼25%)"
        },
        "referenced_paper_title": {
          "value": "This paper (Improved baselines for vision-language pre-training)",
          "justification": "CLIP ðŸš€ is contributed and detailed in the current paper itself.",
          "quote": "with our improved training recipe, CLIP ðŸš€, we obtain state-of-the-art performance on four standard datasets"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "YFCC15M",
          "justification": "YFCC15M is one of the primary datasets used for pre-training and evaluation of the models in the paper.",
          "quote": "The Yahoo Flickr Creative Commons dataset is composed of 100M of image-text pairs (Thomee et al., 2016). Noisy text-image pairs have been filtered by Radford et al. (2021) to obtain a cleaner version of 15M pairs. This dataset is referred to as YFCC15M."
        },
        "aliases": [
          "Yahoo Flickr Creative Commons dataset"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "YFCC100M: The new data in multimedia research",
          "justification": "The referenced paper title for YFCC100M by Thomee et al. (2016) is mentioned in the paper.",
          "quote": "The Yahoo Flickr Creative Commons dataset is composed of 100M of image-text pairs (Thomee et al., 2016)."
        }
      },
      {
        "name": {
          "value": "CC3M",
          "justification": "CC3M is another dataset used for pre-training and evaluation of the models in the paper.",
          "quote": "The Conceptual Captions dataset is composed of image-caption pairs that have been filtered... two datasets have been published in the literature: CC3M (Sharma et al., 2018) composed of 3.3M image-text pairs obtained by applying the full filtering pipeline."
        },
        "aliases": [
          "Conceptual Captions"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
          "justification": "The referenced paper title for the Conceptual Captions dataset by Sharma et al. (2018) is mentioned in the paper.",
          "quote": "CC3M (Sharma et al., 2018) composed of 3.3M image-text pairs obtained by applying the full filtering pipeline."
        }
      },
      {
        "name": {
          "value": "CC12M",
          "justification": "CC12M is another dataset mentioned used for pre-training and evaluation in the paper.",
          "quote": "two datasets have been published in the literature: CC3M...and CC12M (Changpinyo et al., 2021) comprising 12.4M pairs obtained from a relaxed version of the pipeline"
        },
        "aliases": [
          "Conceptual Captions"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
          "justification": "The referenced paper title for CC12M by Changpinyo et al. (2021) is mentioned in the paper.",
          "quote": "CC12M (Changpinyo et al., 2021) comprising 12.4M pairs obtained from a relaxed version of the pipeline."
        }
      },
      {
        "name": {
          "value": "Open29M",
          "justification": "Open29M is described as a combined dataset created specifically for the experiments in this paper.",
          "quote": "To experiment with larger scale datasets we follow Li et al. (2021) and create a dataset as the union of CC3M, CC12M, and YFCC15M, obtaining 29M image-text pairs. We refer to this dataset as Open29M."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "The referenced paper title for Li et al. (2021) which inspired the creation of Open29M is mentioned in the paper.",
          "quote": "To experiment with larger scale datasets we follow Li et al. (2021) and create a dataset as the union of CC3M, CC12M, and YFCC15M, obtaining 29M image-text pairs."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used as the primary deep learning framework in the implementation of the models.",
          "quote": "The code is available at https://github.com/facebookresearch/clip-rocket"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "This paper (Improved baselines for vision-language pre-training)",
          "justification": "The referenced paper detailing the use of PyTorch is the current paper itself.",
          "quote": "The code is available at https://github.com/facebookresearch/clip-rocket"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1974,
    "prompt_tokens": 23157,
    "total_tokens": 25131
  }
}