{
  "paper": "875dd4b6f05e1a232b850149661badfb.txt",
  "words": 11990,
  "extractions": {
    "title": {
      "value": "Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules",
      "justification": "The title of the paper is explicitly stated at the beginning of the document.",
      "quote": "Beyond accuracy: generalization properties of bio-plausible temporal credit assignment rules"
    },
    "description": "This paper investigates the generalization properties of biologically-plausible gradient approximations in training recurrent neural networks (RNNs), focusing on their tendency to converge to solutions with varying generalization expressed through loss landscape curvature. Empirical and theoretical analyses are used to explain the worse and more variable performance compared to standard gradient descent techniques, and potential biologically-inspired mechanisms to improve these generalization properties are suggested.",
    "type": {
      "value": "theoretical",
      "justification": "The paper provides theoretical analyses and a theorem to explain the observed phenomena related to generalization and loss landscape curvature.",
      "quote": "Using tools from dynamical systems, we derive theoretical arguments and present a theorem explaining this phenomenon."
    },
    "primary_research_field": {
      "name": {
        "value": "Neuroscience",
        "justification": "The paper addresses biologically-plausible learning rules, which are fundamentally related to neuroscience, to model learning mechanisms in the brain using recurrent neural networks (RNNs).",
        "quote": "To unveil how the brain learns, ongoing work seeks biologically-plausible approximations of gradient descent algorithms for training recurrent neural networks (RNNs)."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Recurrent Neural Networks",
          "justification": "The study focuses on training methods for recurrent neural networks (RNNs) using biologically-plausible gradient approximations.",
          "quote": "bio-plausible learning rules for training RNNs"
        },
        "aliases": [
          "RNN"
        ]
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The paper compares biologically-plausible learning rules with machine learning counterparts, such as stochastic gradient descent.",
          "quote": "generalization performance compared to true (stochastic) gradient descent"
        },
        "aliases": [
          "ML"
        ]
      },
      {
        "name": {
          "value": "Deep Learning Theory",
          "justification": "The research uses deep learning theory, particularly related to the loss landscape, to analyze generalization properties.",
          "quote": "Leveraging results from deep learning theory based on loss landscape curvature"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "RFLO",
          "justification": "RFLO is mentioned as a bio-plausible model analyzed for training RNNs included in the study.",
          "quote": "rules investigated include a three-factor rule using random feedback weights (RFLO [12])"
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "RFLO is listed as one of the models being investigated for their bio-plausibility and performance characteristics.",
          "quote": "rules investigated include... RFLO [12]"
        },
        "is_executed": {
          "value": false,
          "justification": "The text does not explicitly mention the execution of RFLO on specific hardware like GPU or CPU.",
          "quote": "we investigate different learning rules while holding the data, objective function and architecture constant."
        },
        "is_compared": {
          "value": true,
          "justification": "RFLO is part of the comparison to machine learning algorithms like stochastic gradient descent in terms of generalization and learning characteristics.",
          "quote": "show worse and more variable generalization gaps, compared to true (stochastic) gradient descent (Figure 2A-C)."
        },
        "referenced_paper_title": {
          "value": "Local online learning in recurrent networks with random feedback",
          "justification": "The paper lists RFLO and provides a reference for it, showing it's a recognized model.",
          "quote": "rules investigated include... random feedback weights (RFLO [12])"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Sequential MNIST",
          "justification": "The Sequential MNIST dataset is explicitly mentioned as one of the tasks used for the experiments in the paper.",
          "quote": "We performed experiments on three tasks: sequential MNIST [137], pattern generation [138] and delayed match-to-sample tasks [139]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The mnist database of handwritten digits",
          "justification": "The referenced paper title provided aligns with the common citation for MNIST, acknowledging its original introduction.",
          "quote": "We performed experiments on three tasks: sequential MNIST [137]..."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 860,
    "prompt_tokens": 22207,
    "total_tokens": 23067,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}