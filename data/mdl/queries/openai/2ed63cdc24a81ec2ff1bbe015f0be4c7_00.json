{
  "paper": "2ed63cdc24a81ec2ff1bbe015f0be4c7.txt",
  "words": 9132,
  "extractions": {
    "title": {
      "value": "T2VIndexer: A Generative Video Indexer for Efficient Text-Video Retrieval",
      "justification": "The title is clearly mentioned at the very beginning of the paper and is consistent throughout.",
      "quote": "T2VIndexer: A Generative Video Indexer for Efficient Text-Video Retrieval"
    },
    "description": "The paper proposes T2VIndexer, a generative model-based video indexer designed for efficient text-video retrieval. It introduces video identifier encoding and query-identifier augmentation to represent videos as short sequences, significantly reducing retrieval time while maintaining high accuracy. The model is evaluated on several standard datasets, demonstrating its ability to enhance retrieval efficiency and accuracy.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments on several standard datasets and discusses the retrieval performance of their proposed model, indicating an empirical nature.",
      "quote": "Our method consistently enhances the retrieval efficiency of current state-of-the-art models on four standard datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Video Retrieval",
        "justification": "The research primarily focuses on text-video retrieval, developing methods to improve the efficiency and accuracy of retrieving videos based on text queries.",
        "quote": "Given a query text description, text-video retrieval aims to retrieve videos that are semantically relevant to the query."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-modal Learning",
          "justification": "The paper involves the use of cross-modal interactions between text and video, which is a key aspect of multi-modal learning.",
          "quote": "Current text-video retrieval methods mainly rely on cross-modal matching between queries and videos."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Generative Models",
          "justification": "The paper proposes a generative model-based approach for video indexing and retrieval.",
          "quote": "To enhance retrieval efficiency, in this paper, we introduce a model-based video indexer named T2VIndexer, which is a sequence-to-sequence generative model."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "T2VIndexer",
          "justification": "The paper introduces T2VIndexer as the proposed generative model-based video indexer.",
          "quote": "To enhance retrieval efficiency, in this paper, we introduce a model-based video indexer named T2VIndexer."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "T2VIndexer is the main contribution of the paper, aiming to improve text-video retrieval efficiency.",
          "quote": "To this end, we propose a sequence-to-sequence generative network that supports Text query to Video candidate Index, named as T2VIndexer."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper reports experiments conducted using T2VIndexer on various datasets.",
          "quote": "The code is available at https://anonymous.4open.science/r/T2VIndexer-40BE."
        },
        "is_compared": {
          "value": true,
          "justification": "T2VIndexer's performance is compared against existing methods on standard datasets.",
          "quote": "Our method consistently enhances the retrieval efficiency of current state-of-the-art models on four standard datasets."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "T2VIndexer is introduced in this paper and does not reference an external paper for its definition.",
          "quote": "To enhance retrieval efficiency, in this paper, we introduce a model-based video indexer named T2VIndexer, which is a sequence-to-sequence generative model."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MSR-VTT",
          "justification": "The paper uses the MSR-VTT dataset to evaluate the proposed T2VIndexer model.",
          "quote": "MSR-VTT encompasses 10,000 videos, paired with 200,000 captions."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MSR-VTT: A large video description dataset for bridging video and language",
          "justification": "The dataset is identified with a reference to its original paper.",
          "quote": "MSR-VTT [26] encompasses 10,000 videos, paired with 200,000 captions."
        }
      },
      {
        "name": {
          "value": "MSVD",
          "justification": "The paper uses the MSVD dataset to evaluate the proposed T2VIndexer model.",
          "quote": "MSVD contains 1,970 videos, and a wealth of approximately 40 associated English sentences per video."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Collecting highly parallel data for paraphrase evaluation",
          "justification": "The dataset is identified with a reference to its creation and initial purpose.",
          "quote": "MSVD [5] contains 1,970 videos, and a wealth of approximately 40 associated English sentences per video."
        }
      },
      {
        "name": {
          "value": "ActivityNet Caption",
          "justification": "The paper uses the ActivityNet Caption dataset to evaluate the proposed T2VIndexer model.",
          "quote": "ActivityNet [11] consists of 20,000 YouTube video."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Dense-Captioning Events in Videos",
          "justification": "The dataset is identified with a reference to its source paper.",
          "quote": "ActivityNet [11] consists of 20,000 YouTube video."
        }
      },
      {
        "name": {
          "value": "DiDeMo",
          "justification": "The paper uses the DiDeMo dataset to evaluate the proposed T2VIndexer model.",
          "quote": "DiDeMo contains 10,000 videos annotated with 40,000 sentences."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Localizing moments in video with natural language",
          "justification": "The dataset is identified with a reference to its originating paper.",
          "quote": "DiDeMo [1] contains 10,000 videos annotated with 40,000 sentences."
        }
      },
      {
        "name": {
          "value": "TGIF",
          "justification": "The paper uses the TGIF dataset for large-scale testing of the T2VIndexer model.",
          "quote": "we decided to redivide the TGIF dataset into 50,000 training data and 50,000 testing data in a 5:5 ratio."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TGIF: A New Dataset and Benchmark on Animated GIF Description",
          "justification": "The dataset is identified with a reference to its creation and naming in its originating paper.",
          "quote": "we decided to redivide the TGIF dataset [14] into 50,000 training data and 50,000 testing data in a 5:5 ratio."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is used as the image encoder for video representation in the paper's methodology.",
          "quote": "we utilize CLIP to embed each video, and then cluster and encode the semantic embeddings in a hierarchical mode."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "The paper references the source which introduced CLIP.",
          "quote": "we utilize CLIP [19] to embed each video"
        }
      },
      {
        "name": {
          "value": "T5",
          "justification": "T5 is used as the basis of the generative model's architecture for retrieval.",
          "quote": "we propose to train a generative network based on T5 architecture to enable deep interactions between the query and video identifier."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "The paper references the source of the T5 model architecture.",
          "quote": "we propose to train a generative network based on T5 [4] architecture."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1520,
    "prompt_tokens": 17450,
    "total_tokens": 18970,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}