{
  "paper": "98b2208c1a3615d1fe5124e224372f69.txt",
  "words": 8931,
  "extractions": {
    "title": {
      "value": "Implicit Regularization or Implicit Conditioning? Exact Risk Trajectories of SGD in High Dimensions",
      "justification": "The title of the paper is found at the beginning of the document and summarizes the focus of the research on exact risk trajectories of SGD in high dimensions.",
      "quote": "Implicit Regularization or Implicit Conditioning? Exact Risk Trajectories of SGD in High Dimensions"
    },
    "description": "The paper investigates the dynamics of multi-pass stochastic gradient descent (SGD) on high-dimensional convex quadratic problems, demonstrating its asymptotic equivalence to a homogenized stochastic gradient descent (HSGD) via a stochastic differential equation. It challenges the notion of implicit regularization by proving that noise from SGD negatively impacts generalization performance, while introducing the implicit-conditioning ratio (ICR) to assess the efficiency of SGD compared to full-batch gradient methods.",
    "type": {
      "value": "theoretical",
      "justification": "The paper provides a theoretical analysis of SGD by deriving its dynamics and risk trajectories using mathematical models such as stochastic differential equations, without conducting empirical experiments to validate the theory.",
      "quote": "We study the dynamics of multi-pass SGD on high-dimensional convex quadratics and establish an asymptotic equivalence to a stochastic differential equation, which we call homogenized stochastic gradient descent (HSGD)."
    },
    "primary_research_field": {
      "name": {
        "value": "Optimization Algorithms",
        "justification": "The primary research of the paper focuses on the theoretical understanding and improvement of optimization algorithms, particularly stochastic gradient descent and its variants.",
        "quote": "Stochastic gradient descent (SGD) is a pillar of modern machine learning, serving as the go-to optimization algorithm for a diverse array of problems."
      },
      "aliases": [
        "SGD",
        "Gradient Descent"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Machine Learning Theory",
          "justification": "The paper contributes to machine learning theory by providing theoretical insights into the optimization algorithms used in machine learning, particularly in terms of their efficiency and generalization capabilities.",
          "quote": "Even in the simple setting of convex quadratic problems, worst-case analyses give an asymptotic convergence rate for SGD that is no better than full-batch gradient descent (GD), and the purported implicit regularization effects of SGD lack a precise explanation."
        },
        "aliases": [
          "ML Theory"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "homogenized stochastic gradient descent (HSGD)",
          "justification": "HSGD is introduced and analyzed in the paper as an asymptotic equivalent to SGD for theoretical investigations.",
          "quote": "A stochastic differential equation, which we call homogenized stochastic gradient descent (HSGD), whose solutions we characterize explicitly in terms of a Volterra integral equation."
        },
        "aliases": [
          "HSGD"
        ],
        "is_contributed": {
          "value": true,
          "justification": "HSGD is introduced as a novel theoretical model presented and characterized by the authors in the context of the paper.",
          "quote": "...we call homogenized stochastic gradient descent (HSGD), whose solutions we characterize explicitly..."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper explores HSGD theoretically rather than practically implementing or running it computationally.",
          "quote": "We study the dynamics of multi-pass SGD on high-dimensional convex quadratics and establish an asymptotic equivalence to a stochastic differential equation, which we call homogenized stochastic gradient descent (HSGD)."
        },
        "is_compared": {
          "value": false,
          "justification": "HSGD is used as a theoretical tool rather than as a competitive benchmark against other practical algorithms.",
          "quote": "The paper does not directly compare HSGD to other models on numerical performance metrics."
        },
        "referenced_paper_title": {
          "value": "Homogenization of SGD in high-dimensions: exact dynamics and generalization properties",
          "justification": "HSGD is referenced from a paper titled 'Homogenization of SGD in high-dimensions: exact dynamics and generalization properties', providing context to its theoretical framework.",
          "quote": "Homogenization of SGD in high-dimensions: exact dynamics and generalization properties [61]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-5m",
          "justification": "CIFAR-5m is mentioned as part of a benchmark in evaluating theoretical predictions made by HSGD against SGD.",
          "quote": "Single runs of SGD vs. HSGD (Volterra) in streaming on standarized CIFAR-5M [54] with car/plane class vector (1, 000, 000 samples)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers",
          "justification": "The referenced work provides background and context for using CIFAR-5m in machine learning experiments.",
          "quote": "The Deep Bootstrap Framework: Good Online Learners are Good Offline Generalizers. In International Conference on Learning Representations (ICLR), 2021."
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is referenced as one of the datasets traditionally used for benchmarking machine learning models, thus allowing for theoretical generalization claims.",
          "quote": "We use three data sets to verify our claims. All three data sets (MNIST [37], CIFAR-10 [34], and CIFAR-5m are open source and available in TensorFlow."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "\"mnist\" handwritten digit database",
          "justification": "The dataset used for experiments is commonly associated with the cited reference for the MNIST database.",
          "quote": "\"mnist\" handwritten digit database, 2010."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is mentioned in the context of experiments and algorithm evaluations associated with generalization metrics.",
          "quote": "We use three data sets to verify our claims. All three data sets (MNIST [37], CIFAR-10 [34], and CIFAR-5m are open source and available in TensorFlow."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "The referenced technical report is recognized for introducing and describing the CIFAR-10 dataset.",
          "quote": "Learning multiple layers of features from tiny images. Technical report, U. of Toronto, 2009."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1272,
    "prompt_tokens": 16394,
    "total_tokens": 17666,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}