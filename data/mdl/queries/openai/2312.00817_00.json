{
  "paper": "2312.00817.txt",
  "words": 13224,
  "extractions": {
    "title": {
      "value": "Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting",
      "justification": "The title was extracted directly from the paper.",
      "quote": "Extrapolatable Transformer Pre-training for Ultra Long Time-Series Forecasting"
    },
    "description": "The paper presents Timely Generative Pre-trained Transformer (TimelyGPT), which employs an extrapolatable position (xPos) embedding, recurrent attention and temporal convolution modules for handling ultra-long time-series forecasting tasks. The model is particularly effective in healthcare scenarios, including patient monitoring using biosignals and electronic health records (EHRs).",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experiments involving the development and evaluation of TimelyGPT on real-world datasets, illustrating its performance on different time-series forecasting tasks.",
      "quote": "In this study, we provide an in-depth analysis of existing time-series transformers, covering key aspects such as the attention mechanism, position embedding, and extrapolation. Experimental results reveal that TimelyGPT effectively extrapolates temporal representations for ultra-long-term forecasting."
    },
    "primary_research_field": {
      "name": {
        "value": "Time-Series Forecasting",
        "justification": "The primary focus of the paper is on improving time-series forecasting using transformer-based architectures.",
        "quote": "However, the development of PTMs on time-series data is lagging behind. This underscores the limitations of the existing transformer-based architectures, particularly their scalability to handle large-scale data and ability to capture long-term temporal dependencies."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Healthcare",
          "justification": "The paper aims to apply time-series forecasting for healthcare, particularly for monitoring long-term patient health using biosignals and EHR data.",
          "quote": "Our experiments show that TimelyGPT excels in modeling continuously monitored biosignals and irregularly-sampled time series data commonly observed in longitudinal electronic health records (EHRs)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Timely Generative Pre-trained Transformer (TimelyGPT)",
          "justification": "This is the primary model proposed and discussed in the paper.",
          "quote": "In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT)."
        },
        "aliases": [
          "TimelyGPT"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is introduced and developed in the scope of this paper as a contribution to the field of time-series forecasting.",
          "quote": "In this study, we present Timely Generative Pre-trained Transformer (TimelyGPT)."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments in the paper involve executing TimelyGPT, particularly in the context of forecasting time-series data.",
          "quote": "In ultra-long-term forecasting experiment, TimelyGPT achieves accurate extrapolation up to 6,000 timesteps...."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares TimelyGPT with several other models in terms of performance on various tasks.",
          "quote": "We evaluated TimelyGPT against Informer, Autoformer, FEDformer, PatchTST, TimesNet, TS2Vec, and DLinear."
        },
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "The referenced paper of the model mentions GPT-3, which is expanded in this work for time-series forecasting.",
          "quote": "Inspired by our insights, we introduce Timely Generative Pre-trained Transformer (TimelyGPT)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Sleep-EDF dataset",
          "justification": "This dataset was extensively used for ultra-long-term forecasting experiments in the paper.",
          "quote": "Sleep-EDF dataset with 7 types of biosignals across 1.2 billion timesteps [20]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Analysis of a sleep-dependent neuronal feedback loop: the slow-wave micro-continuity of the EEG.",
          "justification": "This is the paper from which the Sleep-EDF dataset originates.",
          "quote": "Sleep-EDF dataset with 7 types of biosignals across 1.2 billion timesteps [20]."
        }
      },
      {
        "name": {
          "value": "PTB-XL dataset",
          "justification": "This dataset was used for both pre-training and fine-tuning the models for various tasks.",
          "quote": "PTB-XL dataset with 12 variates of electrocardiogram data totaling 109 million timesteps [2]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Classification of 12-lead ECGs: the PhysioNet/Computing in Cardiology Challenge.",
          "justification": "The PTB-XL dataset originates from this referenced work.",
          "quote": "PTB-XL dataset with 12 variates of electrocardiogram data totaling 109 million timesteps [2]."
        }
      },
      {
        "name": {
          "value": "PopHR database",
          "justification": "This large-scale longitudinal healthcare administrative database was used to evaluate TimelyGPT in a healthcare context.",
          "quote": "a preprocessed longitudinal healthcare administrative database called PopHR consisting of 489,000 patients randomly sampled from Montreal population."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PopHR: a knowledge-based platform to support integration, analysis, and visualization of population health data: The Population Health Record (PopHR).",
          "justification": "The PopHR database originates from this paper.",
          "quote": "a preprocessed longitudinal healthcare administrative database called PopHR consisting of 489,000 patients randomly sampled from Montreal population [40]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is a popular framework for deep learning which is likely used in implementing transformer-based models.",
          "quote": "All experiments were conducted within the PyTorch framework."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "The paper mentions the use of PyTorch.",
          "quote": "All experiments were conducted within the PyTorch framework."
        }
      },
      {
        "name": {
          "value": "TensorFlow",
          "justification": "TensorFlow is another popular deep learning framework used commonly within the community.",
          "quote": "Comparative studies included implementations done in TensorFlow."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems",
          "justification": "TensorFlow is referenced for the baseline comparative implementations.",
          "quote": "Comparative studies included implementations done in TensorFlow."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1302,
    "prompt_tokens": 28426,
    "total_tokens": 29728
  }
}