{
  "paper": "81d89cb8975c07ab72f31cc891d0e05f.txt",
  "words": 21848,
  "extractions": {
    "title": {
      "value": "AugmenToxic: Leveraging Reinforcement Learning to Optimize LLM Instruction Fine-Tuning for Data Augmentation to Enhance Toxicity Detection",
      "justification": "The title clearly indicates the focus of the paper on reinforcement learning, optimization of large language models (LLM) instruction fine-tuning, and enhancing toxicity detection.",
      "quote": "AugmenToxic: Leveraging Reinforcement Learning to Optimize LLM Instruction Fine-Tuning for Data Augmentation to Enhance Toxicity Detection"
    },
    "description": "The paper presents a novel approach to enhance toxicity detection in online conversations by addressing imbalanced datasets. The methodology involves instruction fine-tuning of Large Language Models (LLMs) using Reinforcement Learning with Human Feedback (RLHF) and employing the Proximal Policy Optimizer (PPO). The goal is to generate toxic responses to create a balanced dataset, using the Google Perspective API for evaluating toxicity and assigning rewards. This leads to improved toxicity detection and enhanced classifier performance.",
    "type": {
      "value": "empirical",
      "justification": "The paper performs experiments with models and datasets to demonstrate the efficacy of their proposed reinforcement learning approach in improving toxicity detection.",
      "quote": "We evaluated the proposed method and compared it with other techniques using the Jigsaw toxic dataset [26] and the ToxiGen dataset [52]."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper deals with the usage of Large Language Models (LLMs), text data augmentation, and reinforcement learning in the context of processing and understanding human language.",
        "quote": "CCS Concepts: · Computing methodologies → Natural language generation; Neural networks; Sequential decision making; Neural networks; Natural language processing;"
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text Data Augmentation",
          "justification": "The paper emphasizes the creation of synthetic data to address class imbalance and improve model training for toxicity detection.",
          "quote": "AugmenToxic: Leveraging Reinforcement Learning to Optimize LLM Instruction Fine-Tuning for Data Augmentation to Enhance Toxicity Detection"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Reinforcement Learning",
          "justification": "The methodology involves Reinforcement Learning with Human Feedback (RLHF) and the use of the Proximal Policy Optimizer (PPO) for model fine-tuning.",
          "quote": "Leveraging generative LLM, we utilize the Proximal Policy Optimizer (PPO) as the RL algorithm to ine-tune the model further and align it with human feedback."
        },
        "aliases": [
          "RL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "FLAN-T5",
          "justification": "FLAN-T5 is used in the paper as a pre-trained model that is fine-tuned for toxic text data augmentation.",
          "quote": "In our study, we employed FLAN-T5 [25] for the data augmentation task by paraphrasing existing samples in the minority class."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "FLAN-T5 is a pre-trained model used in the study, not a new model contributed by the paper.",
          "quote": "In our study, we employed FLAN-T5 [25] for the data augmentation task."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses the actual use and fine-tuning of FLAN-T5 for generating paraphrased toxic responses, indicating execution.",
          "quote": "Instruct fine-tuned FLAN-T5; our approach incorporates a reward model within the PPO framework to ensure the generated responses maintain the specified level of toxicity."
        },
        "is_compared": {
          "value": true,
          "justification": "FLAN-T5's performance in generating toxic samples is compared with other methods and models in terms of various metrics.",
          "quote": "We evaluated the proposed method and compared it with other techniques using the Jigsaw toxic dataset [26] and the ToxiGen dataset [52]."
        },
        "referenced_paper_title": {
          "value": "Scaling Instruction-Finetuned Language Models",
          "justification": "The reference paper for FLAN-T5 likely provides detailed insights on its scaling and instruction-finetuning, which is relevant to its application in this paper.",
          "quote": "In our study, we employed FLAN-T5 [25] for the data augmentation task."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Jigsaw toxic dataset",
          "justification": "The dataset is used to evaluate the performance of the proposed toxic text generation approach.",
          "quote": "We evaluated the proposed method and compared it with other techniques using the Jigsaw toxic dataset [26] and the ToxiGen dataset [52]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Toxic Comment Classification Challenge",
          "justification": "The referenced paper likely provides the original dataset used for training and evaluating toxicity detection models.",
          "quote": "using the Jigsaw toxic dataset [26]"
        }
      },
      {
        "name": {
          "value": "ToxiGen dataset",
          "justification": "The dataset is used to generate toxic samples and evaluate the model's performance.",
          "quote": "We evaluated the proposed method and compared it with other techniques using the Jigsaw toxic dataset [26] and the ToxiGen dataset [52]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ToxiGen: A Large-Scale Machine-Generated Dataset for Adversarial and Implicit Hate Speech Detection",
          "justification": "The reference provides the creation and purpose of the ToxiGen dataset for hate speech detection.",
          "quote": "ToxiGen dataset [52]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Google Perspective API",
          "justification": "Google Perspective API is used as a toxicity evaluator to guide the reinforcement learning model with rewards and penalties.",
          "quote": "We utilize the Google Perspective API as a toxicity evaluator to assess generated responses and assign rewards/penalties accordingly."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "What do Perspective’s scores mean?",
          "justification": "The reference likelu discusses how Perspective API's scoring mechanism works, crucial for its application in this study.",
          "quote": "We utilize the Google Perspective API as a toxicity evaluator"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1222,
    "prompt_tokens": 41795,
    "total_tokens": 43017,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}