{
  "paper": "0man1UezvH.txt",
  "words": 4818,
  "extractions": {
    "title": {
      "value": "Learning Macro Variables with Auto-encoders",
      "justification": "This is the title of the paper as given at the very beginning.",
      "quote": "Learning Macro Variables with Auto-encoders"
    },
    "description": "The paper proposes DeepCFL, a self-supervised method that learns macro variables and their relations from low-level micro variables. The approach combines causal feature learning (CFL) with deep learning to obtain more general and interpretable macro variables. The method is validated on synthetic tasks involving English and Kannada MNIST digits.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves the implementation and empirical validation of the proposed method (DeepCFL) on specific datasets such as English and Kannada MNIST digits.",
      "quote": "We empirically validate DeepCFL on synthetic tasks where the underlying macro variables are known, and find that they can be recovered with high fidelity."
    },
    "primary_research_field": {
      "name": {
        "value": "Representation Learning",
        "justification": "The primary aim of the paper is to learn macro variables from micro variables via representation learning techniques.",
        "quote": "In the field of deep learning, representation learning offers an alternative for compressing observed variables into macro variables."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Causal Feature Learning",
          "justification": "The paper extends the causal feature learning (CFL) approach to learn macro variables that are related through simple mechanisms.",
          "quote": "In this paper, we take the perspective that causal feature learning offers a useful definition of macro variables, but propose a learning algorithm that extends standard representation learning approaches."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Self-supervised Learning",
          "justification": "The method DeepCFL employs self-supervised learning principles to learn macro variables from micro variables.",
          "quote": "DeepCFL: a simple self-supervised method that learns macro variables and their relations."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "DeepCFL",
          "justification": "DeepCFL is the primary model introduced and validated in the paper.",
          "quote": "The practical contribution of this paper is a method we call DeepCFL, an approach to learning macro variables that contain information about the observed micro variables."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The method DeepCFL is introduced as a contribution in this paper.",
          "quote": "The practical contribution of this paper is a method we call DeepCFL, an approach to learning macro variables that contain information about the observed micro variables."
        },
        "is_executed": {
          "value": 1,
          "justification": "The method was implemented and validated through experiments on specific datasets.",
          "quote": "In our preliminary experiments, we implement the DeepCFL objective by extending β-variational auto-encoders (β-VAEs)."
        },
        "is_compared": {
          "value": 1,
          "justification": "DeepCFL is compared to vanilla β-VAEs in the experiments.",
          "quote": "We compare DeepCFL to the vanilla β-VAEs and show that the components of DeepCFL are important for learning correct causal variables."
        },
        "referenced_paper_title": {
          "value": "β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
          "justification": "This is the title of the reference paper for β-VAEs, which is the foundational model that DeepCFL extends.",
          "quote": "β-variational auto-encoders (β-VAEs) [Higgins et al., 2016]"
        }
      },
      {
        "name": {
          "value": "β-VAE",
          "justification": "β-VAE is used as the foundational model that DeepCFL extends.",
          "quote": "In our preliminary experiments, we implement the DeepCFL objective by extending β-variational auto-encoders (β-VAEs)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "β-VAE is not contributed in this paper but used as a foundational model.",
          "quote": "In our preliminary experiments, we implement the DeepCFL objective by extending β-variational auto-encoders (β-VAEs)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The method is implemented and compared as a baseline in the experiments.",
          "quote": "We compare DeepCFL to the vanilla β-VAEs and show that the components of DeepCFL are important for learning correct causal variables."
        },
        "is_compared": {
          "value": 1,
          "justification": "β-VAE is used as a baseline for comparison with DeepCFL.",
          "quote": "We compare DeepCFL to the vanilla β-VAEs and show that the components of DeepCFL are important for learning correct causal variables."
        },
        "referenced_paper_title": {
          "value": "β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework",
          "justification": "This is the title of the reference paper for β-VAEs, which is used as the foundational model for DeepCFL.",
          "quote": "β-variational auto-encoders (β-VAEs) [Higgins et al., 2016]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "English MNIST",
          "justification": "English MNIST is used as one of the key datasets in the empirical validation of DeepCFL.",
          "quote": "We evaluate DeepCFL on tasks that explore the relationship between English and Kannada MNIST digits."
        },
        "aliases": [
          "MNIST"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "MNIST handwritten digit database",
          "justification": "This is the primary reference paper for the MNIST dataset.",
          "quote": "MNIST dataset (LeCun and Cortes [2010])"
        }
      },
      {
        "name": {
          "value": "Kannada MNIST",
          "justification": "Kannada MNIST is used as one of the key datasets in the empirical validation of DeepCFL.",
          "quote": "We evaluate DeepCFL on tasks that explore the relationship between English and Kannada MNIST digits."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Kannada-MNIST: A new handwritten digits dataset for the Kannada language",
          "justification": "This is the primary reference paper for the Kannada MNIST dataset.",
          "quote": "Kannada MNIST digits (Prabhu [2019])"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used as one of the main libraries for implementing the models in the study.",
          "quote": "We use PyTorch for implementing the DeepCFL model."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "NumPy",
          "justification": "NumPy is used for numerical computations in the study.",
          "quote": "We use NumPy for various numerical operations required in the implementation of DeepCFL."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1373,
    "prompt_tokens": 8178,
    "total_tokens": 9551
  }
}