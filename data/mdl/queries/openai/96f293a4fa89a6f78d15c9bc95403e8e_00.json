{
  "paper": "96f293a4fa89a6f78d15c9bc95403e8e.txt",
  "words": 9661,
  "extractions": {
    "title": {
      "value": "Local Structure Matters Most: Perturbation Study in NLU",
      "justification": "The title 'Local Structure Matters Most: Perturbation Study in NLU' is explicitly stated at the beginning of the paper, providing a clear indication of the paper's focus on the importance of local structure in natural language understanding (NLU) models.",
      "quote": "Local Structure Matters Most: Perturbation Study in NLU"
    },
    "description": "The paper investigates the sensitivity of neural language models to word-order perturbations, particularly focusing on the effects of perturbations at different granularities (words, subwords, and characters) on model performance in natural language understanding (NLU) tasks. The study finds that models primarily rely on the local structure of text and are relatively insensitive to the global order, supporting this with experiments controlling for various factors.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves a series of experiments on neural models to analyze their sensitivity to text perturbations, which is typical of empirical research.",
      "quote": "We demonstrate the sensitivity to local structure of model performances in English natural language understanding (NLU) (GLUE (Wang et al., 2019a)) and their relative insensitivity to the global structure."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on neural models and their performance in natural language understanding tasks, which is a central topic within Natural Language Processing (NLP).",
        "quote": "...show that neural models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding..."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Understanding",
          "justification": "The paper specifically addresses natural language understanding tasks and examines how perturbations affect model performance in this area.",
          "quote": "...models, invariant of their inductive biases, pretraining scheme, or the choice of tokenization, mostly rely on the local structure of text to build understanding and make limited use of the global structure."
        },
        "aliases": [
          "NLU"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BiLSTM",
          "justification": "The BiLSTM model is mentioned as part of the experiments conducted to observe the effects of perturbations.",
          "quote": "We experiment with BiLSTMs (Schuster and Paliwal, 1997)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The BiLSTM model is used in experiments but is not a novel contribution of this paper.",
          "quote": "We experiment with BiLSTMs (Schuster and Paliwal, 1997)..."
        },
        "is_executed": {
          "value": true,
          "justification": "The BiLSTM model was executed as part of the experiments to test perturbation effects.",
          "quote": "We experiment with BiLSTMs..."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of BiLSTM is compared with other models to understand sensitivity to perturbations.",
          "quote": "...a greater chance to be sensitive to this perturbation..."
        },
        "referenced_paper_title": {
          "value": "Bidirectional recurrent neural networks",
          "justification": "Cited as the original source of the BiLSTM architecture that is being employed in the experiments.",
          "quote": "We experiment with BiLSTMs (Schuster and Paliwal, 1997)..."
        }
      },
      {
        "name": {
          "value": "Transformer",
          "justification": "The Transformer model is mentioned as part of the experiments to compare sensitivity to perturbations.",
          "quote": "We experiment with...Transformers (Vaswani et al., 2017)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The Transformer model is not a novel contribution of this paper but is used in the experiments.",
          "quote": "We experiment with...Transformers (Vaswani et al., 2017)..."
        },
        "is_executed": {
          "value": true,
          "justification": "The Transformer model was executed as part of the experimental analysis.",
          "quote": "We experiment with...Transformers..."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of Transformers is compared with other models to evaluate their structural sensitivity.",
          "quote": "...a greater chance to be sensitive to this perturbation..."
        },
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "Cited as the original introduction of the Transformer model architecture.",
          "quote": "We experiment with...Transformers (Vaswani et al., 2017)..."
        }
      },
      {
        "name": {
          "value": "ConvNet",
          "justification": "ConvNet models are included in the experiments to understand their reaction to perturbations.",
          "quote": "We experiment with BiLSTMs...and ConvNets..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "ConvNet is utilized in the study to test its properties but is not developed further in this paper.",
          "quote": "We experiment with BiLSTMs...and ConvNets..."
        },
        "is_executed": {
          "value": true,
          "justification": "ConvNet was executed as part of the experimental set up to compare its sensitivity to perturbations.",
          "quote": "We experiment with...ConvNets..."
        },
        "is_compared": {
          "value": true,
          "justification": "The ConvNet's performance is compared with other models to analyze sensitivity to structural changes.",
          "quote": "...a greater chance to be sensitive to this perturbation..."
        },
        "referenced_paper_title": {
          "value": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
          "justification": "ConvNet's original development and use case cited in the paper are for experiments.",
          "quote": "The ConvNet architecture is the one described in Collobert and Weston (2008)..."
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "RoBERTa is used in experiments to evaluate perturbation sensitivity.",
          "quote": "We experiment with three flavor of PT Transformers (RoBERTa-Base..."
        },
        "aliases": [
          "RoBERTa-Base"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The RoBERTa model is not introduced in this paper but used for experimentation.",
          "quote": "We experiment with three flavor of PT Transformers (RoBERTa-Base..."
        },
        "is_executed": {
          "value": true,
          "justification": "RoBERTa was executed to test its performance on perturbed textual datasets.",
          "quote": "We experiment with three flavor of PT Transformers (RoBERTa-Base..."
        },
        "is_compared": {
          "value": true,
          "justification": "RoBERTa's performance is compared with other models.",
          "quote": "While the IDC metric correlates somewhat with performance, it fails to distinguish between neighbor flipping perturbations and phrase shuffle perturbations..."
        },
        "referenced_paper_title": {
          "value": "Roberta: A robustly optimized bert pretraining approach",
          "justification": "Referenced in the paper as a pre-existing architecture used in the experimentation phase.",
          "quote": "We experiment with three flavor of PT Transformers (RoBERTa-Base (Liu et al., 2019c)..."
        }
      },
      {
        "name": {
          "value": "BART",
          "justification": "BART is used to evaluate perturbation effects in the experiments.",
          "quote": "BART-Base..."
        },
        "aliases": [
          "BART-Base"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The BART model is used for experimentation but is not a new development in this paper.",
          "quote": "BART-Base..."
        },
        "is_executed": {
          "value": true,
          "justification": "BART was executed and analyzed for its sensitivity to various perturbations.",
          "quote": "BART-Base..."
        },
        "is_compared": {
          "value": true,
          "justification": "BART's performance is compared against other models in the study.",
          "quote": "The PT BART model has results that are very much inline with the PT RoBERTa model."
        },
        "referenced_paper_title": {
          "value": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
          "justification": "Cited as a pre-existing model architecture used in empirical research.",
          "quote": "We experiment with three flavor of PT Transformers (RoBERTa-Base, BART-Base..."
        }
      },
      {
        "name": {
          "value": "CharBERT",
          "justification": "CharBERT is included among the models used to explore the effects of perturbations.",
          "quote": "CharBERT-Base..."
        },
        "aliases": [
          "CharBERT-Base"
        ],
        "is_contributed": {
          "value": false,
          "justification": "CharBERT is recognized from previous work and used in the experiments.",
          "quote": "CharBERT-Base..."
        },
        "is_executed": {
          "value": true,
          "justification": "CharBERT was executed during the experiments to assess its response to text perturbations.",
          "quote": "CharBERT-Base..."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of CharBERT is compared to other models in assessing perturbation impacts.",
          "quote": "The PT CharBERT seem roughly inline with the other PT models..."
        },
        "referenced_paper_title": {
          "value": "Charbert: Character-aware pre-trained language model",
          "justification": "Mentioned in the references as an existing model employed during experimental phases.",
          "quote": "...CharBERT-Base (Ma et al., 2020))..."
        }
      },
      {
        "name": {
          "value": "BERT",
          "justification": "BERT is discussed in the context of syntactic understanding and sensitivity for comparison purposes, though not directly experimented on in this paper.",
          "quote": "Goldberg (2019); Hewitt and Manning (2019) both show that BERT (Devlin et al., 2019) models have some syntactic capacity."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The BERT model is referenced from prior work for comparative insights but is not directly contributed by this paper.",
          "quote": "Glavaš and Vulić (2020) showed that pretraining BERT on syntax does not seem to improve downstream performance much."
        },
        "is_executed": {
          "value": false,
          "justification": "BERT is referenced for previous findings and insights but is not practically executed in this research.",
          "quote": "Goldberg (2019); Hewitt and Manning (2019) both show that BERT (Devlin et al., 2019) models have some syntactic capacity."
        },
        "is_compared": {
          "value": false,
          "justification": "BERT is mentioned to provide context on syntactic capability but not numerically compared within this paper's experiments.",
          "quote": "BERT is not directly compared in empirical results in this paper; rather discussed for previous findings."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of deep bidirectional transformers for language understanding",
          "justification": "Referenced for understanding the syntactic capacity and capabilities of models.",
          "quote": "Devlin et al. (2019)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GLUE Benchmark",
          "justification": "GLUE is used extensively in the experiments to assess model performance under perturbations.",
          "quote": "We experiment with the GLUE Benchmark (Wang et al., 2019a) datasets..."
        },
        "aliases": [
          "GLUE"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
          "justification": "GLUE is a widely recognized dataset referenced in the context of the paper's experimental framework.",
          "quote": "We experiment with the GLUE Benchmark (Wang et al., 2019a) datasets..."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2342,
    "prompt_tokens": 18529,
    "total_tokens": 20871,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}