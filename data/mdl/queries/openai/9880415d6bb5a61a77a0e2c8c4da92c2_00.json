{
  "paper": "9880415d6bb5a61a77a0e2c8c4da92c2.txt",
  "words": 10445,
  "extractions": {
    "title": {
      "value": "Towards Scaling Difference Target Propagation by Learning Backprop Targets",
      "justification": "The title is explicitly mentioned at the top of the paper and at several other instances, including the footers of pages.",
      "quote": "Towards Scaling Difference Target Propagation by Learning Backprop Targets"
    },
    "description": "This paper proposes a novel feedback weight training scheme for the Difference Target Propagation (DTP) algorithm to ensure it matches backpropagation (BP) performance on complex tasks while remaining biologically plausible. The proposed method introduces the Jacobian Matching Condition (JMC) and a Local Difference Reconstruction Loss (L-DRL), which significantly improves the alignment of feedback and feedforward weights. Experiments on datasets like CIFAR-10 and ImageNet 32×32 show that this method achieves state-of-the-art DTP performance, closely matching that of BP.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experimental results validating the proposed method's effectiveness on different datasets, implying it is an empirical study.",
      "quote": "Our theory is corroborated by experimental results and we report the best performance ever achieved by DTP on CIFAR-10 and ImageNet 32×32."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper deals with an advancement in learning algorithms (Difference Target Propagation) within neural networks, which is a subfield of Deep Learning.",
        "quote": "The development of biologically-plausible learning algorithms is important for understanding learning in the brain..."
      },
      "aliases": [
        "DL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Neural Networks",
          "justification": "The paper focuses on learning algorithms applicable to neural network architectures, specifically proposing improvements to the Difference Target Propagation algorithm.",
          "quote": "...Difference Target Propagation (DTP), a biologically-plausible learning algorithm whose close relation with Gauss-Newton (GN) optimization has been recently established."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Difference Target Propagation (DTP)",
          "justification": "Difference Target Propagation is the central model discussed and improved upon in the paper, with a focus on its performance and training scheme.",
          "quote": "One such algorithm is Difference Target Propagation (DTP), a biologically-plausible learning algorithm..."
        },
        "aliases": [
          "DTP"
        ],
        "is_contributed": {
          "value": false,
          "justification": "DTP is not newly introduced in this paper but rather modified and improved.",
          "quote": "One such algorithm is Difference Target Propagation (DTP), a biologically-plausible learning algorithm..."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments including training results on datasets imply that the model was executed.",
          "quote": "Finally, we validate our novel implementation of DTP on training experiments on MNIST, Fashion MNIST, CIFAR-10 and ImageNet 32×32..."
        },
        "is_compared": {
          "value": true,
          "justification": "DTP's performance is compared to the standard backpropagation and other implementations in terms of accuracy and efficiency.",
          "quote": "...we observe that L-DRL achieves an angle of ≈ 3 ◦ and a relative distance of ≈ 0, while DRL can only reduce these quantities to 18.7 ◦ and ≈ 1.8 respectively..."
        },
        "referenced_paper_title": {
          "value": "Difference target propagation",
          "justification": "The original implementation and theoretical background of DTP are discussed, and a referenced work titled 'Difference target propagation' by Lee et al. is noted.",
          "quote": "Difference Target Propagation (Lee et al., 2015)..."
        }
      },
      {
        "name": {
          "value": "Backpropagation (BP)",
          "justification": "Backpropagation is the baseline against which DTP's performance is compared in the paper.",
          "quote": "Moreover, good alignment between DTP weight updates and loss gradients is only loosely guaranteed..."
        },
        "aliases": [
          "BP"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Backpropagation is a well-known algorithm and not newly contributed by this paper.",
          "quote": "Moreover, good alignment between DTP weight updates and loss gradients is only loosely guaranteed..."
        },
        "is_executed": {
          "value": false,
          "justification": "While BP is consistently used as a benchmark, there is no direct execution evidence provided in the paper for BP itself.",
          "quote": "We say that such an architecture satisfies the Gradient Matching Property (GMP)."
        },
        "is_compared": {
          "value": true,
          "justification": "BP is used as the key comparison point for evaluating the effectiveness of the new DTP variant.",
          "quote": "...our DTP implementation subsequently outperforms DDTP (Meulemans et al., 2020) on all training tasks and approaches the BP baseline performance."
        },
        "referenced_paper_title": {
          "value": "Backpropagation applied to handwritten zip code recognition",
          "justification": "This is a classic and foundational work referenced multiple times when discussing BP context in the paper.",
          "quote": "LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551, 1989."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is mentioned as a dataset used in the experimental validation of the models discussed in the paper.",
          "quote": "...we report the best performance ever achieved by DTP on CIFAR-10 and ImageNet 32×32."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "This is a foundational dataset paper commonly referred to when CIFAR-10 is used in experiments.",
          "quote": "Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images."
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is utilized for experiments in the paper to validate the proposed improvements in DTP.",
          "quote": "Finally, we validate our novel implementation of DTP on training experiments on MNIST, Fashion MNIST, CIFAR-10 and ImageNet 32×32..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-based learning applied to document recognition",
          "justification": "The seminal paper introducing the MNIST dataset.",
          "quote": "LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 1998."
        }
      },
      {
        "name": {
          "value": "Fashion MNIST",
          "justification": "Fashion MNIST is used alongside MNIST as another dataset for testing the algorithms.",
          "quote": "Finally, we validate our novel implementation of DTP on training experiments on MNIST, Fashion MNIST, CIFAR-10 and ImageNet 32×32..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms",
          "justification": "The paper that introduces Fashion MNIST is likely referred to when discussing datasets.",
          "quote": "Xiao, H., Rasul, K., & Vollgraf, R. (2017). Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms."
        }
      },
      {
        "name": {
          "value": "ImageNet 32×32",
          "justification": "ImageNet 32×32 is used in experiments to evaluate the performance of the proposed DTP implementation.",
          "quote": "Finally, we validate our novel implementation of DTP on training experiments on MNIST, Fashion MNIST, CIFAR-10 and ImageNet 32×32..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Conditional image generation with pixelcnn decoders",
          "justification": "The downsampled version of ImageNet used matches the description and likely references this work due to its relevance in similar contexts.",
          "quote": "van den Oord, A., Kalchbrenner, N., Espeholt, L., kavukcuoglu, k., Vinyals, O., and Graves, A. Conditional image generation with pixelcnn decoders."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1688,
    "prompt_tokens": 20880,
    "total_tokens": 22568,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}