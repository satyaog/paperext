{
  "paper": "bf81741bd1e0319da7785636f561504b.txt",
  "words": 12365,
  "extractions": {
    "title": {
      "value": "Mixture of Experts in a Mixture of RL settings",
      "justification": "The title 'Mixture of Experts in a Mixture of RL settings' is clearly stated at the beginning of the paper, indicating the focus of the work.",
      "quote": "Mixture of Experts in a Mixture of RL settings"
    },
    "description": "This paper explores the use of Mixture of Experts (MoEs) in Deep Reinforcement Learning (DRL) settings characterized by non-stationarity. The study investigates how MoEs can enhance the performance of DRL in multi-task and continual learning environments by improving learning capacity and mitigating plasticity loss.",
    "type": {
      "value": "empirical",
      "justification": "The paper is empirical as it conducts experiments using the PureJaxRL codebase and evaluates the performance of MoEs in different DRL settings, providing quantitative analysis and results.",
      "quote": "We chose to run our experiments with the PureJaxRL codebase (Lu et al., 2022b;a; 2023), which is a high-performance and parallelisable library including an implementation of Proximal Policy Optimisation (Schulman et al., 2017, PPO)."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Reinforcement Learning",
        "justification": "The primary research field is Deep Reinforcement Learning, as the paper focuses on improving DRL performance using Mixture of Experts in various multi-task settings.",
        "quote": "Mixtures of Experts can significantly boost Deep Reinforcement Learning (DRL) performance by expanding the network’s parameter count while reducing dormant neurons."
      },
      "aliases": [
        "DRL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Task Learning",
          "justification": "The paper explores the application of MoEs in Multi-Task Reinforcement Learning settings, where multiple tasks are learned concurrently.",
          "quote": "Specifically, we investigate the incorporation of a variety of MoE architectures in Multi-Task Reinforcement Learning (MTRL) and Continual Reinforcement Learning (CRL) settings."
        },
        "aliases": [
          "MTRL"
        ]
      },
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The paper investigates MoEs in Continual Reinforcement Learning settings, where tasks are learned sequentially over time.",
          "quote": "We investigate the incorporation of a variety of MoE architectures in Multi-Task Reinforcement Learning (MTRL) and Continual Reinforcement Learning (CRL) settings."
        },
        "aliases": [
          "CRL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Mixture of Experts (MoEs)",
          "justification": "The focus of the paper is on the application and evaluation of Mixture of Experts in DRL settings.",
          "quote": "Mixtures of Experts (MoEs) have gained prominence in (self-)supervised learning due to their enhanced inference efficiency, adaptability to distributed training, and modularity."
        },
        "aliases": [
          "MoE"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper builds upon existing MoE models and does not claim the introduction of a new MoE model itself.",
          "quote": "MoEs stand out by facilitating the scaling of networks to encompass trillions of parameters, a feat made possible through their modular design that seamlessly integrates with distributed computing techniques (Fedus et al., 2022)."
        },
        "is_executed": {
          "value": true,
          "justification": "MoEs were experimented with in various settings using the PureJaxRL codebase as mentioned in the experimental setup.",
          "quote": "We chose to run our experiments with the PureJaxRL codebase (Lu et al., 2022b;a; 2023), which is a high-performance and parallelisable library including an implementation of Proximal Policy Optimisation (Schulman et al., 2017, PPO)."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of different MoE architectures was compared with a baseline throughout the experiments.",
          "quote": "We present further analysis with different task orders in Section 4."
        },
        "referenced_paper_title": {
          "value": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
          "justification": "This is a key paper referenced concerning the MoE model and its architecture.",
          "quote": "Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MinAtar",
          "justification": "MinAtar environments were used for experiments, providing insights comparable to the full ALE suite.",
          "quote": "We rely on the Gymnax suite (Lange, 2022) to implement optimised versions of MinAtar environments (Young & Tian, 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MinAtar: An Atari-inspired testbed for thorough and reproducible reinforcement learning experiments",
          "justification": "The referenced paper provides the original description and utility of the MinAtar environments.",
          "quote": "Kenny Young and Tian Tian. Minatar: An Atari-inspired testbed for thorough and reproducible reinforcement learning experiments. arXiv preprint arXiv:1903.03176, 2019."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PureJaxRL",
          "justification": "The PureJaxRL library was used for running experiments and contains implementations crucial to the DRL methods explored.",
          "quote": "We chose to run our experiments with the PureJaxRL codebase (Lu et al., 2022b;a; 2023), which is a high-performance and parallelisable library."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Discovered Policy Optimisation",
          "justification": "The library is part of the referenced research used to implement the practical aspects of the experiments in this paper.",
          "quote": "Chris Lu, Jakub Kuba, Alistair Letcher, Luke Metz, Christian Schroeder de Witt, and Jakob Foerster. Discovered Policy Optimisation. Advances in Neural Information Processing Systems, 35: 16455–16468, 2022a."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1254,
    "prompt_tokens": 26224,
    "total_tokens": 27478,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 26112
    }
  }
}