{
  "paper": "bd0278041b085844aea4dff42fe93245.txt",
  "words": 15593,
  "extractions": {
    "title": {
      "value": "Towards Modular LLMs by Building and Reusing a Library of LoRAs",
      "justification": "The title is clearly stated at the beginning of the document.",
      "quote": "Towards Modular LLMs by Building and Reusing a Library of LoRAs"
    },
    "description": "This paper explores the reuse of parameter-efficient adaptations, namely LoRAs, to build modular large language models (LLMs) optimized for multi-task learning. It introduces a model-based clustering method to create a library of adapters based on parameter similarity and suggests the Arrow routing method for effective zero-shot generalization without retraining.",
    "type": {
      "value": "empirical",
      "justification": "The paper provides experimental results on zero-shot and supervised adaptation, and discusses performance improvements analytically.",
      "quote": "We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks, verifying that MBC-based adapters and Arrow routing lead to superior generalization to new tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper discusses techniques relevant to language models, such as LoRAs, clustering tasks for generalization, and routing strategies for improved language understanding.",
        "quote": "We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Transfer Learning",
          "justification": "The paper focuses on reusing and adapting trained models for new tasks, a core concept in transfer learning.",
          "quote": "Reusing LoRAs in a zero-shot manner is challenging because there is no labelled data to learn a routing mechanism. We propose Arrow (â†—), a routing mechanism that automatically selects relevant LoRAs."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Phi-2",
          "justification": "Phi-2 is mentioned as one of the LLMs experimented with in the study.",
          "quote": "We experiment with several LLMs, such as Phi-2 and Mistral."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Phi-2 is used for experimentation but not introduced as a novel model in the paper.",
          "quote": "First, given a base LLM, such as Phi-2 (Microsoft Research, 2023) or Mistral (Jiang et al., 2023)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper states that experiments were conducted using Phi-2.",
          "quote": "We experiment with several LLMs, such as Phi-2 and Mistral."
        },
        "is_compared": {
          "value": true,
          "justification": "Experiments with Phi-2 involve comparisons on tasks for generalization and performance.",
          "quote": "We experiment with several LLMs, such as Phi-2 and Mistral, on a wide array of held-out tasks."
        },
        "referenced_paper_title": {
          "value": "Phi-2: The Surprising Power of Small Language Models",
          "justification": "The referenced title for Phi-2 is specified as part of its citation in the text.",
          "quote": "First, given a base LLM, such as Phi-2 (Microsoft Research, 2023)..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Flan-v2",
          "justification": "Flan-v2 is used to build the adapter library as part of the experimentation process.",
          "quote": "building a library of adapters by leveraging 256 tasks from Flan-v2 (Longpre et al., 2023)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The flan collection: Designing data and methods for effective instruction tuning",
          "justification": "The dataset's reference title is noted in the document.",
          "quote": "Flan-v2 (Longpre et al., 2023)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "LoRA",
          "justification": "LoRA is central to the paper's method for building the modular LLM library.",
          "quote": "For each linear transformation in the base LM, LoRA modifies the base model parameters as follows:"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "LoRA: Low-rank adaptation of large language models",
          "justification": "The library referred to is LoRA as specified in the text.",
          "quote": "LoRA (Hu et al., 2022)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 895,
    "prompt_tokens": 31151,
    "total_tokens": 32046,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}