{
  "paper": "cf06998c57188ae6d1dfbb2ae488d861.txt",
  "words": 13401,
  "extractions": {
    "title": {
      "value": "Adaptive Exploration for Data-Efficient General Value Function Evaluations",
      "justification": "The paper introduces an approach called GVFExplorer for aiding data-efficient evaluations of General Value Functions (GVFs).",
      "quote": "To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel."
    },
    "description": "This paper introduces a method called GVFExplorer, aimed at adaptively learning a single behavior policy. This policy efficiently collects data for the parallel evaluation of multiple General Value Functions (GVFs). The method optimizes the behavior policy by minimizing overall variance, thus reducing prediction errors and environmental interactions. The presented theoretical insights and empirical results demonstrate performance improvements in both tabular and complex environments (like Mujoco) across various settings, including stationary and non-stationary reward signals.",
    "type": {
      "value": "empirical",
      "justification": "The paper empirically demonstrates the proposed method's effectiveness using various environments and settings.",
      "quote": "We empirically demonstrate in both tabular and Mujoco environments that GVFExplorer lowers the total MSE when estimating multiple GVFs compared to baseline approaches and enables evaluating a larger number of GVFs in parallel."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary focus of the paper is on improving exploration and evaluation methods within reinforcement learning.",
        "quote": "The ability to make multiple predictions is a key attribute of human, animal, and artificial intelligence. Sutton et al. (2011) introduced General Value Functions (GVFs) which consists of several independent sub-agents, each responsible for answering specific predictive knowledge about the environment."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "General Value Functions",
          "justification": "The paper specifically explores methods to evaluate multiple General Value Functions (GVFs) efficiently.",
          "quote": "Sutton et al. (2011) introduced General Value Functions (GVFs) which consists of several independent sub-agents, each responsible for answering specific predictive knowledge about the environment."
        },
        "aliases": [
          "GVFs"
        ]
      },
      {
        "name": {
          "value": "Exploration Strategies",
          "justification": "The paper focuses on developing adaptive exploration strategies to improve data efficiency.",
          "quote": "To achieve better value estimation in fewer samples, it is essential to focus on state-action pairs with high variance in return, as these pairs would exhibit greater uncertainty in their mean return."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GVFExplorer",
          "justification": "GVFExplorer is the main model proposed in the paper to improve data collection efficiency in evaluating multiple GVFs.",
          "quote": "To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "GVFExplorer is a novel method introduced in the paper for improving the efficiency of evaluating multiple GVFs.",
          "quote": "To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel."
        },
        "is_executed": {
          "value": true,
          "justification": "GVFExplorer is executed in experimental settings, including both tabular and Mujoco environments.",
          "quote": "We empirically demonstrate in both tabular and Mujoco environments that GVFExplorer lowers the total MSE when estimating multiple GVFs compared to baseline approaches and enables evaluating a larger number of GVFs in parallel."
        },
        "is_compared": {
          "value": true,
          "justification": "The method is compared against several baseline approaches in various empirical experiments.",
          "quote": "We empirically demonstrate in both tabular and Mujoco environments that GVFExplorer lowers the total MSE when estimating multiple GVFs compared to baseline approaches and enables evaluating a larger number of GVFs in parallel."
        },
        "referenced_paper_title": {
          "value": "None",
          "justification": "GVFExplorer is a contribution of the current paper and doesn't reference another model for its definition.",
          "quote": "To address this, we introduce GVFExplorer, which adaptively learns a single behavior policy that efficiently collects data for evaluating multiple GVFs in parallel."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Mujoco environments",
          "justification": "The Mujoco environments are used in experiments to demonstrate the model's performance in nonlinear function approximation settings.",
          "quote": "We empirically demonstrate in both tabular and Mujoco environments that GVFExplorer lowers the total MSE when estimating multiple GVFs compared to baseline approaches and enables evaluating a larger number of GVFs in parallel."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Deepmind control suite",
          "justification": "Mujoco is referenced as a standard benchmark environment for reinforcement learning tasks in the paper.",
          "quote": "We use DM-Control (Tassa et al., 2018) based continuous state-action tasks to experiment with Mujoco environments, Walker and Cheetah domain."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Soft Actor-Critic (SAC)",
          "justification": "SAC is utilized as a component in the experimental section for handling continuous state-action tasks.",
          "quote": "For these experiments, we used Soft Actor-Critic (SAC) which provides stability in such settings."
        },
        "aliases": [
          "SAC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
          "justification": "SAC is referenced in the section describing its use in Mujoco experiments.",
          "quote": "For these experiments, we used Soft Actor-Critic (SAC) which provides stability in such settings."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1132,
    "prompt_tokens": 23663,
    "total_tokens": 24795,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}