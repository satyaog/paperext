{
  "paper": "PhpCHXUx9D.txt",
  "words": 15080,
  "extractions": {
    "title": {
      "value": "Deep Networks as Paths on the Manifold of Neural Representations",
      "justification": "The exact title as stated in the paper.",
      "quote": "Deep Networks as Paths on the Manifold of Neural Representations"
    },
    "description": "This paper explores a novel perspective on deep neural networks by conceptualizing them as traversing paths on a high-dimensional manifold of neural representations. It leverages advanced representational similarity metrics to analyze and visualize these paths, thereby offering new insights into the layer-wise transformations and overall geometry of neural network computations.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily presents a theoretical framework for understanding deep neural networks through geometric and mathematical concepts.",
      "quote": "We formalize this intuitive idea by leveraging recent advances in metric representational similarity."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The research is centered around understanding and conceptualizing the behavior of deep neural networks.",
        "quote": "Deep neural networks implement a sequence of layer-by-layer operations that are each relatively easy to understand, but the resulting overall computation is generally difficult to understand."
      },
      "aliases": [
        "Deep Neural Networks"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The focus is on analyzing how neural representations evolve through the layers of a network.",
          "quote": "We imagine that outputs are 'far' from inputs if the mapping between them is complex, or 'close' if it is simple. In this spatial analogy, one layer of a neural network contributes a single step, and the composition of many steps transports representations along a path towards the desired target representation."
        },
        "aliases": [
          "Representational Similarity Analysis",
          "RSA"
        ]
      },
      {
        "name": {
          "value": "Geometric Deep Learning",
          "justification": "The paper uses geometrical concepts like manifolds, geodesics, and tangent spaces to analyze neural networks.",
          "quote": "Embedding hidden-layers on a smooth manifold endows every operation in the network with a sense of both distance and direction."
        },
        "aliases": [
          "Geometric Representation"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet",
          "justification": "The paper uses ResNet models to illustrate its concepts and findings.",
          "quote": "Schematic of model architecture with color-coded layers. Gray boxes correspond to residual blocks."
        },
        "aliases": [
          "Residual Networks"
        ],
        "is_contributed": {
          "value": false,
          "justification": "ResNet is not a new model introduced by this paper; it is used as a tool for experiments.",
          "quote": "We then demonstrate these tools by visualizing and comparing the paths taken by a collection of trained neural networks with a variety of architectures."
        },
        "is_executed": {
          "value": true,
          "justification": "The ResNet models are executed to visualize and compare paths on the manifold.",
          "quote": "We visualize and compare the paths taken by a collection of trained neural networks with a variety of architectures."
        },
        "is_compared": {
          "value": true,
          "justification": "ResNet models are compared with other architectures in terms of their paths on the manifold.",
          "quote": "We compare paths taken by different models, different datasets, and changes over training."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "The seminal paper introducing ResNet.",
          "quote": "He, K., Zhang, X., Ren, S., and Sun, J. Deep Residual Learning for Image Recognition. CVPR, 2016."
        }
      },
      {
        "name": {
          "value": "VGG",
          "justification": "VGG models are used alongside ResNet models for experiments.",
          "quote": "We trained a variety of standard 'wide' and 'deep' ResNet models (He et al., 2016; Nguyen et al., 2021), as well as models from the VGG family (Simonyan & Zisserman, 2015), on the CIFAR-10 dataset."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The VGG model is not a new model introduced by this paper; it is used for comparison purposes.",
          "quote": "We compare paths taken by different models, different datasets, and changes over training."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves executing VGG models to visualize and compare paths on the manifold.",
          "quote": "We visualize and compare the paths taken by a collection of trained neural networks with a variety of architectures."
        },
        "is_compared": {
          "value": true,
          "justification": "VGG models are compared with ResNet models and possibly others in terms of their paths on the manifold.",
          "quote": "We compare paths taken by different models, different datasets, and changes over training."
        },
        "referenced_paper_title": {
          "value": "Very Deep Convolutional Networks for Large-Scale Image Recognition",
          "justification": "The seminal paper introducing VGG.",
          "quote": "Simonyan, K. and Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1409.1556, 2014."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used for training the neural networks analyzed in the paper.",
          "quote": "For our main analyses, we trained a variety of standard 'wide' and 'deep' ResNet models... on the CIFAR-10 dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the original reference for the CIFAR-10 dataset.",
          "quote": "Krizhevsky, A. Learning Multiple Layers of Features from Tiny Images. 2009."
        }
      },
      {
        "name": {
          "value": "Tiny ImageNet",
          "justification": "Tiny ImageNet is used as a different dataset to compare against CIFAR-10.",
          "quote": "We trained another set of models with similar architecture on the related but different dataset known as TinyImagenet."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Tiny ImageNet Visual Recognition Challenge",
          "justification": "This is the standard reference for the Tiny ImageNet dataset.",
          "quote": "TinyImagenet available at http://cs231n.stanford.edu/tiny-imagenet-200.zip"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1245,
    "prompt_tokens": 27151,
    "total_tokens": 28396
  }
}