{
  "paper": "e89d927d036d55b89e61312655631078.txt",
  "words": 14166,
  "extractions": {
    "title": {
      "value": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving",
      "justification": "The title clearly indicates that the paper explores the metacognitive capabilities of large language models in the context of solving mathematical problems.",
      "quote": "Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving"
    },
    "description": "The paper investigates whether large language models (LLMs) possess metacognitive knowledge, particularly in mathematical problem solving. It introduces a prompt-guided interaction process to assign skill labels to math problems, demonstrating how this can improve accuracy on datasets like GSM8K and MATH. The study also highlights how these skills can be transferred to weaker models.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments with LLMs, like GPT-4, to validate the metacognitive knowledge through quantitative improvements in performance on math problems.",
      "quote": "To validate that these skill labels are meaningful and relevant to the LLM’s reasoning processes we perform the following experiments."
    },
    "primary_research_field": {
      "name": {
        "value": "Artificial Intelligence",
        "justification": "The paper explores the capabilities of large language models, a core area within artificial intelligence, by examining their reasoning and metacognitive abilities.",
        "quote": "Today’s best LLMs clearly possess some reasoning processes."
      },
      "aliases": [
        "AI"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Natural Language Processing",
          "justification": "The research revolves around language models, which are a part of natural language processing.",
          "quote": "Large language models (LLMs) have demonstrated remarkable advancements"
        },
        "aliases": [
          "NLP"
        ]
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The work involves using machine learning models to explore metacognitive capabilities, which is a machine learning concept.",
          "quote": "The methodology presented is domain-agnostic, even though this article applies it to math problems."
        },
        "aliases": [
          "ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is explicitly mentioned as the large language model used in the experiments to validate metacognitive skills.",
          "quote": "Using a strong LLM - GPT-4 - to identify skills, we validate the usefulness of these skills"
        },
        "aliases": [
          "GPT-4-0613"
        ],
        "is_contributed": {
          "value": false,
          "justification": "GPT-4 is a pre-existing model used in the research rather than being developed as part of this paper's contributions.",
          "quote": "The paper gives evidence that they also have metacognitive knowledge."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves running experiments with GPT-4 to assign skills and validate results.",
          "quote": "To validate that these skill labels are meaningful and relevant to the LLM’s reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions."
        },
        "is_compared": {
          "value": true,
          "justification": "GPT-4's performance is compared with baselines to show improvements in solving problems.",
          "quote": "We validate the usefulness of these skills by demonstrates a significant 11.6% enhancement over CoT on the MATH Dataset."
        },
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "The reference is likely included to define GPT-4.",
          "quote": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "GSM8K",
          "justification": "GSM8K is identified as one of the datasets where skill labels are assigned and performance is measured.",
          "quote": "To validate that these skill labels are meaningful and relevant to the LLM’s reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Training verifiers to solve math word problems",
          "justification": "The GSM8K dataset reference details its origin.",
          "quote": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021."
        }
      },
      {
        "name": {
          "value": "MATH",
          "justification": "The MATH dataset is used to measure and compare the impact of skill labels on solving competition-level math problems.",
          "quote": "To validate that these skill labels are meaningful and relevant to the LLM’s reasoning processes we perform the following experiments. (a) We ask GPT-4 to assign skill labels to training questions in math datasets GSM8K and MATH."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Measuring mathematical problem solving with the math dataset",
          "justification": "The MATH dataset reference provides additional background on its content.",
          "quote": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1115,
    "prompt_tokens": 24303,
    "total_tokens": 25418,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}