{
  "paper": "f061e35996d6d07dfbd3b0b6320382c3.txt",
  "words": 19097,
  "extractions": {
    "title": {
      "value": "Interpolation consistency training for semi-supervised learning",
      "justification": "The title accurately reflects the main subject addressed in the paper, which is about introducing a new training algorithm for semi-supervised learning called Interpolation Consistency Training.",
      "quote": "We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm."
    },
    "description": "This paper presents Interpolation Consistency Training (ICT), an innovative algorithm that enhances semi-supervised learning in deep neural networks. ICT works by ensuring that predictions at interpolations of unlabeled data points are consistent with interpolations of the predictions of those points, moving decision boundaries to regions of low data density.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes experiments and presents empirical results on benchmark datasets like CIFAR-10 and SVHN, comparing ICT with other methods, thus contributing empirical evidence.",
      "quote": "Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets."
    },
    "primary_research_field": {
      "name": {
        "value": "Semi-supervised Learning",
        "justification": "The paper focuses on developing a technique to enhance the performance of semi-supervised learning models.",
        "quote": "The goal of Semi-Supervised Learning (SSL) (Chapelle et al., 2010) is to leverage large amounts of unlabeled data to improve the performance of supervised learning over small datasets."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Neural Networks",
          "justification": "The paper focuses on training deep neural networks using semi-supervised learning algorithms.",
          "quote": "We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Π-model",
          "justification": "This model is mentioned as part of the consistency regularization techniques used in semi-supervised learning.",
          "quote": "The low-density separation assumption has inspired many recent consistency-regularization semi-supervised learning techniques, including the Π-model (Laine & Aila, 2017; Sajjadi et al., 2016)."
        },
        "aliases": [
          "Pi-model"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The Π-model is referenced as part of existing techniques in the related work section and not contributed by this paper.",
          "quote": "The low-density separation assumption has inspired many recent consistency-regularization semi-supervised learning techniques, including the Π-model (Laine & Aila, 2017; Sajjadi et al., 2016)."
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not provide experiments or data specific to the execution of the Π-model; it is only mentioned conceptually.",
          "quote": "The low-density separation assumption has inspired many recent consistency-regularization semi-supervised learning techniques, including the Π-model."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the proposed model with the Π-model to establish the effectiveness of their model.",
          "quote": "Our experimental results on the benchmark datasets... outperform (or are competitive with) the state-of-the-art methods."
        },
        "referenced_paper_title": {
          "value": "Temporal Ensembling for Semi-Supervised Learning",
          "justification": "The Π-model is associated with the paper titled 'Temporal Ensembling for Semi-Supervised Learning' authored by Laine & Aila, 2017.",
          "quote": "The low-density separation assumption has inspired many recent consistency-regularization semi-supervised learning techniques, including the Π-model (Laine & Aila, 2017; Sajjadi et al., 2016)."
        }
      },
      {
        "name": {
          "value": "Mean Teacher",
          "justification": "The Mean Teacher model is referred to as part of the experimental setup and consistency regularization techniques.",
          "quote": "We use its variant (i.e., without additive Gaussian noise in the input layer) as implemented in Athiwaratkun et al. (2019)."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Mean Teacher is a referenced model used for comparison and not contributed by this paper.",
          "quote": "Despite this, our method outperforms other methods in several experimental settings."
        },
        "is_executed": {
          "value": false,
          "justification": "The execution of Mean Teacher on any experiment is not detailed within this paper.",
          "quote": "Despite this, our method outperforms other methods in several experimental settings."
        },
        "is_compared": {
          "value": true,
          "justification": "Mean Teacher model is part of the comparative study to evaluate the ICT model.",
          "quote": "Despite this, our method outperforms other methods in several experimental settings."
        },
        "referenced_paper_title": {
          "value": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
          "justification": "The Mean Teacher is associated with a paper by Tarvainen & Valpola, 2017.",
          "quote": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results."
        }
      },
      {
        "name": {
          "value": "Virtual Adversarial Training (VAT)",
          "justification": "VAT is mentioned as a related model in the domain of semi-supervised learning techniques.",
          "quote": "To alleviate this issue, Virtual Adversarial Training or VAT (Miyato et al., 2018), searches for small perturbations δ that maximize the change in the prediction of the model."
        },
        "aliases": [
          "VAT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper references VAT as part of existing methods rather than contributing to its development.",
          "quote": "VAT (Miyato et al., 2018), searches for small perturbations δ that maximize the change in the prediction of the model."
        },
        "is_executed": {
          "value": false,
          "justification": "The execution details of VAT are not covered; it's mentioned for conceptual comparison.",
          "quote": "To alleviate this issue, Virtual Adversarial Training or VAT (Miyato et al., 2018), searches for small perturbations δ that maximize the change in the prediction of the model."
        },
        "is_compared": {
          "value": true,
          "justification": "VAT is one of the methods against which ICT is benchmarked in experimental results.",
          "quote": "Our experimental results... outperform (or are competitive with) the state-of-the-art methods."
        },
        "referenced_paper_title": {
          "value": "Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning",
          "justification": "The VAT technique is credited to the paper by Miyato et al., 2018.",
          "quote": "VAT (Miyato et al., 2018), searches for small perturbations δ that maximize the change in the prediction of the model."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is used as part of the experimental setup to evaluate the proposed ICT model.",
          "quote": "Our experimental results on the benchmark datasets CIFAR10 and SVHN and neural network architectures CNN-13 (Laine & Aila, 2017; Luo et al., 2018; Miyato et al., 2018; Park et al., 2018; Tarvainen & Valpola, 2017) and WRN28-2 (Oliver et al., 2018) outperform (or are competitive with) the state-of-the-art methods."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "CIFAR-10: Note on the parameters",
          "justification": "CIFAR-10 is a well-known dataset often referenced with its original publication by Krizhevsky et al.",
          "quote": "The CIFAR-10 dataset consists of 60,000 color images each of size 32 × 32, split between 50K training and 10K test images."
        }
      },
      {
        "name": {
          "value": "SVHN",
          "justification": "SVHN is also used as part of the experimental evaluation of the ICT approach.",
          "quote": "Our experimental results on the benchmark datasets CIFAR10 and SVHN and neural network architectures CNN-13 (Laine & Aila, 2017; Luo et al., 2018; Miyato et al., 2018; Park et al., 2018; Tarvainen & Valpola, 2017) and WRN28-2 (Oliver et al., 2018) outperform (or are competitive with) the state-of-the-art methods."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Reading Digits in Natural Images with Unsupervised Feature Learning",
          "justification": "SVHN dataset details are often referenced to the work on digit recognition as stated by Goodfellow et al.",
          "quote": "The SVHN dataset consists of 73 257 training samples and 26 032 test samples each of size 32 × 32."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1815,
    "prompt_tokens": 33677,
    "total_tokens": 35492,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}