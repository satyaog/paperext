{
  "paper": "d42700fe940e1f974e3bc60d0660cd28.txt",
  "words": 9358,
  "extractions": {
    "title": {
      "value": "Recipe for a General, Powerful, Scalable Graph Transformer",
      "justification": "The title is clearly stated at the beginning of the paper and in various headers.",
      "quote": "Recipe for a General, Powerful, Scalable Graph Transformer"
    },
    "description": "The paper introduces a general framework for constructing scalable Graph Transformers with linear complexity, termed as GPS (General, Powerful, Scalable). It outlines a modular architecture that combines positional/structural encoding, local message-passing, and global attention to enhance scalability and expressivity on graph neural networks. The framework supports diverse encoding schemes and aims to scale to graphs with several thousand nodes, addressing shortcomings in existing models.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experimental setup and benchmarking of the proposed model on various datasets to demonstrate its empirical benefits.",
      "quote": "We test our architecture on 16 benchmarks and show highly competitive results..."
    },
    "primary_research_field": {
      "name": {
        "value": "Graph Representation Learning",
        "justification": "The research is focused on improving Graph Transformers for Graph Representation Learning, addressing issues specific to graph domain applications.",
        "quote": "Graph Transformers (GTs) have gained popularity in the field of graph representation learning..."
      },
      "aliases": [
        "Graph Neural Networks",
        "GNN"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Graph Transformers",
          "justification": "The paper is developing a framework specifically for Graph Transformers.",
          "quote": "Recipe for a General, Powerful, Scalable Graph Transformer"
        },
        "aliases": [
          "GT",
          "Graph Transformer"
        ]
      },
      {
        "name": {
          "value": "Scalable Graph Neural Networks",
          "justification": "The emphasis on building scalable models is a significant part of the paper.",
          "quote": "...propose a recipe for building general, powerful, and scalable (GPS) graph Transformers."
        },
        "aliases": [
          "Scalable GNN"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Graph Transformer",
          "justification": "The model is explicitly referred to as 'Graph Transformer' throughout the paper.",
          "quote": "Graph Transformers (GTs) have gained popularity in the field of graph representation learning..."
        },
        "aliases": [
          "GT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper builds upon previous Graph Transformer models, proposing improvements but not introducing a fundamentally new model with a unique name.",
          "quote": "...fully-connected Graph Transformer models [14, 36, 63, 44, 31]..."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper discusses execution and performance benchmarks of the proposed model.",
          "quote": "We test our architecture on 16 benchmarks and show highly competitive results."
        },
        "is_compared": {
          "value": true,
          "justification": "The model is compared against several other graph-based models as part of the experimental results.",
          "quote": "We test our architecture on 16 benchmarks and show highly competitive results..."
        },
        "referenced_paper_title": {
          "value": "A Generalization of Transformer Networks to Graphs",
          "justification": "The paper references prior Graph Transformer models.",
          "quote": "...fully-connected Graph Transformer models [14, 36, 63, 44, 31]..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ZINC",
          "justification": "ZINC is one of the datasets listed for benchmarking the performance.",
          "quote": "We test on datasets from different sources to ensure diversity, providing their detailed description in Appendix A.1. From the Benchmarking GNNs [15], we test on the ZINC, PATTERN, CLUSTER, MNIST, CIFAR10."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Benchmarking Graph Neural Networks",
          "justification": "The dataset is part of the Benchmarking GNNs used for performance evaluation.",
          "quote": "From the Benchmarking GNNs [15], we test on the ZINC, PATTERN, CLUSTER, MNIST, CIFAR10."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch Geometric",
          "justification": "The library is used as a base for implementing the G RAPH GPS package, as stated in the methods section.",
          "quote": "we provide an extensible implementation of GPS in G RAPH GPS package, built on top of PyG [20]..."
        },
        "aliases": [
          "PyG"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Fast Graph Representation Learning with PyTorch Geometric",
          "justification": "This is the reference paper cited for PyTorch Geometric.",
          "quote": "built on top of PyG [20] and GraphGym [65]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 898,
    "prompt_tokens": 19471,
    "total_tokens": 20369,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}