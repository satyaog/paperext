{
  "paper": "TLercqxR4f.txt",
  "words": 9015,
  "extractions": {
    "title": {
      "value": "HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science",
      "justification": "Title is stated at the top of the paper",
      "quote": "HoneyBee: Progressive Instruction Finetuning of\nLarge Language Models for Materials Science"
    },
    "description": "This paper introduces HoneyBee, a finetuned large language model (LLM) based on the LLaMa architecture, specialized for materials science. The model is trained using MatSci-Instruct, a novel instruction-based data curation process designed to create high-quality, domain-specific datasets. The paper demonstrates the effectiveness of HoneyBee through various evaluations, showing its superior performance on materials science NLP tasks compared to existing models.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes the process of fine-tuning an LLM and validates its performance through experiments and evaluations.",
      "quote": "We study the quality of HoneyBee’s\nlanguage modeling through automatic evaluation and analyze case studies to further\nunderstand the model’s capabilities and limitations."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on the development and fine-tuning of a large language model (LLM) and its application to materials science.",
        "quote": "Natural language processing (NLP) holds considerable promise in expediting the discovery and understanding of novel material systems"
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Materials Science",
          "justification": "The paper aims to develop an LLM specifically for tasks within the field of materials science.",
          "quote": "HoneyBee is the first billion-parameter language model specialized to materials science"
        },
        "aliases": [
          "MatSci"
        ]
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The paper discusses the fine-tuning and iterative instruction of an LLM, which is a fundamental aspect of machine learning.",
          "quote": "we apply a Progressive Refinement-Feedback strategy to finetune a LLaMa model, culminating in the HoneyBee model"
        },
        "aliases": [
          "ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "HoneyBee",
          "justification": "HoneyBee is the main model introduced and discussed throughout the paper.",
          "quote": "HoneyBee is the first billion-parameter language model specialized to materials science"
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "HoneyBee is explicitly introduced as the main contribution of the paper.",
          "quote": "we propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee)."
        },
        "is_executed": {
          "value": 1,
          "justification": "Multiple experiments and evaluations are described, implying execution.",
          "quote": "Our evaluation on the MatSci-NLP benchmark shows HoneyBee’s outperformance of existing language models on materials science tasks"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of HoneyBee is compared with other existing language models in various tasks.",
          "quote": "HoneyBee’s outperformance of existing language models on materials science tasks"
        },
        "referenced_paper_title": {
          "value": "LLaMA: Open and Efficient Foundation Language Models",
          "justification": "The paper mentions that HoneyBee is based on the LLaMa architecture.",
          "quote": "a billion-parameter specialized materials science language model based on\nthe LLaMa architecture [Touvron et al., 2023]"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MatSci-NLP",
          "justification": "The MatSci-NLP benchmark is used for evaluating the performance of the HoneyBee model.",
          "quote": "Our evaluation on the MatSci-NLP benchmark shows HoneyBee’s outperformance of existing language models on materials science tasks"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling",
          "justification": "The MatSci-NLP benchmark is cited as a reference for evaluating the performance of various models, including HoneyBee.",
          "quote": "We evaluate the performance of HoneyBee using a materials science language benchmark [Song et al., 2023]"
        }
      },
      {
        "name": {
          "value": "MatSci-Instruct",
          "justification": "MatSci-Instruct is created as part of this research to fine-tune HoneyBee.",
          "quote": "we propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct)"
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "MatSci-Instruct: A Methodology for Trustworthy Instruction Generation in Scientific Domains",
          "justification": "MatSci-Instruct is introduced in this paper as the methodology for data generation and fine-tuning of HoneyBee.",
          "quote": "MatSci-Instruct follows a structured instruction generation template and ensures\ninstruction quality through an iterative verification loop described in Section 3.1."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "LLaMa",
          "justification": "The HoneyBee model is based on the LLaMA foundation model.",
          "quote": "HoneyBee, a billion-parameter specialized materials science language model based on the LLaMa architecture [Touvron et al., 2023]"
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "LLaMA: Open and Efficient Foundation Language Models",
          "justification": "The LLaMa architecture is referenced as the basis for HoneyBee.",
          "quote": "a billion-parameter specialized materials science language model based on the LLaMa architecture [Touvron et al., 2023]"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1116,
    "prompt_tokens": 16903,
    "total_tokens": 18019
  }
}