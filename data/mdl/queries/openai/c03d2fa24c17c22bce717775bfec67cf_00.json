{
  "paper": "c03d2fa24c17c22bce717775bfec67cf.txt",
  "words": 9054,
  "extractions": {
    "title": {
      "value": "Beyond Mahalanobis-Based Scores for Textual OOD Detection",
      "justification": "The title is explicitly provided at the beginning of the paper.",
      "quote": "Beyond Mahalanobis-Based Scores for\nTextual OOD Detection"
    },
    "description": "The paper introduces TRUSTED, a novel OOD detector for textual classifiers based on Transformer architectures. TRUSTED leverages the concept of data depth and uses information from all hidden layers of the model for effective OOD detection. It improves previous AUROC scores while meeting operational requirements of being unsupervised and fast to compute.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts extensive numerical experiments involving 51k model configurations to demonstrate the effectiveness of the proposed method.",
      "quote": "Our extensive\nnumerical experiments involve 51k model configurations, including various check-\npoints, seeds, and datasets, and demonstrate that TRUSTED achieves state-of-the-art\nperformances."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on out-of-distribution detection for NLP systems, using Transformer architectures such as BERT, DistilBERT, and RoBERTa.",
        "quote": "This paper is about a critical safety issue, namely Out-Of-Distribution (OOD) detection [11], which refers to a change of distribution of incoming data that may cause failures of in-production AI systems."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Out-of-Distribution Detection",
          "justification": "The study specifically targets OOD detection challenges in NLP models.",
          "quote": "This paper is about a critical safety issue, namely Out-Of-Distribution (OOD) detection [11], which refers to a change of distribution of incoming data that may cause failures of in-production AI systems."
        },
        "aliases": [
          "OOD Detection"
        ]
      },
      {
        "name": {
          "value": "Transformer Architectures",
          "justification": "The paper evaluates its proposed OOD detection methodologies on Transformer models like BERT, DistilBERT, and RoBERTa.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT [34], DistilBERT [83], and RoBERTa [68] architectures."
        },
        "aliases": [
          "Transformers"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BERT",
          "justification": "The paper uses BERT as one of the architectures for evaluating its novel OOD detection method.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT [34], DistilBERT [83], and RoBERTa [68] architectures."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BERT is a pre-existing model and not a contribution of this paper.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT [34]..."
        },
        "is_executed": {
          "value": true,
          "justification": "BERT is part of the extensive numerical experiments conducted in the paper.",
          "quote": "We conduct extensive numerical experiments on three transformers architectures..."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of different models including BERT as part of the experiments.",
          "quote": "Our extensive\nnumerical experiments involve 51k model configurations, including various checkpoints, seeds, and datasets..."
        },
        "referenced_paper_title": {
          "value": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "justification": "The referenced paper is related to the original BERT model, as indicated by the citation [34] in this paper.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT [34]..."
        }
      },
      {
        "name": {
          "value": "DistilBERT",
          "justification": "The paper uses DistilBERT as one of the architectures for evaluating its novel OOD detection method.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT [34], DistilBERT [83], and RoBERTa [68] architectures."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "DistilBERT is a pre-existing model and not a contribution of this paper.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT... DistilBERT [83]..."
        },
        "is_executed": {
          "value": true,
          "justification": "DistilBERT is part of the extensive numerical experiments conducted in the paper.",
          "quote": "We conduct extensive numerical experiments on three transformers architectures..."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of different models including DistilBERT as part of the experiments.",
          "quote": "Our extensive\nnumerical experiments involve 51k model configurations..."
        },
        "referenced_paper_title": {
          "value": "DistilBERT, a Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter",
          "justification": "The referenced paper is related to the DistilBERT model, as indicated by the citation [83] in this paper.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT... DistilBERT [83]..."
        }
      },
      {
        "name": {
          "value": "RoBERTa",
          "justification": "The paper uses RoBERTa as one of the architectures for evaluating its novel OOD detection method.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT [34], DistilBERT [83], and RoBERTa [68] architectures."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "RoBERTa is a pre-existing model and not a contribution of this paper.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT... and RoBERTa [68] architectures."
        },
        "is_executed": {
          "value": true,
          "justification": "RoBERTa is part of the extensive numerical experiments conducted in the paper.",
          "quote": "We conduct extensive numerical experiments on three transformers architectures..."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of different models including RoBERTa as part of the experiments.",
          "quote": "Our extensive\nnumerical experiments involve 51k model configurations..."
        },
        "referenced_paper_title": {
          "value": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
          "justification": "The referenced paper is related to the RoBERTa model, as indicated by the citation [68] in this paper.",
          "quote": "In this paper, we focus on classifiers for textual data and on the ubiquitous BERT... and RoBERTa [68] architectures."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "SST2",
          "justification": "SST2 is used as one of the datasets for in-distribution (IN-DS) evaluation in the experiments.",
          "quote": "The considered benchmark is composed of three different types of in\ndistribution datasets (referred to as IN-DS) which are used to train the classifiers: sentiment analysis\n(i.e., SST2 [88] and IMDB [70])..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
          "justification": "The referenced paper is the original paper for the SST2 dataset, as indicated by the citation [88] in this paper.",
          "quote": "The considered benchmark is composed of three different types of in\ndistribution datasets (referred to as IN-DS) which are used to train the classifiers: sentiment analysis\n(i.e., SST2 [88]..."
        }
      },
      {
        "name": {
          "value": "IMDB",
          "justification": "IMDB is used as one of the datasets for in-distribution (IN-DS) evaluation in the experiments.",
          "quote": "The considered benchmark is composed of three different types of in\ndistribution datasets (referred to as IN-DS) which are used to train the classifiers: sentiment analysis\n(i.e., SST2 [88] and IMDB [70])..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Word Vectors for Sentiment Analysis",
          "justification": "The referenced paper is the original paper for the IMDB dataset, as indicated by the citation [70] in this paper.",
          "quote": "The considered benchmark is composed of three different types of in\ndistribution datasets (referred to as IN-DS) which are used to train the classifiers: sentiment analysis\n(i.e., ... IMDB [70])..."
        }
      },
      {
        "name": {
          "value": "20Newsgroup",
          "justification": "20Newsgroup is used as one of the datasets for in-distribution (IN-DS) evaluation in the experiments.",
          "quote": "The considered benchmark is composed of three different types of in\ndistribution datasets (referred to as IN-DS) which are used to train the classifiers: ... topic classification (i.e., 20Newsgroup [54]) ..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization",
          "justification": "The referenced paper is the original paper for the 20Newsgroup dataset, as indicated by the citation [54] in this paper.",
          "quote": "The considered benchmark is composed of three different types of in\ndistribution datasets (referred to as IN-DS) which are used to train the classifiers: ... topic classification (i.e., 20Newsgroup [54]) ..."
        }
      },
      {
        "name": {
          "value": "TREC-10",
          "justification": "TREC-10 is used as one of the datasets for in-distribution (IN-DS) evaluation in the experiments.",
          "quote": "The considered benchmark is composed of three different types of in\ndistribution datasets (referred to as IN-DS) which are used to train the classifiers: ... and question answering\n (i.e., TREC-10 [61]) ..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Question Classifiers",
          "justification": "The referenced paper is the original paper for TREC-10, as indicated by the citation [61] in this paper.",
          "quote": "... and question answering (i.e., TREC-10 [61]) ..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "AdamW",
          "justification": "The paper mentions using the AdamW optimizer for training model architectures during experiments.",
          "quote": "We trained all models with a dropout rate [89] of 0.2, a batch size of 32, we use ADAMW [55]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Adam: A Method for Stochastic Optimization",
          "justification": "The reference paper describes the Adam optimizer, which AdamW is based on as indicated by citation [55].",
          "quote": "We trained all models with a dropout rate [89] of 0.2, a batch size of 32, we use ADAMW [55]."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2276,
    "prompt_tokens": 18913,
    "total_tokens": 21189,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}