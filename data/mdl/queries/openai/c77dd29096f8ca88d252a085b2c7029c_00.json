{
  "paper": "c77dd29096f8ca88d252a085b2c7029c.txt",
  "words": 9918,
  "extractions": {
    "title": {
      "value": "Modeling Caption Diversity in Contrastive Vision-Language Pretraining",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Modeling Caption Diversity in Contrastive Vision-Language Pretraining"
    },
    "description": "The paper introduces Llip, Latent Language Image Pretraining, which models the diversity of captions that could match an image. Llip's vision encoder outputs a set of visual features that are mixed into a final representation by conditioning on information from the text. The paper demonstrates that Llip outperforms non-contextualized baselines like CLIP and SigLIP on various tasks, achieving improvements in zero-shot classification and retrieval.",
    "type": {
      "value": "empirical",
      "justification": "The paper provides empirical results demonstrating the performance of Llip compared to other models.",
      "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The paper focuses on image representation and pretraining models for understanding visual content.",
        "quote": "Contrastive Language Pretraining (CLIP) on the other hand, works by mapping an image and its caption to a single vector."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Vision-Language Pretraining",
          "justification": "The paper is specifically about improving models that do vision-language pretraining such as CLIP.",
          "quote": "Contrastive Language-Image Pre-training (CLIP; Radford et al. (2021)) combined with a large-scale weakly supervised dataset has become the standard Visual Language Pre-training (VLP) approach to learn visual representation."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Zero-shot Learning",
          "justification": "The paper evaluates models based on their zero-shot classification performance.",
          "quote": "Specifically, Llip attains a zero-shot top-1 accuracy of 83.5% on ImageNet outperforming a similarly sized CLIP by 1.4%."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Llip",
          "justification": "Llip is introduced as a new model in the paper specifically designed to handle caption diversity.",
          "quote": "In this work, we introduce Llip, Latent Language Image Pretraining, which models the diversity of captions that could match an image."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Llip is introduced and developed within this paper.",
          "quote": "In this work, we introduce Llip, Latent Language Image Pretraining."
        },
        "is_executed": {
          "value": true,
          "justification": "Llip is tested and compared against other models in various experiments.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "Llip is compared to other models like CLIP and SigLIP throughout the paper.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "Llip is an original contribution from this paper, so there is no previous paper referring to it.",
          "quote": "In this work, we introduce Llip, Latent Language Image Pretraining."
        }
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is a baseline model against which Llip is compared.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "CLIP is an existing model developed by others.",
          "quote": "Contrastive Language-Image Pre-training (CLIP; Radford et al. (2021))"
        },
        "is_executed": {
          "value": false,
          "justification": "The paper doesn't execute CLIP but references its results for comparison.",
          "quote": "We compare Llip to CLIP and SigLIP for several backbones with different scales."
        },
        "is_compared": {
          "value": true,
          "justification": "CLIP is frequently compared against Llip in the paper.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        },
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models From Natural Language Supervision",
          "justification": "The original title of the CLIP paper, providing context for its previous development.",
          "quote": "Contrastive Language-Image Pre-training (CLIP; Radford et al. (2021))"
        }
      },
      {
        "name": {
          "value": "SigLIP",
          "justification": "SigLIP is used as a baseline for comparison with Llip.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "SigLIP is an existing model not developed in this paper.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        },
        "is_executed": {
          "value": false,
          "justification": "SigLIP is referenced for comparison purposes but not executed in the paper.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "SigLIP is frequently compared against Llip in the paper.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        },
        "referenced_paper_title": {
          "value": "Sigmoid Loss for Language Image Pre-Training",
          "justification": "Referenced paper title for SigLIP, providing details on its origin and context.",
          "quote": "We show that Llip outperforms non-contextualized baselines like CLIP and SigLIP on a variety of tasks."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MS-COCO",
          "justification": "MS-COCO is mentioned as a benchmark for evaluating Llip's performance.",
          "quote": "We also demonstrate improvement on zero-shot retrieval on MS-COCO by 6.0%."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "Microsoft COCO: Common Objects in Context",
          "justification": "The referenced paper for the MS-COCO dataset is indicated in the citations.",
          "quote": "Microsoft COCO: Common Objects in Context"
        }
      },
      {
        "name": {
          "value": "MetaCLIP dataset",
          "justification": "The MetaCLIP dataset is used to train and evaluate the models.",
          "quote": "We pretrain a family of vision transformer (ViT) encoders on the recent MetaCLIP (Xu et al., 2023) dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Demystifying CLIP Data",
          "justification": "The referenced paper for the MetaCLIP dataset, which is used in this study.",
          "quote": "MetaCLIP (Xu et al., 2023)"
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is a benchmark used for evaluating the performance of Llip in the paper.",
          "quote": "Specifically, Llip attains a zero-shot top-1 accuracy of 83.5% on ImageNet outperforming a similarly sized CLIP by 1.4%."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "Reference to the original paper introducing the ImageNet dataset, used extensively for evaluation in this work.",
          "quote": "ImageNet Large Scale Visual Recognition Challenge"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly mentioned as being used for implementing and running models in this research.",
          "quote": "To increase the training efficiency, we leverage compilation and mixed-precision in PyTorch (Paszke et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "PyTorch's reference indicates its previous development and usage context in the field.",
          "quote": "PyTorch: An Imperative Style, High-Performance Deep Learning Library"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1720,
    "prompt_tokens": 20282,
    "total_tokens": 22002,
    "completion_tokens_details": null,
    "prompt_tokens_details": null
  }
}