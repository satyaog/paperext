{
  "paper": "ccb2b044e476b39277efa1f30ce8d463.txt",
  "words": 10818,
  "extractions": {
    "title": {
      "value": "In-Simulation Testing of Deep Learning Vision Models in Autonomous Robotic Manipulators",
      "justification": "The title clearly describes the focus of the research on testing deep learning vision models specifically within the context of autonomous robotic manipulators.",
      "quote": "In-Simulation Testing of Deep Learning Vision Models in Autonomous Robotic Manipulators"
    },
    "description": "The paper discusses the challenges of testing autonomous robotic manipulators, which involve complex software interactions between vision and control components, particularly focusing on deep learning-based object detection models. It introduces the MARTENS framework that integrates NVIDIA Isaac Sim simulator with evolutionary search to enhance the development of deep learning vision models by identifying critical scenarios and system design flaws. The study concludes that the MARTENS framework effectively improves model performance and reveals design flaws through in-simulation testing, with significant results achieved in industrial use cases.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves practical evaluation and results derived from two industrial case studies, demonstrating the effectiveness of the MARTENS framework in identifying failures and improving model performance.",
      "quote": "Evaluation of two industrial case studies demonstrated that MARTENS effectively reveals robotic manipulator system failures, detecting 25% to 50% more failures with greater diversity compared to random test generation."
    },
    "primary_research_field": {
      "name": {
        "value": "Robotics",
        "justification": "The primary focus of the paper is on autonomous robotic manipulators, which are a key component of robotics. The study emphasizes testing and improving the interaction between the vision models and robotic systems.",
        "quote": "Testing autonomous robotic manipulators is challenging due to the complex software interactions between vision and control com-ponents."
      },
      "aliases": [
        "Autonomous Robotic Manipulators",
        "ARM"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Learning",
          "justification": "Deep learning models are a crucial component of the research, particularly in their application to object detection and vision systems within robotic systems.",
          "quote": "A crucial element of modern robotic manipulators is the deep learning based object detection model."
        },
        "aliases": [
          "Deep Neural Networks",
          "DNN"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The research particularly focuses on the application of deep learning models to computer vision tasks, specifically in detecting visual objects in 2D images for robotic guidance.",
          "quote": "...deep neural networks (DNNs), which excel at detecting visual objects in 2D images, making them suitable for robotic guidance applications."
        },
        "aliases": [
          "Vision Systems"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "YOLOv8",
          "justification": "The paper mentions the use of YOLOv8 for oriented bounding box detection, which is a significant component of the vision system being tested and improved.",
          "quote": "The vision system should send the coordinates of the cardbox center and its rotation angle w.r.t the z-axis. This information can be inferred from the 2D images using YOLOv8 [42] that natively supports the oriented bounding box detection."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model was used within the research but is not a contribution of this particular study.",
          "quote": "This information can be inferred from the 2D images using YOLOv8 [42]..."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented and its outputs were analyzed in the study, specifically for detecting objects in test scenarios.",
          "quote": "The vision system should send the coordinates of the cardbox center and its rotation angle...using YOLOv8 [42]..."
        },
        "is_compared": {
          "value": false,
          "justification": "There is no indication in the excerpt that YOLOv8 was numerically compared to other models within this paper.",
          "quote": "This information can be inferred from the 2D images using YOLOv8 [42]..."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "The text mentions the use of YOLOv8 but does not provide the title of any reference paper for this model within the provided excerpt.",
          "quote": "This information can be inferred from the 2D images using YOLOv8 [42]..."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Synthetic Data from NVIDIA Isaac Sim",
          "justification": "The research uses synthetic 2D annotated images generated by the NVIDIA Isaac Sim simulator as part of their testing framework.",
          "quote": "Using NVIDIA Isaac Sim, we created a photorealistic simulation environment for ARM applications, enabling the collection of synthetic 2D annotated images for training the DL vision models."
        },
        "aliases": [
          "Synthetic 2D Annotated Images"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "While the simulator is mentioned, there is no specific mention of a paper that references the synthetic dataset from the NVIDIA Isaac Sim.",
          "quote": "Using NVIDIA Isaac Sim, we created a photorealistic simulation environment for ARM applications, enabling the collection of synthetic 2D annotated images..."
        }
      },
      {
        "name": {
          "value": "Real-world Images Dataset",
          "justification": "The paper uses a dataset of real-world images to evaluate the performance of models trained and repaired using their approach.",
          "quote": "The model trained and repaired using the MARTENS approach achieved mean average precision (mAP) scores of 0.91 and 0.82 on real-world images with no prior retraining."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper discusses use of real-world images but does not provide a specific reference to a paper for this dataset.",
          "quote": "The model trained and repaired using the MARTENS approach achieved mean average precision (mAP) scores of 0.91 and 0.82 on real-world images..."
        }
      },
      {
        "name": {
          "value": "Test Datasets",
          "justification": "The paper references various test datasets to evaluate the performance of the frameworks and methods used, although they are not specifically named.",
          "quote": "The rest of this paper is structured as follows: Section 2 describes the details of MARTENS approach for in-simulation DNN testing and improvement."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "No specific paper reference is provided for the test datasets used in evaluating the framework.",
          "quote": "The rest of this paper is structured as follows: Section 2 describes the details of MARTENS approach for in-simulation DNN testing and improvement."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "NVIDIA Isaac Sim",
          "justification": "This library is integral to the simulation environment which is a core part of the MARTENS framework described in the paper.",
          "quote": "Using NVIDIA Isaac Sim, we created a photorealistic simulation environment for ARM applications..."
        },
        "aliases": [
          "Isaac Sim"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The NVIDIA Isaac Sim is mentioned as a tool used in the research but no specific reference paper is indicated.",
          "quote": "Using NVIDIA Isaac Sim, we created a photorealistic simulation environment for ARM applications..."
        }
      },
      {
        "name": {
          "value": "RMPFlow",
          "justification": "RMPFlow is used for robot control within the Isaac Sim environment as part of the testing framework described in the paper.",
          "quote": "To implement robot control within Isaac Sim, we used the RMPFlow controller..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "While RMPFlow is mentioned, there is no specific reference paper given.",
          "quote": "To implement robot control within Isaac Sim, we used the RMPFlow controller..."
        }
      },
      {
        "name": {
          "value": "Intel RealSense",
          "justification": "The library supports the camera systems used in testing the manipulators, as described in the experimental setup.",
          "quote": "The camera is an Intel RealSense D435 [21], configured to capture images of the size 640 x 514 pixels."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper mentions the Intel RealSense but does not provide a specific reference paper.",
          "quote": "The camera is an Intel RealSense D435 [21], configured to capture images..."
        }
      },
      {
        "name": {
          "value": "Ultralytics YOLO",
          "justification": "The YOLO model, particularly YOLOv8, is utilized as part of the vision model for detecting oriented bounding boxes in the experiment.",
          "quote": "This information can be inferred from the 2D images using YOLOv8 [42] that natively supports the oriented bounding box detection."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "While the utilization of YOLOv8 by Ultralytics is discussed, no specific reference paper is mentioned in the text.",
          "quote": "This information can be inferred from the 2D images using YOLOv8 [42]..."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1765,
    "prompt_tokens": 17775,
    "total_tokens": 19540,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}