{
  "paper": "c6a15c27a3f13c7d5a054966b9be6957.txt",
  "words": 11697,
  "extractions": {
    "title": {
      "value": "Recurrent neural networks learn robust representations by dynamically balancing compression and expansion",
      "justification": "The title is explicitly stated at the beginning of the paper.",
      "quote": "Recurrent neural networks learn robust representations by dynamically balancing compression and expansion"
    },
    "description": "The paper investigates how recurrent neural networks (RNNs) can balance the dimensionality of neural representations. It shows that RNNs naturally oscillate between expanding and compressing the dimensionality to match task demands, using chaos and learning. Strongly chaotic networks are particularly effective at balancing dimensionality expansion and compression, aiding in low-dimensional input classification and improving generalization.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes simulations and mathematical analyses that demonstrate how RNNs manage dimensionality.",
      "quote": "Through simulations and mathematical analysis, we show how the dimensionality of RNN activity evolves over time and over stages of learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on how deep learning models, specifically recurrent neural networks, balance dimensionality in representations.",
        "quote": "In this work we address these questions using recurrent neural network (RNN) models, which have recently shown promise in predicting and explaining brain dynamics."
      },
      "aliases": [
        "DL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Recurrent Neural Networks",
          "justification": "The paper examines how RNNs function in terms of dimensionality management.",
          "quote": "Recurrent neural networks (RNNs) can learn to balance tendencies to expand and compress dimensionality."
        },
        "aliases": [
          "RNNs"
        ]
      },
      {
        "name": {
          "value": "Neuroscience",
          "justification": "The study draws parallels between RNNs and brain dynamics.",
          "quote": "Bridging between machine learning and neuroscience, artificial networks are powerful tools for investigating dynamical representations in controlled settings."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Recurrent Neural Networks",
          "justification": "RNNs are central to the study, exploring their capacity to handle dimensionality.",
          "quote": "In this work, we address these questions using recurrent neural network (RNN) models."
        },
        "aliases": [
          "RNN",
          "RNN models"
        ],
        "is_contributed": {
          "value": false,
          "justification": "RNNs are well-established models and are not introduced as novel in this paper.",
          "quote": "Our network model is based on standard RNN models used in machine learning."
        },
        "is_executed": {
          "value": true,
          "justification": "The study involves executing RNN models as part of the experimentation.",
          "quote": "We investigate the dynamics of recurrent networks learning to classify static inputs."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares different initialization schemes like edge-of-chaos and strongly chaotic networks in the context of RNNs.",
          "quote": "We compare chaotic networks initialized near the transition point to chaos (said to be on the 'edge of chaos'), to 'strongly chaotic' networks."
        },
        "referenced_paper_title": {
          "value": "Supervised Sequence Labelling with Recurrent Neural Networks",
          "justification": "The reference title is mentioned in relation to standard RNN models used in the paper.",
          "quote": "Our network model is based on standard RNN models used in machine learning [22]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST is used as a reference dataset to validate the findings related to RNNs in the study.",
          "quote": "See Fig. S4, where we train our network on the MNIST digit classification database [34]."
        },
        "aliases": [
          "MNIST digit classification database"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-based learning applied to document recognition",
          "justification": "The referenced paper is the original publication related to the MNIST dataset, as cited in the paper.",
          "quote": "See Fig. S4, where we train our network on the MNIST digit classification database [34]."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 788,
    "prompt_tokens": 20581,
    "total_tokens": 21369,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}