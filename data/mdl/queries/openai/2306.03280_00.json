{
  "paper": "2306.03280.txt",
  "words": 15421,
  "extractions": {
    "title": {
      "value": "AHA!: Facilitating AI Impact Assessment by Generating Examples of Harms",
      "justification": "Title of the paper as provided by the user.",
      "quote": "AHA!: Facilitating AI Impact Assessment by Generating Examples of Harms"
    },
    "description": "This paper presents AHA! (Anticipating Harms of AI), a generative framework designed to help AI practitioners and decision-makers anticipate potential harms and unintended consequences of AI systems before they are developed or deployed. AHA! generates descriptions of possible harms for different stakeholders using vignettes and prompts provided to crowd workers and large language models. The paper evaluates AHA! by examining harms generated for five different AI deployment scenarios and discusses design implications for tools facilitating ethical reflection in AI practice.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments applying AHA! to different AI deployment scenarios and conducts interviews with AI professionals to assess the framework's utility.",
      "quote": "To evaluate AHA!’s capacity to surface possible harms, we first ran a series of experiments where we applied AHA! to five different AI deployment scenarios: hiring, loan application, content moderation, communication compliance, and disease diagnosis. Our analyses in Section 4 show that AHA! generates meaningful examples of possible harms."
    },
    "primary_research_field": {
      "name": {
        "value": "Ethical AI",
        "justification": "The primary focus of the paper is on anticipating and mitigating harms and unintended consequences of AI systems, which aligns with the research field of Ethical AI.",
        "quote": "We developed AHA! (Anticipating Harms of AI), a generative framework to assist AI practitioners and decision-makers in anticipating potential harms and unintended consequences of AI systems prior to development or deployment."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Human-Computer Interaction",
          "justification": "The paper involves user interaction by evaluating the harms generated by AHA! and conducting interviews with AI professionals to gather their insights, which falls under Human-Computer Interaction.",
          "quote": "To gauge AHA!’s potential practical utility, we also conducted semi-structured interviews with responsible AI professionals (N = 9)."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GPT-3",
          "justification": "GPT-3 was explicitly mentioned and used in the paper for generating examples of potential harms by completing vignettes.",
          "quote": "In another example, Park et al. [56] used LLMs to simulate social interactions between distinct LLM-powered personas in social computing systems to help system designers refine community rules of engagement. AHA! also uses LLMs to complete vignettes with possible examples of harms, complementing examples elicited from crowds."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "GPT-3 is not a novel contribution of this paper but was used as a tool for generating examples of harms.",
          "quote": "AHA! also uses LLMs to complete vignettes with possible examples of harms, complementing examples elicited from crowds."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper describes using GPT-3 for generating examples, implying it was executed as part of the study.",
          "quote": "we prompted GPT-3 (davinci model with temperature set to 0.95)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The examples generated by GPT-3 were compared to those generated by crowds.",
          "quote": "the combination of crowds + GPT3 produced significantly larger and more diverse examples of harms than either the crowd or the model alone."
        },
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "This is the reference paper that introduced GPT-3.",
          "quote": "Brown, T. B., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901."
        }
      }
    ],
    "datasets": [],
    "libraries": [
      {
        "name": {
          "value": "GPT-3",
          "justification": "GPT-3 is used as a language model library for generating text.",
          "quote": "In another example, Park et al. [56] used LLMs to simulate social interactions between distinct LLM-powered personas in social computing systems to help system designers refine community rules of engagement. AHA! also uses LLMs to complete vignettes with possible examples of harms, complementing examples elicited from crowds."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "The referenced paper is the one that introduced GPT-3.",
          "quote": "Brown, T. B., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems, 33, 1877-1901."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 970,
    "prompt_tokens": 25034,
    "total_tokens": 26004
  }
}