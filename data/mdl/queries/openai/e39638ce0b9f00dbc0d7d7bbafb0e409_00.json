{
  "paper": "e39638ce0b9f00dbc0d7d7bbafb0e409.txt",
  "words": 20528,
  "extractions": {
    "title": {
      "value": "Continual Learning with Foundation Models: An Empirical Study of Latent Replay",
      "justification": "The title of the paper explicitly defines its focus on continual learning using foundation models and investigating latent replay, which is evident from the structure and content of the study.",
      "quote": "CONTINUAL LEARNING WITH FOUNDATION MODELS: AN EMPIRICAL STUDY OF LATENT REPLAY"
    },
    "description": "This paper investigates the efficacy of foundation models in continual learning frameworks, particularly focusing on class incremental learning scenarios. It compares the performance of large-scale pre-trained vision models using latent space representations with traditional raw data approaches. Key insights include how factors such as task similarity and the encoder's characteristics affect continual learning performance, and how self-supervised pre-training can benefit out-of-distribution domains compared to the pre-training domain. The paper introduces a set of diverse datasets as an efficient playground for further continual learning research.",
    "type": {
      "value": "empirical",
      "justification": "The paper reports on experiments conducted with various pre-trained models and benchmarks to evaluate continual learning performance, fitting the empirical research definition.",
      "quote": "For this, we conduct an extensive analysis of CL using the ER strategy applied in the latent space of pretrained models (latent ER) using 26 different large-scale models pre-trained on a variety of datasets, architectures and with different pre-training algorithms."
    },
    "primary_research_field": {
      "name": {
        "value": "Continual Learning",
        "justification": "The paper focuses on approaches and challenges in continual learning, primarily examining how these are addressed by foundation models and latent replay strategies.",
        "quote": "The goal of continual learning (CL) is to design machine learning (ML) algorithms that can learn tasks presented in the form of a non-stationary stream..."
      },
      "aliases": [
        "CL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper discusses the use of large-scale pre-trained vision models and their application to downstream tasks, indicating a focus on computer vision approaches.",
          "quote": "Recent years have witnessed development of large scale models pre-trained on broad data – the so called foundation models. These models are intended to be universal feature extractors that can produce useful features for a large number of downstream tasks."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet",
          "justification": "The paper mentions ResNet as one of the architectures of the encoders used in the experiments.",
          "quote": "We consider a total of up to 32 encoders (most experiments used 26) with either ResNet (He et al., 2020) or Vision Transformer (ViT) (Dosovitskiy et al., 2020) architectures."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "ResNet is a well-known model not introduced by this paper.",
          "quote": "ResNet (He et al., 2020)"
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments with ResNet models are part of the study's empirical evaluations.",
          "quote": "We consider a total of up to 32 encoders (most experiments used 26) with either ResNet ... architectures."
        },
        "is_compared": {
          "value": true,
          "justification": "ResNet is compared to other models like Vision Transformer in the study's empirical analysis.",
          "quote": "We compare the efficacy of various pre-trained models in large-scale benchmarking scenarios."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "This is the original paper introducing the ResNet architecture, often cited in works that discuss or use ResNet.",
          "quote": "ResNet (He et al., 2020)"
        }
      },
      {
        "name": {
          "value": "Vision Transformer (ViT)",
          "justification": "Vision Transformer is explicitly mentioned as one of the model architectures used in the study's experiments.",
          "quote": "We consider a total of up to 32 encoders (most experiments used 26) with either ResNet ... or Vision Transformer (ViT) architectures."
        },
        "aliases": [
          "ViT"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Vision Transformer is not a new model proposed in this paper.",
          "quote": "Vision Transformer (ViT) (Dosovitskiy et al., 2020)"
        },
        "is_executed": {
          "value": true,
          "justification": "ViT is one of the encoders used in the experimental evaluation.",
          "quote": "We consider a total of up to 32 encoders (most experiments used 26) with either ResNet ... or Vision Transformer (ViT) architectures."
        },
        "is_compared": {
          "value": true,
          "justification": "ViT is evaluated alongside ResNet and other models in the study.",
          "quote": "We compare the efficacy of various pre-trained models in large-scale benchmarking scenarios."
        },
        "referenced_paper_title": {
          "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "justification": "This paper introduced the Vision Transformer model which is referenced in the provided study.",
          "quote": "Vision Transformer (ViT) (Dosovitskiy et al., 2020)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CUB-200",
          "justification": "The dataset is specifically mentioned as one of the six different datasets used for evaluating downstream tasks in the experiments.",
          "quote": "We consider six different datasets for downstream tasks. CUB-200 (Welinder et al., 2010) which contains 6K images of birds distributed in 200 different categories."
        },
        "aliases": [
          "Caltech-UCSD Birds 200"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Caltech-UCSD Birds 200",
          "justification": "The dataset reference aligns with the naming convention and characteristics as described in the paper.",
          "quote": "CUB-200 (Welinder et al., 2010)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "timm",
          "justification": "The library timm is mentioned as a source for the model implementations used in the study.",
          "quote": "Models from timm include their own data-preprocessing functions."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch Image Models",
          "justification": "This matches the typical reference associated with the ‘timm’ library.",
          "quote": "Models from timm include their own data-preprocessing functions."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1271,
    "prompt_tokens": 50288,
    "total_tokens": 51559,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}