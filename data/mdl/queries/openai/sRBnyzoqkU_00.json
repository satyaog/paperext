{
  "paper": "sRBnyzoqkU.txt",
  "words": 6207,
  "extractions": {
    "title": {
      "value": "Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks",
      "justification": "This is the full title as given in the paper.",
      "quote": "Model Breadcrumbs: Scaling Multi-Task Model Merging with Sparse Masks"
    },
    "description": "This paper introduces the innovative Model Breadcrumbs strategy, which efficiently merges fine-tuned foundation models across multiple tasks using sparse masks, thereby enhancing task performance and scalability.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes extensive experimentation involving various models and tasks to establish the effectiveness of the introduced Model Breadcrumbs strategy.",
      "quote": "Through extensive experimentation involving various models and tasks, we establish that integrating Model Breadcrumbs offers a straightforward, efficient, and highly effective approach for constructing multi-task models and facilitating updates to foundation models."
    },
    "primary_research_field": {
      "name": {
        "value": "Multi-Task Learning",
        "justification": "The paper focuses on creating and evaluating methods for merging models to perform well on multiple tasks concurrently.",
        "quote": "This paper introduces an innovative strategy termed Model Breadcrumbs, which addresses the need to merge multiple fine-tunings of the same foundation model across a spectrum of auxiliary tasks."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Model Merging",
          "justification": "The method proposed in the paper primarily revolves around merging multiple models.",
          "quote": "To address the challenges of scalability, practical constraints, and unlock the untapped potential of the growing pool of publicly available fine-tuned models, recent developments in neural network weight averaging techniques have gained attention."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP",
          "justification": "The paper mentions using CLIP models for fine-tuning and evaluation.",
          "quote": "Using the above datasets, we first fine-tune a series of CLIP models Radford et al. (2021)."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The paper uses existing CLIP models; they are not an original contribution of this paper.",
          "quote": "Using the above datasets, we first fine-tune a series of CLIP models Radford et al. (2021)."
        },
        "is_executed": {
          "value": 1,
          "justification": "The experiments involve fine-tuning and evaluating CLIP models.",
          "quote": "Using the above datasets, we first fine-tune a series of CLIP models Radford et al. (2021)."
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of fine-tuned CLIP models is compared.",
          "quote": "In our fine-tuning process, we adopt a procedure similar to that outlined in a previous study Ilharco et al. (2022b)."
        },
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "This title is provided as the reference for CLIP models.",
          "quote": "Using the above datasets, we first fine-tune a series of CLIP models Radford et al. (2021)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Cars",
          "justification": "The Cars dataset is explicitly mentioned as used for experiments.",
          "quote": "We assess our findings using an extensive set of 8 datasets: Cars (Krause et al., 2013)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "3D Object Representations for Fine-Grained Categorization",
          "justification": "The referenced paper title for this dataset.",
          "quote": "We assess our findings using an extensive set of 8 datasets: Cars (Krause et al., 2013)."
        }
      },
      {
        "name": {
          "value": "DTD",
          "justification": "The DTD dataset is explicitly mentioned as used for experiments.",
          "quote": "We assess our findings using an extensive set of 8 datasets: DTD (Cimpoi et al., 2014)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Describing Textures in the Wild",
          "justification": "The referenced paper title for this dataset.",
          "quote": "We assess our findings using an extensive set of 8 datasets: DTD (Cimpoi et al., 2014)."
        }
      },
      {
        "name": {
          "value": "EuroSAT",
          "justification": "The EuroSAT dataset is explicitly mentioned as used for experiments.",
          "quote": "We assess our findings using an extensive set of 8 datasets: EuroSAT (Helber et al., 2019)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification",
          "justification": "The referenced paper title for this dataset.",
          "quote": "We assess our findings using an extensive set of 8 datasets: EuroSAT (Helber et al., 2019)."
        }
      },
      {
        "name": {
          "value": "German Traffic Sign Recognition Benchmark (GTSRB)",
          "justification": "The GTSRB dataset is explicitly mentioned as used for experiments.",
          "quote": "We assess our findings using an extensive set of 8 datasets: GTSRB (Houben et al., 2013)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Detection of Traffic Signs in Real-World Images: The German Traffic Sign Detection Benchmark",
          "justification": "The referenced paper title for this dataset.",
          "quote": "We assess our findings using an extensive set of 8 datasets: GTSRB (Houben et al., 2013)."
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "The MNIST dataset is explicitly mentioned as used for experiments.",
          "quote": "We assess our findings using an extensive set of 8 datasets: MNIST (LeCun et al., 2010)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MNIST handwritten digit database",
          "justification": "The referenced paper title for this dataset.",
          "quote": "We assess our findings using an extensive set of 8 datasets: MNIST (LeCun et al., 2010)."
        }
      },
      {
        "name": {
          "value": "RESISC45",
          "justification": "The RESISC45 dataset is explicitly mentioned as used for experiments.",
          "quote": "We assess our findings using an extensive set of 8 datasets: RESISC45 (Cheng et al., 2017)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Remote Sensing Image Scene Classification: Benchmark and State of the Art",
          "justification": "The referenced paper title for this dataset.",
          "quote": "We assess our findings using an extensive set of 8 datasets: RESISC45 (Cheng et al., 2017)."
        }
      },
      {
        "name": {
          "value": "SUN397",
          "justification": "The SUN397 dataset is explicitly mentioned as used for experiments.",
          "quote": "We assess our findings using an extensive set of 8 datasets: SUN397 (Xiao et al., 2010)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SUN database: Large-scale scene recognition from abbey to zoo",
          "justification": "The referenced paper title for this dataset.",
          "quote": "We assess our findings using an extensive set of 8 datasets: SUN397 (Xiao et al., 2010)."
        }
      },
      {
        "name": {
          "value": "SVHN",
          "justification": "The SVHN dataset is explicitly mentioned as used for experiments.",
          "quote": "We assess our findings using an extensive set of 8 datasets: SVHN (Netzer et al., 2011)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Reading Digits in Natural Images with Unsupervised Feature Learning",
          "justification": "The referenced paper title for this dataset.",
          "quote": "We assess our findings using an extensive set of 8 datasets: SVHN (Netzer et al., 2011)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2003,
    "prompt_tokens": 11632,
    "total_tokens": 13635
  }
}