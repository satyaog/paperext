{
  "paper": "c8180c6dd88d3ac34287cdf07f60c6e9.txt",
  "words": 10023,
  "extractions": {
    "title": {
      "value": "An Attentive Approach for Building Partial Reasoning Agents from Pixels",
      "justification": "The title was explicitly listed at the beginning of the document.",
      "quote": "An Attentive Approach for Building Partial Reasoning Agents from Pixels"
    },
    "description": "The paper introduces an approach for building model-based reinforcement learning agents that can generalize effectively by focusing on relevant environmental features, particularly using pixel-based inputs for reasoning.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experiments and quantitative analyses on specific domains to demonstrate the effectiveness of the proposed approach.",
      "quote": "Our quantitative analyses show that the proposed approach allows for effective generalization in high-dimensional domains with raw observational inputs."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The primary aim of the paper is to improve model-based reinforcement learning agents through partial reasoning and pixel-based inputs.",
        "quote": "The goal in model-based reinforcement learning (RL) is to build reasoning agents that maximize long-term cumulative reward through trial and error interaction with the environment."
      },
      "aliases": [
        "RL",
        "Model-Based Reinforcement Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Attention Mechanisms",
          "justification": "The paper extensively discusses the use of attention mechanisms to identify and attend to relevant inputs in the environment.",
          "quote": "The aspect identification module... uses an iterative attention mechanism to produce the set of slots."
        },
        "aliases": [
          "Soft Attention",
          "Self-Attention"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The approach deals with processing pixel-based inputs, which falls under the domain of computer vision.",
          "quote": "Unlike prior work, our end-to-end approach allows for building reasoning agents that automatically identify the distinct aspects of the environment and then dynamically attend to the relevant ones... which makes it possible for them to work on high-dimensional domains with raw observational inputs."
        },
        "aliases": [
          "Vision Processing"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Partial Reasoning Agent",
          "justification": "This agent was proposed in the paper as a novel model to improve generalization by focusing on relevant aspects of the environment using pixel-based inputs.",
          "quote": "Partial Reasoning (PR) Agent. A partial reasoning agent that was built by our proposed approach in Sec. 3."
        },
        "aliases": [
          "PR Agent"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The model is introduced as a new approach by the authors.",
          "quote": "A partial reasoning agent that was built by our proposed approach in Sec. 3."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes experiments where the PR agent is evaluated, indicating execution.",
          "quote": "We perform our experiments on the (i) MiniGrid (Chevalier-Boisvert et al., 2018) and (ii) Procgen domains (Cobbe et al., 2020)."
        },
        "is_compared": {
          "value": true,
          "justification": "The PR agent is compared to other agents such as MuZero and Model-Free agents in performance experiments.",
          "quote": "We compare the generalization performances of the PR, MZ and MF agents across two image-based MiniGrid and Procgen domains."
        },
        "referenced_paper_title": {
          "value": "A consciousness-inspired planning agent for model-based reinforcement learning",
          "justification": "This referenced paper by Zhao et al. (2021) serves as a basis for some of the concepts explored in the current paper.",
          "quote": "The study of Zhao et al. (2021) demonstrated that partial reasoning can allow for effective generalization over regular reasoning."
        }
      },
      {
        "name": {
          "value": "MuZero",
          "justification": "MuZero is used as a baseline model in the experiments to compare against the proposed PR agent.",
          "quote": "MuZero (MZ) Agent. A MuZero (Schrittwieser et al., 2020) agent."
        },
        "aliases": [
          "MZ"
        ],
        "is_contributed": {
          "value": false,
          "justification": "MuZero is an existing model used for comparison, not introduced by the paper.",
          "quote": "We compare the generalization performances of the PR, MZ and MF agents across two image-based MiniGrid and Procgen domains."
        },
        "is_executed": {
          "value": true,
          "justification": "Performance evaluations include the execution of the MuZero model for baseline comparisons.",
          "quote": "A MuZero (Schrittwieser et al., 2020) agent. Unlike the PR agent, the MZ agent performs regular reasoning."
        },
        "is_compared": {
          "value": true,
          "justification": "MuZero's performance is compared against the PR agent to highlight differences in generalization capability.",
          "quote": "Performance curves in Fig. 3 consistently show that, the PR agent displays a better generalization performance than the MZ agent."
        },
        "referenced_paper_title": {
          "value": "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
          "justification": "The paper references MuZero by Schrittwieser et al. (2020) as part of its baseline discussion.",
          "quote": "MuZero (Schrittwieser et al., 2020) agent."
        }
      },
      {
        "name": {
          "value": "Model-Free Agent",
          "justification": "Model-Free agents are mentioned as a contrast to the model-based approach advocated by the paper.",
          "quote": "Model-Free (MF) Agent. A version of the MZ agent that does not perform any planning."
        },
        "aliases": [
          "MF"
        ],
        "is_contributed": {
          "value": false,
          "justification": "This type of agent is used for comparison purposes and is not a contribution of the paper.",
          "quote": "Model-Free (MF) Agent. A version of the MZ agent that does not perform any planning."
        },
        "is_executed": {
          "value": true,
          "justification": "The Model-Free agent was executed as part of the experiments to evaluate its generalization performance.",
          "quote": "We compare the generalization performances of the PR, MZ and MF agents across two image-based MiniGrid and Procgen domains."
        },
        "is_compared": {
          "value": true,
          "justification": "The Model-Free agent's performance is contrasted with that of the PR and MuZero agents.",
          "quote": "We compare the generalization performances of the PR, MZ and MF agents across two image-based MiniGrid and Procgen domains."
        },
        "referenced_paper_title": {
          "value": "The predictron: End-to-end learning and planning",
          "justification": "This is similar to a baseline model referenced in the context of model-based reinforcement learning.",
          "quote": "MuZero (Schrittwieser et al., 2020) agent. Unlike the PR agent, the MZ agent performs regular reasoning."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MiniGrid",
          "justification": "The MiniGrid domain is used for experiments to test the agent's performance.",
          "quote": "We perform our experiments on the (i) MiniGrid (Chevalier-Boisvert et al., 2018) and (ii) Procgen domains (Cobbe et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Minimalistic Gridworld Environment for OpenAI Gym",
          "justification": "The reference paper by Chevalier-Boisvert et al. (2018) describes the MiniGrid environment used in the experiments.",
          "quote": "MiniGrid (Chevalier-Boisvert et al., 2018)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Slot Attention",
          "justification": "Slot attention was specifically mentioned as part of the model architecture used in the paper.",
          "quote": "We prefer slot attention over other techniques as it allows for better generalization (Locatello et al., 2020)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Object-centric Learning with Slot Attention",
          "justification": "The Slot Attention concept is taken from the paper by Locatello et al. (2020) and used for aspect identification in the current research.",
          "quote": "We prefer slot attention over other techniques as it allows for better generalization (Locatello et al., 2020)."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1612,
    "prompt_tokens": 17632,
    "total_tokens": 19244,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}