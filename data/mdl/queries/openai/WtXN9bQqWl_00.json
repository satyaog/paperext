{
  "paper": "WtXN9bQqWl.txt",
  "words": 9071,
  "extractions": {
    "title": {
      "value": "Discovering Object-Centric Generalized Value Functions From Pixels",
      "justification": "Title appears at the top of the first page and in the header of subsequent pages",
      "quote": "Discovering Object-Centric Generalized Value Functions From Pixels"
    },
    "description": "The paper proposes a new method for automatically discovering object-centric General Value Functions (OC-GVFs) from pixel inputs to improve representation learning in reinforcement learning tasks. The proposed method leverages Slot Attention for object discovery and uses these discovered objects to create cumulants for training value functions, which are then used as features for learning downstream control policies.",
    "type": {
      "value": "empirical",
      "justification": "The authors present comparative experimental results with other baselines, demonstrating the improved performance of their proposed method across different environments and settings.",
      "quote": "In this paper, we introduce a method that tries to discover meaningful features from objects, translating them to temporally coherent ‘question’ functions and leveraging the subsequent learned general value functions for control. We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on improving representation learning and policy learning in Reinforcement Learning (RL) by discovering object-centric General Value Functions (GVFs).",
        "quote": "Deep Reinforcement Learning has shown significant progress in extracting useful representations from high-dimensional inputs albeit using handcrafted auxiliary tasks and pseudo rewards."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper involves learning from high-dimensional visual inputs (pixels) and uses a slot attention mechanism for object discovery in images.",
          "quote": "1. Introduction Learning control from high-dimensional input such as images is a complex problem relevant to many real world applications."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Object-Centric General Value Functions (OC-GVFs)",
          "justification": "The authors introduce and develop this new model in the paper.",
          "quote": "We propose OC-GVFs: an end-to-end approach to automatically discover object-centric General Value Functions from pixels."
        },
        "aliases": [
          "OC-GVFs"
        ],
        "is_contributed": {
          "value": 1,
          "justification": "This model is introduced and validated within the scope of the paper.",
          "quote": "We propose OC-GVFs: an end-to-end approach to automatically discover object-centric General Value Functions from pixels."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was implemented and executed as part of the empirical evaluation in the paper.",
          "quote": "We demonstrate that OC-GVFs can outperform the current state-of-the-art algorithms for GVF discovery in both stationary and non-stationary environments."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares OC-GVFs with several baseline methods and state-of-the-art techniques.",
          "quote": "We compare our approach with state-of-the-art techniques alongside other ablations and show competitive performance in both stationary and non-stationary settings."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no specific reference paper for OC-GVFs; it is introduced in this paper.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Collect Objects",
          "justification": "The paper describes using a customized version of a gridworld environment named Collect-objects for experiments.",
          "quote": "Collect-objects Environment: is a customized version of the four-room gridworld environment similar to the one used in Veeriah et al."
        },
        "aliases": [
          "Collect Objects"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Discovery of useful questions as auxiliary tasks",
          "justification": "The primary paper for the Collect-objects environment appears to be referenced here.",
          "quote": "Collect-objects Environment: is a customized version of the four-room gridworld environment similar to the one used in Veeriah et al.."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper involves implementing deep learning models, which are commonly done using PyTorch.",
          "quote": "All our experiments were run on a single V100 GPU."
        },
        "aliases": [
          "Torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper does not specifically reference another paper for PyTorch.",
          "quote": ""
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 872,
    "prompt_tokens": 15459,
    "total_tokens": 16331
  }
}