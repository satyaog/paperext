{
  "paper": "UROBiQEOLP.txt",
  "words": 11576,
  "extractions": {
    "title": {
      "value": "E-FORCING: IMPROVING AUTOREGRESSIVE MODELS BY TREATING IT AS AN ENERGY-BASED ONE",
      "justification": "The title is taken directly from the paper.",
      "quote": "E-FORCING: IMPROVING AUTOREGRESSIVE MODELS BY TREATING IT AS AN ENERGY-BASED ONE"
    },
    "description": "This paper proposes a method called E-Forcing to improve autoregressive generative models (ARGMs) by integrating them with energy-based models using a specific energy-based learning objective.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves extensive empirical results and experimentation to demonstrate the effectiveness of the proposed approach.",
      "quote": "Extensive empirical results, covering numerous benchmarks demonstrate the effectiveness of the proposed approach."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on autoregressive generative models, which are widely used in natural language processing tasks like machine translation and language modeling.",
        "quote": "They can be trained efficiently via maximum likelihood and generate samples of exceptional quality, making this technique popular for modeling distributions, especially for sequential data."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Energy-Based Models",
          "justification": "The paper aims to integrate energy-based models with autoregressive generative models.",
          "quote": "EBMs are a popular class of generative models that have demonstrated their effectiveness in modeling high-dimensional distributions in a variety of machine learning applications."
        },
        "aliases": [
          "EBM"
        ]
      },
      {
        "name": {
          "value": "Autoregressive Generative Models",
          "justification": "The core focus of the paper is on improving autoregressive generative models.",
          "quote": "autoregressive generative models (abbr. ARGMs) simplify the difficult challenge of modeling high-dimensional joint distributions."
        },
        "aliases": [
          "ARGM"
        ]
      },
      {
        "name": {
          "value": "Machine Translation",
          "justification": "One of the applications explored in the paper is neural machine translation.",
          "quote": "We further evaluate E-Forcing’s effectiveness on neural machine translation (NMT), which can be regarded as a conditional generation task."
        },
        "aliases": [
          "NMT"
        ]
      },
      {
        "name": {
          "value": "Language Modeling",
          "justification": "Another key application tested in the paper is language modeling.",
          "quote": "For the language modeling task, three different datasets, WikiText-103, Toronto Book Corpus, and CC-news, are chosen as testbeds."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "E-Forcing",
          "justification": "The paper introduces a new method called E-Forcing for improving ARGMs by treating them as energy-based models.",
          "quote": "In this paper, we propose a unique method termed E-Forcing for training autoregressive generative models that takes advantage of a well-designed energy-based learning objective."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "E-Forcing is the primary contribution of the paper.",
          "quote": "In this paper, we propose a unique method termed E-Forcing for training autoregressive generative models."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is executed in various experiments covered in the paper, across different tasks.",
          "quote": "Extensive empirical results, covering numerous benchmarks demonstrate the effectiveness of the proposed approach."
        },
        "is_compared": {
          "value": 1,
          "justification": "E-Forcing is compared numerically with baseline models in several experiments.",
          "quote": "we also compared our E-Forcing with the residual EBM Deng et al. (2020) method, which is a typical method to improve the performance of language models by utilizing EBMs."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "E-Forcing is a new model introduced in this paper, and therefore does not have a referenced paper.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "WikiText-103",
          "justification": "WikiText-103 is used as a testbed for evaluating the language modeling task.",
          "quote": "For the language modeling task, three different datasets, WikiText-103, Toronto Book Corpus, and CC-news, are chosen as testbeds;"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pointer Sentinel Mixture Models",
          "justification": "Paper published by Merity et al., outlining WikiText-103 dataset.",
          "quote": "Merity, S., Xiong, C., Bradbury, J., & Socher, R. (2017). Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843."
        }
      },
      {
        "name": {
          "value": "Toronto Book Corpus",
          "justification": "Toronto Book Corpus is another testbed for evaluating the language modeling task.",
          "quote": "For the language modeling task, three different datasets, WikiText-103, Toronto Book Corpus, and CC-news, are chosen as testbeds;"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books",
          "justification": "Paper published by Zhu et al., outlining Toronto Book Corpus dataset.",
          "quote": "Zhu, Y., Kiros, R., Zemel, R. S., Salakhutdinov, R., Urtasun, R., Torralba, A., & Fidler, S. (2015). Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision (pp. 19-27)."
        }
      },
      {
        "name": {
          "value": "CC-News",
          "justification": "CC-News is used as a testbed for evaluating the language modeling task.",
          "quote": "For the language modeling task, three different datasets, WikiText-103, Toronto Book Corpus, and CC-news, are chosen as testbeds;"
        },
        "aliases": [
          "Common Crawl News"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "CC-News: A large English news corpus",
          "justification": "Paper published by Mackenzie outlining the CC-News dataset.",
          "quote": "Mackenzie et al. (2020). CC-News: A large English news corpus."
        }
      },
      {
        "name": {
          "value": "IWSLT14",
          "justification": "IWSLT14 dataset is used to evaluate E-Forcing's performance in neural machine translation tasks.",
          "quote": "We mainly analyze E-Forcing on the IWSLT14 dataset, which includes six different language pairs."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Report on the 11th IWSLT Evaluation Campaign",
          "justification": "Paper outlining the IWSLT14 dataset.",
          "quote": "Cettolo, M., Niehues, J., Bentivogli, L., Bertoldi, N., & Federico, M. (2014). Report on the 11th IWSLT Evaluation Campaign."
        }
      },
      {
        "name": {
          "value": "WMT16",
          "justification": "WMT16 dataset is used to evaluate E-Forcing's performance in neural machine translation tasks.",
          "quote": "Table 3 shows the performance of E-Forcing on the WMT16 English → German benchmark, which is a relatively larger dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Findings of the 2016 Conference on Machine Translation (WMT16)",
          "justification": "Paper outlining the WMT16 dataset.",
          "quote": "Bojar, O., Chatterjee, R., Federmann, C., Graham, Y., Haddow, B., Huang, S., ... & Specia, L. (2016). Findings of the 2016 conference on machine translation (WMT16)."
        }
      },
      {
        "name": {
          "value": "MNIST",
          "justification": "MNIST dataset is used for the image generation experiment involving E-Forcing.",
          "quote": "To illustrate the effectiveness and generality of our method in processing different modality tasks, we further show the results of applying E-Forcing to image generation in this section. We apply E-Forcing to Pixel-CNN (Van Oord et al., 2016) and its variant Gated Pixel-CNN (Oord et al., 2016). Experiments are carried out on the MNIST and CIFAR-10 datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Gradient-based learning applied to document recognition",
          "justification": "Paper published by LeCun et al., outlining the MNIST dataset.",
          "quote": "LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278-2324."
        }
      },
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 dataset is used for the image generation experiment involving E-Forcing.",
          "quote": "To illustrate the effectiveness and generality of our method in processing different modality tasks, we further show the results of applying E-Forcing to image generation in this section. We apply E-Forcing to Pixel-CNN (Van Oord et al., 2016) and its variant Gated Pixel-CNN (Oord et al., 2016). Experiments are carried out on the MNIST and CIFAR-10 datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning multiple layers of features from tiny images",
          "justification": "Paper published by Krizhevsky et al., outlining the CIFAR-10 dataset.",
          "quote": "Krizhevsky, A., & Hinton, G. (2009). Learning multiple layers of features from tiny images."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is mentioned as the deep learning library used to implement and test the models in the paper.",
          "quote": "We uniformly use the Adam optimizer. The training will be stopped once the model has not obtained better performance for 20 epochs on the validation set. As for the model structures of the image generation task, we use the official structure reported by PixelCNN (van den Oord et al., 2016c) and Gated PixelCNN (van den Oord et al., 2016b) without modification. The source code will be released once upon acceptance."
        },
        "aliases": [
          "torch"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "Paper by Paszke et al., outlining PyTorch library is referenced.",
          "quote": "Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., ... & Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 2202,
    "prompt_tokens": 21899,
    "total_tokens": 24101
  }
}