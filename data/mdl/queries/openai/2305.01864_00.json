{
  "paper": "2305.01864.txt",
  "words": 4575,
  "extractions": {
    "title": {
      "value": "UNSUPERVISED IMPROVEMENT OF AUDIO-TEXT CROSS-MODAL REPRESENTATIONS",
      "justification": "The title is explicitly mentioned at the beginning of the paper.",
      "quote": "UNSUPERVISED IMPROVEMENT OF AUDIO-TEXT CROSS-MODAL REPRESENTATIONS"
    },
    "description": "This paper studies unsupervised approaches to improve audio-text cross-modal representations using unpaired text and audio. It explores domain-unspecific and domain-specific methods to curate audio-text pairs for training and demonstrates significant improvements in zero-shot classification performance for sound event and acoustic scene classification tasks.",
    "type": {
      "value": "empirical",
      "justification": "The paper proposes methods and conducts experiments to evaluate the improvements in zero-shot classification tasks.",
      "quote": "In this paper, we study unsupervised approaches to improve the learning framework of such representations with unpaired text and audio ... We also show that when domain-specific curation is used in conjunction with a soft-labeled contrastive loss, we are able to obtain significant improvement in terms of zero-shot classification performance on downstream sound event classification or acoustic scene classification tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Audio-Text Representation Learning",
        "justification": "The paper focuses on learning cross-modal representations between audio and text, improving methods for this specific purpose.",
        "quote": "Recent advances in using language models to obtain crossmodal audio-text representations have overcome the limitations of conventional training approaches ... In this paper, we study unsupervised approaches to improve the learning framework of such representations with unpaired text and audio."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Contrastive Learning",
          "justification": "The paper employs contrastive learning methods to train its models for audio-text representation learning.",
          "quote": "In the CLAP model, the latent representation is obtained by passing the text and audio through the text and audio encoders ... The model then tries to maximize the diagonal entries on the matrix C = ta⊤."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Zero-shot Classification",
          "justification": "The paper aims to improve zero-shot classification performance using curated audio-text pairs.",
          "quote": "We also show that when domain-specific curation is used in conjunction with a soft-labeled contrastive loss, we are able to obtain significant improvement in terms of zero-shot classification performance on downstream sound event classification or acoustic scene classification tasks."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLAP (Contrastive Language-Audio Pretraining)",
          "justification": "The CLAP model is discussed extensively as both a teacher and student model for training audio-text representations.",
          "quote": "The CLAP model is able to perform zero-shot classification by simply calculating the similarity of a given audio to a fixed set of text prompts constructed from class labels."
        },
        "aliases": [
          "CLAP"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper builds on the existing CLAP model and proposes improvements rather than contributing a new model.",
          "quote": "In the CLAP model, the latent representation is obtained by passing the text and audio through the text and audio encoders ft (.), and fa (.) ... The model then tries to maximize the diagonal entries on the matrix C = ta⊤."
        },
        "is_executed": {
          "value": true,
          "justification": "The CLAP model is executed in the experiments presented in the paper.",
          "quote": "The models are trained with 2 Quadro RTX 6000 GPUs using a batch size of 64 per GPU. We perform three runs for each setup and obtain the average results."
        },
        "is_compared": {
          "value": true,
          "justification": "The model’s performance is compared with various adjustments and curated datasets within the scope of the paper.",
          "quote": "In Table 2, we showcase the improvements obtained when the teacher model is trained with the full training set."
        },
        "referenced_paper_title": {
          "value": "CLAP: Learning audio concepts from natural language supervision",
          "justification": "The original CLAP model is referenced multiple times throughout the paper and it is crucial to the methodology presented.",
          "quote": "... follows the original paper [9] by using 128k paired audio and text captions from FSD50k [16], ClothoV2 [17], AudioCaps [18], and MACS [19]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ESC-50",
          "justification": "The dataset is used for zero-shot evaluation of the proposed methods.",
          "quote": "We consider the following datasets for downstream evaluation with sound event classification and acoustic scene classification tasks: ESC-50 [22] with 2000, 5-second audio clips from 50 environmental sound classes."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ESC: Dataset for Environmental Sound Classification",
          "justification": "The paper references the original source of the ESC-50 dataset to provide context and background.",
          "quote": "ESC-50 [22] with 2000, 5-second audio clips from 50 environmental sound classes."
        }
      },
      {
        "name": {
          "value": "UrbanSound8K",
          "justification": "The dataset is used for zero-shot evaluation of the proposed methods.",
          "quote": "We consider the following datasets for downstream evaluation with sound event classification and acoustic scene classification tasks: UrbanSound8K [23] with 8732, 4-second recordings from 10 possible urban sound classes."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A dataset and taxonomy for urban sound research",
          "justification": "The paper references the original source of the UrbanSound8K dataset to provide context and background.",
          "quote": "UrbanSound8K [23] with 8732, 4-second recordings from 10 possible urban sound classes."
        }
      },
      {
        "name": {
          "value": "TUT Acoustic Scenes 2017 (TUT17)",
          "justification": "The dataset is used for zero-shot evaluation of the proposed methods.",
          "quote": "We consider the following datasets for downstream evaluation with sound event classification and acoustic scene classification tasks: TUT Acoustic Scenes 2017 (TUT17) [24] with 6300, 10-second clips from 15 possible scene classes."
        },
        "aliases": [
          "TUT17"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "DCASE 2017 challenge setup: Tasks, datasets and baseline system",
          "justification": "The paper references the original source of the TUT Acoustic Scenes 2017 dataset to provide context and background.",
          "quote": "TUT Acoustic Scenes 2017 (TUT17) [24] with 6300, 10-second clips from 15 possible scene classes."
        }
      },
      {
        "name": {
          "value": "AudioSet",
          "justification": "The dataset is used to match with captions for domain-unspecific improvement.",
          "quote": "We match the captions of the training data with audio from a large dataset such as AudioSet [13]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Audio Set: An ontology and human-labeled dataset for audio events",
          "justification": "The paper references the original source of the AudioSet dataset to provide context and background.",
          "quote": "AudioSet [13] by excluding recordings that are corrupted."
        }
      },
      {
        "name": {
          "value": "FSD50k",
          "justification": "The dataset is part of the paired audio and text data used for training the teacher CLAP model.",
          "quote": "For training the teacher CLAP, we follow the original paper [9] by using 128k paired audio and text captions from FSD50k [16], ClothoV2 [17], AudioCaps [18], and MACS [19]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "FSD50K: an open dataset of human-labeled sound events",
          "justification": "The paper references the original source of the FSD50k dataset to provide context and background.",
          "quote": "FSD50k [16]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper implies that the models were likely implemented using PyTorch, given its common usage and the context of the implementations presented.",
          "quote": "Implementation available at https://github.com/zhepeiw/clap_curation"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "The paper does not reference a specific source for PyTorch, but its use is implied.",
          "quote": "Implementation available at https://github.com/zhepeiw/clap_curation"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3339,
    "prompt_tokens": 20078,
    "total_tokens": 23417
  }
}