{
  "paper": "d06c9f09cf33b97e2df59421b28aab58.txt",
  "words": 8723,
  "extractions": {
    "title": {
      "value": "Image Retrieval from Contextual Descriptions",
      "justification": "The title is taken directly from the header of the paper.",
      "quote": "Image Retrieval from Contextual Descriptions"
    },
    "description": "The paper proposes a new multimodal challenge called IMAGE CODE, where models must retrieve the correct image from a set based on a contextual description. The challenge aims to evaluate the current vision-and-language models' ability to incorporate context, including visual and temporal cues. The paper investigates the performance of several state-of-the-art models like ViLBERT and CLIP on this task. It highlights the models' significant performance gap compared to human accuracy and discusses the implications for multimodal systems. The paper introduces IMAGE CODE dataset, which is sourced from static pictures and video frames, featuring contrasting image sets and complex descriptions to emphasize contextual understanding.",
    "type": {
      "value": "empirical",
      "justification": "The paper focuses on proposing a challenge and performing experiments to benchmark models like CLIP and ViLBERT, which falls under empirical research.",
      "quote": "We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT and bi-encoders such as CLIP, on IMAGE CODE."
    },
    "primary_research_field": {
      "name": {
        "value": "Vision-and-Language",
        "justification": "The paper deals with a vision-and-language task where models must retrieve images based on textual descriptions.",
        "quote": "We benchmark a series of vision-and-language models that achieve state-of-the-art performance on other multimodal tasks."
      },
      "aliases": [
        "Multimodal Systems"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Retrieval",
          "justification": "The paper specifically addresses the task of image retrieval based on contextual descriptions.",
          "quote": "Image Retrieval from Contextual Descriptions (IMAGE CODE)"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Pragmatics in Language Understanding",
          "justification": "The challenge involves pragmatics, where linguistic hints must be resolved in the context of multiple images.",
          "quote": "To measure to what extent current vision-and-language models master this ability, we propose a new multimodal challenge"
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "ViLBERT",
          "justification": "ViLBERT is mentioned as one of the state-of-the-art models benchmarked on the IMAGE CODE dataset.",
          "quote": "We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "ViLBERT was used as a benchmark model, not introduced in this paper.",
          "quote": "We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT."
        },
        "is_executed": {
          "value": true,
          "justification": "ViLBERT was executed and benchmarked on the IMAGE CODE dataset.",
          "quote": "We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT."
        },
        "is_compared": {
          "value": true,
          "justification": "ViLBERT’s performance on the dataset was compared to other models like CLIP.",
          "quote": "We benchmark several state-of-the-art models, including both cross-encoders such as ViLBERT."
        },
        "referenced_paper_title": {
          "value": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks",
          "justification": "The referenced paper title is provided in the references.",
          "quote": "Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee (2019). ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks."
        }
      },
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is another model that was tested extensively in the challenge.",
          "quote": "We benchmark a series of vision-and-language models that achieve state-of-the-art performance on other multimodal tasks, specifically CLIP as a bi-encoder."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "CLIP is used as a benchmark model, not introduced in this paper.",
          "quote": "We benchmark a series of vision-and-language models, specifically CLIP as a bi-encoder."
        },
        "is_executed": {
          "value": true,
          "justification": "CLIP was evaluated on the IMAGE CODE dataset as part of the empirical study.",
          "quote": "We benchmark a series of vision-and-language models, specifically CLIP as a bi-encoder."
        },
        "is_compared": {
          "value": true,
          "justification": "CLIP’s performance was compared against other models like ViLBERT in the experiments.",
          "quote": "We benchmark a series of vision-and-language models, specifically CLIP as a bi-encoder."
        },
        "referenced_paper_title": {
          "value": "Learning Transferable Visual Models from Natural Language Supervision",
          "justification": "The referenced paper title for CLIP is provided in the references section.",
          "quote": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh (2021). Learning Transferable Visual Models from Natural Language Supervision."
        }
      },
      {
        "name": {
          "value": "UNITER",
          "justification": "UNITER is mentioned as one of the models evaluated in the study.",
          "quote": "We benchmark a series of vision-and-language models, specifically UNITER as a cross-encoder variant."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The model is not a contribution of this paper; it is used for benchmarking.",
          "quote": "We benchmark a series of vision-and-language models, specifically UNITER as a cross-encoder variant."
        },
        "is_executed": {
          "value": true,
          "justification": "UNITER was executed as part of the paper's experiments on IMAGE CODE dataset.",
          "quote": "We benchmark a series of vision-and-language models, specifically UNITER as a cross-encoder variant."
        },
        "is_compared": {
          "value": true,
          "justification": "UNITER's performance is compared to other models in the paper.",
          "quote": "We benchmark a series of vision-and-language models, specifically UNITER as a cross-encoder variant."
        },
        "referenced_paper_title": {
          "value": "UNIVERSAL Image-TExt Representation Learning",
          "justification": "The paper references the original paper on UNITER for context.",
          "quote": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy (2020). UNIVERSAL Image-TExt Representation Learning."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "IMAGE CODE",
          "justification": "The paper introduces the IMAGE CODE dataset specifically for the proposed challenge.",
          "quote": "A new multimodal challenge, Image Retrieval from Contextual Descriptions (IMAGE CODE)."
        },
        "aliases": [
          "Image Retrieval from Contextual Descriptions"
        ],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "IMAGE CODE is introduced in this paper, so there is no prior paper to reference.",
          "quote": "A new multimodal challenge, Image Retrieval from Contextual Descriptions (IMAGE CODE)."
        }
      },
      {
        "name": {
          "value": "Open Images",
          "justification": "Open Images is used as a source for static pictures included in the IMAGE CODE dataset.",
          "quote": "Static Pictures. We obtain image sets from one of the largest repositories of static pictures, the Open Images Dataset V6."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale",
          "justification": "The referenced paper title is provided in the references.",
          "quote": "Alina Kuznetsova, Hassan Rom, Neil Alldrin (2020). The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale."
        }
      },
      {
        "name": {
          "value": "Video-Storytelling",
          "justification": "Video-Storytelling is one of the sources for video frames used in the IMAGE CODE dataset.",
          "quote": "As sources for our video frames, we use i) Video-Storytelling."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Video storytelling: Textual summaries for events",
          "justification": "The reference paper for Video-Storytelling is mentioned in the references.",
          "quote": "Junnan Li, Yongkang Wong, Qi Zhao (2019). Video storytelling: Textual summaries for events."
        }
      },
      {
        "name": {
          "value": "MSR-VTT",
          "justification": "MSR-VTT is mentioned as another source for video frames used in the dataset.",
          "quote": "As sources for our video frames, we use ii) general-domain MSR-VTT."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "MSR-VTT: A large video description dataset for bridging video and language",
          "justification": "The reference paper for MSR-VTT is listed in the references.",
          "quote": "Jun Xu, Tao Mei, Ting Yao, Yong Rui (2016). MSR-VTT: A large video description dataset for bridging video and language."
        }
      },
      {
        "name": {
          "value": "YouCook",
          "justification": "YouCook is named as a dataset providing video frames for the challenge's dataset.",
          "quote": "As sources for our video frames, we use iii) YouCook."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching",
          "justification": "The reference for the YouCook dataset is provided in the references section.",
          "quote": "Pradipto Das, Chenliang Xu, Richard F Doell, Jason J Corso (2013). A thousand frames in just a few words: Lingual description of videos through latent topics and sparse object stitching."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1939,
    "prompt_tokens": 15473,
    "total_tokens": 17412,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}