{
  "paper": "af0209d3dce13923d725d18ba72264a4.txt",
  "words": 5795,
  "extractions": {
    "title": {
      "value": "The Power of Prompt Tuning for Low-Resource Semantic Parsing",
      "justification": "The title is explicitly mentioned at the top of the provided paper.",
      "quote": "The Power of Prompt Tuning for Low-Resource Semantic Parsing"
    },
    "description": "This paper investigates the effectiveness of prompt tuning in adapting pre-trained language models for the task of low-resource semantic parsing. It evaluates the performance of prompt tuned T5 models against fine-tuned models and strong baselines like GPT-3 and BART on datasets like Overnight and TOPv2, especially in low-resource settings.",
    "type": {
      "value": "empirical",
      "justification": "The paper evaluates prompt tuning on real datasets, conducts ablation studies, and compares performance with baseline models, indicating empirical research.",
      "quote": "In this paper, we investigate prompt tuning for semantic parsing... On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on adapting language models for semantic parsing, which is a sub-field of Natural Language Processing.",
        "quote": "Prompt tuning has recently emerged as an effective method for adapting pre-trained language models to a number of language understanding and generation tasks."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Semantic Parsing",
          "justification": "The specific task investigated in the paper is semantic parsing, mapping natural language to formal meaning representations.",
          "quote": "In this paper, we investigate prompt tuning for semantic parsingâ€”the task of mapping natural language utterances onto formal meaning representations."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "T5-large",
          "justification": "T5-large is listed as one of the models evaluated in the paper.",
          "quote": "T5-large"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "T5-large is used in the paper, but it is not presented as a new contribution.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves experimentation with the T5-large, indicating it was executed.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "is_compared": {
          "value": true,
          "justification": "T5-large is numerically compared to other models like GPT-3 and BART in the experiments.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines."
        },
        "referenced_paper_title": {
          "value": "Exploring the limits of transfer learning with a unified text-to-text transformer",
          "justification": "The referenced paper for T5 models is the one by Raffel et al., which is about the T5 model.",
          "quote": "Lester et al. (2021) show that prompt tuning becomes competitive with fine-tuning for the largest pre-trained T5 models (Raffel et al., 2020)."
        }
      },
      {
        "name": {
          "value": "T5-xl",
          "justification": "T5-xl is specifically mentioned as being evaluated and significantly outperforming fine-tuned counterparts.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "T5-xl is used as an existing model for experimentation rather than as a new contribution.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "is_executed": {
          "value": true,
          "justification": "The performance of T5-xl is measured in experiments, so it was executed.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared to other models in a numerical context.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines."
        },
        "referenced_paper_title": {
          "value": "Exploring the limits of transfer learning with a unified text-to-text transformer",
          "justification": "The paper by Raffel et al. about T5 is referenced for models used in this research.",
          "quote": "Lester et al. (2021) show that prompt tuning becomes competitive with fine-tuning for the largest pre-trained T5 models (Raffel et al., 2020)."
        }
      },
      {
        "name": {
          "value": "T5-base",
          "justification": "T5-base is one of the models used in the paper's experiments.",
          "quote": "T5-base"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "T5-base is an existing model applied in the study, not a new contribution.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "is_executed": {
          "value": true,
          "justification": "As T5-base is involved in the experiments, it was executed to obtain results.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "is_compared": {
          "value": true,
          "justification": "T5-base is part of the comparative analysis against other models in context.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines."
        },
        "referenced_paper_title": {
          "value": "Exploring the limits of transfer learning with a unified text-to-text transformer",
          "justification": "The referenced work by Raffel et al. is about T5 models, which includes the T5-base.",
          "quote": "Lester et al. (2021) show that prompt tuning becomes competitive with fine-tuning for the largest pre-trained T5 models (Raffel et al., 2020)."
        }
      },
      {
        "name": {
          "value": "T5-small",
          "justification": "The paper contains mention and discussion of T5-small within experiments.",
          "quote": "T5-small"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "T5-small is used from existing models for tests, not a new model contribution.",
          "quote": "With growing model size, prompt tuned T5 models are increasingly capable of outputting diverse target representations (see Figure 1)."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper conducts experiments with T5-small, indicating it was run.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "is_compared": {
          "value": true,
          "justification": "T5-small's performance is compared with other T5 variants and baselines.",
          "quote": "On Overnight, we find that the disparity between canonical and meaning representations shrinks from 17% to 4% for T5-small and T5-xl, respectively."
        },
        "referenced_paper_title": {
          "value": "Exploring the limits of transfer learning with a unified text-to-text transformer",
          "justification": "The T5 model paper by Raffel et al. is the referenced work for these models.",
          "quote": "Lester et al. (2021) show that prompt tuning becomes competitive with fine-tuning for the largest pre-trained T5 models (Raffel et al., 2020)."
        }
      },
      {
        "name": {
          "value": "BART",
          "justification": "BART is explicitly mentioned as a baseline model in the experiments.",
          "quote": "...as well as strong GPT-3 and BART baselines."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BART is used as a baseline model, not a new contribution.",
          "quote": "as well as strong GPT-3 and BART baselines."
        },
        "is_executed": {
          "value": true,
          "justification": "BART was used as a baseline and thus executed in experiments.",
          "quote": "...as well as strong GPT-3 and BART baselines."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares BART's performance to other models like T5 and GPT-3.",
          "quote": "...as well as strong GPT-3 and BART baselines."
        },
        "referenced_paper_title": {
          "value": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "justification": "This paper is the key reference for BART, as indicated in the context of using BART in research.",
          "quote": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension."
        }
      },
      {
        "name": {
          "value": "GPT-3",
          "justification": "GPT-3 is listed among the strong baselines against which T5 models were evaluated.",
          "quote": "...as well as strong GPT-3 and BART baselines."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "The paper uses GPT-3 as a baseline model, not as a new research contribution.",
          "quote": "as well as strong GPT-3 and BART baselines."
        },
        "is_executed": {
          "value": true,
          "justification": "GPT-3 was involved in experiments as a baseline model.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines."
        },
        "is_compared": {
          "value": true,
          "justification": "GPT-3's performance was compared to other models like T5-xl and BART.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart, as well as strong GPT-3 and BART baselines."
        },
        "referenced_paper_title": {
          "value": "Language Models are Few-Shot Learners",
          "justification": "This is the primary paper where GPT-3 was introduced and studied.",
          "quote": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Overnight",
          "justification": "The Overnight dataset is central to the experiments conducted in the paper, mentioned explicitly by name.",
          "quote": "Our main finding is that prompt tuned T5 models become better at generating meaning representations with increased model size. On Overnight, we see the absolute difference between canonical and meaning representations shrink..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Building a Semantic Parser Overnight",
          "justification": "The paper relevant to the Overnight dataset is referenced in relation to this dataset.",
          "quote": "Yushi Wang, Jonathan Berant, and Percy Liang. 2015. Building a Semantic Parser Overnight."
        }
      },
      {
        "name": {
          "value": "TOPv2",
          "justification": "The TOPv2 dataset is explicitly used for evaluation in the research work.",
          "quote": "On the low-resource splits of Overnight and TOPv2, we find that a prompt tuned T5-xl significantly outperforms its fine-tuned counterpart."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing",
          "justification": "The reference title corresponds to the use of the TOPv2 dataset, as indicated in the paper.",
          "quote": "Xilun Chen, Asish Ghoshal, Yashar Mehdad, Luke Zettlemoyer, and Sonal Gupta. 2020. Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly mentioned in the context of conducting experiments in the paper.",
          "quote": "All experiments were run with PyTorch (v. 1.8.1)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An imperative style, high-performance deep learning library",
          "justification": "This is the main reference paper for the PyTorch library, as mentioned in the context.",
          "quote": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An imperative style, high-performance deep learning library."
        }
      },
      {
        "name": {
          "value": "Hugging Face Transformers",
          "justification": "The Hugging Face Transformers library is used in the experiments conducted in the paper.",
          "quote": "and the Huggingface Transformers (v. 4.8.2) library"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-art natural language processing",
          "justification": "This reference corresponds to the Transformers library from Hugging Face, used in the paper's experiments.",
          "quote": "Wolf et al., 2020"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 3034,
    "prompt_tokens": 12965,
    "total_tokens": 15999,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}