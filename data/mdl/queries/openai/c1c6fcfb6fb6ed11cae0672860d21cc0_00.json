{
  "paper": "c1c6fcfb6fb6ed11cae0672860d21cc0.txt",
  "words": 12574,
  "extractions": {
    "title": {
      "value": "Simplifying Constraint Inference with Inverse Reinforcement Learning",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Simplifying Constraint Inference with Inverse Reinforcement Learning"
    },
    "description": "The paper introduces a simplified approach to constraint inference in inverse reinforcement learning (IRL) for learning safety constraints from expert data. It argues that by reducing the complexity of the optimization process traditionally used in inverse constrained reinforcement learning (ICRL), the proposed method performs as well or better than prior approaches on various benchmarks. This simplified method facilitates implementation and provides new opportunities for extensions like offline constraint inference.",
    "type": {
      "value": "theoretical",
      "justification": "The paper primarily focuses on theoretical advancements in constraint inference within reinforcement learning, discussing equivalence between methodologies and providing mathematical proofs.",
      "quote": "This result is significant because it allows us to simplify the training dynamics and complexity of constraint inference methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper is centered around the challenges and methodologies associated with constraint learning in reinforcement learning.",
        "quote": "Learning safe policies has presented a longstanding challenge for the reinforcement learning (RL) community."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Inverse Reinforcement Learning",
          "justification": "The core of the paper revolves around applying inverse reinforcement learning techniques to infer constraints.",
          "quote": "However, prior work in this area has relied on complex tri-level optimizations in order to infer safe behavior (constraints)."
        },
        "aliases": [
          "IRL"
        ]
      },
      {
        "name": {
          "value": "Constraint Inference",
          "justification": "The paper explicitly discusses constraint inference as a field where the simplification is applied to improve outcomes.",
          "quote": "In this work, we present a simplified version of constraint inference that performs as well or better than prior work."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Inverse Constrained Reinforcement Learning",
          "justification": "The paper compares traditional inverse constrained reinforcement learning methods with the newly proposed simplified approach.",
          "quote": "This paper explores inverse constrained reinforcement learning methodologies."
        },
        "aliases": [
          "ICRL"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Simplified inverse constrained reinforcement learning",
          "justification": "The paper proposes a simplified model of the existing inverse constrained reinforcement learning methods.",
          "quote": "In this work, we present a simplified version of constraint inference that performs as well or better than prior work."
        },
        "aliases": [
          "ICRL"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The proposed simplified method is a novel contribution presented by the authors.",
          "quote": "we present a simplified version of constraint inference that performs as well or better than prior work."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper provides empirical validation of the proposed simplified method across various benchmarks.",
          "quote": "Next, we experimentally validate this claim."
        },
        "is_compared": {
          "value": true,
          "justification": "The simplified version is compared against prior ICRL methods such as MECL.",
          "quote": "First, we compare our IRL implementation to the baseline methods."
        },
        "referenced_paper_title": {
          "value": "Inverse constrained reinforcement learning",
          "justification": "The paper discusses various previous works in ICRL as the base for its comparisons.",
          "quote": "inverse constrained RL (ICRL) [Malik et al., 2021, Liu et al., 2023a, Kim et al., 2023]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "MuJoCo Environments",
          "justification": "The paper specifies the use of MuJoCo environments as part of its benchmark testing for constraint inference.",
          "quote": "The environments include five MuJoCo environments, Ant, Half Cheetah, Walker 2D, Swimmer and Inverted Pendulum, modified to include constraints."
        },
        "aliases": [
          "MuJoCo"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Benchmarking constraint inference in inverse reinforcement learning",
          "justification": "The paper references Liu et al. (2023a) which provides the benchmark environments used in the experiments.",
          "quote": "introduced by Liu et al. [2023a] since these were specially designed to test the performance of constraint inference tasks."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SAC (Soft Actor-Critic)",
          "justification": "SAC is mentioned as the forward RL algorithm used in the experiments.",
          "quote": "We use SAC for policy optimization."
        },
        "aliases": [
          "SAC"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Spinning Up in Deep Reinforcement Learning",
          "justification": "The paper credits SAC to Tianshou as part of the setup for implementing the experiments.",
          "quote": "As mentioned, we use SAC for policy optimization."
        }
      },
      {
        "name": {
          "value": "FSRL",
          "justification": "The FSRL library is used for implementing SAC-Lagrangian algorithms.",
          "quote": "Tianshou [Weng et al., 2022] and FSRL [Liu et al., 2023b] implementations of SAC and SAC-Lagrangian, respectively."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Datasets and benchmarks for offline safe reinforcement learning",
          "justification": "This paper is referenced for the implementation details of FSRL.",
          "quote": "FSRL version 0.1.0"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1059,
    "prompt_tokens": 21126,
    "total_tokens": 22185,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}