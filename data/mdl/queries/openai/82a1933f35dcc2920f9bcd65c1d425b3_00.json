{
  "paper": "82a1933f35dcc2920f9bcd65c1d425b3.txt",
  "words": 6224,
  "extractions": {
    "title": {
      "value": "Learning Representations for New Sound Classes With Continual Self-Supervised Learning",
      "justification": "The title accurately reflects the focus of the paper, which is on developing continual self-supervised learning methods for new sound classes.",
      "quote": "Learning Representations for New Sound Classes With Continual Self-Supervised Learning"
    },
    "description": "The paper introduces a framework for sound recognition systems that continually incorporate new sound classes without relying on labeled data. It proposes the use of continual self-supervised learning (CSSL) methods for representation learning in this context. The main idea is to train an encoder using unlabeled data, which can then be fine-tuned for specific tasks. The study compares CSSL with continual supervised representation learning (CSUP) and evaluates their performance in handling new sound classes and overcoming catastrophic forgetting.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments comparing different continual learning methods, indicating its empirical nature.",
      "quote": "We show that this approach obtains similar performance compared to several distillation-based continual learning methods when employed on self-supervised representation learning methods."
    },
    "primary_research_field": {
      "name": {
        "value": "Sound Recognition",
        "justification": "The paper focuses on sound recognition systems and addresses challenges in learning new sound classes continually.",
        "quote": "In this paper, we work on a sound recognition system that continually incorporates new sound classes."
      },
      "aliases": [
        "Sound Classification",
        "Audio Classification"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Continual Learning",
          "justification": "The paper investigates methods for handling new sound classes continuously, aligning with the field of continual learning.",
          "quote": "Continual/lifelong learning has been a field of rising interest to address the aforementioned concerns."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper adopts self-supervised learning within the continual learning framework, focusing on learning without labeled data.",
          "quote": "We propose using similarity-based self-supervised algorithms for representation learning in this framework."
        },
        "aliases": [
          "SSL"
        ]
      },
      {
        "name": {
          "value": "Representation Learning",
          "justification": "The paper emphasizes representation learning as a core component of their proposed framework for handling new sound classes.",
          "quote": "Representation learning decouples the learning into two stages: a) Learning of an encoder with a representation-specific objective."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "SimCLR",
          "justification": "The paper uses SimCLR as one of the self-supervised methods evaluated in the continual representation learning framework.",
          "quote": "We consider SimCLR as candidates for the continual representation learning framework."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "SimCLR is used as a reference self-supervised method and is not presented as a new contribution of this paper.",
          "quote": "We consider SimCLR as candidates for the continual representation learning framework."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes experiments using SimCLR within the framework discussed.",
          "quote": "Table II (No distillation) summarizes the average accuracy and forgetting for the in-domain protocols."
        },
        "is_compared": {
          "value": true,
          "justification": "SimCLR is compared with other methods like MoCo and Barlow Twins in the study.",
          "quote": "We empirically observe that even if we do not employ an explicit mechanism to combat forgetting, employing similarity-based self-supervised learning within CRL yields better performance than continual supervised representation learning, and comparable performance to distillation-based continual learning methods."
        },
        "referenced_paper_title": {
          "value": "A simple framework for contrastive learning of visual representations",
          "justification": "This is the foundational paper for SimCLR, which is a key method discussed.",
          "quote": "We consider SimCLR [28], [41] as candidates for the continual representation learning framework."
        }
      },
      {
        "name": {
          "value": "MoCo",
          "justification": "The paper uses MoCo as another self-supervised method in evaluating continual learning strategies.",
          "quote": "We consider MoCo as candidates for the continual representation learning framework."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "MoCo is already an existing self-supervised learning algorithm being evaluated.",
          "quote": "We consider MoCo as candidates for the continual representation learning framework."
        },
        "is_executed": {
          "value": true,
          "justification": "MoCo is a part of the experiments conducted to assess the continual learning framework.",
          "quote": "Table II (No distillation) summarizes the average accuracy and forgetting for the in-domain protocols."
        },
        "is_compared": {
          "value": true,
          "justification": "MoCo is compared against SimCLR and Barlow Twins in the experiments.",
          "quote": "Table II (No distillation) summarizes the average accuracy and forgetting for the in-domain protocols."
        },
        "referenced_paper_title": {
          "value": "Momentum contrast for unsupervised visual representation learning",
          "justification": "This is the main paper introducing MoCo, which the current paper uses as a method.",
          "quote": "We consider MoCo [27] as candidates for the continual representation learning framework."
        }
      },
      {
        "name": {
          "value": "Barlow Twins",
          "justification": "The paper evaluates Barlow Twins as one of the self-supervised learning methods within their framework.",
          "quote": "We consider Barlow Twins as candidates for the continual representation learning framework."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Barlow Twins is an existing method used for comparison, not a contribution of this paper.",
          "quote": "We consider Barlow Twins as candidates for the continual representation learning framework."
        },
        "is_executed": {
          "value": true,
          "justification": "Experiments involving Barlow Twins are performed to test its effectiveness in their framework.",
          "quote": "Table II (No distillation) summarizes the average accuracy and forgetting for the in-domain protocols."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares Barlow Twins with SimCLR and MoCo while evaluating learning methods.",
          "quote": "Table II (No distillation) summarizes the average accuracy and forgetting for the in-domain protocols."
        },
        "referenced_paper_title": {
          "value": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction",
          "justification": "Barlow Twins is discussed as part of the analysis of self-supervised methods.",
          "quote": "We consider Barlow Twins [42] as candidates for the continual representation learning framework."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "UrbanSound8K",
          "justification": "UrbanSound8K is used as a dataset for training and evaluation in the paper.",
          "quote": "We use UrbanSound8K to continually train the encoder."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "A dataset and taxonomy for urban sound research",
          "justification": "This reference discusses the UrbanSound8K dataset used in the research.",
          "quote": "UrbanSound8K [44]"
        }
      },
      {
        "name": {
          "value": "TAU Urban Acoustic Scenes 2019",
          "justification": "This dataset is used for experiment evaluations, as stated in the methodology.",
          "quote": "The TAU Urban Acoustic Scenes 2019 dataset is used for the DCASE 2019 Task-1(A) challenge for acoustic scene classification."
        },
        "aliases": [
          "DCASE TAU19"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "A multi-device dataset for urban acoustic scene classification",
          "justification": "The paper references the dataset's original publication for more context.",
          "quote": "TAU Urban Acoustic Scenes 2019 [45]"
        }
      },
      {
        "name": {
          "value": "VGGSound",
          "justification": "VGGSound is mentioned as a dataset for evaluating and learning representations within the framework.",
          "quote": "VGGSound is used to learn representations for OOD experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "VGGSound: A Large-Scale Audio-Visual Dataset",
          "justification": "The description of VGGSound closely aligns with its referenced publication.",
          "quote": "VGGSound [46]"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "SpeechBrain",
          "justification": "The paper mentions using the SpeechBrain toolkit for implementation purposes.",
          "quote": "Additional details can be found in our SpeechBrain implementation."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "SpeechBrain: A General-Purpose Speech Toolkit",
          "justification": "SpeechBrain is a toolkit utilized by the authors for the experimental setup.",
          "quote": "Additional details can be found in our SpeechBrain implementation."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1670,
    "prompt_tokens": 11475,
    "total_tokens": 13145,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}