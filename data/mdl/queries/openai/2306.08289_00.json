{
  "paper": "2306.08289.txt",
  "words": 13027,
  "extractions": {
    "title": {
      "value": "A2CiD2: Accelerating Asynchronous Communication in Decentralized Deep Learning",
      "justification": "The title appears at the top of the paper, clearly indicating the name of the research work.",
      "quote": "A2CiD2: Accelerating Asynchronous Communication in Decentralized Deep Learning"
    },
    "description": "This paper introduces A2CiD2, an optimization algorithm designed to accelerate asynchronous communication in decentralized deep learning. The method allows each worker to process mini-batches without stopping and run peer-to-peer averaging routines in parallel, which significantly reduces idle time and communication costs. The algorithm demonstrates improvements in communication efficiency and is validated both theoretically and empirically.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents theoretical analysis as well as empirical validation through experiments on CIFAR-10 and ImageNet datasets, indicating that it is primarily an empirical study.",
      "quote": "Our theoretical analysis proves accelerated rates compared to previous asynchronous decentralized baselines and we empirically show that using our A2CiD2 momentum significantly decrease communication costs in poorly connected networks."
    },
    "primary_research_field": {
      "name": {
        "value": "Distributed Deep Learning",
        "justification": "The primary focus of the paper is on distributed training methods for deep learning models, aiming to improve communication efficiency in decentralized settings.",
        "quote": "Distributed training of Deep Learning models has been critical to many recent successes in the field."
      },
      "aliases": [
        "Distributed DL",
        "Decentralized Deep Learning"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper introduces an optimization algorithm (A2CiD2) designed to improve the efficiency of decentralized training methods.",
          "quote": "we introduce a principled asynchronous, randomized, gossip-based optimization algorithm which works thanks to a continuous local momentum named A2CiD2."
        },
        "aliases": [
          "Optimization Algorithms",
          "Decentralized Optimization"
        ]
      },
      {
        "name": {
          "value": "Algorithm",
          "justification": "A significant portion of the paper is dedicated to the theoretical and practical implementation of the A2CiD2 algorithm.",
          "quote": "We demonstrate that our method effectively minimizes the gap between centralized settings in environments hosting up to 64 asynchronous GPUs."
        },
        "aliases": [
          "Algorithms"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "ResNet18",
          "justification": "ResNet18 is mentioned as one of the models used in the experiments for decentralized training behavior on CIFAR-10.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet18 is a pre-existing model commonly used for image recognition tasks, not a contribution of this paper.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is used in the experiments, as indicated by the description of empirical results.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not focus on comparing ResNet18 with other models but rather on the communication efficiency of the A2CiD2 algorithm.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "This is the original paper introducing ResNet18.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        }
      },
      {
        "name": {
          "value": "ResNet50",
          "justification": "ResNet50 is mentioned as one of the models used in the experiments for decentralized training behavior on ImageNet.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "ResNet50 is a pre-existing model commonly used for image recognition tasks, not a contribution of this paper.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        },
        "is_executed": {
          "value": 1,
          "justification": "The model is used in the experiments, as indicated by the description of empirical results.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        },
        "is_compared": {
          "value": 0,
          "justification": "The paper does not focus on comparing ResNet50 with other models but rather on the communication efficiency of the A2CiD2 algorithm.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "This is the original paper introducing ResNet50.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CIFAR-10",
          "justification": "CIFAR-10 is one of the datasets used for the experiments in the paper.",
          "quote": "To investigate how our method scales with the number of workers, we run multiple experiments using up to 64 NVIDIA A100 GPUs... Following [2], we pick a ResNet18 for CIFAR-10 [24]."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Learning Multiple Layers of Features from Tiny Images",
          "justification": "This is the original paper introducing the CIFAR-10 dataset.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24]."
        }
      },
      {
        "name": {
          "value": "ImageNet",
          "justification": "ImageNet is one of the datasets used for the experiments in the paper.",
          "quote": "We experimentally compare A2CiD2 to a synchronous baseline All-Reduce SGD (AR-SGD), and an asynchronous baseline using randomized pairwise communications... the large-scale ImageNet dataset."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet: A Large-Scale Hierarchical Image Database",
          "justification": "This is the original paper introducing the ImageNet dataset.",
          "quote": "Following [2], we pick a ResNet18 for CIFAR-10 [24] and ResNet50 for ImageNet [11]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "The paper mentions that PyTorch was used to implement the code for the experiments.",
          "quote": "Our code is implemented in Pytorch [35], remove locks put on previous asynchronous implementations by circumventing their deadlocks, and can be found in an open-source repository."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This is the original paper introducing the PyTorch library.",
          "quote": "Our code is implemented in Pytorch [35], remove locks put on previous asynchronous implementations by circumventing their deadlocks, and can be found in an open-source repository."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1886,
    "prompt_tokens": 25266,
    "total_tokens": 27152
  }
}