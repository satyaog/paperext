{
  "paper": "88ed7ea13db39a72b52b50646e8d908c.txt",
  "words": 42681,
  "extractions": {
    "title": {
      "value": "4+3 Phases of Compute-Optimal Neural Scaling Laws",
      "justification": "The paper's title is clearly stated multiple times, including in the header and footer of each page and consistently throughout the paper.",
      "quote": "4+3 Phases of Compute-Optimal Neural Scaling Laws"
    },
    "description": "This paper analyzes a neural scaling model to derive compute-optimal scaling laws. It identifies four main phases and three sub-phases in the data-complexity and target-complexity plane. These are used to compute optimal model-parameter counts relative to compute budgets with a thorough mathematical framework, including a proposed model called power-law random features (PLRF).",
    "type": {
      "value": "theoretical",
      "justification": "The paper is primarily focused on deriving theoretical results regarding neural scaling laws and provides mathematical proofs for its claims, with no mention of empirical experiments.",
      "quote": "We derive, with mathematical proof and extensive numerical evidence, the scaling-law exponents in all of these phases, in particular computing the optimal model-parameter-count as a function of floating point operation budget."
    },
    "primary_research_field": {
      "name": {
        "value": "Deep Learning",
        "justification": "The paper focuses on neural scaling laws within the context of optimization of deep learning models, a subfield of deep learning.",
        "quote": "The advent of large language models (LLMs) has changed our perceptions of the landscape of optimization and is resulting in the emergence of new interesting questions related to scaling."
      },
      "aliases": [
        "DL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Optimization",
          "justification": "The paper deals with optimizing neural scaling laws and model-parameter-counts specifically in the context of deep learning, which falls under the umbrella of optimization.",
          "quote": "This leads to the following natural question: given an architecture, given a fixed compute budget, and having unlimited data, how should one select the model size to minimize loss?"
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Large Language Models",
          "justification": "The paper discusses scaling laws in the context of large language models, which are a specific application within the broader deep learning field.",
          "quote": "The advent of large language models (LLMs) has changed our perceptions of the landscape of optimization."
        },
        "aliases": [
          "LLMs"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "power-law random features model",
          "justification": "The paper introduces the power-law random features model as the main model for studying scaling laws.",
          "quote": "Main contributions. In this work, we analyze a three parameter simple model, which we call power-law random features (PLRF)."
        },
        "aliases": [
          "PLRF"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The power-law random features model is presented as a new theoretical framework developed by the authors for analyzing scaling laws.",
          "quote": "In this work, we analyze a three parameter simple model, which we call power-law random features (PLRF)."
        },
        "is_executed": {
          "value": false,
          "justification": "While the paper theoretically analyzes the PLRF model, there is no indication that the model is executed in the form of experiments or simulations.",
          "quote": "We derive a deterministic equivalent for the expected loss, as a function of α, β, and d, that captures the training dynamics of one-pass SGD."
        },
        "is_compared": {
          "value": false,
          "justification": "There is no mention of the PLRF model being numerically compared to other models within the paper.",
          "quote": "We derive a deterministic equivalent for the expected loss, as a function of α, β, and d, that captures the training dynamics of one-pass SGD."
        },
        "referenced_paper_title": {
          "value": "A Solvable Model of Neural Scaling Laws",
          "justification": "The referenced paper title for the PLRF model is mentioned in the citations, supporting its theoretical basis.",
          "quote": "The problem setup was formulated by [30], where additionally data-limited scalings were considered, but compute optimality was not (nor indeed any algorithmic considerations); see also [8] where gradient flow is considered in the same setting."
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 816,
    "prompt_tokens": 78123,
    "total_tokens": 78939,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}