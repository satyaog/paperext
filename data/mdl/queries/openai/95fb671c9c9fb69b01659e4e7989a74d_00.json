{
  "paper": "95fb671c9c9fb69b01659e4e7989a74d.txt",
  "words": 8836,
  "extractions": {
    "title": {
      "value": "Long Range Graph Benchmark",
      "justification": "The title \"Long Range Graph Benchmark\" is clearly stated at the beginning of the paper, reflecting the focus on benchmarking graph neural networks and graph transformers for long-range interactions.",
      "quote": "Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct..."
    },
    "description": "This paper introduces the Long Range Graph Benchmark (LRGB), a collection of five graph learning datasets designed to facilitate the study of long-range interactions in graph neural networks and transformers. The benchmark is intended to address limitations in existing datasets, which often prioritize local interactions, and highlights the importance of long-range dependencies in graph-based tasks. The paper showcases several baseline experiments to demonstrate how different graph models perform on these datasets.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical results from experiments conducted with various graph models on the proposed benchmark datasets, focusing on their ability to model long-range interactions.",
      "quote": "We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Graph Neural Networks",
        "justification": "The paper is centered around Graph Neural Networks (GNNs) and their ability to handle long-range interactions on graph-based datasets.",
        "quote": "Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm generally exchange information between 1-hop neighbors to build node representations at each layer."
      },
      "aliases": [
        "GNNs"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Transformer-based Graph Models",
          "justification": "The paper discusses transformer-based methods for graphs, highlighting their ability to model full node connectivity and long-range interactions.",
          "quote": "Recently, there has been an increasing interest in development of Transformer-based methods for graphs..."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Benchmarking",
          "justification": "The paper introduces a new benchmark (LRGB) specifically designed to evaluate the ability of graph models to incorporate long-range interactions, aligning with benchmarking efforts in the field.",
          "quote": "we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets..."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GCN",
          "justification": "GCN is mentioned as one of the baseline models used in experiments on the LRGB datasets.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GCN is a well-established model referenced as a baseline, not a novel contribution of this paper.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes experimental results with GCN on the LRGB datasets, indicating its execution.",
          "quote": "We conduct baseline experiments on our proposed LRGB datasets by training and evaluating two GNN classes: (i) local MP-GNNs, and (ii) fully connected Graph Transformers."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares the performance of GCN against other models, such as Transformers, on the benchmarks.",
          "quote": "The baseline results for PascalVOC-SP and COCO-SP benchmarks are reported..."
        },
        "referenced_paper_title": {
          "value": "Semi-supervised classification with graph convolutional networks",
          "justification": "The provided reference corresponds to the original paper introducing the GCN model.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        }
      },
      {
        "name": {
          "value": "GCNII",
          "justification": "GCNII is included among the baseline models evaluated on the LRGB datasets.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GCNII is cited as a baseline model, indicating it's established in prior literature, not a new contribution.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        },
        "is_executed": {
          "value": true,
          "justification": "GCNII is part of the experimental setup, making it executed as per the paper details.",
          "quote": "We conduct baseline experiments on our proposed LRGB datasets by training and evaluating two GNN classes: (i) local MP-GNNs, and (ii) fully connected Graph Transformers."
        },
        "is_compared": {
          "value": true,
          "justification": "GCNII's performance is compared to other models in the experiments conducted.",
          "quote": "The baseline results for PascalVOC-SP and COCO-SP benchmarks are reported..."
        },
        "referenced_paper_title": {
          "value": "Simple and deep graph convolutional networks",
          "justification": "The reference to the original paper describing GCNII is required for accurate context.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        }
      },
      {
        "name": {
          "value": "GINE",
          "justification": "The GINE model is mentioned as a baseline in the experiments.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GINE is used as a known baseline, indicating it was not newly introduced in this work.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper describes executing GINE as part of its experimental analysis.",
          "quote": "We conduct baseline experiments on our proposed LRGB datasets by training and evaluating two GNN classes: (i) local MP-GNNs, and (ii) fully connected Graph Transformers."
        },
        "is_compared": {
          "value": true,
          "justification": "GINE's performance is analyzed alongside other models in the experimental results.",
          "quote": "The baseline results for PascalVOC-SP and COCO-SP benchmarks are reported..."
        },
        "referenced_paper_title": {
          "value": "Strategies for pre-training graph neural networks",
          "justification": "The referenced article is the relevant literature for the GINE model used in experiments.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        }
      },
      {
        "name": {
          "value": "GatedGCN",
          "justification": "The GatedGCN model is specifically listed as part of the baseline evaluations within the paper.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GatedGCN is not a new model; rather, it serves as an existing baseline in this research.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        },
        "is_executed": {
          "value": true,
          "justification": "The research document the empirical results obtained by executing the GatedGCN model.",
          "quote": "We conduct baseline experiments on our proposed LRGB datasets by training and evaluating two GNN classes: (i) local MP-GNNs, and (ii) fully connected Graph Transformers."
        },
        "is_compared": {
          "value": true,
          "justification": "GatedGCN's results are juxtaposed with those of other graph models in the paper's experiments.",
          "quote": "The baseline results for PascalVOC-SP and COCO-SP benchmarks are reported..."
        },
        "referenced_paper_title": {
          "value": "Residual gated graph convnets",
          "justification": "This is an appropriate reference to understand the origin and formulation of GatedGCN employed in the study.",
          "quote": "We select GCN [33], GCNII [11], GINE [62, 28] and GatedGCN [8] models from the local MP-GNN class."
        }
      },
      {
        "name": {
          "value": "Transformer+LapPE",
          "justification": "Transformer with Laplacian Positional Encoding (LapPE) is featured as a baseline transformer model used in the experiments.",
          "quote": "For the baselines, we select ... a fully connected Transformer [58] with Laplacian PE (LapPE) [14, 13] and SAN [34] models from the Transformer class."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "This configuration (Transformer with LapPE) is used as a baseline model, suggesting it is not a novel development in this paper.",
          "quote": "For the baselines, we select ... a fully connected Transformer [58] with Laplacian PE (LapPE) [14, 13] and SAN [34] models from the Transformer class."
        },
        "is_executed": {
          "value": true,
          "justification": "The conducted experiments mention the execution of this transformer model with LapPE on the datasets.",
          "quote": "We conduct baseline experiments on our proposed LRGB datasets by training and evaluating two GNN classes: (i) local MP-GNNs, and (ii) fully connected Graph Transformers."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper presents comparisons involving the Transformer+LapPE model against others in the benchmark.",
          "quote": "The baseline results for PascalVOC-SP and COCO-SP benchmarks are reported..."
        },
        "referenced_paper_title": {
          "value": "Attention is all you need",
          "justification": "This broadly corresponds to foundational Transformer model descriptions which apply to the Transformer variants used.",
          "quote": "For the baselines, we select ... a fully connected Transformer [58] with Laplacian PE (LapPE) [14, 13] and SAN [34] models from the Transformer class."
        }
      },
      {
        "name": {
          "value": "SAN",
          "justification": "Spectral Attention Network (SAN) is described as an advanced baseline transformer model applied in the experiments.",
          "quote": "For the baselines, we select ... a fully connected Transformer [58] with Laplacian PE (LapPE) [14, 13] and SAN [34] models from the Transformer class."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "SAN is utilized as a known model introduced prior to this paper's publication, hence it isn't a novel contribution.",
          "quote": "For the baselines, we select ... a fully connected Transformer [58] with Laplacian PE (LapPE) [14, 13] and SAN [34] models from the Transformer class."
        },
        "is_executed": {
          "value": true,
          "justification": "The experiments detailed in the paper include the execution of the SAN model along with other baselines.",
          "quote": "We conduct baseline experiments on our proposed LRGB datasets by training and evaluating two GNN classes: (i) local MP-GNNs, and (ii) fully connected Graph Transformers."
        },
        "is_compared": {
          "value": true,
          "justification": "The SAN model's performance is compared to other models' performance within the experimental results presented.",
          "quote": "The baseline results for PascalVOC-SP and COCO-SP benchmarks are reported..."
        },
        "referenced_paper_title": {
          "value": "Rethinking graph transformers with spectral attention",
          "justification": "The reference title corresponds to the paper likely describing the SAN model as applied in this work.",
          "quote": "For the baselines, we select ... a fully connected Transformer [58] with Laplacian PE (LapPE) [14, 13] and SAN [34] models from the Transformer class."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "PascalVOC-SP",
          "justification": "PascalVOC-SP is named as one of the benchmark datasets in the LRGB suite.",
          "quote": "Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct..."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "The PASCAL visual object classes (VOC) challenge",
          "justification": "The dataset is derived from the Pascal VOC dataset 2011, and is transformed into a graph format for this benchmark.",
          "quote": "PascalVOC-SP is a node classification dataset, based on the Pascal VOC 2011 image dataset [18], where each node corresponds to a region of the image belonging to a particular class."
        }
      },
      {
        "name": {
          "value": "COCO-SP",
          "justification": "COCO-SP is listed as part of the five datasets featured in the LRGB benchmark.",
          "quote": "Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct..."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "Microsoft COCO: Common objects in context",
          "justification": "COCO-SP dataset is based on the MS COCO dataset, adapted for node classification tasks in graph format.",
          "quote": "COCO-SP is a node classification dataset based on the MS COCO image dataset [38] where each superpixel node denotes an image region belonging to a particular class."
        }
      },
      {
        "name": {
          "value": "PCQM-Contact",
          "justification": "PCQM-Contact is specifically mentioned as one of the LRGB benchmark datasets focusing on long-range interactions.",
          "quote": "Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct..."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "OGB-LSC: A large-scale challenge for machine learning on graphs",
          "justification": "PCQM-Contact dataset is related to the Open Graph Benchmark and designed to highlight tasks requiring long-range graph interactions.",
          "quote": "In PCQM-Contact, we design a task that explicitly requires LRI since it needs to understand the interaction between distant atoms."
        }
      },
      {
        "name": {
          "value": "Peptides-func",
          "justification": "Peptides-func is part of the LRGB, explicitly crafted for understanding peptide-related functions via graph-based methods.",
          "quote": "Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct..."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "SATPdb: a database of structurally annotated therapeutic peptides",
          "justification": "Peptides-func dataset draws from SATPdb, using molecular graphs for functional prediction tasks.",
          "quote": "Peptides-func and Peptides-struct datasets, derived from 15,535 peptides retrieved from SATPdb [54]. Both datasets use the same set of graphs but differ in their prediction tasks."
        }
      },
      {
        "name": {
          "value": "Peptides-struct",
          "justification": "Peptides-struct is one of the datasets in LRGB, aimed at leveraging peptide structural data for graph predictions.",
          "quote": "Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: PascalVOC-SP, COCO-SP, PCQM-Contact, Peptides-func and Peptides-struct..."
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "SATPdb: a database of structurally annotated therapeutic peptides",
          "justification": "Peptides-struct, like Peptides-func, utilizes structural information from SATPdb to support predictions about peptide properties.",
          "quote": "Peptides-func and Peptides-struct datasets, derived from 15,535 peptides retrieved from SATPdb [54]. Both datasets use the same set of graphs but differ in their prediction tasks."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 3410,
    "prompt_tokens": 16373,
    "total_tokens": 19783,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}