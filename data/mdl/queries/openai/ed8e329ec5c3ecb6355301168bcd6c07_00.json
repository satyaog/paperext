{
  "paper": "ed8e329ec5c3ecb6355301168bcd6c07.txt",
  "words": 5934,
  "extractions": {
    "title": {
      "value": "Training Matters: Unlocking Potentials of Deeper Graph Convolutional Neural Networks",
      "justification": "The paper focuses on improving the training process of deeper Graph Convolutional Networks (GCNs) to enhance performance.",
      "quote": "Contrary to this belief, our paper demonstrates several ways to achieve such improvements. We begin by highlighting the training challenges of GCNs from the perspective of graph signal energy loss."
    },
    "description": "The paper discusses the performance limitations of deep Graph Convolutional Networks (GCNs), often attributed to the inherent limitations of GCN layers. It challenges this notion by suggesting that the training procedure itself may be the limiting factor. The authors propose strategies to improve training from an energy perspective and validate these methodologies, demonstrating that better training can significantly boost performance without changing model architecture.",
    "type": {
      "value": "empirical",
      "justification": "The paper provides empirical validation for its claims by conducting experiments to confirm that changes in the training process improve performance.",
      "quote": "After empirical validation, we confirm that these changes of operator lead to significant decrease in the training difficulties and notable performance boost, without changing the composition of parameters."
    },
    "primary_research_field": {
      "name": {
        "value": "Graph Neural Networks",
        "justification": "The paper focuses on Graph Convolutional Networks (GCNs), which are a type of Graph Neural Network, addressing training challenges and performance improvements.",
        "quote": "As a structure that is capable of modeling relational information, graph has inspired the emerge of Graph Neural Networks (GNNs)."
      },
      "aliases": [
        "GNNs"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Graph Signal Processing",
          "justification": "The paper discusses graph signal energy loss and other signal processing aspects in the context of improving GCN training.",
          "quote": "from the perspective of graph signal energy loss. More specifically, we find that the loss of energy in the backward pass during training hinders the learning of the layers closer to the input."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "GCN",
          "justification": "The paper centers its experiments and analyses around Graph Convolutional Networks (GCNs), aiming to improve their training process.",
          "quote": "GCN [16], being arguably the most popular method of all GNNs, is applied pervasively for being lightweight and having relatively capable performance."
        },
        "aliases": [
          "Graph Convolutional Networks"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper works on improving the existing GCN architecture rather than contributing a new model.",
          "quote": "However, the development of GCNs on more complicated tasks is hindered by the fact that their performance is still relatively limited and cannot be easily boosted."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper involves empirical validation and experiments, which implies execution of the GCN model to gather results.",
          "quote": "After empirical validation, we confirm that these changes of operator lead to significant decrease in the training difficulties and notable performance boost, without changing the composition of parameters."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper involves comparing the performance of GCNs before and after applying the proposed training modifications.",
          "quote": "For this purpose, we patch the most-popular baseline GCN with the ideas in previous section to form a set of detailed comparative tests."
        },
        "referenced_paper_title": {
          "value": "Semi-supervised classification with graph convolutional networks",
          "justification": "The referenced paper by Kipf and Welling is mentioned as the source of GCNs, the main model discussed in this paper.",
          "quote": "GCN [16], being arguably the most popular method of all GNNs, is applied pervasively for being lightweight and having relatively capable performance."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Cora",
          "justification": "Cora is used for the node classification tasks in the experiments conducted.",
          "quote": "We have selected the node classification tasks on Cora, CiteSeer and PubMed, the three most popular datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Revisiting semi-supervised learning with graph embeddings",
          "justification": "The paper uses Cora dataset as part of the experiment setup, likely referring to this paper which is commonly associated with Cora dataset usage.",
          "quote": "We use the most classic setting on training, which is identical to the one suggested in [35]."
        }
      },
      {
        "name": {
          "value": "CiteSeer",
          "justification": "CiteSeer is used for the node classification tasks in the experiments conducted.",
          "quote": "We have selected the node classification tasks on Cora, CiteSeer and PubMed, the three most popular datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Revisiting semi-supervised learning with graph embeddings",
          "justification": "The paper uses CiteSeer dataset as part of the experiment setup, likely referring to this paper which is commonly associated with CiteSeer dataset usage.",
          "quote": "We use the most classic setting on training, which is identical to the one suggested in [35]."
        }
      },
      {
        "name": {
          "value": "PubMed",
          "justification": "PubMed is used for the node classification tasks in the experiments conducted.",
          "quote": "We have selected the node classification tasks on Cora, CiteSeer and PubMed, the three most popular datasets."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Revisiting semi-supervised learning with graph embeddings",
          "justification": "The paper uses PubMed dataset as part of the experiment setup, likely referring to this paper which is commonly associated with PubMed dataset usage.",
          "quote": "We use the most classic setting on training, which is identical to the one suggested in [35]."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is explicitly mentioned as the library used for implementing the experiments.",
          "quote": "For all experiments, we used Adam optimizer and ReLU as the activation function (PyTorch implementation)."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
          "justification": "While not directly referenced alongside PyTorch, this paper on weight normalization might relate to optimization or implementation aspects within PyTorch.",
          "quote": "Normalization is a natural method to control the energy flow."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1274,
    "prompt_tokens": 11600,
    "total_tokens": 12874,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}