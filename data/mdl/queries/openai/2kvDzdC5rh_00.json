{
  "paper": "2kvDzdC5rh.txt",
  "words": 13267,
  "extractions": {
    "title": {
      "value": "INTENTGPT: FEW-SHOT INTENT DISCOVERY WITH LARGE LANGUAGE MODELS",
      "justification": "Extracted from the title of the paper.",
      "quote": "INTENTGPT: FEW-SHOT INTENT DISCOVERY WITH LARGE LANGUAGE MODELS"
    },
    "description": "The paper presents IntentGPT, a novel approach for few-shot intent discovery using pre-trained large language models such as GPT-4. The proposed method includes an In-Context Prompt Generator, an Intent Predictor, and a Semantic Few-Shot Sampler to identify new user intents with minimal labeled data. Experiments show IntentGPT outperforms previous models on benchmarks like CLINC and BANKING.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes experiments and results comparing the proposed method with existing methods, indicating it's an empirical study.",
      "quote": "Our experiments show that IntentGPT outperforms previous methods that require extensive domain-specific data and fine-tuning, in popular benchmarks, including CLINC and BANKING."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on tasks such as Intent Detection and Discovery, which are central topics in Natural Language Processing.",
        "quote": "Intent Discovery is a natural language processing (NLP) task that involves classifying user-written sentences into intents within an open-world context, where new intents will emerge and need to be identified."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Intent Detection",
          "justification": "One of the primary tasks discussed in the paper is Intent Detection.",
          "quote": "To overcome this, we introduce IntentGPT, a novel method that efficiently prompts Large Language Models (LLMs) such as GPT-4 to effectively discover new intents with minimal labeled data."
        },
        "aliases": [
          "Intent Recognition"
        ]
      },
      {
        "name": {
          "value": "Few-Shot Learning",
          "justification": "The paper describes a method that leverages few-shot learning using pre-trained LLMs to identify new user intents.",
          "quote": "To overcome this, we introduce IntentGPT, a novel method that efficiently prompts Large Language Models (LLMs) such as GPT-4 to effectively discover new intents with minimal labeled data."
        },
        "aliases": [
          "Few-Shot"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "IntentGPT",
          "justification": "The primary model introduced in the paper is IntentGPT.",
          "quote": "To overcome this, we introduce IntentGPT, a novel method that efficiently prompts Large Language Models (LLMs) such as GPT-4 to effectively discover new intents with minimal labeled data."
        },
        "aliases": [
          "In-Context Prompt Generator",
          "Intent Predictor",
          "Semantic Few-Shot Sampler"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The paper proposes IntentGPT as a novel model for few-shot intent discovery.",
          "quote": "To overcome this, we introduce IntentGPT, a novel method that efficiently prompts Large Language Models (LLMs) such as GPT-4 to effectively discover new intents with minimal labeled data."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is executed using large language models such as GPT-4 which typically run on GPUs.",
          "quote": "To overcome this, we introduce IntentGPT, a novel method that efficiently prompts Large Language Models (LLMs) such as GPT-4 to effectively discover new intents with minimal labeled data."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of IntentGPT is compared with existing models in the paper.",
          "quote": "Our experiments show that IntentGPT outperforms previous methods that require extensive domain-specific data and fine-tuning, in popular benchmarks, including CLINC and BANKING."
        },
        "referenced_paper_title": {
          "value": "Language models are few-shot learners",
          "justification": "The referenced paper for GPT-3 and related LLMs.",
          "quote": "Pre-trained models, such as GPT 3-4 (Brown et al., 2020; OpenAI, 2023), have undergone extensive training on vast and diverse text corpora encompassing a wide range of domains and contexts."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CLINC",
          "justification": "The CLINC dataset is used in the experimental evaluation of IntentGPT.",
          "quote": "Our experiments show that IntentGPT outperforms previous methods that require extensive domain-specific data and fine-tuning, in popular benchmarks, including CLINC and BANKING."
        },
        "aliases": [
          "CLINC150",
          "CLINC OOS"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction",
          "justification": "The referenced paper for the CLINC dataset.",
          "quote": "Intent Detection focuses on classifying pre-defined user intents, typically through text sentences (Liu et al., 2019)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "GPT-4",
          "justification": "GPT-4 is explicitly used as the large language model in the context of IntentGPT.",
          "quote": "To overcome this, we introduce IntentGPT, a novel method that efficiently prompts Large Language Models (LLMs) such as GPT-4 to effectively discover new intents with minimal labeled data."
        },
        "aliases": [
          "gpt-4"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "GPT-4 Technical Report",
          "justification": "The referenced paper for GPT-4.",
          "quote": "Pre-trained models, such as GPT 3-4 (Brown et al., 2020; OpenAI, 2023), have undergone extensive training on vast and diverse text corpora encompassing a wide range of domains and contexts."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1108,
    "prompt_tokens": 23925,
    "total_tokens": 25033
  }
}