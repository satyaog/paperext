{
  "paper": "cb95800a2d1fec02bdeaee56422274d1.txt",
  "words": 8620,
  "extractions": {
    "title": {
      "value": "Plot Twist: Multimodal Models Don’t Comprehend Simple Chart Details",
      "justification": "The title explicitly states the central claim and investigation focus of the paper, which is assessing the comprehension of multimodal models on simple chart details.",
      "quote": "Plot Twist: Multimodal Models Don’t Comprehend Simple Chart Details"
    },
    "description": "This paper evaluates the fundamental capabilities of multimodal models in understanding basic chart details, despite their advanced performance on complex benchmarks. It investigates these models' limitations by posing elementary questions about chart data, revealing critical shortcomings in their visual comprehension abilities.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts empirical evaluations of multimodal models by posing elementary questions to them on both synthetic and real-world datasets and comparing their performance.",
      "quote": "The empirical evaluation of 5 popular multimodal models on our dataset reveals shortfalls in understanding charts and figures."
    },
    "primary_research_field": {
      "name": {
        "value": "Multimodal Model Evaluation",
        "justification": "The paper primarily focuses on evaluating different multimodal models' ability to comprehend chart details, which falls under the domain of model evaluation within multimodal learning.",
        "quote": "In this paper, we probe multimodal models to understand whether they can answer elementary questions about the specific visual content in charts."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Chart Understanding",
          "justification": "The paper specifically targets the chart understanding capabilities of multimodal models, making this a significant sub-research field discussed in the study.",
          "quote": "Assessing chart understanding capabilities offers a crucial benchmark for evaluating foundational models’ reasoning skills beyond text."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Gemini Pro Vision",
          "justification": "The paper includes empirical evaluations of the Gemini Pro Vision model as part of its study.",
          "quote": "For instance, Gemini Pro Vision only achieves 57.9% accuracy on our elementary set of questions on real-world plots."
        },
        "aliases": [
          "Gemini Pro"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The paper does not claim to have developed the Gemini Pro Vision model, but rather evaluates its performance.",
          "quote": "We include Gemini Pro Vision (Gemini-Team et al., 2023), GPT-4V (OpenAI et al., 2023), PaLI-3 (Chen et al., 2023), ChartLlama (Han et al., 2023) and CogVLM (Wang et al., 2024) models in our empirical analysis."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper mentions using these models, indicating execution during empirical analysis.",
          "quote": "Models struggle to answer basic synthetic chart questions reliably. We initially assess model performance on our synthetic subset."
        },
        "is_compared": {
          "value": true,
          "justification": "The study compares the performance of various multimodal models on elementary chart understanding tasks.",
          "quote": "...there is often a high drop in performance, such as around 10% for Gemini Pro Vision."
        },
        "referenced_paper_title": {
          "value": "Gemini: A family of highly capable multimodal models",
          "justification": "The referenced paper for the Gemini Pro Vision model is cited within the document.",
          "quote": "We include Gemini Pro Vision (Gemini-Team et al., 2023), ... in our empirical analysis."
        }
      },
      {
        "name": {
          "value": "GPT-4V",
          "justification": "The model GPT-4V is evaluated in the paper for its performance on chart understanding.",
          "quote": "Moreover, other models such as Gemini Pro Vision and GPT-4V also get less than 60% performance."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "GPT-4V is an existing model that the authors evaluated, not one they contributed to creating.",
          "quote": "We include Gemini Pro Vision ... and GPT-4V (OpenAI et al., 2023) ... models in our empirical analysis."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper uses the term 'evaluate,' indicating the model was executed in their tests.",
          "quote": "We include Gemini Pro Vision ... GPT-4V (OpenAI et al., 2023) ... models in our empirical analysis."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares GPT-4V's performance to other models in elementary chart question tasks.",
          "quote": "Moreover, other models such as Gemini Pro Vision and GPT-4V also get less than 60% performance."
        },
        "referenced_paper_title": {
          "value": "GPT-4 technical report",
          "justification": "GPT-4V's referenced paper is cited in the document.",
          "quote": "GPT-4V (OpenAI et al., 2023) (OpenAI et al., 2023)"
        }
      },
      {
        "name": {
          "value": "PaLI-3",
          "justification": "The model PaLI-3 is explicitly evaluated in the paper as part of their study on chart understanding capabilities.",
          "quote": "For example, PaLI-3 only gets 37.7% accuracy on our straightforward questions on the real-world plots."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "PaLI-3 is an existing model used in the evaluations, not created by the authors.",
          "quote": "We include Gemini Pro Vision ... PaLI-3 (Chen et al., 2023) ... models in our empirical analysis."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper evaluates PaLI-3's performance, which indicates execution during tests.",
          "quote": "For example, PaLI-3 only gets 37.7% accuracy on our straightforward questions on the real-world plots."
        },
        "is_compared": {
          "value": true,
          "justification": "PaLI-3 is compared in performance with other models on specific tasks in the study.",
          "quote": "For example, PaLI-3 only gets 37.7% accuracy on our straightforward questions on the real-world plots."
        },
        "referenced_paper_title": {
          "value": "PaLI-3 vision language models: Smaller, faster, stronger",
          "justification": "PaLI-3's referenced paper is cited in the document.",
          "quote": "PaLI-3 (Chen et al., 2023)"
        }
      },
      {
        "name": {
          "value": "ChartLlama",
          "justification": "ChartLlama is included and analyzed for its capabilities in chart understanding within the study.",
          "quote": "Public models perform considerably worse, even ChartLlama, which is specialized explicitly for charts."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "ChartLlama is a model discussed in the paper but not developed by its authors.",
          "quote": "We include Gemini Pro Vision ... ChartLlama (Han et al., 2023) ... models in our empirical analysis."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper makes comparisons using this model, indicating it was executed during tests.",
          "quote": "Public models perform considerably worse, even ChartLlama, which is specialized explicitly for charts."
        },
        "is_compared": {
          "value": true,
          "justification": "ChartLlama's performance is compared to other models in the context of the research questions posed.",
          "quote": "Public models perform considerably worse, even ChartLlama, which is specialized explicitly for charts."
        },
        "referenced_paper_title": {
          "value": "Chartllama: A multimodal llm for chart understanding and generation",
          "justification": "The referenced paper for ChartLlama is cited in the document.",
          "quote": "ChartLlama (Han et al., 2023)"
        }
      },
      {
        "name": {
          "value": "CogVLM",
          "justification": "CogVLM is discussed and evaluated within the context of the study for its performance on chart-related tasks.",
          "quote": "We evaluate multimodal models, including CogVLM, in our empirical analysis."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "CogVLM is an existing model that the authors analyzed, not one they contributed to creating.",
          "quote": "...CogVLM (Wang et al., 2024) models in our empirical analysis."
        },
        "is_executed": {
          "value": true,
          "justification": "The empirical analysis and evaluation of CogVLM suggest its execution during these tests.",
          "quote": "We evaluate multimodal models, including CogVLM, in our empirical analysis."
        },
        "is_compared": {
          "value": true,
          "justification": "The study involves comparison of performance among the included models, including CogVLM.",
          "quote": "We evaluate multimodal models, including CogVLM, in our empirical analysis."
        },
        "referenced_paper_title": {
          "value": "Cogvlm: Visual expert for pretrained language models",
          "justification": "The referenced paper for CogVLM is cited in the document.",
          "quote": "CogVLM (Wang et al., 2024)"
        }
      },
      {
        "name": {
          "value": "Chart101",
          "justification": "The paper introduces a new probing dataset explicitly mentioned as Chart101, contributing to the field.",
          "quote": "The dataset used in this paper is available at Chart101."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "Chart101 is a newly introduced dataset probing tool, indicated as part of their results and contributions.",
          "quote": "The dataset used in this paper is available at Chart101."
        },
        "is_executed": {
          "value": false,
          "justification": "Chart101 is a dataset introduced in the research and does not entail model execution itself.",
          "quote": "The dataset used in this paper is available at Chart101."
        },
        "is_compared": {
          "value": false,
          "justification": "As a dataset, Chart101 does not involve numerical comparison directly with models.",
          "quote": "The dataset used in this paper is available at Chart101."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "Chart101 is named as a newly introduced dataset and therefore might not have a previous reference title.",
          "quote": "The dataset used in this paper is available at Chart101."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ChartQA",
          "justification": "The paper frequently references the ChartQA dataset for assessing model performance on real-world data.",
          "quote": "Recent advances in multimodal models show remarkable performance in real-world benchmarks for chart and figure understanding like ChartQA."
        },
        "aliases": [],
        "role": "referenced",
        "referenced_paper_title": {
          "value": "ChartQA: A benchmark for question answering about charts with visual and logical reasoning",
          "justification": "The referenced paper discussing ChartQA is cited within the document.",
          "quote": "...complex, human-written questions reflecting real-world applications (Methani et al., 2019; Masry et al., 2022)."
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2129,
    "prompt_tokens": 17148,
    "total_tokens": 19277,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}