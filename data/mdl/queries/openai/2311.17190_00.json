{
  "paper": "2311.17190.txt",
  "words": 6516,
  "extractions": {
    "title": {
      "value": "Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play",
      "justification": "The title is clearly stated at the beginning of the paper.",
      "quote": "Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play"
    },
    "description": "The paper proposes the Minimax Exploiter, an approach to Competitive Self-Play Multi-Agent Reinforcement Learning (CSP-MARL) aimed at increasing data efficiency and reducing computational costs by leveraging the knowledge of opponents to generate better counter-strategies. The approach is validated across different game settings, including simple turn-based games, Atari Boxing, and For Honor, a modern video game.",
    "type": {
      "value": "empirical",
      "justification": "The paper describes various experiments conducted to validate the proposed Minimax Exploiter approach across different environments, including Tic-Tac-Toe, Connect 4, Atari Boxing, and For Honor.",
      "quote": "We validate our approach in a diversity of settings, including simple turn based games, the arcade learning environment, and For Honor, a modern video game."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper focuses on improving Competitive Self-Play Multi-Agent Reinforcement Learning methods.",
        "quote": "To this end, we propose an alternative to the standard Exploiter archetype, we call the Minimax Exploiter, that utilises a game theoretic inspired reward function that aims to minimize the maximum Q-value of its opponent at every step."
      },
      "aliases": [
        "RL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multi-Agent Reinforcement Learning",
          "justification": "The paper focuses on Competitive Self-Play Multi-Agent Reinforcement Learning methods.",
          "quote": "Recent advances in Competitive Self-Play (CSP) have achieved, or even surpassed, human level performance in complex game environments such as Dota 2 and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL)."
        },
        "aliases": [
          "MARL"
        ]
      },
      {
        "name": {
          "value": "Game Theory",
          "justification": "The approach proposed involves a game theoretic inspired reward function.",
          "quote": "To this end, we propose an alternative to the standard Exploiter archetype, we call the Minimax Exploiter, that utilises a game theoretic inspired reward function that aims to minimize the maximum Q-value of its opponent at every step."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "Though primarily focused on RL, the use of game environments and evaluation on video game 'For Honor' suggests relevance to this field as well.",
          "quote": "We first evaluate the efficiency of the Minimax Exploiter on Tic-Tac-Toe, Connect 4 and Atari Boxing, before stress testing our approach on a modern AAA 1 video game environment, For Honor, where we deploy a complete league training setup to evaluate its efficiency."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Minimax Exploiter",
          "justification": "The paper introduces the Minimax Exploiter as a new model for improving data efficiency in Competitive Self-Play.",
          "quote": "In this paper, we propose the Minimax Exploiter, a game theoretic approach to exploiting Main Agents that leverages knowledge of its opponents, leading to significant increases in data efficiency."
        },
        "aliases": [],
        "is_contributed": {
          "value": 1,
          "justification": "The Minimax Exploiter is the main contribution of the paper.",
          "quote": "In this paper, we propose the Minimax Exploiter, a game theoretic approach to exploiting Main Agents that leverages knowledge of its opponents, leading to significant increases in data efficiency."
        },
        "is_executed": {
          "value": 1,
          "justification": "The paper mentions using GPUs for executing models as part of the league training setup, implying that the models were executed on GPUs.",
          "quote": "...roughly 1.3 × 104 CPUs and 144 GPUs, compared to Alphastar’s [17] 139 million parameter model in 44 days with 5 × 105 CPUs and 3 × 103 GPUs..."
        },
        "is_compared": {
          "value": 1,
          "justification": "The paper compares the performance of the Minimax Exploiter against other baselines like standard DQN agents and vanilla exploiters.",
          "quote": "We compare the performance of the resulting agents across different exploiters in a modern AAA video game..."
        },
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The Minimax Exploiter is introduced in this paper and doesn't reference a previous paper for its introduction.",
          "quote": "N/A"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Tic-Tac-Toe",
          "justification": "The paper uses Tic-Tac-Toe for evaluating the Minimax Exploiter.",
          "quote": "We first evaluate the efficiency of the Minimax Exploiter on Tic-Tac-Toe..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The Tic-Tac-Toe dataset used in the paper does not reference another paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Connect 4",
          "justification": "The paper uses Connect 4 for evaluating the Minimax Exploiter.",
          "quote": "We first evaluate the efficiency of the Minimax Exploiter on Tic-Tac-Toe, Connect 4..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The Connect 4 dataset used in the paper does not reference another paper.",
          "quote": "N/A"
        }
      },
      {
        "name": {
          "value": "Atari Boxing",
          "justification": "The paper uses the Atari Boxing environment for evaluating the Minimax Exploiter.",
          "quote": "... before stress testing our approach on a modern AAA video game environment, For Honor."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Mnih et al. 2013",
          "justification": "The paper builds on the framework of Atari games introduced by Mnih et al.",
          "quote": "Reinforcement learning (RL) has demonstrated its utility in reliably solving tasks in static environments, such as with Atari games [7]..."
        }
      },
      {
        "name": {
          "value": "For Honor",
          "justification": "The paper uses For Honor for evaluating the Minimax Exploiter.",
          "quote": "... before stress testing our approach on a modern AAA video game environment, For Honor, where we deploy a complete league training setup to evaluate its efficiency."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "N/A",
          "justification": "The For Honor used in the paper does not reference another paper.",
          "quote": "N/A"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Deep Q-Network (DQN)",
          "justification": "The Deep Q-Network (DQN) algorithm is used as the baseline in the experiments.",
          "quote": "Our benchmarks are Deep Q-Network (DQN) agents utilizing a double Q-function."
        },
        "aliases": [
          "DQN"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Mnih et al. 2013",
          "justification": "The DQN algorithm is originally introduced by Mnih et al.",
          "quote": "Playing Atari with Deep Reinforcement Learning"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1449,
    "prompt_tokens": 12483,
    "total_tokens": 13932
  }
}