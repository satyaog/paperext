{
  "paper": "n6D25aqdV3.txt",
  "words": 1773,
  "extractions": {
    "title": {
      "value": "Contrast-agnostic Spinal Cord Segmentation: A Comparative Study of ConvNets and Vision Transformers",
      "justification": "The title is explicitly mentioned in the beginning of the paper.",
      "quote": "Contrast-agnostic Spinal Cord Segmentation: A Comparative Study of ConvNets and Vision Transformers"
    },
    "description": "This paper presents a comparative study on the performance of convolutional neural networks (ConvNets) and vision transformers (ViTs) in the task of contrast-agnostic spinal cord segmentation. The study uses a fixed dataset size and evaluates seven different deep learning models to determine their effectiveness in producing robust spinal cord segmentations across various MRI contrasts.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experimental comparisons of different deep learning models for a specific task, and thus is empirical.",
      "quote": "In this study, we extend our recent work (Be패dard et al., 2023) by evaluating the contrast-agnostic SC segmentation capabilities of different classes of DL architectures."
    },
    "primary_research_field": {
      "name": {
        "value": "Medical Imaging",
        "justification": "The paper focuses on medical imaging techniques, specifically spinal cord segmentation using deep learning models.",
        "quote": "Keywords: Spinal Cord, MRI, Contrasts, Segmentation, Deep Learning, CNNs, Vision Transformers"
      },
      "aliases": [
        "Medical Imaging"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Deep Learning",
          "justification": "The study involves deep learning models such as ConvNets and vision transformers for segmentation tasks.",
          "quote": "In this study, we extend our recent work (Be패dard et al., 2023) by evaluating the contrast-agnostic SC segmentation capabilities of different classes of DL architectures, namely, ConvNeXt, vision transformers (ViTs), and hierarchical ViTs."
        },
        "aliases": [
          "DL"
        ]
      },
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The paper deals with vision tasks related to segmentation in medical imaging, which falls under the scope of computer vision.",
          "quote": "The segmentation is essentially a computer vision task applied in the medical domain."
        },
        "aliases": [
          "CV"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "DeepSeg 2D",
          "justification": "The model is specifically mentioned as one of the CNNs compared in the study.",
          "quote": "In CNNs, we compared DeepSeg 2D (Gros et al., 2018) (implemented in the open-source Spinal Cord Toolbox (De Leener et al., 2017))"
        },
        "aliases": [
          "DeepSeg 2D"
        ],
        "is_contributed": {
          "value": false,
          "justification": "DeepSeg 2D is cited as a model used for comparison, not as a novel contribution by the authors.",
          "quote": "(Gros et al., 2018)"
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed to compare its performance against other models.",
          "quote": "In CNNs, we compared DeepSeg 2D..."
        },
        "is_compared": {
          "value": true,
          "justification": "The performance of DeepSeg 2D was compared with other models in the paper.",
          "quote": "In CNNs, we compared DeepSeg 2D..."
        },
        "referenced_paper_title": {
          "value": "Automatic segmentation of the spinal cord and intramedullary multiple sclerosis lesions with convolutional neural networks",
          "justification": "The referenced paper for DeepSeg 2D is noted in the study.",
          "quote": "(Gros et al., 2018)"
        }
      },
      {
        "name": {
          "value": "nnUNet",
          "justification": "The nnUNet is identified as one of the models compared in the research.",
          "quote": "nnUNet (Isensee et al., 2021)"
        },
        "aliases": [
          "nnUNet"
        ],
        "is_contributed": {
          "value": false,
          "justification": "nnUNet is used for comparison, not a new contribution in this paper.",
          "quote": "(Isensee et al., 2021)"
        },
        "is_executed": {
          "value": true,
          "justification": "The nnUNet was used in experiments presented in the paper.",
          "quote": "we compared 7 models spanning 3 different classes of DL architectures."
        },
        "is_compared": {
          "value": true,
          "justification": "nnUNet's performance is compared against other models in the study.",
          "quote": "Among the CNN-based models, the contrast-agnostic model achieved the lowest CSA error, with MedNeXt following closely."
        },
        "referenced_paper_title": {
          "value": "nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation",
          "justification": "The study mentions this reference in relation to nnUNet.",
          "quote": "(Isensee et al., 2021)"
        }
      },
      {
        "name": {
          "value": "MedNeXt",
          "justification": "MedNeXt is evaluated as part of the ConvNeXt models compared in the paper.",
          "quote": "Then, we trained MedNeXt (Roy et al., 2023), a state-of-the-art ConvNeXt model designed for 3D medical images."
        },
        "aliases": [
          "MedNeXt"
        ],
        "is_contributed": {
          "value": false,
          "justification": "MedNeXt is a model cited from other sources, not introduced by the authors.",
          "quote": "(Roy et al., 2023)"
        },
        "is_executed": {
          "value": true,
          "justification": "MedNeXt was specifically trained and evaluated in the experiments.",
          "quote": "Then, we trained MedNeXt..."
        },
        "is_compared": {
          "value": true,
          "justification": "MedNeXt was compared against other models in the research findings.",
          "quote": "with MedNeXt following closely."
        },
        "referenced_paper_title": {
          "value": "Mednext: Transformer-driven scaling of convnets for medical image segmentation",
          "justification": "The paper references this title for MedNeXt.",
          "quote": "(Roy et al., 2023)"
        }
      },
      {
        "name": {
          "value": "UNETR",
          "justification": "UNETR is one of the ViTs compared in the study.",
          "quote": "among ViTs, we compared three models, namely, UNETR (Hatamizadeh et al., 2021)"
        },
        "aliases": [
          "UNETR"
        ],
        "is_contributed": {
          "value": false,
          "justification": "UNETR is a previously developed model and not a new contribution by the authors.",
          "quote": "(Hatamizadeh et al., 2021)"
        },
        "is_executed": {
          "value": true,
          "justification": "UNETR was implemented to generate experimental results.",
          "quote": "among ViTs, we compared three models, namely, UNETR..."
        },
        "is_compared": {
          "value": true,
          "justification": "UNETR's performance metrics were compared with other models in the paper.",
          "quote": "while the SwinUNETR models showed similar performance as the CNNs."
        },
        "referenced_paper_title": {
          "value": "UNETR: Transformers for 3D medical image segmentation",
          "justification": "This is the reference provided for UNETR in the study.",
          "quote": "(Hatamizadeh et al., 2021)"
        }
      },
      {
        "name": {
          "value": "SwinUNETR",
          "justification": "SwinUNETR model is evaluated in the study as one of the hierarchical ViTs.",
          "quote": "We compared ... SwinUNETR (Hatamizadeh et al., 2022)"
        },
        "aliases": [
          "SwinUNETR"
        ],
        "is_contributed": {
          "value": false,
          "justification": "SwinUNETR is sourced from existing literature and is not a novel model from this study.",
          "quote": "(Hatamizadeh et al., 2022)"
        },
        "is_executed": {
          "value": true,
          "justification": "SwinUNETR was used in the experiments to assess its performance.",
          "quote": "We compared ... SwinUNETR..."
        },
        "is_compared": {
          "value": true,
          "justification": "SwinUNETR's performance was directly compared against other models in the study.",
          "quote": "while the SwinUNETR models showed similar performance as the CNNs."
        },
        "referenced_paper_title": {
          "value": "Swin UNETR: Swin transformers for semantic segmentation of brain tumors in MRI images",
          "justification": "The referenced paper is noted for SwinUNETR.",
          "quote": "(Hatamizadeh et al., 2022)"
        }
      },
      {
        "name": {
          "value": "contrast-agnostic model",
          "justification": "The study mentions a contrast-agnostic model developed in previous work which was compared in this paper.",
          "quote": "and our recently-introduced contrast-agnostic model (Be패dard et al., 2023)."
        },
        "aliases": [
          "contrast-agnostic model"
        ],
        "is_contributed": {
          "value": true,
          "justification": "The contrast-agnostic model is recently introduced by one of the authors of the study.",
          "quote": "and our recently-introduced contrast-agnostic model (Be패dard et al., 2023)."
        },
        "is_executed": {
          "value": true,
          "justification": "The contrast-agnostic model was executed as part of the experiments in this paper.",
          "quote": "Among the CNN-based models, the contrast-agnostic model achieved the lowest CSA error."
        },
        "is_compared": {
          "value": true,
          "justification": "The model's performance is compared to other models in the research findings.",
          "quote": "Among the CNN-based models, the contrast-agnostic model achieved the lowest CSA error."
        },
        "referenced_paper_title": {
          "value": "Towards contrast-agnostic soft segmentation of the spinal cord",
          "justification": "The paper references this title related to the model in its evaluation.",
          "quote": "(Be패dard et al., 2023)"
        }
      },
      {
        "name": {
          "value": "pretrained SwinUNETR",
          "justification": "The pretrained version of SwinUNETR mentioned explicitly as part of the model cohort.",
          "quote": "an open-source, pretrained SwinUNETR2"
        },
        "aliases": [
          "pretrained SwinUNETR"
        ],
        "is_contributed": {
          "value": false,
          "justification": "Pretrained SwinUNETR is sourced from external resources, not a contribution of this study.",
          "quote": "an open-source, pretrained SwinUNETR2"
        },
        "is_executed": {
          "value": true,
          "justification": "Was fine-tuned and used to conduct experiments in the study.",
          "quote": "an open-source, pretrained SwinUNETR2 . Except for the pretrained model which we fine-tuned on our datasets..."
        },
        "is_compared": {
          "value": true,
          "justification": "Comparative analysis was performed involving the pretrained SwinUNETR model.",
          "quote": "the SwinUNETR models showed similar performance as the CNNs."
        },
        "referenced_paper_title": {
          "value": "Swin UNETR: Swin transformers for semantic segmentation of brain tumors in MRI images",
          "justification": "The paper refers to this title for the pretrained version of SwinUNETR.",
          "quote": "(Hatamizadeh et al., 2022)"
        }
      },
      {
        "name": {
          "value": "ConvNeXt",
          "justification": "ConvNeXt is one of the types of DL architecture studied for SC segmentation capabilities.",
          "quote": "In this study, we extend our recent work (Be패dard et al., 2023) by evaluating the contrast-agnostic SC segmentation capabilities of different classes of DL architectures, namely, ConvNeXt..."
        },
        "aliases": [
          "ConvNeXt"
        ],
        "is_contributed": {
          "value": false,
          "justification": "ConvNeXt is explored as an existing model, not as a new contribution.",
          "quote": "ConvNeXt"
        },
        "is_executed": {
          "value": true,
          "justification": "ConvNeXt models, including MedNeXt, were executed to assess their segmentation performance.",
          "quote": "we extend our recent work... by evaluating... ConvNeXt..."
        },
        "is_compared": {
          "value": true,
          "justification": "ConvNeXt's segmentation ability is analyzed against other models.",
          "quote": "we extend our recent work... by evaluating... ConvNeXt..."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "There appears to be no specific reference title mentioned for ConvNeXt within the context given.",
          "quote": ""
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Spine Generic Database",
          "justification": "The paper uses the Spine Generic Database for evaluating deep learning architectures.",
          "quote": "We compared 7 different DL models using the open-source Spine Generic Database of healthy participants (n = 243)"
        },
        "aliases": [
          "Spine Generic Database"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "",
          "justification": "There is no referenced paper title provided for the Spine Generic Database in the paper.",
          "quote": ""
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 2556,
    "prompt_tokens": 4383,
    "total_tokens": 6939,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}