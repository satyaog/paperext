{
  "paper": "2305.08675.txt",
  "words": 12324,
  "extractions": {
    "title": {
      "value": "Improved baselines for vision-language pre-training",
      "justification": "The paper aims to explore improved training recipes for vision-language pre-training, especially focusing on models like CLIP. Several non-contrastive baseline models are proposed and analyzed.",
      "quote": "Improved baselines for vision-language pre-training"
    },
    "description": "This paper investigates the combination of contrastive learning with recent advances in self-supervised learning to improve vision-language pre-training models like CLIP. The study includes proposing several non-contrastive baseline models and implementing an improved training recipe for CLIP. The findings indicate that with advanced training recipes, simple CLIP baselines can achieve state-of-the-art performance.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves implementing and evaluating several models and methods to empirically determine the best training practices for vision-language pre-training.",
      "quote": "In this paper, we first propose, implement and evaluate several baselines obtained by combining contrastive learning with recent advances in self-supervised learning."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The primary focus of the study is on vision-language pre-training, which falls under the domain of Computer Vision.",
        "quote": "Vision-language pre-training (VLP) is a recent learning paradigm that enables neural networks to learn multimodal representations from images and text."
      },
      "aliases": [
        "Vision-Language Pre-Training",
        "VLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Multimodal Learning",
          "justification": "The paper extensively deals with learning representations from both images and text simultaneously.",
          "quote": "Vision-language pre-training (VLP) is a recent learning paradigm that enables neural networks to learn multimodal representations from images and text."
        },
        "aliases": [
          "Multi-Modal Learning"
        ]
      },
      {
        "name": {
          "value": "Self-Supervised Learning",
          "justification": "The paper proposes using non-contrastive losses inspired by self-supervised learning to improve vision-language models.",
          "quote": "...we use the loss functions that were proven successful for visual self-supervised learning to align image and text modalities."
        },
        "aliases": [
          "SSL",
          "SSL-inspired Learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "CLIP",
          "justification": "CLIP is the baseline model used in the paper for vision-language pre-training. Various improvements and modifications are benchmarked against the basic CLIP implementation.",
          "quote": "Arguably, the most prominent approach to VLP is CLIP (Radford et al., 2021), which leverages a contrastive loss to learn aligned image and text representations from large-scale image-text datasets scraped from the internet."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "CLIP is not an original contribution of this paper but serves as a baseline for comparison.",
          "quote": "Arguably, the most prominent approach to VLP is CLIP (Radford et al., 2021), which leverages a contrastive loss to learn aligned image and text representations from large-scale image-text datasets scraped from the internet."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was executed in the paper to serve as a baseline for comparison.",
          "quote": "...we find that a simple CLIP baseline can also be improved substantially, up to a 25% relative improvement on downstream zero-shot tasks, by using well-known training techniques that are popular in other subfields."
        },
        "is_compared": {
          "value": true,
          "justification": "CLIP's performance was compared against several proposed non-contrastive baselines and improved recipes.",
          "quote": "...we present four new VLP baselines: SiamLIP, BYOLIP, BarLIP, and SwALIP, by translating the non-constrastive losses of the SSL methods, SimSiam, BYOL, Barlow Twins, and SwAV to the multimodal case."
        },
        "referenced_paper_title": {
          "value": "Learning transferable visual models from natural language supervision",
          "justification": "This is the original paper where CLIP was introduced.",
          "quote": "(Radford et al., 2021)"
        }
      },
      {
        "name": {
          "value": "SiamLIP",
          "justification": "SiamLIP is one of the new baseline models proposed in this paper, leveraging losses inspired by SimSiam for vision-language pre-training.",
          "quote": "In particular, we present four new VLP baselines: SiamLIP, BYOLIP, BarLIP, and SwALIP, by translating the non-constrastive losses of the SSL methods, SimSiam, BYOL, Barlow Twins, and SwAV to the multimodal case."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "SiamLIP is an original contribution of this paper.",
          "quote": "In particular, we present four new VLP baselines: SiamLIP, BYOLIP, BarLIP, and SwALIP, by translating the non-constrastive losses of the SSL methods, SimSiam, BYOL, Barlow Twins, and SwAV to the multimodal case."
        },
        "is_executed": {
          "value": true,
          "justification": "The model was implemented, executed, and evaluated in empirical experiments.",
          "quote": "SiamLIP & BYOLIP (see Fig. 2a) are two VLP models that complement the contrastive training objective of CLIP with a consistency-based objective."
        },
        "is_compared": {
          "value": true,
          "justification": "SiamLIP's performance was compared against CLIP and other baselines like BYOLIP, BarLIP, and SwALIP.",
          "quote": "In our non-contrastive baselines, we consider four types of non-contrastive losses and combine them with CLIP to build four models: SiamLIP, BYOLIP, BarLIP, and SwaVLIP."
        },
        "referenced_paper_title": {
          "value": "Exploring simple siamese representation learning",
          "justification": "The model builds on the SimSiam technique introduced in this referenced paper.",
          "quote": "SiamLIP & BYOLIP (see Fig. 2a) are two VLP models that complement the contrastive training objective of CLIP with a consistency-based objective. In the case of SiamLIP & BYOLIP, the complementary training objective is inspired by SimSiam (Chen & He, 2021) and BYOL (Grill et al., 2020)."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CC3M",
          "justification": "The CC3M dataset was used for pre-training and evaluating the proposed models.",
          "quote": "The Conceptual Captions dataset is composed of image-caption pairs that have been filtered based on... CC3M (Sharma et al., 2018) composed of 3.3M image-text pairs obtained by applying the full filtering pipeline."
        },
        "aliases": [
          "Conceptual Captions 3M"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Conceptual Captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
          "justification": "This is the original paper where CC3M was introduced.",
          "quote": "CC3M (Sharma et al., 2018)"
        }
      },
      {
        "name": {
          "value": "CC12M",
          "justification": "The CC12M dataset was used for pre-training and evaluating the proposed models.",
          "quote": "CC12M (Changpinyo et al., 2021) comprising 12.4M pairs obtained from a relaxed version of the pipeline (relaxed uni-modal filters (i), (ii), and (iv))."
        },
        "aliases": [
          "Conceptual Captions 12M"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
          "justification": "This is the original paper where CC12M was introduced.",
          "quote": " CC12M (Changpinyo et al., 2021) comprising 12.4M pairs obtained from a relaxed version of the pipeline (relaxed uni-modal filters (i), (ii), and (iv))."
        }
      },
      {
        "name": {
          "value": "YFCC15M",
          "justification": "The YFCC15M dataset was used for pre-training and evaluating the proposed models.",
          "quote": "The Yahoo Flickr Creative Commons dataset is composed of 100M of image-text pairs (Thomee et al., 2016). Noisy text-image pairs have been filtered by Radford et al. (2021) to obtain a cleaner version of 15M pairs. This dataset is referred to as YFCC15M."
        },
        "aliases": [
          "Yahoo Flickr Creative Commons 15M"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "YFCC100M: The new data in multimedia research",
          "justification": "This is the original paper where the YFCC dataset, from which YFCC15M is derived, was introduced.",
          "quote": "The Yahoo Flickr Creative Commons dataset is composed of 100M of image-text pairs (Thomee et al., 2016)."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch was likely used for implementing the mentioned models as it is widely used for deep learning research.",
          "quote": "The code is available at https://github.com/facebookresearch/clip-rocket"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
          "justification": "This paper introduces PyTorch, which is likely the library used for this research.",
          "quote": "The code is available at https://github.com/facebookresearch/clip-rocket"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1945,
    "prompt_tokens": 23847,
    "total_tokens": 25792
  }
}