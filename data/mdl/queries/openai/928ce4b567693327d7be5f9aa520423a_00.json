{
  "paper": "928ce4b567693327d7be5f9aa520423a.txt",
  "words": 15922,
  "extractions": {
    "title": {
      "value": "Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning",
      "justification": "The extracted title matches the title provided at the beginning of the paper.",
      "quote": "Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning"
    },
    "description": "The paper explores a method called Head2Toe for improving transfer learning by selecting features from all layers of a pretrained model, instead of just the last layer, to train a classification head for the target domain. This method matches fine-tuning performance while significantly reducing training and storage costs and performs better in out-of-distribution transfer scenarios.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents experiments on the Visual Task Adaptation Benchmark (VTAB) and compares different methods empirically.",
      "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018) and ViT-B/16 (Dosovitskiy et al., 2021), both pretrained on ImageNet-2012."
    },
    "primary_research_field": {
      "name": {
        "value": "Transfer Learning",
        "justification": "The paper is primarily focused on transfer learning, as indicated by its exploration of methods to transfer learned features from a source domain to a target domain.",
        "quote": "Transfer learning is a widely used method for obtaining strong performance in a variety of tasks where training data is scarce."
      },
      "aliases": []
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Computer Vision",
          "justification": "The experiments and methods discussed, like the Visual Task Adaptation Benchmark (VTAB) and architectures such as ResNet, are typical within Computer Vision.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018) and ViT-B/16 (Dosovitskiy et al., 2021), both pretrained on ImageNet-2012."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Head2Toe",
          "justification": "The proposed method in the paper, Head2Toe, is clearly mentioned throughout as a novel model contribution.",
          "quote": "We propose a method, Head-to-Toe probing (H EAD 2T OE ), that selects features from all layers of the source model to train a classification head for the target domain."
        },
        "aliases": [
          "H EAD 2T OE"
        ],
        "is_contributed": {
          "value": true,
          "justification": "Head2Toe is a novel contribution of the paper as suggested by phrases like 'we propose.'",
          "quote": "We propose a method, Head-to-Toe probing (H EAD 2T OE ), that selects features from all layers of the source model to train a classification head for the target domain."
        },
        "is_executed": {
          "value": true,
          "justification": "The method was applied across various datasets and evaluated empirically.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018) and ViT-B/16 (Dosovitskiy et al., 2021), both pretrained on ImageNet-2012."
        },
        "is_compared": {
          "value": true,
          "justification": "Head2Toe was compared against other methods like fine-tuning and linear probing in terms of performance benchmarks.",
          "quote": "On the VTAB collection of data sets, we show that H EAD 2T OE outperforms L INEAR and matches the performance of the more computationally costly F INE - T UNING with only 0.6% of the training FLOPs and 1% of the storage cost."
        },
        "referenced_paper_title": {
          "value": "Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning",
          "justification": "This is the paper where the Head2Toe method is introduced, hence it forms the basis of its own reference.",
          "quote": "In this work, we propose and explore methods for selecting useful features from all layers of a pretrained net,...Our approach, called H EAD 2T OE."
        }
      },
      {
        "name": {
          "value": "ResNet-50",
          "justification": "ResNet-50 is mentioned as a model used for benchmarking.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018) and ViT-B/16 (Dosovitskiy et al., 2021), both pretrained on ImageNet-2012."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "ResNet-50 is not an original contribution in this paper as it is a pre-existing architecture.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018) and ViT-B/16 (Dosovitskiy et al., 2021), both pretrained on ImageNet-2012."
        },
        "is_executed": {
          "value": true,
          "justification": "ResNet-50 is executed as part of the paper's methodology for evaluating Head2Toe.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018)..."
        },
        "is_compared": {
          "value": false,
          "justification": "ResNet-50 is not the primary subject of comparison, it serves rather as a backbone architecture for the experiments.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures"
        },
        "referenced_paper_title": {
          "value": "Deep Residual Learning for Image Recognition",
          "justification": "The ResNet-50 model originates from the 'Deep Residual Learning for Image Recognition' paper by He et al.",
          "quote": "Deep Residual Learning for Image Recognition (He et al., 2016)"
        }
      },
      {
        "name": {
          "value": "ViT-B/16",
          "justification": "ViT-B/16 is mentioned as a model utilized for benchmarking alongside ResNet-50.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018) and ViT-B/16 (Dosovitskiy et al., 2021), both pretrained on ImageNet-2012."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "ViT-B/16 is not a contribution of this paper as it is a pre-existing architecture.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018) and ViT-B/16 (Dosovitskiy et al., 2021), both pretrained on ImageNet-2012."
        },
        "is_executed": {
          "value": true,
          "justification": "ViT-B/16 is executed as part of the paper's methodology for evaluating Head2Toe.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50... and ViT-B/16."
        },
        "is_compared": {
          "value": false,
          "justification": "ViT-B/16, similar to ResNet-50, is not the subject of primary comparison but is utilized for backbone evaluation.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures"
        },
        "referenced_paper_title": {
          "value": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
          "justification": "ViT-B/16 originates from the 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale' paper.",
          "quote": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "ImageNet-2012",
          "justification": "ImageNet-2012 is used as the source domain for pretraining models in the paper.",
          "quote": "Source domain and backbone models. In our experiments, we use source models pretrained on ImageNet-2012 (Russakovsky et al., 2015), a large scale image classification benchmark with 1000 classes and over 1M natural images."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "ImageNet Large Scale Visual Recognition Challenge",
          "justification": "This is the reference associated with ImageNet that describes the dataset.",
          "quote": "ImageNet Large Scale Visual Recognition Challenge (Russakovsky et al., 2015)"
        }
      },
      {
        "name": {
          "value": "Visual Task Adaptation Benchmark (VTAB)",
          "justification": "The VTAB is used extensively to evaluate performance in various transfer learning scenarios.",
          "quote": "We evaluate H EAD 2T OE on the VTAB-1k benchmark using two popular vision architectures, ResNet-50 (Wu et al., 2018) and ViT-B/16 (Dosovitskiy et al., 2021), both pretrained on ImageNet-2012."
        },
        "aliases": [
          "VTAB"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Visual Task Adaptation Benchmark",
          "justification": "VTAB has a distinct reference discussing its use and purpose.",
          "quote": "The Visual Task Adaptation Benchmark (Zhai et al., 2019)"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1965,
    "prompt_tokens": 34531,
    "total_tokens": 36496,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}