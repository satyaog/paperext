{
  "paper": "2110.02355.txt",
  "words": 16613,
  "extractions": {
    "title": {
      "value": "Robustness and sample complexity of model-based MARL for general-sum Markov games",
      "justification": "The title is clearly mentioned at the beginning of the paper.",
      "quote": "Robustness and sample complexity of model-based MARL for general-sum Markov games"
    },
    "description": "The paper investigates the fundamental question of sample complexity for model-based multi-agent reinforcement learning (MARL) algorithms in general-sum Markov games. The authors show that certain sample bounds are sufficient to obtain an approximate Markov perfect equilibrium. They explore the robustness of these equilibria to model approximations and provide explicit bounds on the approximation error.",
    "type": {
      "value": "empirical",
      "justification": "The paper presents empirical results and numerical examples to illustrate the theoretical findings on sample complexity and robustness.",
      "quote": "We illustrate the results via a numerical example."
    },
    "primary_research_field": {
      "name": {
        "value": "Reinforcement Learning",
        "justification": "The paper deals with multi-agent reinforcement learning (MARL) within the framework of general-sum Markov games.",
        "quote": "Multi-agent reinforcement learning (MARL) is often modeled using the framework of Markov games."
      },
      "aliases": [
        "MARL"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Game Theory",
          "justification": "The research is specifically about general-sum Markov games, which are a topic within game theory.",
          "quote": "MARL concentrates on zero-sum Markov games but is not applicable to general-sum Markov games."
        },
        "aliases": [
          "Markov games"
        ]
      },
      {
        "name": {
          "value": "Sample Complexity",
          "justification": "The paper explores the sample complexity required for learning in general-sum Markov games.",
          "quote": "We investigate the fundamental question of sample complexity for model-based MARL algorithms in general-sum Markov games."
        },
        "aliases": [
          "Model-based learning"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "Minimax Q-learning",
          "justification": "The paper mentions minimax Q-learning as a variant applied to zero-sum games.",
          "quote": "the action-value function can be learnt using variants of Q-learning (called minimax Q-learning) because the Shapley operator is a contraction"
        },
        "aliases": [
          "minimax Q-learning"
        ],
        "is_contributed": {
          "value": false,
          "justification": "The model is not presented as a new contribution of this paper.",
          "quote": "minimax Q-learning (called minimax Q-learning)"
        },
        "is_executed": {
          "value": false,
          "justification": "The execution of this model is not discussed in the paper.",
          "quote": "N/A"
        },
        "is_compared": {
          "value": true,
          "justification": "The paper compares various algorithms for general-sum games, including minimax Q-learning.",
          "quote": "we refer the reader to Shoham et al. (2003) for an overview of MARL for zero-sum games."
        },
        "referenced_paper_title": {
          "value": "Value-function reinforcement learning in Markov games",
          "justification": "The model references previous work by Littman on minimax Q-learning.",
          "quote": "Littman 2001"
        }
      }
    ],
    "datasets": [],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 617,
    "prompt_tokens": 30650,
    "total_tokens": 31267
  }
}