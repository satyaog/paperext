{
  "paper": "2304.06845.txt",
  "words": 11466,
  "extractions": {
    "title": {
      "value": "SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval)",
      "justification": "This is the title of the paper.",
      "quote": "SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval)"
    },
    "description": "The paper presents the AfriSenti-SemEval, a sentiment classification challenge for 14 African languages, as part of the SemEval-2023. The challenge includes three subtasks: monolingual, multilingual, and zero-shot classification. The study describes the datasets, task settings, and the approaches used by the top-performing systems.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes details of datasets, models, and empirical evaluation of different approaches in a shared task setting.",
      "quote": "We received submissions from 44 teams, with 29 submitting a system description paper. The top-ranked teams for the different subtasks used pre-trained language models (PLMs)."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The focus is on sentiment analysis, a key task in Natural Language Processing (NLP).",
        "quote": "Sentiment Analysis is a prominent sub-field of Natural Language Processing that focuses on the automatic identification of sentiments or opinions expressed through online content"
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Sentiment Analysis",
          "justification": "The paper specifically addresses sentiment analysis tasks for multiple languages.",
          "quote": "AfriSenti-SemEval targets sentiment analysis in low-resource African languages."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "AfroXLMR",
          "justification": "AfroXLMR was one of the best performing models according to the paper.",
          "quote": "AfroXLMR (Alabi et al., 2022), an Africa-centric model, was the best performing model in both Tasks A (monolingual) and B (multilingual) with an average weighted F1 score of 71.30% and 75.06% respectively."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model was used in the study but not developed by the authors of this paper.",
          "quote": "AfroXLMR (Alabi et al., 2022)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments in the tasks.",
          "quote": "AfroXLMR (Alabi et al., 2022), an Africa-centric model, was the best performing model in both Tasks A (monolingual) and B (multilingual)"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of AfroXLMR was compared with many other models in the paper.",
          "quote": "The top-ranked teams for the different subtasks used pre-trained language models (PLMs). In particular, AfroXLMR (Alabi et al., 2022), an Africa-centric model, was the best performing model in both Tasks A (monolingual) and B (multilingual)"
        },
        "referenced_paper_title": {
          "value": "Adapting Pretrained Language Models to African Languages via Multilingual Adaptive Fine-Tuning",
          "justification": "This is the referenced paper where AfroXLMR is described.",
          "quote": "AfroXLMR (Alabi et al., 2022)"
        }
      },
      {
        "name": {
          "value": "AfriBERTa",
          "justification": "AfriBERTa was mentioned as one of the models used in the tasks.",
          "quote": "PLMs such as XLM-R (Conneau et al., 2020), mDeBERTaV3 (He et al., 2021), AfriBERTa (Ogueji et al., 2021), AfroXLMR (Alabi et al., 2022) and XLM-T (Barbieri et al., 2022) provide state-of-the-art performance for sentiment classification in different languages."
        },
        "aliases": [],
        "is_contributed": {
          "value": 0,
          "justification": "The model was used in the study but not developed by the authors.",
          "quote": "AfriBERTa (Ogueji et al., 2021)"
        },
        "is_executed": {
          "value": 1,
          "justification": "The model was executed as part of the experiments in the tasks.",
          "quote": "PLMs such as XLM-R (Conneau et al., 2020), mDeBERTaV3 (He et al., 2021), AfriBERTa (Ogueji et al., 2021)"
        },
        "is_compared": {
          "value": 1,
          "justification": "The performance of AfriBERTa was compared with many other models in the paper.",
          "quote": "The top-ranked teams for the different subtasks used pre-trained language models (PLMs)."
        },
        "referenced_paper_title": {
          "value": "Small Data? No Problem! Exploring the Viability of Pretrained Multilingual Language Models for Low-resourced Languages",
          "justification": "This is the referenced paper where AfriBERTa is described.",
          "quote": "AfriBERTa (Ogueji et al., 2021)"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "AfriSenti",
          "justification": "The paper introduced the AfriSenti dataset for the tasks described.",
          "quote": "We provide researchers interested in African NLP with 110K sentiment-labeled tweets that were collected using the Twitter API. These tweets are in 14 languages"
        },
        "aliases": [],
        "role": "contributed",
        "referenced_paper_title": {
          "value": "AfriSenti: A Twitter Sentiment Analysis Benchmark for African Languages",
          "justification": "This is the referenced paper where AfriSenti is described.",
          "quote": "For more information on the AfriSenti dataset collection, annotation, and linguistic challenges, please refer to the AfriSenti dataset paper (Muhammad et al., 2023)."
        }
      },
      {
        "name": {
          "value": "NaijaSenti",
          "justification": "NaijaSenti was used as additional data for some models in the tasks.",
          "quote": "AfriBERTa PLM trained on the NaijaSenti dataset (Muhammad et al., 2022)"
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis",
          "justification": "This is the referenced paper where NaijaSenti is described.",
          "quote": "NaijaSenti dataset (Muhammad et al., 2022)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Huggingface Transformers",
          "justification": "Huggingface's library was used for implementation in the tasks.",
          "quote": "Models were implemented using the Huggingface Transformers library."
        },
        "aliases": [
          "Transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "Transformers: State-of-the-Art Natural Language Processing",
          "justification": "The citation is for Huggingface's Transformers library.",
          "quote": "huggingface/transformers"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1444,
    "prompt_tokens": 25167,
    "total_tokens": 26611
  }
}