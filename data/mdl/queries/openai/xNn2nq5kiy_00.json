{
  "paper": "xNn2nq5kiy.txt",
  "words": 11060,
  "extractions": {
    "title": {
      "value": "Plan-based Prompting Improves Literature Review Generation",
      "justification": "This value is directly obtained from the title of the research paper under review.",
      "quote": "Plan-based Prompting Improves Literature Review Generation"
    },
    "description": "This paper explores the capability of large language models (LLMs) in generating literature reviews for scientific research papers, particularly focusing on a novel strategy of creating an intermediate plan before generating the actual text. The study demonstrates that this intermediate planning step improves the quality of the literature reviews generated by LLMs compared to vanilla zero-shot generation. Additionally, the authors introduce a new corpus consisting of recent arXiv papers and extend the existing Multi-XScience dataset to include full-text research papers rather than just abstracts.",
    "type": {
      "value": "empirical",
      "justification": "The paper conducts experiments to evaluate the effectiveness of plan-based prompting in literature review generation using empirical data from various datasets.",
      "quote": "our empirical study shows that these intermediate plans improve the quality of generated literature reviews over vanilla zero-shot generation"
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper deals with literature review generation, which involves tasks related to natural language understanding and generation, falling under the broader field of Natural Language Processing (NLP).",
        "quote": "We explore the zero-shot abilities of recent large language models (LLMs) for the task of writing the literature review of a scientific research paper conditioned on its abstract and the content of related papers."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text Generation",
          "justification": "The primary focus of the paper is on generating literature reviews, which is a task related to text generation within NLP.",
          "quote": "We propose and examine a novel strategy for literature review generation with an LLM"
        },
        "aliases": [
          "Generation"
        ]
      },
      {
        "name": {
          "value": "Prompt Engineering",
          "justification": "The paper introduces and evaluates a novel strategy involving intermediate planning for literature review generation, which falls under the domain of prompt engineering.",
          "quote": "Our prompt engineering techniques are reminiscent of the traditional modular pipelines of Natural Language Generation"
        },
        "aliases": [
          "Prompting"
        ]
      },
      {
        "name": {
          "value": "Multi-Document Summarization",
          "justification": "The task involves synthesizing information from multiple documents (research papers) into a cohesive literature review, fitting into the multi-document summarization sub-field.",
          "quote": "summarizing and/or contextualizing the content of all papers to be cited represents a challenging variation of the classical \\"
        },
        "aliases": [
          "Summarization"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "BART",
          "justification": "BART is explicitly mentioned as a model with strong results in summarization tasks, relevant to the paper's discussion on literature review generation.",
          "quote": "Many other subsequent methods such as the well-known BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020) models have also demonstrated strong results"
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "BART is referenced as an existing model with strong results, not as a new model introduced by this study.",
          "quote": "Many other subsequent methods such as the well-known BART (Lewis et al., 2020)…"
        },
        "is_executed": {
          "value": false,
          "justification": "The paper does not explicitly indicate that BART was executed during their experiments.",
          "quote": "demonstrated strong results"
        },
        "is_compared": {
          "value": true,
          "justification": "BART is mentioned in the context of its performance on similar tasks, indicating a comparison by reference.",
          "quote": "demonstrated strong results for the tasks of news and patent summarization"
        },
        "referenced_paper_title": {
          "value": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
          "justification": "This information is referenced within the paper, indicating the source study for the BART model.",
          "quote": "Lewis et al., 2020"
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Multi-XScience",
          "justification": "The Multi-XScience dataset is used extensively in the experiments conducted in the paper.",
          "quote": "we use the Multi-XScience dataset (Lu et al., 2020) for our experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Multi-XScience: A Large-Scale Dataset for Extreme Multi-Document Summarization of Scientific Articles",
          "justification": "This is the original reference for the dataset used within the study.",
          "quote": "(Lu et al., 2020)"
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "PyTorch",
          "justification": "PyTorch is used for the experiments in the study.",
          "quote": "We use PyTorch (Paszke et al., 2017) for our experiments."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Automatic differentiation in PyTorch",
          "justification": "This provides the exact reference for the PyTorch library as used in the study.",
          "quote": "(Paszke et al., 2017)"
        }
      },
      {
        "name": {
          "value": "HuggingFace Transformers",
          "justification": "The HuggingFace Transformers library is used within the study for implementing LLMs.",
          "quote": "we use HuggingFace Transformers library with bitsandbytes integration which allows for mixed-precision, quantized and LoRA training"
        },
        "aliases": [
          "Transformers"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "HuggingFace’s Transformers: State-of-the-art Natural Language Processing",
          "justification": "This provides the exact reference for the HuggingFace Transformers library as used in the study.",
          "quote": "(Wolf et al., 2019)"
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1146,
    "prompt_tokens": 20029,
    "total_tokens": 21175
  }
}