{
  "paper": "9db1e4c0b3de2997ad8cc2c6bdad83e5.txt",
  "words": 10184,
  "extractions": {
    "title": {
      "value": "Near-Optimal Glimpse Sequences for Improved Hard Attention Neural Network Training",
      "justification": "The title is explicitly stated on the first page of the paper.",
      "quote": "Near-Optimal Glimpse Sequences for Improved Hard Attention Neural Network Training"
    },
    "description": "The paper introduces a methodology for framing hard attention in image classification as a Bayesian optimal experimental design problem, proposing a new approach that uses 'near-optimal' sequences of attention locations to improve model training.",
    "type": {
      "value": "empirical",
      "justification": "The paper involves experiments with datasets (CelebA-HQ and Caltech-UCSD Birds) and evaluates a proposed method against baselines.",
      "quote": "We empirically investigate the performance of PS-NOGS and find that it speeds up training compared to our baselines, and leads to qualitatively different behaviour with competitive accuracy."
    },
    "primary_research_field": {
      "name": {
        "value": "Computer Vision",
        "justification": "The research focuses on developing methods for improving hard attention mechanisms in image classification, which is a central task in the field of Computer Vision.",
        "quote": "Hard visual attention is a promising approach to reduce the computational burden of modern computer vision methodologies."
      },
      "aliases": [
        "CV"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Image Classification",
          "justification": "The paper discusses improving hard attention for the specific task of image classification.",
          "quote": "Our focus in this paper is on hard attention mechanisms. Their foremost advantage is the ability to solve certain tasks using orders of magnitude less sensor bandwidth and computation than the alternatives."
        },
        "aliases": []
      }
    ],
    "models": [
      {
        "name": {
          "value": "Recurrent Attention Model (RAM)",
          "justification": "The paper mentions RAM as a baseline model for comparison in experiments involving hard attention mechanisms.",
          "quote": "RAM baseline On both experiments, we train our architecture with the algorithm used for the recurrent attention model (RAM) of Mnih et al. [6] as a baseline."
        },
        "aliases": [
          "RAM"
        ],
        "is_contributed": {
          "value": false,
          "justification": "RAM is used as a baseline and not introduced as a novel contribution in this paper.",
          "quote": "RAM baseline On both experiments, we train our architecture with the algorithm used for the recurrent attention model (RAM) of Mnih et al. [6] as a baseline."
        },
        "is_executed": {
          "value": true,
          "justification": "The model is used in experiments as a baseline for evaluating the proposed approach.",
          "quote": "RAM baseline On both experiments, we train our architecture with the algorithm used for the recurrent attention model (RAM) of Mnih et al. [6] as a baseline."
        },
        "is_compared": {
          "value": true,
          "justification": "The model is compared against the proposed method (PS-NOGS) to demonstrate improvements.",
          "quote": "We compare this to our method of partially-supervising training using near-optimal glimpse sequences (PS-NOGS)."
        },
        "referenced_paper_title": {
          "value": "Recurrent models of visual attention",
          "justification": "The referenced paper titled 'Recurrent models of visual attention' by Mnih et al. is cited for RAM.",
          "quote": "RAM baseline On both experiments, we train our architecture with the algorithm used for the recurrent attention model (RAM) of Mnih et al. [6] as a baseline."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "CelebA-HQ",
          "justification": "CelebA-HQ is explicitly mentioned as one of the datasets used for evaluating the proposed method.",
          "quote": "Datasets and network architectures We test our approach on CelebA-HQ [29] and..."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Progressive growing of gans for improved quality, stability, and variation",
          "justification": "The referenced paper 'Progressive growing of gans for improved quality, stability, and variation' is associated with CelebA-HQ dataset.",
          "quote": "We test our approach on CelebA-HQ [29] and ..."
        }
      },
      {
        "name": {
          "value": "Caltech-UCSD Birds 200",
          "justification": "The Caltech-UCSD Birds 200 dataset is used for experiments to test the proposed method in another domain of image classification.",
          "quote": "We test our approach on ... a cropped variant of Caltech-UCSD Birds (CUB) [30], ..."
        },
        "aliases": [
          "CUB"
        ],
        "role": "used",
        "referenced_paper_title": {
          "value": "The Caltech-UCSD Birds-200-2011 Dataset",
          "justification": "The datasetâ€™s title is given in the references as 'The Caltech-UCSD Birds-200-2011 Dataset'.",
          "quote": "We test our approach on ... a cropped variant of Caltech-UCSD Birds (CUB) [30], ..."
        }
      }
    ],
    "libraries": [
      {
        "name": {
          "value": "Pyro",
          "justification": "Pyro is mentioned in the context of implementing Hamiltonian Monte Carlo for a component of the proposed methodology.",
          "quote": "We used an implementation of HMC in Pyro [43], a probabilistic programming language."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pyro: Deep universal probabilistic programming",
          "justification": "The reference 'Pyro: Deep universal probabilistic programming' is related to the usage of the Pyro library.",
          "quote": "We used an implementation of HMC in Pyro [43], a probabilistic programming language."
        }
      }
    ]
  },
  "usage": {
    "completion_tokens": 1088,
    "prompt_tokens": 17213,
    "total_tokens": 18301,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    }
  }
}