{
  "paper": "2404.05961.txt",
  "words": 12561,
  "extractions": {
    "title": {
      "value": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders",
      "justification": "The title is explicitly mentioned at the very beginning of the paper.",
      "quote": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"
    },
    "description": "The paper introduces LLM2Vec, an approach to transform decoder-only large language models (LLMs) into strong text encoders. It modifies LLMs using bidirectional attention, masked next token prediction, and unsupervised contrastive learning, and demonstrates improved performance on text embedding tasks compared to existing models.",
    "type": {
      "value": "empirical",
      "justification": "The paper includes both the proposal of a new method, LLM2Vec, and experimental validation against benchmarks such as MTEB, demonstrating its empirical focus.",
      "quote": "We apply LLM2Vec to 3 decoder-only LLMs ranging from 1.3B to 7B parameters and evaluate the resulting models on word- and sequence-level tasks."
    },
    "primary_research_field": {
      "name": {
        "value": "Natural Language Processing",
        "justification": "The paper focuses on transforming language models for text encoding tasks, which is a central area in Natural Language Processing.",
        "quote": "Large decoder-only language models (LLMs) are the state-of-the-art models on most of todayâ€™s NLP tasks and benchmarks."
      },
      "aliases": [
        "NLP"
      ]
    },
    "sub_research_fields": [
      {
        "name": {
          "value": "Text Embeddings",
          "justification": "The paper develops LLM2Vec, an approach for modifying LLMs to improve text embedding capabilities.",
          "quote": "LLM2Vec consists of three simple steps: 1) enabling bidirectional attention, 2) masked next token prediction, and 3) unsupervised contrastive learning."
        },
        "aliases": []
      },
      {
        "name": {
          "value": "Machine Learning",
          "justification": "The paper involves developing methods to enhance the capabilities of ML models like LLMs for specific tasks.",
          "quote": "Our strong empirical results and extensive analysis demonstrate that LLMs can be effectively transformed into universal text encoders."
        },
        "aliases": [
          "ML"
        ]
      }
    ],
    "models": [
      {
        "name": {
          "value": "LLM2Vec",
          "justification": "LLM2Vec is the primary model introduced and analyzed throughout the paper.",
          "quote": "In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder."
        },
        "aliases": [],
        "is_contributed": {
          "value": true,
          "justification": "LLM2Vec is a new contribution proposed in this research to enhance text encoding through modification of existing LLMs.",
          "quote": "In this work, we introduce LLM2Vec, a simple unsupervised approach that can transform any decoder-only LLM into a strong text encoder."
        },
        "is_executed": {
          "value": true,
          "justification": "The paper includes implementation details and practical experiments with LLM2Vec, indicating execution.",
          "quote": "We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed models on English word- and sequence-level tasks."
        },
        "is_compared": {
          "value": true,
          "justification": "The paper numerically evaluates LLM2Vec against existing state-of-the-art models on benchmarks like MTEB.",
          "quote": "On the Massive Text Embeddings Benchmark (MTEB), LLM2Vec-transformed models set a new state-of-the-art for unsupervised models."
        },
        "referenced_paper_title": {
          "value": "",
          "justification": "LLM2Vec is introduced in this paper, so there is no external reference.",
          "quote": ""
        }
      },
      {
        "name": {
          "value": "Mistral-7B",
          "justification": "Mistral-7B is one of the specific LLMs used in the experiments to demonstrate LLM2Vec's effectiveness.",
          "quote": "We apply LLM2vec to 3 decoder-only LLMs ranging from 1.3B to 7B parameters (S-LLaMA-1.3B, LLaMA-2-7B, and Mistral-7B)..."
        },
        "aliases": [],
        "is_contributed": {
          "value": false,
          "justification": "Mistral-7B is an existing model used for experiments, not contributed by this paper.",
          "quote": "We apply LLM2vec to 3 decoder-only LLMs ranging from 1.3B to 7B parameters (S-LLaMA-1.3B, LLaMA-2-7B, and Mistral-7B)..."
        },
        "is_executed": {
          "value": true,
          "justification": "Mistral-7B was executed in experiments with the LLM2Vec method.",
          "quote": "We apply LLM2vec to 3 decoder-only LLMs ranging from 1.3B to 7B parameters..."
        },
        "is_compared": {
          "value": true,
          "justification": "Mistral-7B's performance with and without LLM2Vec is compared across tasks.",
          "quote": "We demonstrate the effectiveness of LLM2Vec by applying it to 3 popular LLMs... On the Massive Text Embeddings Benchmark (MTEB), LLM2Vec-transformed models set a new state-of-the-art for unsupervised models."
        },
        "referenced_paper_title": {
          "value": "Mistral 7B",
          "justification": "Mistral-7B is previously referenced for its architecture and performance characteristics, though this detailed reference isn't included in the provided text.",
          "quote": "...an intriguing property of Mistral-7B, the currently best performing architecture on MTEB."
        }
      }
    ],
    "datasets": [
      {
        "name": {
          "value": "Wikitext-103",
          "justification": "The paper mentions using Wikitext-103 as part of the training data for the MNTP step.",
          "quote": "Specifically, we use the Wikitext-103 dataset (Merity et al., 2017) for the MNTP step."
        },
        "aliases": [],
        "role": "used",
        "referenced_paper_title": {
          "value": "Pointer Sentinel Mixture Models",
          "justification": "The reference paper 'Pointer Sentinel Mixture Models' by Merity et al. is explicitly mentioned alongside Wikitext-103.",
          "quote": "Merity et al., 2017"
        }
      }
    ],
    "libraries": []
  },
  "usage": {
    "completion_tokens": 1278,
    "prompt_tokens": 25899,
    "total_tokens": 27177,
    "completion_tokens_details": {
      "accepted_prediction_tokens": null,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": null
    },
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 1152
    }
  }
}