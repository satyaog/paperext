High-Probability Bounds for Stochastic Optimization and
Variational Inequalities: the Case of Unbounded Variance

arXiv:2302.00999v2 [math.OC] ^{18} ^{Jul} ^{2023}

Abdurakhmon Sadiev ^{1} Marina Danilova ^{2} Eduard Gorbunov ^{3} Samuel Horv√°th ^{3} Gauthier Gidel ^{4} ^{5}
Pavel Dvurechensky ^{6} Alexander Gasnikov ^{2} ^{7} ^{8} Peter Richt√°rik ^{1}

1. Introduction

Abstract

Training of machine learning models is usually per-
formed via stochastic first-order optimization methods, e.g.,
Stochastic Gradient Descent (SGD) (Robbins & Monro,
1951)
x ^{k+1} = x ^{k} ‚àí Œ≥‚àáf Œæ k (x ^{k} ),
(1)

During recent years the interest of optimiza-
tion and machine learning communities in high-
probability convergence of stochastic optimiza-
tion methods has been growing. One of the main
reasons for this is that high-probability complex-
ity bounds are more accurate and less studied
than in-expectation ones. However, SOTA high-
probability non-asymptotic convergence results
are derived under strong assumptions such as
the boundedness of the gradient noise variance
or of the objective‚Äôs gradient itself. In this pa-
per, we propose several algorithms with high-
probability convergence results under less restric-
tive assumptions. In particular, we derive new
high-probability convergence results under the
assumption that the gradient/operator noise has
bounded central Œ±-th moment for Œ± ‚àà (1, 2] in
the following setups: (i) smooth non-convex /
Polyak-≈Åojasiewicz / convex / strongly convex
/ quasi-strongly convex minimization problems,
(ii) Lipschitz / star-cocoercive and monotone /
quasi-strongly monotone variational inequalities.
These results justify the usage of the considered
methods for solving problems that do not fit stan-
dard functional classes studied in stochastic opti-
mization.

where ‚àáf Œæ k (x ^{k} ) represents the stochastic gradient of the
objective/loss function f at point x ^{k} . Despite numerous
empirical studies and observations validating the good per-
formance of such methods, it is also important for the field
to understand their theoretical convergence properties, e.g.,
under what assumptions a method converges and what the
rate is. However, since the methods of interest are stochas-
tic, one needs to specify what type of convergence is con-
sidered before moving on to further questions.

Typically, the convergence of the stochastic methods is
studied only in expectation, i.e., for some performance met-
^{ric} ^{1} P(x), ^{upper} ^{bounds} ^{are} ^{derived} ^{for} ^{the} ^{number} ^{of} ^{iter-}
ations K needed to achieve E[P(x ^{K} )] ‚â§ Œµ, where x ^{K} is
the output of the method after K steps, Œµ is an optimization
error, and E[¬∑] is the full expectation. These bounds can
be ‚Äúblind‚Äù to some important properties like light-/heavy-
tailedness of the noise distribution and, as a result, such
guarantees do not accurately describe the methods‚Äô conver-
gence in practice (Gorbunov et al., 2020). In contrast, high-
probability convergence guarantees are more sensitive to
the noise distribution and thus are more accurate. Such
results provide upper bounds for the number of iterations
K needed to achieve P{P(x ^{K} ) ‚â§ Œµ} ‚â• 1 ‚àí Œ≤ for some
confidence level Œ≤ ‚àà (0, 1], where P{¬∑} denotes some prob-
ability measure determined by a setup.

1

King Abdullah University of Science and Technology,
KSA ^{2} Moscow Institute of Physics and Technology, Russia
3
Mohamed bin Zayed University of Artificial Intelligence, UAE
4
Universit√© de Montr√©al and Mila, Canada ^{5} Canada CIFAR AI
Chair ^{6} Weierstrass Institute for Applied Analysis and Stochas-
tics, Germany ^{7} Skolkovo Institute of Science and Technol-
ogy, Russia ^{8} Institute for Information Transmission Problems
RAS, Russia. Correspondence to: Eduard Gorbunov <ed-
uard.gorbunov@mbzuai.ac.ae>.

With the ultimate goal of bridging the theory and practice of
stochastic methods, recent works on high-probability con-
vergence guarantees (Nazin et al., 2019; Davis et al., 2021;
Gorbunov et al., 2020; 2021; 2022a; Cutkosky & Mehta,
2021) focus on an important direction of the relaxing the as-
sumptions under which these guarantees are derived. Our

Proceedings of the 40 ^{th} International Conference on Machine
Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright
2023 by the author(s).

1

Examples of performance metrics for minimization of func-
tion f : P(x) = f (x) ‚àí f (x ^{‚àó} ), P(x) = k‚àáf (x)k ^{2} , P(x) =
kx ‚àí x ^{‚àó} k ^{2} , where x ^{‚àó} ‚àà arg min x‚ààR d f (x).

1

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

(ii) for problem (3) E Œæ‚àºD [F Œæ (x)] = F (x) and

paper further extensively complements this line of works in
two main aspects: for a plethora of settings, we derive new
high-probability results allowing the variance of the noise
and the gradient of the objective to be unbounded.

Œ±

E Œæ‚àºD [kF Œæ (x) ‚àí F (x)k ] ‚â§ œÉ ^{Œ±} .

When Œ± = 2, the above assumption recovers the standard
uniformly bounded variance assumption (Nemirovski et al.,
2009; Ghadimi & Lan, 2012; 2013). However, Assump-
tion 1.1 allows the variance of the estimator to be un-
bounded when Œ± ‚àà (1, 2), i.e., the noise can follow
some heavy-tailed distribution. For example, the distribu-
tion of the gradient noise in the training of large attention
models resembles L√©vy Œ±-stable distribution with Œ± < 2
(Zhang et al., 2020b). There exist also other versions of
Assumption 1.1, see (Patel et al., 2022).

1.1. Technical Preliminaries

Before we move on to the main part of the paper, we intro-
duce the problems considered in the work and all necessary
preliminaries. In particular, we consider stochastic uncon-
strained optimization problems

min {f (x) = E Œæ‚àºD [f Œæ (x)]} ,

x‚ààR ^{d}

(2)

where Œæ is a random variable with distribution D.
Such problems often arise in machine learning, where
f Œæ (x) represents the loss function on the data sample Œæ
(Shalev-Shwartz & Ben-David, 2014).

Assumptions on f . We start with a very mild assumption
since without it, problem (2) does not make sense.
Assumption 1.2. We assume that there exist some set Q ‚äÜ
R ^{d} such that f is uniformly lower-bounded on Q: f ‚àó =
^{inf} x‚ààQ ^{f} ^{(x)} ^{>} ‚àí‚àû.

Another class of problems that we consider this work is un-
constrained variational inequality problems (VIP), i.e., non-
linear equations (Harker & Pang, 1990; Ryu & Yin, 2021):

find x ^{‚àó} ‚àà R ^{d} such that F (x ^{‚àó} ) = 0,

Moreover, when working with minimization problems (2),
we always assume smoothness of f .
Assumption 1.3. We assume that there exist some set Q ‚äÜ
R ^{d} and constant L > 0 such that for all x, y ‚àà Q

(3)

where F (x) = E Œæ‚àºD [F Œæ (x)]. These problems arise in
adversarial/game formulations of machine learning tasks
(Goodfellow et al., 2014; Gidel et al., 2019).
p
^{Notation.} ^{We} ^{use} ^{standard} ^{notation:} kxk ^{=} hx, ^{xi} ^{de-}
notes the standard Euclidean norm in R ^{d} , E Œæ [¬∑] denotes
an expectation w.r.t. the randomness coming from random
^{variable} ^{Œæ,} ^{B} R ^{(x)} ^{=} {y ‚àà ^{R} ^{d} | ky ‚àíxk ‚â§ ^{R}} ^{is} ^{a} ^{ball} ^{with}
center at x and radius R. We define restricted gap-function
as Gap R (x) = max y‚ààB R (x ‚àó ) hF (y), x ‚àí yi ‚Äì a standard
convergence criterion for monotone VIP (Nesterov, 2007).
e hides poly-
^{Finally,} O(¬∑) ^{hides} ^{numerical} ^{factors} ^{and} O(¬∑)
logarithmic and numerical factors.

k‚àáf ^{(x)} ‚àí ‚àáf ^{(y)k} ‚â§

k‚àáf ^{(x)k}

‚â§

Lkx ‚àí yk,

2L (f (x) ‚àí f ‚àó ) ,

(6)

(7)

We notice here that (7) follows from (6) for Q = R ^{d} , but in
the general case, the implication is slightly more involved
(see the details in Appendix B). When Q is a compact set,
the function f is allowed to be non-L-smooth on the whole
R ^{d} , which is related to local-Lipschitzness of the gradients
(Patel et al., 2022; Patel & Berahas, 2022).

In each particular special case, we also make one of the fol-
lowing assumptions about the structured non-convexity of
the objective function. The previous two assumptions hold
for a very broad class of functions. The next assumption ‚Äì
Polyak-≈Åojasiewicz condition (Polyak, 1963; Lojasiewicz,
1963) ‚Äì narrows the class of non-convex functions.
Assumption 1.4. We assume that there exist some set
Q ‚äÜ R ^{d} and constant Œº > 0 such that f satisfies Polyak-
≈Åojasiewicz (P≈Å) condition/inequality on Q, i.e., for all
x ‚àà Q and x ‚àó = arg min x‚ààR d f (x)

Stochastic oracle. We assume that at given point x we
have an access to the unbiased stochastic oracle returning
‚àáf Œæ (x) or F Œæ (x) that satisfy the following conditions.

k‚àáf ^{(x)k} ^{2} ‚â• ^{2Œº} ^{(f} ^{(x)} ‚àí ^{f} ^{(x} ^{‚àó} ^{))} ^{.}

Assumption 1.1. We assume that there exist some set Q ‚äÜ
R ^{d} and values œÉ ‚â• 0, Œ± ‚àà (1, 2] such that for all x ‚àà Q

(8)

When function f is Œº-strongly convex, it satisfies P≈Å condi-
tion. However, P≈Å inequality can hold even for non-convex
functions. Some analogs of this assumption have been ob-
served for over-parameterized models (Liu et al., 2022).

(i) for problem (2) E Œæ‚àºD [‚àáf Œæ (x)] = ‚àáf (x) and

Œ±

2

^{where} ^{f} ‚àó ^{=} ^{inf} x‚ààQ ^{f} ^{(x)} ^{>} ‚àí‚àû.

Assumptions on a subset. Although we consider uncon-
strained problems, our analysis does not require any as-
sumptions to hold on the whole space. For our purposes,
it is sufficient to introduce all assumptions only on some
subset of R ^{d} , since we prove that the considered methods
do not leave some ball around the solution or some level-set
of the objective function with high probability. This allows
us to consider quite large classes of problems.

E Œæ‚àºD [k‚àáf Œæ (x) ‚àí ‚àáf (x)k ] ‚â§ œÉ ^{Œ±} ,

(5)

(4)

We also consider another relaxation of convexity.

2

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Assumption 1.5. We assume that there exist some set
Q ‚äÜ R ^{d} and constant Œº ‚â• 0 such that f is Œº-quasi-strongly
convex, i.e., for all x ‚àà Q and x ‚àó = arg min x‚ààR d f (x)

^{f} ^{(x} ^{‚àó} ^{)} ‚â• ^{f} ^{(x)} ^{+} h‚àáf ^{(x),} ^{x} ^{‚àó} ‚àí ^{xi} ^{+}

Another structured non-monotonicity assumption that we
consider in this paper is star-cocoercivity.
Assumption 1.10. We assume that there exist some set
Q ‚äÜ R ^{d} and constant l > 0 such that F is star-cocoercive
on Q, i.e., for all x ‚àà Q and x ‚àó such that F (x ‚àó ) = 0

Œº
kx ‚àí x ^{‚àó} k ^{2} . (9)
2

kF (x)k ^{2} ‚â§ lhF (x), x ‚àí x ^{‚àó} i.

As P≈Å condition, this assumption holds for any Œº-strongly
convex function but does not imply convexity. Neverthe-
less, for the above two assumptions, some standard deter-
ministic methods such as Gradient Descent (GD) converge
linearly; see more details and examples in (Necoara et al.,
2019).

(14)

This assumption can be seen as a relaxation of the stan-
dard cocoercivity: kF (x) ‚àí F (y)k ^{2} ‚â§ lhF (x) ‚àí F (y), x ‚àí
yi. However, unlike cocoercivity, star-cocoercivity im-
plies neither monotonicity nor Lipschitzness of operator F
(Loizou et al., 2021, Appendix A.6).

In the analysis of the accelerated method, we also need stan-
dard (strong) convexity.

1.2. Closely Related Works and Our Contributions

Assumption 1.6. We assume that there exist some set Q ‚äÜ
R ^{d} and constant Œº ‚â• 0 such that f is Œº-strongly convex,
i.e., for all x, y ‚àà Q

In this subsection, we overview closely related works and
describe the contributions of our work. Additional related
works are discussed in Appendix A.

^{f} ^{(y)} ‚â• ^{f} ^{(x)} ^{+} h‚àáf ^{(x),} ^{y} ‚àí ^{xi} ^{+}

Œº
ky ‚àí xk 2 .
2

(10)

Convex optimization and monotone VIPs. Classical
high-probability results for (strongly) convex minimization
(Nemirovski et al., 2009; Ghadimi & Lan, 2012) and mono-
tone VIP (Juditsky et al., 2011) are derived under the so-
called light-tails assumption, meaning that the noise in
the stochastic gradients/operators is assumed to be sub-
2
Gaussian: E Œæ‚àºD [exp( k‚àáf Œæ ^{(x)‚àí‚àáf} ^{(x)k} / œÉ ^{2} )] ‚â§ exp(1) or
2
E Œæ‚àºD [exp( kF Œæ ^{(x)‚àíF} ^{(x)k} / œÉ ^{2} )] ‚â§ exp(1). In these settings,
optimal (up to logarithmic factors) rates of convergence are
derived in the mentioned papers.

When Œº = 0 function f is called convex.

Assumptions on F . In the context of solving (3), we as-
sume Lipschitzness of F ‚Äì a standard assumption for VIP.

Assumption 1.7. We assume that there exist some set Q ‚äÜ
R ^{d} and constant L > 0 such that for all x, y ‚àà Q

kF (x) ‚àí F (y)k ‚â§ Lkx ‚àí yk,

(11)

The first high-probability results with logarithmic depen-
dence ^{2} on ^{1} / Œ≤ under just bounded variance assumption are
given by Nazin et al. (2019), where the authors show non-
accelerated rates of convergence for a version of Mirror
Descent with a special truncation operator for smooth con-
vex and strongly convex problems defined on the bounded
sets. Then, Davis et al. (2021) derive accelerated rates in
the strongly convex case using robust distance estimation
techniques. Gorbunov et al. (2020; 2021) propose an accel-
erated method with clipping for unconstrained (strongly)
convex problems with Lipschitz / H√∂lder continuous gradi-
ents and derive the first high-probability results for clipped-
SGD. In the context of VIP, Gorbunov et al. (2022a) derive
the first high-probability results for the stochastic methods
for solving VIP under bounded variance assumption and
different assumptions on structured non-monotonicity.

Similarly to the case of minimization problems, we make
one or two of the following assumptions about the struc-
tured non-monotonicity of the operator F . The first as-
sumption we consider is the standard monotonicity.

Assumption 1.8. We assume that there exist some set Q ‚äÜ
R ^{d} such that F is monotone on Q, i.e., for all x, y ‚àà Q

hF (x) ‚àí F (y), x ‚àí yi ‚â• 0.

(12)

Monotonicity can be seen as an analog of convexity for VIP.
When (12) holds with Œºkx ‚àí yk ^{2} in the r.h.s. instead of just
0, operator F is called Œº-strongly monotone.

Next,
we
consider
quasi-strong
monotonicity
2019;
Song et al.,
2020;
(Mertikopoulos & Zhou,
Loizou et al., 2021) ‚Äì a relaxation of strong mono-
tonicity. There exist examples of non-monotone problems
such that the assumption below holds (Loizou et al., 2021,
Appendix A.6).

2
Note that from in-expectation convergence guarantee, one
can always get a high-probability one using Markov‚Äôs inequality.
For example, under bounded variance, smoothness, and strong
convexity assumptions SGD achieves Ekx ^{k} ‚àí x ^{‚àó} k ^{2} ‚â§ Œµ after
e
^{L} / Œº , ^{œÉ} ^{2} / ŒºŒµ }) iterations. Therefore, taking k such
k = O(max{
k
that Ekx ‚àí x ^{‚àó} k ^{2} ‚â§ ŒµŒ≤ we get from Markov‚Äôs inequality that
P{kx ^{k} ‚àí x ^{‚àó} k ^{2} ‚â§ Œµ} ‚â§ Œ≤. However, in this case, we get bound
e
^{L} / Œº , ^{œÉ} ^{2} / ŒºŒµŒ≤ }), having undesirable inverse-power
k = O(max{
dependence on Œ≤.

Assumption 1.9. We assume that there exist some set
Q ‚äÜ R ^{d} and constant Œº > 0 such that F is Œº-quasi
strongly monotone on Q, i.e., for all x ‚àà Q and x ‚àó such
that F (x ‚àó ) = 0 we have

hF (x), x ‚àí x ‚àó i ‚â• Œºkx ‚àí x ‚àó k ^{2} .

(13)

3

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Table 1: Summary of known and new high-probability complexity results for solving smooth problem (2). Column 0 ‚ÄúSetup‚Äù ‚àó indicates the assumptions made in addition

to Assumptions 1.1 and 1.3. All assumptions are made only on some ball around the solution with radius ‚àº R ‚â• kx ‚àí x k (unless the opposite is indicated). By the
complexity we mean the number of stochastic oracle calls needed for a method to guarantee that P{Metric ‚â§ Œµ} ‚â• 1 ‚àí Œ≤ for some Œµ > 0, Œ≤ ‚àà (0, 1] and ‚ÄúMetric‚Äù
is taken from the corresponding column. For simplicity, we omit numerical and logarithmic factors in the complexity bounds. Column ‚ÄúŒ±‚Äù shows the allowed values of Œ±,
‚ÄúUD?‚Äù shows whether the analysis works on unbounded domains, and ‚ÄúUG?‚Äù indicates whether the analysis works without assuming boundedness of the gradient. Notation:
L = Lipschitz constant; D = diameter of the domain (for the result from (Nazin et al., 2019)); œÉ = parameter from Assumption 1.1; R = any upper bound on kx ^{0} ‚àí x ^{‚àó} k; Œº
^{=} ^{(quasi-)strong} ^{convexity/Polyak-≈Åojasiewicz} ^{parameter;} ^{‚àÜ} ^{=} ^{any} ^{upper} ^{bound} ^{on} ^{f} ^{(x} ^{0} ^{)} ‚àí ^{f} ‚àó ^{;} ^{G} ^{=} ^{parameter} ^{such} ^{that} ^{E} Œæ‚àºD k‚àáf Œæ ^{(x)k} ^{Œ±} ‚â§ ^{G} ^{Œ±} ^{(for} ^{the} ^{result} ^{from}
(Cutkosky & Mehta, 2021)). The results of this paper are highlighted in blue.

Setup

Method

Citation

Metric

RSMD

(Nazin et al., 2019) ^{(1)}
(Gorbunov et al., 2020)
(Gorbunov et al., 2021)
(Gorbunov et al., 2020)
(Gorbunov et al., 2021)

f (x K ) ‚àí f (x ‚àó )

clipped-SGD

As. 1.6
(Œº = 0)

As. 1.6
(Œº > 0)

clipped-SSTM

(2)

K

‚àó

f (y ) ‚àí f (x )

f (x ) ‚àí f (x )

Theorems 3.2 & F.2

f (y K ) ‚àí f (x ‚àó )

(Nazin et al., 2019)

proxBoost

(Davis et al., 2021) ^{(1)}
(Gorbunov et al., 2020)
(Gorbunov et al., 2021)
(Gorbunov et al., 2020)
(Gorbunov et al., 2021)

R-clipped-SGD

clipped-SGD

clipped-NMSGD

clipped-SGD

K

(1)

restarted-RSMD

clipped-SGD

As. 1.4

‚àó

Theorems 3.1 & E.6

MSGD

(1)

K

f (x ) ‚àí f (x )

clipped-SGD

R-clipped-SSTM

As. 1.2

‚àó

clipped-SSTM

R-clipped-SSTM

As. 1.5
(Œº > 0)

K

f (x ) ‚àí f (x )

f (x K ) ‚àí f (x ‚àó )

K

‚àó

K

‚àó

K

‚àó 2

f (y ) ‚àí f (x )

kx

(Li & Orabona, 2020) ^{(1)}

Theorems 3.1 & E.4 ^{(5)}

‚àó

f (y ) ‚àí f (x )

Theorems 3.1 & E.8

Theorems 3.1 & E.2 ^{(5)}

K

f (x ) ‚àí f (x )

Theorems 3.2 & F.3

(Cutkosky & Mehta, 2021) ^{(1)}

‚àó

1
K+1



1
K+1

1
K+1

K
P

k=0
K
P

k=0
K
P

‚àí x k

k‚àáf ^{(x} ^{k} ^{)k} ^{2}
 2
(4)
k‚àáf ^{(x} ^{k} ^{)k}

k=0

k‚àáf ^{(x} ^{k} ^{)k} ^{2}

f (x K ) ‚àí f (x ‚àó )

Complexity
o
n
LD 2 œÉ 2 D 2
Œµ ,
Œµ 2
o
n
2 œÉ 2 R 2
max ^{LR}
Œµ ,
Œµ 2
q

LR 2 œÉ 2 R 2
max
2
Œµ ,
Œµ
n
 Œ± o
2
Œ±‚àí1
max ^{LR}
, œÉR
Œµ
Œµ

q
Œ±

2
LR
^{œÉR} Œ±‚àí1
max
Œµ ,
Œµ
o
n
2
max ^{L}
, œÉ
nq Œº ŒºŒµ 2 o
(2)
L œÉ
max
Œº , ŒºŒµ
o
n
œÉ 2
max ^{L}
Œº , ŒºŒµ
nq
o
L œÉ 2
max
Œº , ŒºŒµ
q
 2  Œ± 
L
2(Œ±‚àí1)
œÉ
max
Œº ,
ŒºŒµ


 2  Œ±
2(Œ±‚àí1)
œÉ
max ^{L}
Œº , Œº 2 Œµ
o
n 2 2
4
max ^{L} Œµ ^{‚àÜ} , ^{œÉ}
Œµ 2

max

max

max





 2  3Œ±‚àí2
2Œ±‚àí2
G
Œµ

L‚àÜ
Œµ ,

L
Œº ,

 ‚àö



L‚àÜœÉ
Œµ

LœÉ 2
Œº 2 Œµ





Œ± 
Œ±‚àí1


Œ±
2(Œ±‚àí1)

Œ±

UD?

UG?

2

‚úó

‚úì

2

‚úì

‚úì

2

‚úì

‚úì

(1, 2]

‚úì

‚úì

(1, 2]

‚úì

‚úì

2

‚úó

‚úì

2

‚úì

‚úì

2

‚úì

‚úì

2

‚úì

‚úì

(1, 2]

‚úì

‚úì

(1, 2]

‚úì

‚úì

‚úó (3)

‚úì

‚úì

(1, 2]

‚úì

‚úó

(1, 2]

‚úì

‚úì

(1, 2]

‚úì

‚úì

All assumptions are made on the whole domain.
Complexity has extra logarithmic factor of ln( ^{L} / Œº ).

h

i
Li & Orabona (2020) assume that the noise is sub-Gaussian: E exp k‚àáfŒæ ^{(x)‚àí‚àáf} ^{(x)k} ^{2} / œÉ ^{2}
‚â§ exp(1) for all x from the domain.
 2

P K
P K
(4)
k
k
2
1
1
^{We} ^{notice} ^{that} K+1
k‚àáf
(x
)k
‚â§
k‚àáf
(x
)k
and
in
the
worst
case
the left-hand side is K + 1 times smaller than the right-hand
k=0
k=0
K+1
side.
‚àö
‚àö
(5)
All assumptions are made on the level set Q = {x ‚àà R ^{d} | ‚àÉy ‚àà R ^{d} : f (y) ‚â§ f ‚àó + 2‚àÜ and kx ‚àí yk ‚â§ ^{‚àÜ} / 20 L }.

(3)

in-expectation lower bound (Zhang et al., 2020b) and de-
terministic lower bound (Nemirovskij & Yudin, 1983). In
other words, we derive the first optimal high-probability
complexity results for smooth strongly convex optimiza-
tion. Noticeably, the derived results have clear separa-
tion between accelerated part and stochastic part that em-
phasizes a potential of clipped-SSTM for efficient par-
allelization. Next, we derive high-probability results for
clipped-SGD for smooth star-convex and quasi-strongly
convex objectives under Assumption 1.1. Finally, under
the same assumption, we prove the high-probability conver-
gence of Clipped Stochastic Extragradient (clipped-SEG)
(Korpelevich, 1976; Juditsky et al., 2011; Gorbunov et al.,
2022a) for Lipschitz monotone and quasi-strongly mono-
tone VIP and also obtain high-probability results for
Clipped Stochastic Gradient Descent-Ascent (clipped-
SGDA) for star-cocoercive and monotone / quasi-strongly
monotone VIP. In the special case of Œ± = 2, our analy-
sis recovers SOTA high-probability results under bounded
variance assumption.

However, there are no high-probability results (with log-
arithmic dependence on the confidence level) for smooth
(strongly) convex minimization problems and Lipschitz
VIP without imposing bounded variance assumption. Only
recently, Zhang & Cutkosky (2022) derived optimal regret-
bounds under Assumption 1.1 in the convex case with
bounded gradients on R ^{d} . However, the bounded gradients
assumption is quite restrictive when assumed on the whole
space. Thus, a noticeable gap in the stochastic optimization
literature remains.

^{Contribution.} We obtain new high-probability conver-
gence results under Assumption 1.1 for smooth convex min-
imization problems and Lipschitz VIP; see the summary in
Tables 1 and 2. In particular, for Clipped Stochastic Sim-
ilar Triangles Method (clipped-SSTM) (Gorbunov et al.,
2020) and its restarted version, we derive high-probability
convergence results for smooth convex and strongly convex
problems. The high-probability complexity in the strongly
convex case matches (up to logarithmic factors) the known

4

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Table 2: Summary of known and new high-probability complexity results for solving 0 (3). Column
‚ÄúSetup‚Äù indicates the assumptions made in addition to Assumption 1.1.
‚àó

All assumptions are made only on some ball around the solution with radius ‚àº R ‚â• kx ‚àí x k (unless the opposite is indicated). By the complexity we mean the number
of stochastic oracle calls needed for a method to guarantee that P{Metric ‚â§ Œµ} ‚â• 1 ‚àí Œ≤ for some Œµ > 0, Œ≤ ‚àà (0, 1] and ‚ÄúMetric‚Äù is taken from the corresponding column.
For simplicity, we omit numerical and logarithmic factors in the complexity bounds. Column ‚ÄúŒ±‚Äù shows the allowed values of Œ±, ‚ÄúUD?‚Äù shows whether the analysis works on
P K
1
e ^{k} (for clipped-SEG),
unbounded domains, and ‚ÄúUG?‚Äù indicates whether the analysis works without assuming boundedness of the gradient. Notation: x
e K
avg ^{=} K+1
k=0 ^{x}
P
K
k
1
x K
=
x
(for
clipped-SGDA);
L
=
Lipschitz
constant;
D
=
diameter
of
the
domain
(used
in
(Juditsky
et
al.,
2011));
Gap
(x)
=
max
y‚ààX hF (y), x‚àíyi,
D
avg
k=0
K+1
where X is a bounded domain with diameter D where the problem is defined (used in (Juditsky et al., 2011)); D = diameter of the domain (for the result from (Juditsky et al.,
2011)); œÉ = parameter from Assumption 1.1; R = any upper bound on kx ^{0} ‚àí x ^{‚àó} k; Œº = quasi-strong monotonicity parameter; l = star-cocoercivity parameter. The results of
this paper are highlighted in blue.

Setup

As. 1.7 & 1.8

As. 1.7 & 1.9

As. 1.8 & 1.10

Method

Citation

Metric

Mirror-Prox

(Juditsky et al., 2011) ^{(1)}

Gap D (e
x K
avg ^{)}

clipped-SEG

(Gorbunov et al., 2022a)

Gap R (e
x K
avg ^{)}

clipped-SEG

Theorems 4.1 & G.2

Gap R (e
x K
avg ^{)}

clipped-SEG

(Gorbunov et al., 2022a)

kx k ‚àí x ‚àó k 2

clipped-SEG

Theorems 4.1 & G.4

kx k ‚àí x ‚àó k 2

clipped-SGDA

(Gorbunov et al., 2022a)

Gap R (x ^{K}
avg ^{)}

clipped-SGDA

Theorems 4.2 & H.3

Gap R (x ^{K}
avg ^{)}

clipped-SGDA

(Gorbunov et al., 2022a)

As. 1.10

clipped-SGDA

As. 1.9 & 1.10

(1)

(2)

Theorems 4.2 & H.4

clipped-SGDA

(Gorbunov et al., 2022a)

clipped-SGDA

Theorems 4.2 & H.6

1
K+1

1
K+1

K
P

k

k=0
K
P

k=0
K

kx

kF (x )k

Complexity
o
n
LD 2 œÉ 2 D 2
2
Œµ ,
Œµ
o
n
2 œÉ 2 R 2
max ^{LR}
Œµ ,
Œµ 2
n
o
Œ±

2
^{œÉR} Œ±‚àí1
max ^{LR}
Œµ ,
Œµ
o
n
œÉ 2
max ^{L}
Œº , Œº 2 Œµ

 2  Œ± ) 
œÉ
2(Œ±‚àí1
max ^{L}
Œº , Œº 2 Œµ
n 2
o
2 2
max ^{lR}
, œÉ Œµ R
2
n 2 Œµ
 Œ± o
^{œÉR} Œ±‚àí1
max ^{lR}
Œµ ,
Œµ
n 2 2 2 2 2 o
max ^{l} Œµ ^{R} , ^{l} ^{œÉ} Œµ 2 ^{R}

2

kF (x k )k 2

max

‚àó 2

‚àí x k

kx K ‚àí x ‚àó k 2

Œ±

UD?

UG?

‚úó (2)

‚úó

‚úì

‚úì

max

max

n 2

l R 2
Œµ ,

n


lœÉR

Œµ

l
œÉ 2
Œº , Œº 2 Œµ

Œ± o
Œ±‚àí1

2

‚úì

(1, 2]

‚úì

‚úì

2

‚úì

‚úì

(1, 2]

‚úì

‚úì

2

‚úì

‚úì

(1, 2]

‚úì

‚úì

2

‚úì

‚úì

(1, 2]

‚úì

‚úì

2

‚úì

‚úì

(1, 2]

‚úì

‚úì

o

max
 
 Œ± ) 
2(Œ±‚àí1
œÉ 2
l
Œº , Œº 2 Œµ

All assumptions are made on the whole domain.
h

i
Juditsky et al. (2011) assume that the noise is sub-Gaussian: E exp kFŒæ ^{(x)‚àíF} ^{(x)k} ^{2} / œÉ ^{2}
‚â§ exp(1) for all x from the domain.

Gradient clipping received a lot of attention in the ma-
chine learning community due to its successful empiri-
cal applications in the training of deep neural networks
(Pascanu et al., 2013; Goodfellow et al., 2016). The clip-
^{ping} ^{operator} ^{is} ^{defined} ^{as} ^{clip(x,} ^{Œª)} ^{=} ^{min} {1, ^{Œª} ^{/} kxk } ^{x}
(clip(x, Œª) = 0, when x = 0). From the theoretical
perspective, gradient clipping is used for multiple different
purposes: to handle structured non-smoothness in the ob-
jective function (Zhang et al., 2020a), to robustify aggrega-
tion (Karimireddy et al., 2021) and to provide privacy guar-
antees (Abadi et al., 2016) in the distributed training. More-
over, as we already mentioned before, gradient clipping
is used to handle heavy-tailed noise (satisfying Assump-
tion 1.1) in the stochastic gradients (Zhang et al., 2020b)
and, in particular, to derive better high-probability guar-
antees under bounded variance assumption (Nazin et al.,
2019; Gorbunov et al., 2020). However, there are no re-
sults showing the necessity of modifying standard meth-
ods like SGD and its accelerated variants to achieve high-
probability convergence with logarithmic dependence on
the confidence level under bounded variance assumption.

Non-convex optimization. Under the light-tails and
smoothness assumption Li & Orabona (2020) derive high-
probability convergence rates to the first-order station-
ary point for SGD. These rates match the known in-
expectation guarantees for SGD and are optimal up
to logarithmic factors (Arjevani et al., 2022). Recently,
Cutkosky & Mehta (2021) derived the first high-probability
results for non-convex optimization under Assumption 1.1
for a version of SGD with gradient clipping and normal-
ization of the momentum. The
P K results are k obtained for the
1
^{non-standard} ^{metric} ^{‚Äì} K+1
k=0 k‚àáf ^{(x} ^{)k} ^{‚Äì} ^{and} ^{match}
in-expectation lower bound for the expected (non-squared)
norm of the gradient from (Zhang et al., 2020b). However,
Cutkosky & Mehta (2021) make an additional assumption
that the norm of the gradient is bounded ^{3} on R ^{d} , which is
quite restrictive.

^{Contribution.} We derive the first high-probability result
with logarithmic dependence on the confidence level for
finding first-order stationary points of smooth (possibly,
non-convex) functions without bounded gradients assump-
tion. The result is derived for simple clipped-SGD. More-
over, we extend the analysis to the functions satisfying
Polyak-≈Åojasiewicz condition; see Table 1 for the sum-
mary.

^{Contribution.} We construct an example of a strongly con-
vex smooth problem and stochastic oracle with bounded
variance such that to achieve
P{kx ^{k} ‚àíx ‚àó k ^{2} > Œµ} ‚â§ Œ≤ SGD
‚àö 
2
œÉ
requires Œ© / Œº ŒµŒ≤ iterations, i.e., the algorithm has
inverse-power dependence on the confidence level. This
justifies the importance of using some non-linearity such
as gradient clipping to achieve logarithmic dependence on

3

1.1,
More
precisely,
instead
of
Assumption
Cutkosky & Mehta (2021) assume E Œæ‚àº k‚àáf Œæ (x)k ^{Œ±} ‚â§ G ^{Œ±}
for some G > 0. This assumption implies Assumption 1.1 and
boundedness of k‚àáf (x)k.

5

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance



2(Œ±‚àí1) /Œ± 2
Œº ‚àÜ / LœÉ 2 A 2(Œ±‚àí1) /Œ± ln 2 (B K ) } ,
Œò max{2, ^{(K+1)}

the confidence level even in the bounded variance case.

‚àö

2. Failure of Standard SGD

It is known that SGD x ^{k+1} = x ^{k} ‚àí Œ≥‚àáf Œæ k (x ^{k} ) can di-
verge in expectation, when Assumption 1.1 is satisfied with
Œ± < 2 (Zhang et al., 2020b, Remark 1). However, it does
converge in expectation when Œ± = 2, i.e., when the vari-
ance is bounded. In contrast, there are no high-probability
convergence results for SGD having logarithmic depen-
dence on ^{1} / Œ≤ . The next theorem establishes the impossi-
bility of deriving such high-probability results.

Œª k = Œò( exp(‚àíŒ≥Œº(1+ ^{k} / 2 ))R / Œ≥A ).
1 P k
k 2
^{Then} ^{to} ^{guarantee} K+1
k=0 k‚àáf ^{(x} ^{)k} ‚â§ ^{Œµ} ^{in} ^{Case} ^{1,}
K
‚àó
K
f (x ) ‚àí f (x ) ‚â§ Œµ in Case 2, f (xÃÑ ) ‚àí f (x ‚àó ) ‚â§ Œµ in
P K
1
k
K
‚àí x ‚àó k 2 ‚â§ Œµ in
^{Case} ^{3} ^{with} ^{xÃÑ} ^{K} ^{=} K+1
k=0 ^{x} ^{,} kx
Case 4 with probability ‚â• 1 ‚àí Œ≤ clipped-SGD requires
Ô£±
Ô£´
! Œ± Ô£º Ô£∂
Ô£≤ L‚àÜ ^{‚àö} L‚àÜœÉ ^{Œ±‚àí1} Ô£Ω
e Ô£≠ max
Ô£∏ (16)
Case 1: O
,
Ô£æ
Ô£≥ Œµ
Œµ
( 
 Œ± )!
^{2} 2(Œ±‚àí1)
L
LœÉ
e max
Case 2: O
(17)
,
Œº
Œº 2 Œµ
(
 Œ± )!

LR ^{2} œÉR Œ±‚àí1
e
,
Case 3: O max
(18)
Œµ
Œµ
)!
( 
Œ±
 2(Œ±‚àí1)
2
L
œÉ
e max
(19)
Case 4: O
,
Œº Œº 2 Œµ

Theorem 2.1. For any Œµ > 0 and sufficiently small Œ≤ ‚àà
(0, 1) there exist problem (2) such that Assumptions 1.1, 1.3,
and 1.6 hold with Q = R ^{d} , Œ± = 2, 0 < Œº ‚â§ L and for the
iterates produced by SGD with any stepsize Œ≥ > 0


 k
œÉ
‚àó 2
‚àö
.
P kx ‚àí x k ‚â• Œµ ‚â§ Œ≤ =‚áí k = Œ©
Œº ŒµŒ≤

The proof is deferred to Appendix D. We believe that sim-
ilar examples can be constructed for any stochastic first-
order methods having linear dependence on the stochastic
gradients in their update rules. Thus, Theorem 2.1 mo-
tivates the use of non-linear operators such as gradient
clipping in stochastic methods to achieve logarithmic de-
pendence on the confidence level in the high-probability
bounds.

oracle calls.

3. Main Results for Minimization Problems

The complete formulation of the result and full proofs are
deferred to Appendix E. As one can see from Table 1,
for Œ± = 2 the derived complexity bounds match the best-
known ones for clipped-SGD in the setups where it was an-
alyzed. Next, we emphasize that the second term under the
maximum in (19) (quasi-strongly convex functions) is opti-
mal up to logarithmic factors (Zhang et al., 2020b). In the
convex case, there are no lower bounds, but we conjecture
that the second term in (18) is optimal (up to logarithms) in
this case as well.

3.1. SGD with Clipping

We start with clipped-SGD:


x ^{k+1} = x ^{k} ‚àí Œ≥ ¬∑ clip ‚àáf Œæ k (x ^{k} ), Œª k ,

‚àö

Œª k = Œò( ^{exp(‚àíŒ≥Œº(1+} ^{k} ^{/} 2 ^{))} ^{‚àÜ} / LŒ≥A ).
Case 3.
Let Assumptions 1.1, 1.3, 1.6 with
Œº = 0 hold for Q = B 3R (x ‚àó ), R ‚â• kx ^{0} ‚àí x ‚àó k
^{and} ^{0} ^{<} ^{Œ≥} ‚â§ O(min{ ^{1} ^{/} ^{LA} ^{,} ^{R} ^{/} ^{œÉK} ^{1} ^{/Œ±} ^{A} ^{(Œ±‚àí1)} ^{/Œ±} }),
Œª k = Œª = Œò( R / Œ≥A ).
^{Case} ^{4.} Let Assumptions 1.1, 1.3, 1.5 with Œº > 0
hold for Q = B 3R (x ‚àó ), R ‚â• kx ^{0} ‚àí x ‚àó k and
^{0}  ^{<} ^{Œ≥} ^{=} O ^{(min{} ^{1} ^{/} ^{LA} ^{,} ^{ln(B} ^{K} ^{)} ^{/} ^{Œº(K+1)} }), ^{B}
 K =
2(Œ±‚àí1) /Œ± 2 2
2 2(Œ±‚àí1) /Œ±
2
(K+1)
Œº
R
Œò max{2,
/ œÉ A
ln (B K ) } ,

(15)

where Œæ ^{k} is sampled from D k independently from previous
steps. We emphasize here and below that distribution of
the noise is allowed to be dependent on k: we require just
independence of Œæ ^{k} from the the previous steps. Our main
convergence results for clipped-SGD are summarized in
the following theorem.

Next, in the case of P≈Å-functions, we are not aware of any
high-probability convergence results in the literature. In the
special case of Œ± = 2, the derived complexity bound (17)
matches the best-known in-expectation complexity bound
for SGD (Karimi et al., 2016; Khaled & Richt√°rik, 2020)
and the first term coincides (up to logarithms) with the
lower bound for deterministic first-order methods in this
setup (Yue et al., 2022).

Theorem 3.1 (Convergence of clipped-SGD). Let k ‚â• 0
and Œ≤ ‚àà (0, 1] are such that A = ln ^{4(K+1)}
‚â• 1.
Œ≤
Case 1.
Let Assumptions 1.1, 1.2, 1.3 hold for
Q = {x ‚àà R d | ‚àö
‚àÉy ‚àà R d :
f (y) ‚â§
‚àö
f ‚àó + 2‚àÜ and kx ‚àí yk ‚â§ ^{‚àÜ} / 20 ‚àö L }, ‚àÜ ‚â• f (x ^{0} ) ‚àí f  ‚àó
‚àö
and 0 < Œ≥ ‚â§ O min{ 1 / LA , ‚àÜ / œÉ LK ^{1} ^{/Œ±} A ^{(Œ±‚àí1)} ^{/Œ±} } ,
‚àö ‚àö
^{Œª} k ^{=} ^{Œª} ^{=} ^{Œò(} ^{‚àÜ} ^{/} LŒ≥A ^{).}
Case 2.
Let Assumptions 1.1, 1.3, 1.4 hold for
Q = {x ‚àà R d | ‚àö
‚àÉy ‚àà R d :
f (y) ‚â§
‚àö
f ‚àó + 2‚àÜ and kx ‚àí yk ‚â§ ^{‚àÜ} / 20 L }, ‚àÜ ‚â• f (x ^{0} ) ‚àí f ‚àó
^{and} ^{0} ^{<} ^{Œ≥} ^{=} O ^{(min{} ^{1} ^{/} ^{LA} ^{,} ^{ln(B} ^{K} ^{)} ^{/} ^{Œº(K+1)} }), ^{B} K ^{=}

Finally, in the non-convex case, bound (16) is the first
high-probability result under Assumption 1.1 without the
additional assumption of the boundedness of the gradi-
ents. For Œ± = 2 it matches (up to logarithms) in-

6

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

The derived high-probability bound matches (see the proof
in Appendix F.1) the best-known one in the case of Œ± =
2. For Œ± < 2 there are no lower bounds in the con-
vex case. However, the first term in (23) is optimal and
matches the deterministic lower bound in the convex case
(Nemirovskij & Yudin, 1983). The second term is the same
as in the bound for clipped-SGD (18) and we conjecture
that it cannot be improved.

expectation lower bound (Arjevani et al., 2022). How-
ever, when Œ±  < 2, bound (16)
 is inferior to the ex-

e G ^{2} / Œµ ^{(3Œ±‚àí2)} ^{/} ^{2(Œ±‚àí1)} by Cutkosky & Mehta
isting one O
(2021), which relies on the stronger assumption that
^{E} Œæ‚àºD k‚àáf Œæ ^{(x)k} ^{Œ±} ‚â§ ^{G} ^{Œ±} ^{for} ^{some} ^{G} ^{>} ^{0} ^{and} ^{all}
x ‚àà R ^{d} , and also do not match the lower bound
by Zhang et al. (2020b) derived for Ek‚àáf (x ^{k} )k, where
x ^{k} is the output of the stochastic first-order method.
It is also worth mentioning that Cutkosky & Mehta
(2021) use a different performance metric: PÃÇ K =
 2

P K
1
k
k‚àáf
(x
)k
. This metric is always smaller
k=0
K+1
1 P K
^{than} P K ^{=} K+1 k=0 k‚àáf ^{(x} ^{k} ^{)k} ^{2} ^{,} ^{which} ^{we} ^{use} ^{in} ^{our}
result. In the worst case, P K can be K +1 times larger than
PÃÇ K . Moreover, the lower bound from (Zhang et al., 2020b)
is derived for Ek‚àáf (x ^{k} )k that is also always smaller than
the standard quantity of interest Ek‚àáf (x ^{k} )k ^{2} . Therefore,
the question of optimality of the bound (16) remains open
for Œ± < 2. Moreover, it will also be interesting to modify
our analysis in this case to derive a better bound for metric
PÃÇ K than (16).

In the strongly convex case, we consider clipped-SSTM
with restarts (R-clipped-SSTM). This method consists of
œÑ stages. On the t-th stage R-clipped-SSTM runs clipped-
SSTM for K 0 iterations from the starting point xÃÇ ^{t} , which
is the output from the previous stage (xÃÇ ^{t} = x ^{0} ), and de-
fines the obtained point as xÃÇ ^{t+1} ; see Algorithm 3 in Ap-
pendix F.2. For this procedure we have the following re-
sult.

Theorem 3.3 (Convergence of R-clipped-SSTM). Let
Assumptions 1.1, 1.3, 1.6 with Œº > 0 hold for Q =
B 3R (x ‚àó ), R ‚â• kx ^{0} ‚àí x ‚àó k ^{2} and R-clipped-SSTM
ŒºR 2
runs clipped-SSTM
p ^{œÑ} 2 ^{=} ‚åàlog 2 ^{(} ^{/} Œ± ^{2Œµ} ^{)‚åâ} ^{times.} ^{Let}
e
^{LR} ^{t‚àí1} ^{/} ^{Œµ} t ^{,} ^{(} ^{œÉR} ^{t‚àí1} ^{/} ^{Œµ} t ^{)} ^{/} ^{(Œ±‚àí1)} }), ^{a} t ^{=}
K t = Œò(max{
Œ±+1 /Œ±
e
e ^{R} / Œ± tk+1 ) be the param-
œÉK t
Œò(max{1,
^{/} ^{LR} t }), ^{Œª} ^{tk} ^{=} ^{Œò(}
eters for the stage t of R-clipped-SSTM, where R t‚àí1 =
2
2 ‚àí ^{(t‚àí1)} ^{/} ^{2} R, Œµ t = ŒºR t‚àí1 / 4 , ln ^{4œÑ} Œ≤ ^{K} ^{t} ‚â• 1 for all t = 1, . . . , œÑ
and some Œ≤ ‚àà (0, 1]. Then to guarantee f (xÃÇ ^{œÑ} )‚àíf (x ‚àó ) ‚â§ Œµ
with probability ‚â• 1 ‚àí Œ≤ R-clipped-SSTM requires
(s   Œ± )!
L œÉ ^{2} 2(Œ±‚àí1)
e max
O
oracle calls. (24)
,
Œº ŒºŒµ

3.2. Acceleration

Next, we focus on the accelerated version of clipped-
SGD called Clipped Stochastic Similar Triangles Method
clipped-SSTM (Gorbunov et al., 2020). The method
constructs three sequences of points {x ^{k} } k‚â•0 , {y ^{k} } k‚â•0 ,
{z ^{k} } k‚â•0 satisfying the following update rules: x ^{0} = y ^{0} =
z ^{0} and

x k+1 =

z

k+1

^{A} k ^{y} ^{k} ^{+} ^{Œ±} k+1 ^{z} ^{k}
,
^{A} k+1

(20)

k+1

k

= z ‚àí Œ± k+1 ¬∑ clip ‚àáf Œæ k (x

y k+1 =

k

^{A} k ^{y} ^{+} ^{Œ±} k+1 ^{z}
^{A} k+1

k+1

,



), Œª k ,

Moreover, with probability ‚â• 1 ‚àí Œ≤ the iterates of R-
clipped-SSTM at stage t stay in the ball B 2R t‚àí1 (x ‚àó ).

(21)

The obtained complexity bound (see the proof in Ap-
pendix F.2) is the first optimal (up to logarithms) high-
probability complexity bound under Assumption 1.1 for
the smooth strongly convex problems. Indeed, the first
term cannot be improved in view of the deterministic lower
bound by Nemirovskij & Yudin (1983), and the second
term is optimal due to Zhang et al. (2020b).

(22)

where A 0 = Œ± 0 = 0, Œ± k+1 = ^{k+2}
2aL ^{,} ^{A} ^{k+1} ^{=} ^{A} ^{k} ^{+} ^{Œ±} ^{k+1} ^{,}
and Œæ ^{k} is sampled from D k independently from previous
steps. Our main convergence result for clipped-SSTM is
given in the following theorem.
Theorem 3.2 (Convergence of clipped-SSTM). Let
Assumptions 1.1, 1.3, 1.6 with Œº = 0 hold for
Q = B 3R (x ‚àó ), R ‚â• kx ^{0} ‚àí x ‚àó k ^{2} and a =
(Œ±+1) /Œ± (Œ±‚àí1) /Œ±
A
Œò(max{A ^{2} , ^{œÉK}
^{/} ^{LR} }), ^{Œª} k ^{=} ^{Œò(} ^{R} ^{/} ^{(Œ±} k+1 ^{A)} ^{),}
4K
where A = ln Œ≤ , Œ≤ ‚àà (0, 1] are such that A ‚â• 1. Then
to guarantee f (y ^{K} ) ‚àí f (x ‚àó ) ‚â§ Œµ with probability ‚â• 1 ‚àí Œ≤
clipped-SSTM requires
(r

 Œ± )!
LR ^{2} œÉR Œ±‚àí1
e
oracle calls. (23)
,
O max
Œµ
Œµ

4. Main Results for Variational Inequalities

4.1. Clipped Stochastic Extragradient

For (quasi strongly) monotone VIPs we consider Clipped
Stochastic Extragradient method (clipped-SEG):

x
e ^{k} = x ^{k} ‚àí Œ≥ ¬∑ clip(F Œæ 1 k (x ^{k} ), Œª k ),

x ^{k+1} = x ^{k} ‚àí Œ≥ ¬∑ clip(F Œæ 2 k (e
x k ), Œª k ),

(25)

(26)

where Œæ 1 ^{k} , Œæ 2 ^{k} are sampled from D k independently from
previous steps. Our main convergence results for clipped-
SEG are summarized below.

Moreover, with probability ‚â•
1 ‚àí Œ≤ the iter-
ates of clipped-SSTM stay in the ball B 2R (x ‚àó ):
k K
k K
‚àó
{x k } K+1
k=0 ^{,} {y } k=0 ^{,} {z } k=0 ‚äÜ ^{B} ^{2R} ^{(x} ^{).}

7

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Œª k = Œª = Œò ( R / Œ≥A ), where A = ln ^{4(K+1)}
, Œ≤ ‚àà (0, 1] are
Œ≤
such that A ‚â• 1.
Case 3.
Let Assumptions 1.1, 1.10, 1.9 with
Œº > 0 hold for Q = B 2R (x ‚àó ) and 0 <
Œ≥  =
O ^{(min{} ^{1} ^{/} ^{lA} ^{,} ^{ln(B} ^{K} ^{)} ^{/} ^{Œº(K+1)} }), ^{B} K 
=

Theorem 4.1 (Convergence of clipped-SEG). curiosity
^{Case} ^{1.} Let Assumptions 1.1, 1.3, 1.8 hold for Q =

B 4R (x ‚àó ) and 0 < Œ≥ = O min{ 1 / LA , R / K ^{1} ^{/Œ±} œÉA ^{(Œ±‚àí1)} ^{/Œ±} } ,
Œª k = Œª = Œò ( R / Œ≥A ), where A = ln ^{6(K+1)}
‚â• 1,
Œ≤
Œ≤ ‚àà (0, 1].
Case 2.
Let Assumptions 1.1, 1.3, 1.9 with
Œº > 0 hold for Q = B 3R (x ‚àó ) and 0 <
Œ≥  =
O ^{(min{} ^{1} ^{/} ^{LA} ^{,} ^{ln(B} ^{K} ^{)} ^{/} ^{Œº(K+1)} }), ^{B} K 
=
2(Œ±‚àí1) /Œ± 2 2
Œº R / œÉ 2 A 2(Œ±‚àí1) /Œ± ln 2 (B K ) } ,
Œò max{2, ^{(K+1)}

Œò max{2, ^{(K+1)}

One can find the proofs in Appendix H. The derived high-
probability results generalize the existing SOTA results
from the case of Œ± = 2 (Gorbunov et al., 2022a) to the case
of Œ± < 2.

The proofs are deferred to Appendix G. When Œ± = 2,
the above bounds recover SOTA high-probability bounds
for monotone and quasi-strongly monotone Lipschitz VIP
(Gorbunov et al., 2022a). For the case of Œ± < 2 (27) and
(28) are the first high-probability results for the mentioned
classes. Next, the first terms in these complexity bounds are
optimal (up to logarithms) due to the lower bounds for the
deterministic methods (Ouyang & Xu, 2021; Zhang et al.,
2022). The second term in (28) is also optimal (up to loga-
rithms) due to the lower bounds for stochastic strongly con-
vex minimization (Zhang et al., 2020b). Similarly to the
convex case in minimization, we conjecture that the sec-
ond term in (27) cannot be improved in the monotone case
as well.

5. Key Lemma and Intuition Behind the
Proofs

The proofs of all results in this paper follow a very similar
pattern. To illustrate the main idea, we consider the anal-
ysis of clipped-SGD in the non-convex case. Mimicking
the proof of deterministic GD we derive the following in-
equality:

Œ≥

K
X

k=0

4.2. Clipped Stochastic Gradient Descent-Ascent

x

= x ‚àí Œ≥ ¬∑ clip(F Œæ k (x ), Œª k ),

K
X

k=0

h‚àáf ^{(x} ^{k} ^{),} ^{Œ∏} k i ^{+} ^{LŒ≥} ^{2}

(32)

K‚àí1
X

k=0

kŒ∏ k k 2 ,

e Œæ k (x k ) ‚àí ‚àáf (x k ).
where ‚àÜ k = f (x ^{k} ) ‚àí f ‚àó and Œ∏ k = ‚àáf
In other words, we separate the deterministic part of the
method from its stochastic part. To obtain the result of
Theorem 3.1 (Case 1) it remains to upper bound with high-
probability the sums from the second line of the formula
above. We do it with the help of Bernstein‚Äôs inequality
(Lemma B.2). However, it requires several preliminary
steps. In particular, Bernstein‚Äôs inequality needs the ran-
dom variables to be bounded. The magnitudes of sum-
mands depend on ‚àáf (x ^{k} ) that can be arbitrarily large
due to the stochasticity in x ^{k} . However, (32) allows to
bound ‚àÜ K+1 inductively and, using smoothness, to bound

(29)

where Œæ ^{k} is sampled from D k independently from previous
steps. For this method we derive the following convergence
guarantees.

Theorem 4.2 (Convergence of clipped-SGDA). curios
^{Case} ^{1.} Let Assumptions 1.1, 1.10, 1.8 hold for Q =

B 2R (x ‚àó ) and 0 < Œ≥ = O min{ 1 / lA , R / K ^{1} ^{/Œ±} œÉA ^{(Œ±‚àí1)} ^{/Œ±} } ,
Œª k = Œª = Œò ( ^{R} / Œ≥A ), Œ≤ ‚àà (0, 1] are such that A ‚â• 1.
^{Case} ^{2.} Let Assumptions 1.1, 1.10 hold for Q = B 2R (x ‚àó  )
and 0 < Œ≥ = O min{ 1 / lA , R / K ^{1} ^{/Œ±} œÉA ^{(Œ±‚àí1)} ^{/Œ±} } ,

k‚àáf ^{(x} ^{k} ^{)k} ^{2} ^{/} ^{‚àÜ} 0 ‚àí ^{‚àÜ} K+1

‚àí Œ≥

In the star-cocoercive case, we focus on Clipped Stochastic
Gradient Descent-Ascent (clipped-SGDA):

k

,

oracle calls.

oracle calls.

k

Œº ^{2} R ^{2} / œÉ 2 A 2(Œ±‚àí1) /Œ± ln 2 (B K ) }

,
Œª k = Œò( exp(‚àíŒ≥Œº(1+ k / 2 ))R / Œ≥A ), where A = ln ^{4(K+1)}
Œ≤
Œ≤ ‚àà (0, 1] are such that A ‚â• 1.
Then to guarantee Gap R (e
x K
avg ) ‚â§ Œµ in ^{Case} ^{1} with
P
K
1 P k
1
k
K
e ^{,} K+1 k=0 kF ^{(x} ^{k} ^{)k} ^{2} ‚â§ ^{lŒµ} ^{in}
x
e avg ^{=} K+1 k=0 ^{x}
K
‚àó 2
^{Case} ^{2,} kx ‚àí x k ‚â§ Œµ in ^{Case} ^{3} with probability
‚â• 1 ‚àí Œ≤ clipped-SGDA requires
(
 Œ± )!

lR ^{2} œÉR Œ±‚àí1
e
(30)
Case 1 and 2: O max
,
Œµ
Œµ
( 
)!
Œ±
 2(Œ±‚àí1)
2
l
œÉ
e max
Case 2: O
(31)
,
Œº Œº 2 Œµ

,
Œª k = Œò( exp(‚àíŒ≥Œº(1+ k / 2 ))R / Œ≥A ), where A = ln ^{6(K+1)}
Œ≤
Œ≤ ‚àà (0, 1] are such that A ‚â• 1.
Then to guarantee Gap R (e
x K
avg ) ‚â§ Œµ in ^{Case} ^{1} with
P
K
1
K
k
K
x
e avg ^{=} K+1 k=0 ^{x}
e , kx ‚àí x ‚àó k ^{2} ‚â§ Œµ in ^{Case} ^{2} with
probability ‚â• 1 ‚àí Œ≤ clipped-SEG requires
(
 Œ± )!

LR ^{2} œÉR Œ±‚àí1
e
(27)
,
Case 1: O max
Œµ
Œµ
( 
)!
Œ±
 2(Œ±‚àí1)
2
L
œÉ
e max
Case 2: O
,
(28)
Œº Œº 2 Œµ

k+1

2(Œ±‚àí1) /Œ±

8

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

k‚àáf ^{(x} ^{K+1} ^{)k.} ^{Secondly,} ^{Bernstein‚Äôs} ^{inequality} ^{requires}
knowing the bounds on the bias and variance of the clipped
stochastic estimator. For such purposes, we derive the
following result, which is a generalization of Lemma F.5
from (Gorbunov et al., 2020); see also Lemma 10 from
(Zhang et al., 2020b).

ACM SIGSAC conference on computer and communica-
tions security, pp. 308‚Äì318, 2016. (Cited on page 5)

Arjevani, Y., Carmon, Y., Duchi, J. C., Foster, D. J., Srebro,
N., and Woodworth, B. Lower bounds for non-convex
stochastic optimization. Mathematical Programming, pp.
1‚Äì50, 2022. (Cited on pages 5 and 7)

e =
Lemma 5.1. Let X be a random vector in R ^{d} and X
e
e
clip(X, Œª). Then, k X ‚àí E[ X]k ‚â§ 2Œª. Moreover, if for
some œÉ ‚â• 0 and Œ± ‚àà (1, 2] we have E[X] = x ‚àà R ^{d} ,
^{E[kX} ‚àí ^{xk} ^{Œ±} ^{]} ‚â§ ^{œÉ} ^{Œ±} ^{,} ^{and} kxk ‚â§ ^{Œª} ^{/} ^{2} ^{,} ^{then}

E

E





e ‚àí x
E[ X]

e ‚àí x
X

e ‚àí E[ X]
e
X

2

2





‚â§

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1

(33)

‚â§

18Œª ^{2‚àíŒ±} œÉ ^{Œ±} ,

(34)

‚â§

18Œª ^{2‚àíŒ±} œÉ ^{Œ±} .

(35)

Bennett, G. Probability inequalities for the sum of indepen-
dent random variables. Journal of the American Statisti-
cal Association, 57(297):33‚Äì45, 1962. (Cited on page 14)

Cutkosky, A. and Mehta, H. High-probability bounds for
non-convex stochastic optimization with heavy tails. Ad-
vances in Neural Information Processing Systems, 34,
2021. (Cited on pages 1, 4, 5, and 7)

Davis, D., Drusvyatskiy, D., Xiao, L., and Zhang, J. From
low probability to high confidence in stochastic convex
optimization. Journal of Machine Learning Research, 22
(49):1‚Äì38, 2021. (Cited on pages 1, 3, and 4)

This lemma can be useful on its own for analyses involving
clipping operators. Moreover, our high-probability analy-
sis does not rely on the choice of clipping explicitly: in the
e ‚â§ Œª and inequalities (33)-(35).
proofs, we use only k Xk
Therefore, our results hold for the methods considered in
this work with any other non-linearity œÜ Œª (x) (not neces-
sary clipping), if it satisfies the conditions from the above
e = œÜ Œª (X).
lemma for X

Dvurechenskii, P., Dvinskikh, D., Gasnikov, A., Uribe, C.,
and Nedich, A. Decentralize and randomize: Faster al-
gorithm for wasserstein barycenters. Advances in Neu-
ral Information Processing Systems, 31, 2018. (Cited on

page 40)

Dzhaparidze, K. and Van Zanten, J. On bernstein-type in-
equalities for martingales. Stochastic processes and their
applications, 93(1):109‚Äì117, 2001. (Cited on page 14)

6. Discussion

Freedman, D. A. et al. On tail probabilities for martingales.
the Annals of Probability, 3(1):100‚Äì118, 1975. (Cited on

In this work, we contributed to the stochastic optimization
literature via deriving new high-probability results under
Assumption 1.1. Our results can be extended to the min-
imization of functions with H√∂lder continuous gradients
using similar ideas to (Gorbunov et al., 2021). Another
prominent direction is in obtaining new high-probability
results for other types of non-linearities, e.g., like in
(Polyak & Tsypkin, 1980; Jakovetic et al., 2022).

page 14)

Gasnikov, A. and Nesterov, Y. Universal fast gradi-
ent method for stochastic composit optimization prob-
lems. arXiv preprint arXiv:1604.05275, 2016. (Cited

on page 40)

Ghadimi, S. and Lan, G. Optimal stochastic approximation
algorithms for strongly convex stochastic composite op-
timization i: A generic algorithmic framework. SIAM
Journal on Optimization, 22(4):1469‚Äì1492, 2012. (Cited

Acknowledgements

This work was partially supported by a grant for research
centers in the field of artificial intelligence, provided by the
Analytical Center for the Government of the Russian Fed-
eration in accordance with the subsidy agreement (agree-
ment identifier 000000D730321P5Q0002) and the agree-
ment with the Moscow Institute of Physics and Technology
dated November 1, 2021 No. 70-2021-00138.

on pages 2 and 3)

Ghadimi, S. and Lan, G. Stochastic first-and zeroth-order
methods for nonconvex stochastic programming. SIAM
Journal on Optimization, 23(4):2341‚Äì2368, 2013. (Cited

on page 2)

Gidel, G., Berard, H., Vignoud, G., Vincent, P., and
Lacoste-Julien, S. A variational inequality perspective
on generative adversarial networks. International Con-
ference on Learning Representations, 2019. (Cited on

References

Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B.,
Mironov, I., Talwar, K., and Zhang, L. Deep learning
with differential privacy. In Proceedings of the 2016

page 2)

9

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,
Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.
Generative adversarial nets. In Ghahramani, Z., Welling,
M., Cortes, C., Lawrence, N., and Weinberger, K. Q.
(eds.), Advances in Neural Information Processing Sys-
tems, volume 27. Curran Associates, Inc., 2014. (Cited

conference on machine learning and knowledge discov-
ery in databases, pp. 795‚Äì811. Springer, 2016. (Cited on

page 6)

Karimireddy, S. P., He, L., and Jaggi, M. Learning from
history for byzantine robust optimization. In Interna-
tional Conference on Machine Learning, pp. 5311‚Äì5319.
PMLR, 2021. (Cited on page 5)

on page 2)

Goodfellow, I., Bengio, Y., and Courville, A. Deep learn-
ing. MIT press, 2016. (Cited on page 5)

Khaled, A. and Richt√°rik, P. Better theory for sgd in
the nonconvex world. arXiv preprint arXiv:2002.03329,
2020. (Cited on page 6)

Gorbunov, E., Danilova, M., and Gasnikov, A. Stochastic
optimization with heavy-tailed noise via accelerated gra-
dient clipping. Advances in Neural Information Process-
ing Systems, 33:15042‚Äì15053, 2020. (Cited on pages 1, 3,

Korpelevich, G. M. The extragradient method for finding
saddle points and other problems. Matecon, 12:747‚Äì756,
1976. (Cited on page 4)

4, 5, 7, 9, 14, 40, and 48)

Li, X. and Orabona, F.
A high probability analy-
sis of adaptive sgd with momentum. arXiv preprint
arXiv:2007.14294, 2020. (Cited on pages 4 and 5)

Gorbunov, E., Danilova, M., Shibaev, I., Dvurechen-
sky, P., and Gasnikov, A. Near-optimal high prob-
ability complexity bounds for non-smooth stochastic
optimization with heavy-tailed noise. arXiv preprint
arXiv:2106.05958, 2021. (Cited on pages 1, 3, 4, 9, 40,

Liu, C., Zhu, L., and Belkin, M. Loss landscapes and op-
timization in over-parameterized non-linear systems and
neural networks. Applied and Computational Harmonic
Analysis, 59:85‚Äì116, 2022. (Cited on page 2)

and 43)

Gorbunov, E., Danilova, M., Dobre, D., Dvurechensky,
P., Gasnikov, A., and Gidel, G. Clipped stochastic
methods for variational inequalities with heavy-tailed
noise. arXiv preprint arXiv:2206.01095, 2022a. (Cited

Loizou, N., Berard, H., Gidel, G., Mitliagkas, I., and
Lacoste-Julien, S. Stochastic gradient descent-ascent
and consensus optimization for smooth games: Conver-
gence analysis under expected co-coercivity. Advances
in Neural Information Processing Systems, 34, 2021.

on pages 1, 3, 4, 5, 8, 51, 52, 59, 60, 69, 70, 77, 79, and 80)

Gorbunov, E., Loizou, N., and Gidel, G. Extragradient
method: O( ^{1} / K ) last-iterate convergence for monotone
variational inequalities and connections with cocoerciv-
ity. In International Conference on Artificial Intelligence
and Statistics, pp. 366‚Äì402. PMLR, 2022b. (Cited on

(Cited on page 3)

Lojasiewicz, S. A topological property of real analytic sub-
sets. Coll. du CNRS, Les √©quations aux d√©riv√©es par-
tielles, 117(87-89):2, 1963. (Cited on page 2)

pages 38, 51, and 69)

Mertikopoulos, P. and Zhou, Z. Learning in games
with continuous action sets and unknown payoff func-
tions. Mathematical Programming, 173(1):465‚Äì507,
2019. (Cited on page 3)

Harker, P. T. and Pang, J.-S. Finite-dimensional variational
inequality and nonlinear complementarity problems: a
survey of theory, algorithms and applications. Mathe-
matical programming, 48(1):161‚Äì220, 1990. (Cited on

page 2)

Nazin, A. V., Nemirovsky, A., Tsybakov, A. B., and Ju-
ditsky, A. Algorithms of robust stochastic optimization
based on mirror descent method. Automation and Re-
mote Control, 80(9):1607‚Äì1627, 2019. (Cited on pages 1,

Jakovetic, D., Bajovic, D., Sahu, A. K., Kar, S., Milose-
vic, N., and Stamenkovic, D. Nonlinear gradient map-
pings and stochastic optimization: A general framework
with applications to heavy-tail noise. arXiv preprint
arXiv:2204.02593, 2022. (Cited on page 9)

3, 4, and 5)

Necoara, I., Nesterov, Y., and Glineur, F. Linear conver-
gence of first order methods for non-strongly convex op-
timization. Mathematical Programming, 175(1):69‚Äì107,
2019. (Cited on page 3)

Juditsky, A., Nemirovski, A., and Tauvel, C. Solving varia-
tional inequalities with stochastic mirror-prox algorithm.
Stochastic Systems, 1(1):17‚Äì58, 2011. (Cited on pages 3,

4, and 5)

Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. Ro-
bust stochastic approximation approach to stochastic pro-
gramming. SIAM Journal on optimization, 19(4):1574‚Äì
1609, 2009. (Cited on pages 2 and 3)

Karimi, H., Nutini, J., and Schmidt, M. Linear conver-
gence of gradient and proximal-gradient methods under
the Polyak-Lojasiewicz condition. In Joint European

10

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Nemirovskij, A. S. and Yudin, D. B. Problem complexity
and method efficiency in optimization. 1983. (Cited on

Vural, N. M., Yu, L., Balasubramanian, K., Volgushev, S.,
and Erdogdu, M. A. Mirror descent strikes again: Opti-
mal stochastic convex optimization under infinite noise
variance. In Conference on Learning Theory, pp. 65‚Äì102.
PMLR, 2022. (Cited on page 13)

pages 4, 7, and 13)

Nesterov, Y. Dual extrapolation and its applications to solv-
ing variational inequalities and related problems. Mathe-
matical Programming, 109(2):319‚Äì344, 2007. (Cited on

Yue, P., Fang, C., and Lin, Z. On the lower bound of min-
imizing Polyak-Lojasiewicz functions. arXiv preprint
arXiv:2212.13551, 2022. (Cited on page 6)

page 2)

Nesterov, Y. et al. Lectures on convex optimization, volume
137. Springer, 2018. (Cited on page 14)

Zhang, J. and Cutkosky, A. Parameter-free regret in
high probability with heavy tails.
arXiv preprint
arXiv:2210.14355, 2022. (Cited on page 4)

Ouyang, Y. and Xu, Y. Lower complexity bounds of first-
order methods for convex-concave bilinear saddle-point
problems. Mathematical Programming, 185(1):1‚Äì35,
2021. (Cited on page 8)

Zhang, J., He, T., Sra, S., and Jadbabaie, A. Why
gradient clipping accelerates training: A theoret-
ical justification for adaptivity.
In International
Conference on Learning Representations, 2020a. URL
https://openreview.net/forum?id=BJgnXpVYwS.

Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty
of training recurrent neural networks. In International
conference on machine learning, pp. 1310‚Äì1318, 2013.

(Cited on page 5)

(Cited on pages 5 and 19)

Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S. J.,
Kumar, S., and Sra, S. Why are adaptive methods good
for attention models? Advances in Neural Information
Processing Systems, 33, 2020b. (Cited on pages 2, 4, 5, 6,

Patel, V. and Berahas, A. S. Gradient descent in the ab-
sence of global lipschitz continuity of the gradients: Con-
vergence, divergence and limitations of its continuous
approximation. arXiv preprint arXiv:2210.02418, 2022.

7, 8, 9, and 13)

(Cited on page 2)

Zhang, J., Hong, M., and Zhang, S. On lower iteration
complexity bounds for the convex concave saddle point
problems. Mathematical Programming, 194(1-2):901‚Äì
935, 2022. (Cited on page 8)

Patel, V., Zhang, S., and Tian, B. Global convergence and
stability of stochastic gradient descent. Advances in Neu-
ral Information Processing Systems, 35:36014‚Äì36025,
2022. (Cited on page 2)

Polyak, B. T. Gradient methods for the minimisation
of functionals. USSR Computational Mathematics and
Mathematical Physics, 3(4):864‚Äì878, 1963. (Cited on

page 2)

Polyak, B. T. and Tsypkin, Y. Z. Optimal pseudogradient
adaptation algorithms. Avtomatika i Telemekhanika, (8):
74‚Äì84, 1980. (Cited on page 9)

Robbins, H. and Monro, S. A stochastic approximation
method. The annals of mathematical statistics, pp. 400‚Äì
407, 1951. (Cited on page 1)

Ryu, E. K. and Yin, W. Large-scale convex optimization
via monotone operators, 2021. (Cited on page 2)

Shalev-Shwartz, S. and Ben-David, S. Understanding ma-
chine learning: From theory to algorithms. Cambridge
university press, 2014. (Cited on page 2)

Song, C., Zhou, Z., Zhou, Y., Jiang, Y., and Ma, Y. Op-
timistic dual extrapolation for coherent non-monotone
variational inequalities. Advances in Neural Information
Processing Systems, 33:14303‚Äì14314, 2020. (Cited on

page 3)

11

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Contents

1

1 Introduction

1.1

Technical Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

1.2

Closely Related Works and Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3

2 Failure of Standard SGD

6

3 Main Results for Minimization Problems

6

3.1

SGD with Clipping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6

3.2

Acceleration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

4 Main Results for Variational Inequalities

7

4.1

Clipped Stochastic Extragradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

4.2

Clipped Stochastic Gradient Descent-Ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

8

5 Key Lemma and Intuition Behind the Proofs

8

6 Discussion

9

A Additional Related Work

13

B Useful Facts

14

C Proof of Lemma 5.1

15

D Proof of Theorem 2.1

17

E Missing Proofs for clipped-SGD

19

E.1 Non-Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

E.2 Polyak-≈Åojasiewicz Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25

E.3 Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

32

E.4 Quasi-Strongly Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

38

F Missing Proofs for clipped-SSTM and R-clipped-SSTM

40

F.1

Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

40

F.2

Strongly Convex Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48

G Missing Proofs for clipped-SEG

51

G.1 Monotone Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

51

G.2 Quasi-Strongly Monotone Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

59

H Missing Proofs for clipped-SGDA

69

H.1 Monotone Star-Cocoercive Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

69

H.2 Star-Cocoercive Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

77

H.3 Quasi-Strongly Monotone Star-Cocoercive Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

79

12

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

A. Additional Related Work

In this section, we provide an overview of the existing in-expectation convergence results under Assumption 1.1.

Convex minimization. The first in-expectation result under Assumption 1.1 is given by Nemirovskij & Yudin (1983),
^{who} ^{derive} ^{4} O(Œµ ^{‚àí} ^{Œ±} ^{/} ^{(Œ±‚àí1)} ^{)} ^{complexity} ^{for} ^{Mirror} ^{Descent} ^{applied} ^{to} ^{the} ^{minimization} ^{of} ^{convex} ^{functions} ^{with} ^{bounded}
gradients. This result was recently extended by Vural et al. (2022) to the uniformly convex functions, and matching lower
^{bounds} ^{were} ^{derived.} ^{In} ^{the} ^{strongly} ^{convex} ^{case,} ^{Zhang} ^{et} ^{al.} ^{(2020b)} ^{prove} O(Œµ ^{‚àí} ^{Œ±} ^{/} ^{2(Œ±‚àí1)} ^{)} ^{complexity} ^{for} ^{clipped-SGD.}
However, all these results rely on the boundedness of the gradient. To the best of our knowledge, there are no results for
smooth convex problems under Assumption 1.1 without assuming that the gradient is bounded even in terms of expectation.

^{Non-convex} ^{minimization.} ^{In} ^{the} ^{non-convex} ^{smooth} ^{case,} ^{Zhang} ^{et} ^{al.} ^{(2020b)} ^{prove} O(Œµ ^{‚àí} ^{(3Œ±‚àí2)} ^{/} ^{(Œ±‚àí1)} ^{)} ^{complexity}
for clipped-SGD to produce a point x such that Ek‚àáf (x)k ‚â§ Œµ. In the same work, the authors derive
p the matching lower
bound. However, both upper and lower bounds are derived for Ek‚àáf (x)k which is smaller than Ek‚àáf (x)k ^{2} . The later
one is stronger and is more standard performance metric for stochastic non-convex optimization. Therefore, the question
of deriving lower and matching upper bounds for the standard metric remains open.

4

In this section, we hide in O(¬∑) all dependencies except the dependency on Œµ.

13

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

B. Useful Facts

Smoothness.

If f is L-smooth on convex set Q ‚äÜ R ^{d} , then for all x, y ‚àà Q (Nesterov et al., 2018)

^{f} ^{(y)} ‚â§ ^{f} ^{(x)} ^{+} h‚àáf ^{(x),} ^{y} ‚àí ^{xi} ^{+}

L
ky ‚àí xk 2 .
2

(36)

In particular, if x and y = x ‚àí L ^{1} ‚àáf (x) lie in Q, then the above inequality gives

f (y) ‚â§ f (x) ‚àí

1
1
1
k‚àáf ^{(x)k} ^{2} ^{+}
k‚àáf ^{(x)k} ^{2} ^{=} ^{f} ^{(x)} ‚àí
k‚àáf ^{(x)k} ^{2}
L
2L
2L

and

k‚àáf ^{(x)k} ^{2} ‚â§ ^{2L} ^{(f} ^{(x)} ‚àí ^{f} ^{(y))} ‚â§ ^{2L} ^{(f} ^{(x)} ‚àí ^{f} ‚àó ^{)}

^{under} ^{the} ^{assumption} ^{that} ^{f} ‚àó ^{=} ^{inf} x‚ààQ ^{f} ^{(x)} ^{>} ‚àí‚àû. ^{In} ^{other} ^{words,} ^{(7)} ^{holds} ^{for} ^{any} ^{x} ‚àà ^{Q} ^{such} ^{that} ^{(x‚àí} L ^{1} ‚àáf ^{(x))} ‚àà ^{Q.}
For example, if x ‚àó is an optimum of f , then L-smoothness on B 2R (x ‚àó ) implies that (7) holds on B R (x ‚àó ): indeed, for any
x ‚àà B R (x ‚àó ) we have

x ‚àí

(6)
1
1
‚àáf ^{(x)} ‚àí ^{x} ^{‚àó} ‚â§ kx ‚àí ^{x} ^{‚àó} k ^{+} k‚àáf ^{(x)k} ‚â§ ^{2kx} ‚àí ^{x} ^{‚àó} k ‚â§ ^{2R.}
L
L

This derivation means that, in the worst case, to have (7) on a set Q we need to assume smoothness on a slightly larger set.

Parameters in clipped-SSTM.
A k .

To analyze clipped-SSTM we use the following lemma about its parameters Œ± k and

Lemma B.1 (Lemma E.1 from (Gorbunov et al., 2020)). Let sequences {Œ± k } k‚â•0 and {A k } k‚â•0 satisfy

Œ± 0 = A 0 = 0,

^{A} k+1 ^{=} ^{A} k ^{+} ^{Œ±} k+1 ^{,}

^{Œ±} k+1 ^{=}

k +2
2aL

‚àÄk ‚â• 0,

(37)

where a > 0, L > 0. Then for all k ‚â• 0

^{A} k+1

^{A} k+1

(k + 1)(k + 4)
,
4aL
2
‚â• aLŒ± k+1 .

=

(38)

(39)

Bernstein inequality. One of the final steps in our proofs is in the proper application of the following lemma known as
Bernstein inequality for martingale differences (Bennett, 1962; Dzhaparidze & Van Zanten, 2001; Freedman et al., 1975).

Lemma B.2. Let the sequence of random variables {X i } i‚â•1 form a martingale difference sequence, i.e.


def
E [X i | X i‚àí1 , . . . , X 1 ] = 0 for all i ‚â• 1. Assume that conditional variances œÉ i ^{2} = E X i ^{2} | X i‚àí1 , . . . , X 1 exist and
are bounded and assume also that there exists deterministic constant c > 0 such that |X i | ‚â§ c almost surely for all i ‚â• 1.
Then for all b > 0, G > 0 and n ‚â• 1
)
( n


n
X
X
b 2
2
œÉ i ‚â§ G ‚â§ 2 exp ‚àí
X i > b and
P
.
(40)
2G + 2cb / 3
i=1
i=1

14

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

C. Proof of Lemma 5.1

e = clip(X, Œª). Then,
Lemma C.1 (Lemma 5.1). Let X be a random vector in R ^{d} and X

e ‚àí E[ X]
e ‚â§ 2Œª.
X

Moreover, if for some œÉ ‚â• 0 and Œ± ‚àà [1, 2)

E[X] = x ‚àà R ^{d} ,

(41)

E[kX ‚àí xk ^{Œ±} ] ‚â§ œÉ ^{Œ±}

(42)

‚â§

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1

(43)

‚â§

18Œª ^{2‚àíŒ±} œÉ ^{Œ±} ,

(44)

‚â§

18Œª ^{2‚àíŒ±} œÉ ^{Œ±} .

(45)

^{and} kxk ‚â§ ^{Œª} ^{/} ^{2} ^{,} ^{then}

E

E





e ‚àí x
E[ X]

e ‚àí x
X

e ‚àí E[ X]
e
X

2

2





Proof. Proof of (41): by definition of a clipping operator, we have

h i
e ‚àí E X
e
X

h i
e + E X
e
X

‚â§

^{=} kclip(X, ^{Œª)k} ^{+} kE ^{[clip(X,} ^{Œª)]k}






Œª
Œª
X + E min 1,
X
‚â§
min 1,
kXk
kXk
^{=} ^{min} {kXk ^{,} ^{Œª}} ^{+} ^{E} ^{[min} {kXk ^{,} ^{Œª}]}
‚â§ Œª + Œª = 2Œª.

Proof of (43): To start the proof, we introduce two indicator random variables. Let

^{œá} ^{=} ^{I} {X:kXk>Œª} ^{=}

(

^{1,} ^{if} kXk ^{>} ^{Œª,}
^{,} ^{Œ∑} ^{=} ^{I} { X:kX‚àíxk> ^{Œª} } ^{=}
2
0, otherwise

^{Moreover,} ^{since} kXk ‚â§ kxk ^{+} kX ‚àí ^{xk}
Using that

we obtain

kxk‚â§ ^{Œª} ^{/} ^{2}

‚â§

(

1, if kX ‚àí xk > Œª 2 ,
.
0, otherwise

Œª
2 + kX ‚àí xk, we have œá ‚â§ Œ∑. We are now in a position to show (43).



Œª
e = min 1, ^{Œª}
X
X = œá
X + (1 ‚àí œá)X,
kXk
kXk

h i
e ‚àí x
E X

(46)

=

=

‚â§

=

 


Œª
‚àí 1 X ‚àí x
E X + œá
kXk
 
 
Œª
‚àí 1 X
E œá
kXk
 


Œª
E œá
‚àí ^{1} kXk
kXk


 
Œª
kXk ^{.}
E œá 1 ‚àí
kXk

15

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

^{Since} ^{1} ‚àí ^{Œª} ^{/} kXk ‚àà ^{(0,} ^{1)} ^{when} ^{œá} 6 ^{=} ^{0,} ^{we} ^{derive}
h i
e ‚àí x
E X
‚â§ E [œákXk]

œá‚â§Œ∑

‚â§
‚â§

(‚àó)

‚â§

(42)

‚â§

E [Œ∑kXk]
E [Œ∑kX ‚àí xk + Œ∑kxk]

 Œ±  Œ±‚àí1
Œ± 1 / Œ±
Œ±
^{+} kxkE ^{[Œ∑]}
(E [kX ‚àí xk ])
E Œ∑ Œ±‚àí1
1‚àíŒ±
 Œ±  Œ±
Œª
œÉ E Œ∑ 1‚àíŒ±
+ E [Œ∑] ,
2

where in (‚àó) we applied H√∂lder‚Äôs inequality. By Markov‚Äôs inequality,




 Œ± 
Œª Œ±
Œª
Œ±
1‚àíŒ±
E Œ∑
= P kX ‚àí xk > Œ±
= E [Œ∑] = P kX ‚àí xk >
2
2
Œ±
2
‚â§
E [kX ‚àí xk ^{Œ±} ]
Œ±
Œª
  Œ±
2œÉ
.
‚â§
Œª

Thus, in combination with the previous chain of inequalities, we finally have
  Œ±
  Œ±‚àí1
h i
Œª 2œÉ
2 Œ± œÉ Œ±
2œÉ
e
E X ‚àí x
‚â§ œÉ
+
^{=} Œ±‚àí1 ^{.}
Œª
2 Œª
Œª

e ‚àí xk ‚â§ k Xk
e ^{+} kxk ‚â§ ^{Œª} ^{+} ^{Œª} ^{=} ^{3Œª} ^{,} ^{we} ^{have}
Proof of (44): Using k X
2
2
h
i
h
i
e ‚àí xk 2
e ‚àí xk Œ± k X
e ‚àí xk 2‚àíŒ±
E k X
=
E k X
  2‚àíŒ± h
i
3Œª
e ‚àí xk Œ± œá + k X
e ‚àí xk Œ± (1 ‚àí œá)
‚â§
E k X
2

  2‚àíŒ± 
Œ±
3Œª
Œª
Œ±
E œá
X ‚àí x + kX ‚àí xk (1 ‚àí œá)
=
2
kXk
  2‚àíŒ±  
 Œ±

3Œª
Œª
‚â§
^{X} ^{+} kxk
E œá
+ kX ‚àí xk Œ± (1 ‚àí œá)
2
kXk





 Œ±

Œª
2‚àíŒ±
kxk‚â§ 2
3Œª
3Œª
‚â§
E œá
+ œÉ Œ± ,
2
2

where in the last inequality we applied (42) and 1 ‚àí œá ‚â§ 1. By (47) and œá ‚â§ Œ∑, we obtain
  Œ±   2‚àíŒ±
i
h
3Œª
9Œª 2 2œÉ
2
e
+
œÉ Œ±
‚â§
E k X ‚àí xk
4
Œª
2


9Œª 2
2 Œ± œÉ Œ±
Œ±
‚â§
2 + Œ±
4
3
Œª Œ±

‚â§ 18Œª ^{2‚àíŒ±} œÉ ^{Œ±} .

Proof of (45): Using variance decomposition and (44), we have


i
h
2
e ‚àí xk ^{2} ‚â§ 18Œª ^{2‚àíŒ±} œÉ ^{Œ±} .
e
e
E X ‚àí E[ X]
‚â§ E k X

16

(47)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

D. Proof of Theorem 2.1

In this section, we give an example of the problem for which SGD without clipping leads to a weak high-probability
convergence guarantee even under the strong assumption of bounded variance. Theorem below formally states our result,
showing that, in the worst-case, the bound for SGD scales worse than that of clipped-SGD in terms of the probability Œ≤.

Theorem D.1. For any Œµ > 0, Œ≤ ‚àà (0, 1), and SGD parameterized by the number of steps K and stepsize Œ≥, there exists
problem (2) such that Assumptions 1.1, 1.3, and 1.6 hold with Œ± = 2, 0 < Œº ‚â§ L and for the iterates produced by SGD
with any stepsize 0 < Œ≥ ‚â§ ^{1} / Œº


P kx K ‚àí x ‚àó k 2 ‚â• Œµ ‚â§ Œ≤ =‚áí K = Œ©



œÉ
‚àö
Œº Œ≤Œµ



.

(48)

2

Proof. To prove the above theorem, we consider the simple one-dimensional problem f (x) = ^{Œºx} / 2 . It is easy to see
that the considered problem is Œº-strongly convex, Œº-smooth, and has optimum at x ‚àó = 0. We construct the noise in an
adversarial way with respect to the parameters of the SGD. Concretely, the noise depends on the number of iterates N ,
stepsize Œ≥, target precision Œµ, the starting point x ^{0} , and bound on the variance œÉ ^{2} such that

‚àáf Œæ k (x k ) = Œºx k ‚àí œÉz k ,

where

Ô£±
0,
Ô£¥
Ô£¥
Ô£¥ Ô£±
Ô£≤
1
Ô£¥ ‚àíA, ^{with} ^{probability} 2A
2 ,
z k = Ô£≤
1
Ô£¥
0
with
probability
1
‚àí
Ô£¥ Ô£¥
A 2 ,
Ô£¥
Ô£≥ Ô£≥
1
A,
with probability 2A
2 ,

where A = max

n ‚àö

2 Œµ
Œ≥œÉ , 1

if k < K ‚àí 1 or (1 ‚àí Œ≥Œº) ^{K} |x ^{0} | >

‚àö

Œµ,

‚àÄk ‚àà {0, ^{1,} ^{.} ^{.} ^{.} ^{,} ^{K} ‚àí ^{1},}

otherwise

(49)

o
 


. We note that E z ^{k} = 0. Therefore, E ‚àáf Œæ k (x ^{k} ) = Œºx ^{k} = ‚àáf (x ^{k} ). Furthermore,



 
1 2
1 2
Var z ^{k} = E (z ^{k} ) ^{2} ‚â§
A +
A = 1,
2A 2
2A 2

which implies that Assumption 1.1 holds for Œ± = 2. We note that our construction depends on the parameters of the
algorithm and the target value Œµ. However, our analysis of the methods with clipping works in such generality.

Let us now analyze the properties of the introduced problem. We are interested in the situation when


P kx K ‚àí x ‚àó k 2 ‚â• Œµ ‚â§ Œ≤

‚àö
for Œ≤ ‚àà (0, 1). We first prove that this implies that (1 ‚àí Œ≥Œº) ^{K} |x ^{0} | ‚â§ Œµ. To do that we proceed by contradiction and
assume that
‚àö
(1 ‚àí Œ≥Œº) ^{K} |x ^{0} | > Œµ.
(50)

^{By} ^{construction,} ^{this} ^{implies} ^{that} ^{z} k ^{=} ^{0,} ‚àÄk ‚àà {0, ^{1,} ^{.} ^{.} ^{.} ^{,} ^{K}.} ^{This,} ^{in} ^{turn,} ^{implies} ^{that} ^{x} ^{K} ^{=} ^{(1} ‚àí ^{Œ≥Œº)} ^{K} ^{x} ^{0} ^{,} ^{and,} ^{further,}
by (50) and since x ‚àó = 0, that



P kx K ‚àí x ‚àó k 2 ‚â• Œµ = P kx K k 2 ‚â• Œµ = 1.

‚àö

‚àö

ln |x 0 Œµ |
ln |x 0 Œµ |
0
‚àö
|
1
‚àö . Using
Thus, the contradiction shows that ^{(1} ‚àí ^{Œ≥Œº)} |x | ‚â§ ^{Œµ,} which yields ^{K} ‚â• ln(1‚àíŒ≥Œº)
^{‚â•} ‚àíŒ≥Œº
‚â• Œ≥Œº
= ln |x
Œµ

K

0

0

|
1
‚àö , we obtain
(49) with K ‚â• Œ≥Œº
log ^{|x}
Œµ

kx ^{K} ‚àí x ‚àó k ^{2} = ((1 ‚àí Œ≥Œº) ^{K} x ^{0} + Œ≥œÉz K ) ^{2} .

17

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Furthermore,

‚àö



‚àö
P kx ^{K} ‚àí x ^{‚àó} k ^{2} ‚â• Œµ = P (1 ‚àí Œ≥Œº) ^{K} x ^{0} + Œ≥œÉz K ‚â• Œµ


‚àö
‚àö
= P Œ≥œÉz K ‚â• Œµ ‚àí (1 ‚àí Œ≥Œº) ^{K} x ^{0} + P Œ≥œÉz K ‚â§ ‚àí Œµ ‚àí (1 ‚àí Œ≥Œº) ^{K} x ^{0}


‚àö
‚àö
‚â• P Œ≥œÉz K ‚â• Œµ + (1 ‚àí Œ≥Œº) ^{K} x ^{0} + P Œ≥œÉz K ‚â§ ‚àí Œµ ‚àí (1 ‚àí Œ≥Œº) ^{K} x ^{0}

‚àö
^{=} ^{P} |Œ≥œÉz K | ‚â• ^{Œµ} ^{+} ^{(1} ‚àí ^{Œ≥Œº)} ^{K} ^{x} ^{0}

‚àö 

‚àö
2 Œµ
.
‚â• ^{P} |Œ≥œÉz K | ‚â• ^{2} ^{Œµ} ^{=} ^{P} |z K | ‚â•
Œ≥œÉ

Now if ^{2} Œ≥œÉ ^{Œµ} < 1 then A = 1. Therefore,


‚àö 

2 Œµ
‚â§ P kx K ‚àí x ‚àó k 2 > Œµ < Œ≤,
1 = P |z K | ‚â•
Œ≥œÉ

‚àö

yielding contradiction, which implies that if P kx ^{K} ‚àí x ‚àó k ^{2} > Œµ < Œ≤ for our constructed problem, then ^{2} Œ≥œÉ ^{Œµ} ‚â• 1, i.e.,

‚àö

‚àö

Œ≥ ‚â§ ^{2} œÉ ^{Œµ} . For Œ≥ ‚â§ ^{2} œÉ ^{Œµ} , we have

‚àö


‚àö 
 K
1
Œ≥ 2 œÉ 2
2 Œµ
‚àó 2
= 2 =
.
Œ≤ ‚â• P kx ‚àí x k ‚â• Œµ ‚â• P |z K | ‚â•
Œ≥œÉ
A
4Œµ

0

|
1
‚àö yields
log ^{|x}
This implies that Œ≥ ‚â§ ^{2} œÉ ^{Œ≤Œµ} . Combining this inequality with K ‚â• Œ≥Œº
Œµ

K ‚â•

œÉ
|x 0 |
‚àö log ‚àö
Œµ
2Œº Œ≤Œµ

and concludes the proof.

18

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

E. Missing Proofs for clipped-SGD

In this section, we provide all the missing details and proofs of the results for clipped-SGD. For brevity, we will use the
e Œæ k (x ^{k} ) = clip(‚àáf Œæ k (x ^{k} ), Œª k ).
following notation: ‚àáf

Algorithm 1 Clipped Stochastic Gradient Descent (clipped-SGD) (Pascanu et al., 2013)

Input: starting point x ^{0} , number of iterations K, stepsize Œ≥ > 0, clipping levels {Œª k } ^{K‚àí1}
k=0 ^{.}
1: for k = 0, . . . , K ‚àí 1 do

e Œæ k (x ^{k} ) = clip ‚àáf Œæ k (x ^{k} ), Œª k using a fresh sample Œæ ^{k} ‚àº D k
2:
Compute ‚àáf
k+1
k
e Œæ k (x k )
3:
x
= x ‚àí Œ≥ ‚àáf
4: end for
Output: x ^{K}

E.1. Non-Convex Functions

We start the analysis of clipped-SGD in the non-convex case with the following lemma that follows the proof of determin-
istic GD and separates the stochasticity from the determinisitc part of the method.

‚àö

‚àö

Lemma E.1. Let Assumptions 1.2 and 1.3 hold on Q = {x ‚àà R ^{d} | ‚àÉy ‚àà R ^{d} : f (y) ‚â§ f ‚àó + 2‚àÜ and kx ‚àí yk ‚â§ ^{‚àÜ} / 20 L },
where ‚àÜ ‚â• ‚àÜ 0 = f (x ^{0} ) ‚àí f ‚àó , and let stepsize Œ≥ satisfy Œ≥ < L ^{2} . If x ^{k} ‚àà Q for all k = 0, 1, . . . , K, K ‚â• 0, then after K
iterations of clipped-SGD we have

 K‚àí1

LŒ≥ X
Œ≥ 1 ‚àí
k‚àáf ^{(x} ^{k} ^{)k} ^{2}
2

k=0

‚â§

(f (x ^{0} ) ‚àí f ‚àó ) ‚àí (f (x ^{K} ) ‚àí f ‚àó ) ‚àí Œ≥(1 ‚àí LŒ≥)

+

Œ∏ k

def

=

LŒ≥
2

2 K‚àí1
X

k=0

K‚àí1
X

k=0

h‚àáf ^{(x} ^{k} ^{),} ^{Œ∏} k i

kŒ∏ k k 2 ,

(51)

e Œæ k (x k ) ‚àí ‚àáf (x k ).
‚àáf

(52)

e Œæ k (x ^{k} ) and smoothness of f (1.3) we get that for all k = 0, 1, . . . , K ‚àí 1
Proof. Using x ^{k+1} = x ^{k} ‚àí Œ≥ ‚àáf

f (x k+1 )

‚â§

=

(52)

=

=

^{L} k+1
kx
‚àí x k k 2
2
2
e Œæ k (x k )i + LŒ≥ k ‚àáf
e Œæ k (x k )k 2
f (x ^{k} ) ‚àí Œ≥h‚àáf (x ^{k} ), ‚àáf
2
LŒ≥ 2
f (x ^{k} ) ‚àí Œ≥k‚àáf (x ^{k} )k ^{2} ‚àí Œ≥h‚àáf (x ^{k} ), Œ∏ k i +
kŒ∏ k k 2
2
LŒ≥ 2
k‚àáf ^{(x} ^{k} ^{)k} ^{2} ^{+} ^{LŒ≥} ^{2} h‚àáf ^{(x} ^{k} ^{),} ^{Œ∏} k i
+
2


LŒ≥ 2
LŒ≥
k
k‚àáf ^{(x} ^{k} ^{)k} ^{2} ‚àí ^{Œ≥(1} ‚àí ^{LŒ≥)h‚àáf} ^{(x} ^{k} ^{),} ^{Œ∏} k i ^{+}
kŒ∏ k k 2 .
f (x ) ‚àí Œ≥ 1 ‚àí
2
2

^{f} ^{(x} ^{k} ^{)} ^{+} h‚àáf ^{(x} ^{k} ^{),} ^{x} ^{k+1} ‚àí ^{x} ^{k} i ^{+}

We rearrange the terms and get



LŒ≥
Œ≥ 1 ‚àí
k‚àáf ^{(x} ^{k} ^{)k} ^{2}
2

‚â§

f (x ^{k} ) ‚àí f (x ^{k+1} ) ‚àí Œ≥(1 ‚àí LŒ≥)h‚àáf (x ^{k} ), Œ∏ k i +

19

LŒ≥ 2
kŒ∏ k k 2 .
2

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Finally, summing up these inequalities for k = 0, . . . , K ‚àí 1, we get
 K‚àí1

K‚àí1
K‚àí1
X
X

LŒ≥ X
k‚àáf ^{(x} ^{k} ^{)k} ^{2} ‚â§
f (x ^{k} ) ‚àí f (x ^{k+1} ) ‚àí Œ≥(1 ‚àí LŒ≥)
h‚àáf ^{(x} ^{k} ^{),} ^{Œ∏} k i
Œ≥ 1 ‚àí
2

k=0

k=0

+

LŒ≥
2

k=0

2 K‚àí1
X

k=0

kŒ∏ k k 2

(f (x ^{0} ) ‚àí f ‚àó ) ‚àí (f (x ^{K} ) ‚àí f ‚àó ) ‚àí Œ≥(1 ‚àí LŒ≥)

=

+

LŒ≥
2

2 K‚àí1
X

k=0

K‚àí1
X

k=0

h‚àáf ^{(x} ^{k} ^{),} ^{Œ∏} k i

kŒ∏ k k 2 ,

which concludes the proof.

Using this lemma, we prove the main convergence result for clipped-SGD in the non-convex case.
‚àö
‚àö
^{Theorem} ^{E.2.} ^{Let} ^{Assumptions} ^{1.1,} ^{1.2,} ^{1.3} ^{hold} ^{on} ^{Q} ^{=} {x ‚àà ^{R} ^{d} | ‚àÉy ‚àà ^{R} ^{d} ^{:} ^{f} ^{(y)} ‚â§ ^{f} ‚àó ^{+2‚àÜ} ^{and} kx‚àíyk ‚â§ ^{‚àÜ} ^{/} ^{20} ^{L} },
where ‚àÜ ‚â• ‚àÜ 0 = f (x ^{0} ) ‚àí f ‚àó , stepsize
Ô£±
Ô£º
Ô£¥
Ô£¥
‚àö
Ô£¥
Ô£¥
Ô£≤
Ô£Ω
1
‚àÜ
Œ≥ ‚â§ min
,
,
(53)
Œ±‚àí1


4(K+1)
‚àö
Ô£¥
Œ± Ô£¥
1
1
80L
ln
4(K+1)
Ô£¥
Ô£¥
Ô£≥
Œ≤
Ô£æ
27 Œ± 20œÉ LK Œ± ln

Œ≤

and clipping level

Œª k = Œª =

‚àö
‚àÜ

,
‚àö
20 LŒ≥ ln 4(K+1)
Œ≤

(54)

‚â• 1. Then, after K iterations of clipped-SGD the iterates with
for some K ‚â• 0 and Œ≤ ‚àà (0, 1] such that ln ^{4(K+1)}
Œ≤
probability at least 1 ‚àí Œ≤ satisfy

K

1 X
2‚àÜ

.
k‚àáf ^{(x} ^{k} ^{)k} ^{2} ‚â§ 
K +1
Œ≥ 1 ‚àí LŒ≥ (K + 1)

k=0

(55)

2

In particular, when Œ≥ equals the minimum from (53), then the iterates produced by clipped-SGD after K iterations with
probability at least 1 ‚àí Œ≤ satisfy
Ô£º Ô£∂
Ô£±
Ô£´
‚àö
Œ±‚àí1
K Ô£Ω
K
K
Ô£≤
Œ±
X
L‚àÜœÉ
ln
L‚àÜ
ln
1
Œ≤
Œ≤
Ô£∏ ,
(56)
,
k‚àáf ^{(x} ^{k} ^{)k} ^{2} ^{=} O Ô£≠ ^{max}
Œ±‚àí1
Ô£æ
Ô£≥ K
K +1
Œ±
K

k=0

1
^{meaning} ^{that} ^{to} ^{achieve} K+1

Ô£´

K
P

k=0

k‚àáf ^{(x} ^{k} ^{)k} ^{2} ‚â§ ^{Œµ} ^{with} ^{probability} ^{at} ^{least} ^{1} ‚àí ^{Œ≤} ^{clipped-SGD} ^{requires}

Ô£±
Ô£≤ L‚àÜ

L‚àÜ
ln
,
K = O Ô£≠ max
Ô£≥ Œµ
Œ≤Œµ

‚àö

L‚àÜœÉ
Œµ

Œ±
! Œ±‚àí1

Ô£´

1
ln Ô£≠
Œ≤

Ô£º Ô£∂
Œ± Ô£∂
! Œ±‚àí1
‚àö
Ô£Ω
L‚àÜœÉ
Ô£∏ Ô£∏
Ô£æ
Œµ

iterations/oracle calls.

(57)

Proof. Let ‚àÜ k = f (x ^{k} ) ‚àí f ‚àó for all k ‚â• 0. Next, our goal is to show by induction that ‚àÜ l ‚â§ 2‚àÜ with high probability,
which allows to apply the result of Lemma E.1 and then use Bernstein‚Äôs inequality to estimate the stochastic part of the
upper-bound. More precisely, for each k = 0, . . . , K + 1 we consider probability event E k defined as follows: inequalities

t‚àí1

t‚àí1

l=0

l=1

X
LŒ≥ 2 X
kŒ∏ l k ^{2} ‚àí Œ≥(1 ‚àí LŒ≥)
h‚àáf ^{(x} ^{l} ^{),} ^{Œ∏} l i ‚â§
2

‚àÜ t

20

‚â§

‚àÜ,

(58)

2‚àÜ

(59)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

hold for all t = 0, 1, . . . , k simultaneously. We want to prove via induction that P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) for all k =
0, 1, . . . , K + 1. For k = 0 the statement is trivial. Assume that the statement is true for some k = T ‚àí 1 ‚â§ K:
P{E T ‚àí1 } ‚â• 1 ‚àí ^{(T} ‚àí1)Œ≤ / (K+1) . One needs to prove that P{E T } ‚â• 1 ‚àí ^{T} ^{Œ≤} / (K+1) . First, we notice that probability event
E T ‚àí1 implies that ‚àÜ t ‚â§ 2‚àÜ for all t = 0, 1, . . . , T ‚àí 1, i.e., x ^{t} ‚àà {y ‚àà R ^{d} | f (y) ‚â§ f ‚àó + 2‚àÜ} for t = 0, 1, . . . , T ‚àí 1.
Moreover, due to the choice of clipping level Œª we have
‚àö
‚àö
‚àÜ
‚àÜ
(54)
T
T ‚àí1
T ‚àí1
e
‚àö
‚â§ ‚àö .
kx ‚àí x
k = Œ≥k ‚àáf Œæ T ‚àí1 (x
)k ‚â§ Œ≥Œª =
4K
20 L ln Œ≤
20 L

Therefore, E T ‚àí1 implies {x ^{k} } Tk=0 ‚äÜ Q, meaning that the assumptions of Lemma E.1 are satisfied and we have

A

t‚àí1
X

l=0

k‚àáf ^{(x} ^{l} ^{)k} ^{2}

‚â§

A

def

=

‚àÜ 0 ‚àí ‚àÜ t ‚àí Œ≥(1 ‚àí LŒ≥)



LŒ≥ (53)
Œ≥ 1 ‚àí
‚â• 0
2

t‚àí1
X

t‚àí1

h‚àáf ^{(x} ^{l} ^{),} ^{Œ∏} l i ^{+}

k=0

LŒ≥ 2 X
kŒ∏ l k 2 ,
2

(60)

l=0

(61)

for all t = 0, 1, . . . , T simultaneously and for all t = 1, . . . , T ‚àí 1 this probability event also implies that
!
t‚àí1
t‚àí1
t‚àí1
2 X
X
X
(58), 2‚àÜ
(60) 1
LŒ≥
‚àÜ ‚àí Œ≥(1 ‚àí LŒ≥)
h‚àáf ^{(x} ^{l} ^{),} ^{Œ∏} l i ^{+}
.
kŒ∏ l k 2 ‚â§
k‚àáf ^{(x} ^{l} ^{)k} ^{2} ‚â§
A
2
A

k=0

l=0

Taking into account that A

T P
‚àí1

t=0

(62)

l=0

k‚àáf ^{(x} ^{t} ^{)k} ^{2} ‚â• ^{0,} ^{we} ^{also} ^{derive} ^{that} ^{E} T ‚àí1 ^{implies}

‚àÜ T ‚â§ ‚àÜ +

T
‚àí1
T ‚àí1
X
LŒ≥ 2 X
h‚àáf ^{(x} ^{t} ^{),} ^{Œ∏} t i.
kŒ∏ t k ^{2} ‚àí Œ≥(1 ‚àí LŒ≥)
^{2} t=0
t=0

Next, we define random vectors

Œ∑ t =

(

‚àáf (x t ),
0,

(63)

‚àö
^{if} k‚àáf ^{(x} ^{t} ^{)k} ‚â§ ^{2} ^{L‚àÜ,}
otherwise,

for all t = 0, 1, . . . , T ‚àí 1. By definition these random vectors are bounded with probability 1
‚àö
kŒ∑ t k ‚â§ 2 L‚àÜ.

(64)

Moreover, for t = 1, . . . , T ‚àí 1 event E T ‚àí1 implies
p
‚àö
(53),(54) Œª
(7) q
,
k‚àáf ^{(x} ^{l} ^{)k} ‚â§ ^{2L(f} ^{(x} ^{l} ^{)} ‚àí ^{f} ‚àó ^{)} ^{=} ^{2L‚àÜ} l ‚â§ ^{2} ^{L‚àÜ} ‚â§
2

(65)

meaning that E T ‚àí1 implies that Œ∑ t = ‚àáf (x ^{t} ) for all t = 0, 1, . . . , T ‚àí 1. Next, we define the unbiased part and the bias of
Œ∏ t as Œ∏ t ^{u} and Œ∏ t ^{b} , respectively:
h
h
i
i
e Œæ t (x t ) ‚àí ‚àáf (x t ).
e Œæ t (x t ) , Œ∏ t b = E Œæ t ‚àáf
e Œæ t (x t ) ‚àí E Œæ t ‚àáf
(66)
Œ∏ t u = ‚àáf

We notice that Œ∏ t = Œ∏ t ^{u} + Œ∏ t ^{b} . Using new notation, we get that E T ‚àí1 implies

‚àÜ T

‚â§ ^{‚àÜ} ‚àíŒ≥(1 ‚àí ^{LŒ≥)}

|

T
‚àí1
X

t=0

{z

1

+ LŒ≥ 2

|

T
‚àí1
X

t=0

hŒ∏ t ^{u} ^{,} ^{Œ∑} t i ‚àíŒ≥(1 ‚àí ^{LŒ≥)}

} |

T
‚àí1
X

{z

t=0

hŒ∏ t b , Œ∑ t i + LŒ≥ 2

2

T
‚àí1
i
h
X
2
2
Œ∏ t b .
E Œæ t kŒ∏ t u k + LŒ≥ 2

{z

4

}

|

t=0

{z

5

21

}

}

|

T
‚àí1 
X

t=0

i
h
2
2
kŒ∏ t u k ‚àí E Œæ t kŒ∏ t u k

{z

3

}

(67)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

It remains to derive good enough high-probability upper-bounds for the terms 1, 2, 3, 4, 5, i.e., to finish our inductive
proof we need to show that 1 + 2 + 3 + 4 + 5 ‚â§ ‚àÜ with high probability. In the subsequent parts of the proof, we will
need to use many times the bounds for the norm and second moments of Œ∏ t ^{u} and Œ∏ t ^{b} . First, by definition of clipping operator,
we have with probability 1 that
kŒ∏ t ^{u} k ‚â§ 2Œª.
(68)

^{Moreover,} ^{since} ^{E} T ‚àí1 ^{implies} ^{that} k‚àáf ^{(x} ^{t} ^{)k} ‚â§ ^{Œª} ^{/} ^{2} ^{for} ^{t} ^{=} ^{0,} ^{1,} ^{.} ^{.} ^{.} ^{,} ^{T} ‚àí ^{1} ^{(see} ^{(65)),} ^{then,} ^{in} ^{view} ^{of} ^{Lemma} ^{5.1,} ^{we}
have that E T ‚àí1 implies

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1
2‚àíŒ± Œ±
18Œª
œÉ .

kŒ∏ t b k ‚â§
 u 2 
‚â§
E Œæ t kŒ∏ t k

(69)

(70)

Upper bound for 1. By definition of Œ∏ t ^{u} , we have E Œæ t [Œ∏ t ^{u} ] = 0 and

E Œæ t [‚àíŒ≥(1 ‚àí LŒ≥)hŒ∏ t ^{u} , Œ∑ t i] = 0.

Next, sum 1 has bounded with probability 1 terms:

(64),(68)

(53)

|Œ≥(1 ‚àí ^{LŒ≥)} hŒ∏ t ^{u} ^{,} ^{Œ∑} t i | ‚â§ ^{Œ≥kŒ∏} t ^{u} k ¬∑ kŒ∑ t k

‚â§

‚àö
(54)
4Œ≥Œª L‚àÜ =

‚àÜ

def

5 ln 4(K+1)
Œ≤

= c.

(71)

def

The summands also have bounded conditional variances œÉ t ^{2} = E Œæ t [Œ≥ ^{2} (1 ‚àí LŒ≥) ^{2} hŒ∏ t ^{u} , Œ∑ t i ^{2} ]:

 (64)

 (53)



œÉ t ^{2} ‚â§ E Œæ t Œ≥ ^{2} (1 ‚àí LŒ≥) ^{2} kŒ∏ t ^{u} k ^{2} ¬∑ kŒ∑ t k ^{2} ‚â§ 4Œ≥ ^{2} (1 ‚àí LŒ≥) ^{2} L‚àÜE Œæ t kŒ∏ t ^{u} k ^{2} ‚â§ 4Œ≥ ^{2} L‚àÜE Œæ t kŒ∏ t ^{u} k ^{2} .

(72)

‚àí1
^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {‚àíŒ≥(1 ‚àí ^{LŒ≥)} hŒ∏ t ^{u} ^{,} ^{Œ∑} t i} ^{T} t=0
is a bounded martingale difference sequence with bounded
2 T ‚àí1
^{conditional} ^{variances} {œÉ t } t=0 ^{.} ^{Next,} ^{we} ^{apply} ^{Bernstein‚Äôs} ^{inequality} ^{(Lemma} ^{B.2)} ^{with} ^{X} t ^{=} ‚àíŒ≥(1 ‚àí ^{LŒ≥)} hŒ∏ t ^{u} ^{,} ^{Œ∑} t i,
‚àÜ 2
parameter c as in (71), b = ^{‚àÜ}
4(K+1) ^{:}
5 , G =

150 ln

(

‚àÜ
^{P} |1| ^{>}
5

Œ≤

T
‚àí1
X

and

t=0

œÉ t 2 ‚â§

‚àÜ 2

150 ln ^{4(K+1)}
Œ≤

)


‚â§ 2 exp ‚àí

b 2
2G + 2cb / 3



=

Œ≤
.
2(K + 1)

Equivalently, we have

Œ≤
,
P {E 1 } ‚â• 1 ‚àí
2(K + 1)

for E 1 =

(

T
‚àí1
X

either

t=0

œÉ t 2 >

‚àÜ 2

‚àÜ
^{or} |1| ‚â§
5

150 ln ^{4(K+1)}
Œ≤

)

.

(73)

In addition, E T ‚àí1 implies that

T
‚àí1
X

œÉ t 2

t=0

(72)

‚â§

(54)

=

4Œ≥ 2 L‚àÜ

T
‚àí1
X

 (70)

E Œæ t kŒ∏ t ^{u} k ^{2} ‚â§ 72Œ≥ ^{2} L‚àÜœÉ ^{Œ±} T Œª ^{2‚àíŒ±}

t=0
4‚àíŒ± Œ±

‚àö Œ±
œÉ T L Œ≥ ^{Œ±} (53)
‚àÜ 2
.
‚â§
4(K+1)
50 ln 2‚àíŒ± Œ≤
150 ln ^{4(K+1)}
Œ≤

9 ¬∑ 20

Œ±

‚àö

‚àÜ

(74)

Upper bound for 2. From E T ‚àí1 it follows that

2

=

(54)

=

‚àíŒ≥(1 ‚àí ^{LŒ≥)}

40 Œ±
¬∑
10

T
‚àí1
X

(53)

hŒ∏ t b , Œ∑ t i ‚â§ Œ≥

T
‚àí1
X

t=0
t=0
‚àö 2‚àíŒ± ‚àö Œ± Œ±
Œ±
L Œ≥ (53) ‚àÜ
œÉ T ‚àÜ

ln 1‚àíŒ± 4(K+1)
Œ≤

‚â§

22

5

kŒ∏ t b k ¬∑ kŒ∑ t k

.

(64),(69) 2 ¬∑ 2 ^{Œ±} Œ≥œÉ ^{Œ±} T

‚â§

‚àö
L‚àÜ

Œª Œ±‚àí1

(75)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Upper bound for 3. First, we have

ii
h

h
= 0.
E Œæ t LŒ≥ 2 kŒ∏ t u k 2 ‚àí E Œæ t kŒ∏ t u k 2

Next, sum 3 has bounded with probability 1 terms:
i
h

2
2
‚â§
LŒ≥ 2 kŒ∏ t u k ‚àí E Œæ t kŒ∏ t u k

i
h

2
LŒ≥ 2 kŒ∏ t u k 2 + E Œæ t kŒ∏ t u k

(68)

def
The summands also have bounded conditional variances œÉ
e t 2 = E Œæ t

œÉ
e t 2

50 ln 2 4(K+1)
Œ≤

5 ln

‚â§

‚àÜ

def

5 ln 4(K+1)
Œ≤

= c.

(76)


i 2 
h

2
u
u 2
2 4
:
L Œ≥ kŒ∏ t k ‚àí E Œæ t kŒ∏ t k

i i
h
h
u 2
2
u 2
t kŒ∏ k
t
‚â§
LŒ≥
kŒ∏
k
‚àí
E
E
Œæ
Œæ
t
t
4(K+1)

‚àÜ

(76)

‚â§

‚àÜ

(54)

8LŒ≥ ^{2} Œª ^{2} =

‚â§

Œ≤

2LŒ≥ ^{2} ‚àÜ

5 ln 4(K+1)
Œ≤



E Œæ t kŒ∏ t u k 2 ,

(77)

io ^{T} ^{‚àí1}
h

n
u 2
u 2
2
t
kŒ∏
k
is a bounded martingale difference
kŒ∏
k
‚àí
E
since ln ^{4K}
‚â•
1.
In
other
words,
we
showed
that
LŒ≥
Œæ
t
t
Œ≤

t=0

‚àí1
sequence
conditional
variances {e
^{œÉ} t ^{2} } ^{T} t=0
. Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with X t =
i
h
 with bounded
2
‚àÜ 2
u
u 2
2
, parameter c as in (76), b = ^{‚àÜ}
LŒ≥ kŒ∏ t k ‚àí E Œæ t kŒ∏ t k
4(K+1) ^{:}
5 , G =

150 ln

(

^{P} |3| ^{>}

‚àÜ
5

T
‚àí1
X

and

t=0

Equivalently, we have

Œ≤
P {E 3 } ‚â• 1 ‚àí
,
2(K + 1)

œÉ
e t 2 ‚â§

‚àÜ 2

150 ln ^{4(K+1)}
Œ≤

for E 3 =

(

either

t=0

T
‚àí1
X

œÉ
e t 2 >

b 2
2G + 2cb / 3

‚àÜ 2

150 ln ^{4(K+1)}
Œ≤



=

Œ≤
.
2(K + 1)

‚àÜ
^{or} |3| ‚â§
5

)

 u 2  (70) 36LŒ≥ ^{2} ‚àÜŒª ^{2‚àíŒ±} œÉ ^{Œ±} T
t kŒ∏ k
‚â§
E
Œæ
t
5 ln 4(K+1)
5 ln 4(K+1)
t=0
Œ≤
Œ≤
‚àö 4‚àíŒ± ‚àö Œ± Œ±
Œ±
2
Œ±
(53)
œÉ T ‚àÜ
‚àÜ
L Œ≥
9 ¬∑ 20
¬∑
‚â§
.
3‚àíŒ± 4(K+1)
500
ln
150 ln ^{4(K+1)}

‚â§

(54)

=

.

(78)

T
‚àí1
X

2LŒ≥ ^{2} ‚àÜ

(77)

œÉ
e t 2

Œ≤


‚â§ 2 exp ‚àí

t=0

In addition, E T ‚àí1 implies that

T
‚àí1
X

)

Œ≤

(79)

Œ≤

Upper bound for 4. From E T ‚àí1 it follows that

4 =

‚àö Œ± Œ± Œ± ‚àö 2‚àíŒ±
h
i (70)
Œ±
(53) ‚àÜ
L Œ≥ œÉ T ‚àÜ
(54) ^{9} ¬∑ ^{20}
u 2
2 2‚àíŒ± Œ±
LŒ≥
E Œæ t kŒ∏ t k ‚â§ 18LŒ≥ Œª
œÉ T =
‚â§
¬∑
.
2‚àíŒ± 4(K+1)
200
5
ln
t=0

2

T
‚àí1
X

(80)

Œ≤

Upper bound for 5. From E T ‚àí1 it follows that

5

= LŒ≥ 2

T
‚àí1
X

Œ∏ t b

2 (69) 4

œÉ T LŒ≥ ^{2} (54) 1600 ^{Œ±} œÉ ^{2Œ±} T L ^{Œ±} Œ≥ ^{2Œ±} ‚àÜ ^{1‚àíŒ±} (53) ‚àÜ
=
¬∑
.
‚â§
400
5
Œª 2(Œ±‚àí1)
ln 2(1‚àíŒ±) 4(K+1)

Œ± 2Œ±

‚â§

t=0

Œ≤

Now, we have the upper bounds for 1, 2, 3, 4, 5. In particular, probability event E T ‚àí1 implies

(67)

‚àÜ T ‚â§ ‚àÜ + 1 + 2 + 3 + 4 + 5,

(75) ‚àÜ

2 ‚â§

T
‚àí1
X

t=0

(74)

œÉ t 2 ‚â§

5

(80) ‚àÜ

,

4 ‚â§

‚àÜ 2

150 ln

,
4(K+1)

,

5
T
‚àí1
X

t=0

Œ≤

23

(81) ‚àÜ

5 ‚â§

(79)

œÉ
e t 2 ‚â§

5

,

‚àÜ 2

150 ln ^{4(K+1)}
Œ≤

.

(81)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Moreover, we also have (see (73), (78) and our induction assumption)

P{E T ‚àí1 } ‚â• 1 ‚àí

(T ‚àí 1)Œ≤
,
K +1

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
2(K + 1)

Œ≤
,
2(K + 1)

P{E 3 } ‚â• 1 ‚àí

where

E 1

E 3

=

=

(

(

either

T
‚àí1
X

t=0

either

T
‚àí1
X

t=0

Thus, probability event E T ‚àí1 ‚à© E 1 ‚à© E 3 implies

‚àÜ T

œÉ t 2 >

‚â§

‚àÜ+

œÉ
e t 2 >

‚àÜ 2

150 ln ^{4(K+1)}
Œ≤

‚àÜ 2

150 ln ^{4(K+1)}
Œ≤

‚àÜ
^{or} |1| ‚â§
5

‚àÜ
^{or} |3| ‚â§
5

)

)

,

.

‚àÜ ‚àÜ ‚àÜ ‚àÜ ‚àÜ
+
+
+
+
= 2‚àÜ,
5
5
5
5
5

which is equivalent to (58) and (59) for t = T , and


TŒ≤
.
P{E T } ‚â• P {E T ‚àí1 ‚à© E 1 ‚à© E 3 } = 1 ‚àí P E T ‚àí1 ‚à™ E 1 ‚à™ E 3 ‚â• 1 ‚àí P{E T ‚àí1 } ‚àí P{E 1 } ‚àí P{E 3 } ‚â• 1 ‚àí
K +1

This finishes the inductive part of our proof, i.e., for all k = 0, 1, . . . , K + 1 we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) . In particular,
for k = K + 1 we have that with probability at least 1 ‚àí Œ≤

K

(62)
1 X
2‚àÜ
2‚àÜ
(53)

= 
k‚àáf ^{(x} ^{k} ^{)k} ^{2} ‚â§
LŒ≥
K +1
A(K + 1)
(K + 1)
Œ≥ 1 ‚àí

k=0

2

and {x ^{k} } ^{K}
k=0 ‚äÜ ^{Q,} ^{which} ^{follows} ^{from} ^{(59).}

Finally, if

Ô£º
Ô£¥
‚àö
Ô£¥
Ô£Ω
1
‚àÜ
,
,
Œ≥ ‚â§ min
 Œ±‚àí1

4(K+1)
‚àö
Ô£¥
Œ± Ô£¥
1
1
Ô£¥
Ô£¥
4(K+1)
Ô£æ
Ô£≥ 80L ln Œ≤
Œ±
Œ±
27 20œÉ LK
ln Œ≤

Ô£±
Ô£¥
Ô£¥
Ô£≤

then with probability at least 1 ‚àí Œ≤

K

1 X
k‚àáf ^{(x} ^{k} ^{)k} ^{2}
K +1

k=0

1
^{To} ^{get} K+1

K
P

k=0

2‚àÜ
4‚àÜ


‚â§
LŒ≥
Œ≥(K + 1)
Œ≥ 1 ‚àí 2 (K + 1)
Ô£±
Ô£º

 Œ±‚àí1
‚àö
‚àö
Ô£¥
Œ± Ô£¥
1
1
4(K+1)
Ô£¥
Ô£¥
4(K+1)
Ô£≤ 320‚àÜL ln
Ô£Ω
80 ‚àÜ27 Œ± œÉ LK Œ± ln Œ≤
Œ≤
= max
,
Ô£¥
Ô£¥
K +1
K +1
Ô£¥
Ô£¥
Ô£≥
Ô£æ
Ô£º
Ô£±
Ô£∂
Ô£´
‚àö
Œ±‚àí1
Ô£Ω
Ô£≤ L‚àÜ ln K
L‚àÜœÉ ln Œ± ^{K}
Œ≤
Œ≤
Ô£∏ .
,
= O Ô£≠ max
Œ±‚àí1
Ô£æ
Ô£≥ K
K Œ±

‚â§

k‚àáf ^{(x} ^{k} ^{)k} ^{2} ‚â§ ^{Œµ} ^{with} ^{probability} ^{at} ^{least} ^{1} ‚àí ^{Œ≤} ^{it} ^{is} ^{sufficient} ^{to} ^{choose} ^{K} ^{such} ^{that} ^{both} ^{terms} ^{in} ^{the}

^{maximum} ^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}
Ô£±
Ô£´
Ô£≤ L‚àÜ L‚àÜ
K = O Ô£≠ max
ln
,
Ô£≥ Œµ
ŒµŒ≤

which concludes the proof.

Ô£´
Œ±
! Œ±‚àí1
‚àö
L‚àÜœÉ
1
ln Ô£≠
Œµ
Œ≤

24

Ô£º Ô£∂
Œ± Ô£∂
! Œ±‚àí1
‚àö
Ô£Ω
L‚àÜœÉ
Ô£∏ Ô£∏ ,
Ô£æ
Œµ

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

E.2. Polyak-≈Åojasiewicz Functions

In this subsection, we provide a high-probability analysis of clipped-SGD in the case of Polyak-≈Åojasiewicz functions. As
in the non-convex case, we start with the lemma that handles optimization part of the algorithm and separates it from the
stochastic one.

‚àö

‚àö

Lemma E.3. Let Assumptions 1.3 and 1.4 hold on Q = {x ‚àà R ^{d} | ‚àÉy ‚àà R ^{d} : f (y) ‚â§ f ‚àó + 2‚àÜ and kx ‚àí yk ‚â§ ^{‚àÜ} / 20 L },
where ‚àÜ = f (x ^{0} ) ‚àí f ‚àó , and let stepsize Œ≥ satisfy Œ≥ ‚â§ L ^{1} . If x ^{k} ‚àà Q for all k = 0, 1, . . . , K + 1, K ‚â• 0, then after K
iterations of clipped-SGD for all x ‚àà Q we have

f (x K+1 ) ‚àí f ‚àó

‚â§

(1 ‚àí Œ≥Œº) ^{K+1} (f (x ^{0} ) ‚àí f ‚àó ) ‚àí Œ≥(1 ‚àí LŒ≥)

+

LŒ≥
2

K
2 X

k=0

K
X

^{(1} ‚àí ^{Œ≥Œº)} ^{K‚àík} h‚àáf ^{(x} ^{k} ^{),} ^{Œ∏} k i

k=0

(1 ‚àí Œ≥Œº) ^{K‚àík} kŒ∏ k k ^{2} ,

(82)

where Œ∏ k is defined in (52).

e Œæ k (x ^{k} ) and smoothness of f (1.3) we get that for all k = 0, 1, . . . , K
Proof. Using x ^{k+1} = x ^{k} ‚àí Œ≥ ‚àáf

f (x k+1 )

‚â§

‚â§

(52)

=

1
Œ≥‚â§ L

‚â§

(8)

‚â§

^{L} k+1
kx
‚àí x k k 2
2
2
e Œæ k (x k )k 2
e Œæ k (x k )i + LŒ≥ k ‚àáf
f (x ^{k} ) ‚àí Œ≥h‚àáf (x ^{k} ), ‚àáf
2


LŒ≥
LŒ≥ 2
k
f (x ) ‚àí Œ≥ 1 ‚àí
kŒ∏ k k 2
k‚àáf ^{(x} ^{k} ^{)k} ^{2} ‚àí ^{Œ≥(1} ‚àí ^{LŒ≥)h‚àáf} ^{(x} ^{k} ^{),} ^{Œ∏} k i ^{+}
2
2

^{f} ^{(x} ^{k} ^{)} ^{+} h‚àáf ^{(x} ^{k} ^{),} ^{x} ^{k+1} ‚àí ^{x} ^{k} i ^{+}

LŒ≥ 2
Œ≥
k‚àáf ^{(x} ^{k} ^{)k} ^{2} ‚àí ^{Œ≥(1} ‚àí ^{LŒ≥)h‚àáf} ^{(x} ^{k} ^{),} ^{Œ∏} k i ^{+}
kŒ∏ k k 2
2
2
LŒ≥ 2
kŒ∏ k k 2 .
f (x ^{k} ) ‚àí Œ≥Œº(f (x ^{k} ) ‚àí f ‚àó ) ‚àí Œ≥(1 ‚àí LŒ≥)h‚àáf (x ^{k} ), Œ∏ k i +
2

f (x k ) ‚àí

By rearranging the terms and subtracting f ‚àó , we obtain

f (x k+1 ) ‚àí f ‚àó

‚â§

(1 ‚àí Œ≥Œº)(f (x ^{k} ) ‚àí f ‚àó ) ‚àí Œ≥(1 ‚àí LŒ≥)h‚àáf (x ^{k} ), Œ∏ k i +

LŒ≥ 2
kŒ∏ k k 2 .
2

Unrolling the recurrence, we obtain (82).

^{Theorem} ^{E.4.} ^{Let} ^{Assumptions} ^{1.1,} ^{1.3,} ^{1.4} ^{hold} ^{on} ^{Q} ^{=} {x ‚àà ^{R} ^{d} | ‚àÉy ‚àà ^{R} ^{d} ^{:} ^{f} ^{(y)} ‚â§ ^{f} ‚àó ^{+2‚àÜ} ^{and} kx‚àíyk ‚â§
where ‚àÜ ‚â• ‚àÜ 0 = f (x ^{0} ) ‚àí f ‚àó , stepsize
(
)
1
ln(B K )
0 < Œ≥ ‚â§ min
,
,
Œº(K + 1)
250L ln ^{4(K+1)}
Œ≤
Ô£º
Ô£±
2(Œ±‚àí1)
Ô£Ω
Ô£≤
(K + 1) Œ± Œº 2 ‚àÜ


B K = max 2,
2(Œ±‚àí1)
4(K+1)
Ô£≥ 264600 Œ± ^{2} LœÉ ^{2} ln Œ±
ln 2 (B K ) Ô£æ
Œ≤
Ô£±
Ô£º Ô£∂
Ô£´
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥ Ô£∑
Ô£¨
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£≤
Ô£Ω Ô£∑
2
Ô£¨
Œ±
K
Œº
‚àÜ
Ô£¨
)! Ô£∑
(
= Œò Ô£¨ max 1,
,
 
2(Œ±‚àí1)
Ô£¥
Ô£¥ Ô£∑
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£≠
Ô£∏
2 ‚àÜ
Œ±
Œº
K
2
K
Ô£¥
Ô£¥
2
Ô£¥
Ô£¥
max 2,
2(Œ±‚àí1)
Ô£≥ LœÉ ln Œ±
Ô£æ
Œ≤ ln
LœÉ 2 ln Œ±
( KŒ≤ )

‚àö

‚àö
‚àÜ / 20 L },

(83)

(84)

(85)

and clipping level

‚àö
exp(‚àíŒ≥Œº(1 + ^{k} / 2 )) ‚àÜ
,
Œª k =
‚àö
120 LŒ≥ ln ^{4(K+1)}
Œ≤

25

(86)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

for some K > 0 and Œ≤ ‚àà (0, 1] such that ln ^{4(K+1)}
‚â• 1. Then, after K iterations of clipped-SGD the iterates with
Œ≤
probability at least 1 ‚àí Œ≤ satisfy

(87)
f x ^{K+1} ‚àí f ‚àó ‚â§ 2 exp(‚àíŒ≥Œº(K + 1))‚àÜ.

In particular, when Œ≥ equals the minimum from (83), then the iterates produced by clipped-SGD after K iterations with
probability at least 1 ‚àí Œ≤ satisfy
)! Ô£º Ô£∂
(
Ô£±
Ô£´
 
2(Œ±‚àí1)
2(Œ±‚àí1)
2
Ô£¥
Ô£¥
Œ±
Œº
‚àÜ
K
2
Ô£¥
Ô£¥
K
Ô£¥
Ô£¥
! LœÉ 2 ln Œ±
max 2,
2(Œ±‚àí1)
Ô£¨
Ô£¥
Ô£¥
Œ≤ ln
Ô£≤
Ô£Ω Ô£∑
K
2
Œ±
Ô£¨
Ô£∑
LœÉ ln

(
)
ŒºK
Œ≤
K
Ô£¨
Ô£∑ , (88)
f x ‚àí f ‚àó = O Ô£¨ max ‚àÜ exp ‚àí
,
2(Œ±‚àí1)
Ô£∑
K
Ô£¥
Ô£¥
2
L
ln
Œ±
Ô£¥
Ô£¥
K
Œº
Ô£≠
Ô£∏
Œ≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£æ


meaning that to achieve f x ^{K} ‚àí f ‚àó ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SGD requires
!
Œ±
  ^{2}  2(Œ±‚àí1)

 Œ± !
  
Œ±
L
L
LœÉ
‚àÜ
1 LœÉ ^{2} 2(Œ±‚àí1)
‚àÜ
K = O
ln
,
ln Œ±‚àí1 (B Œµ ) ,
ln
ln
ln
Œº
Œµ
ŒºŒ≤
Œµ
Œº 2 Œµ
Œ≤ Œº 2 Œµ

iterations/oracle calls, where

B Œµ = max

Ô£±
Ô£¥
Ô£¥
Ô£≤

Ô£¥
Ô£¥
Ô£≥

2,

Œµ ln

 

1
Œ≤

‚àÜ

LœÉ 2
Œº 2 Œµ

(89)

Ô£º
Ô£¥
Ô£¥
Ô£Ω

 .
Œ±
 2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£æ

Proof. As in the previous results, the proof is based on the induction argument and shows that the iterates do not leave
some set with high probability. More precisely, for each k = 0, 1, . . . , K + 1 we consider probability event E k as follows:
inequalities
‚àÜ t ‚â§ 2 exp(‚àíŒ≥Œºt)‚àÜ
(90)

hold for t = 0, 1, . . . , k simultaneously, where ‚àÜ t = f (x ^{t} ) ‚àí f ‚àó . We want to prove P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) for all
k = 0, 1, . . . , K + 1 by induction. The base of the induction is trivial: for k = 0 we have ‚àÜ 0 ‚â§ ‚àÜ < 2‚àÜ by definition.
Next, assume that for k = T ‚àí 1 ‚â§ K the statement holds: P{E T ‚àí1 } ‚â• 1 ‚àí ^{(T} ‚àí1)Œ≤ / (K+1) . Given this, we need to
prove P{E T } ‚â• 1 ‚àí ^{T} ^{Œ≤} / (K+1) . Since ‚àÜ t ‚â§ 2 exp(‚àíŒ≥Œºt)‚àÜ ‚â§ 2‚àÜ, we have x ^{t} ‚àà {y ‚àà R ^{d} | f (y) ‚â§ f ‚àó + 2‚àÜ} for
t = 0, 1, . . . , T ‚àí 1, where function f is L-smooth. Thus, E T ‚àí1 implies

k‚àáf ^{(x} ^{t} ^{)k}

(7)

‚â§

for all t = 0, 1, . . . , T ‚àí 1. Moreover

T

p
(83),(86) Œª t
(90) p
2L(f (x ^{t} ) ‚àí f ‚àó ) ‚â§ 2 L exp(‚àíŒ≥Œºt)‚àÜ ‚â§
2

T ‚àí1

kx ‚àí x

(86)

e Œæ T ‚àí1 (x T ‚àí1 )k ‚â§ Œ≥Œª T ‚àí1 ‚â§
k = Œ≥k ‚àáf

‚àö
‚àÜ
‚àö ,
20 L

meaning that E T ‚àí1 implies x ^{T} ‚àà {x ‚àà R ^{d} | ‚àÉy ‚àà R ^{d} : f (y) ‚â§ f ‚àó + 2‚àÜ and kx ‚àí yk ‚â§
and (1 ‚àí Œ≥Œº) ^{T} ‚â§ exp(‚àíŒ≥ŒºT ), we obtain that E T ‚àí1 implies

‚àÜ T

‚â§

exp(‚àíŒ≥ŒºT )‚àÜ ‚àí Œ≥(1 ‚àí LŒ≥)

+

LŒ≥
2

‚àí1
2 T
X

l=0

(91)

‚àö

‚àö
^{‚àÜ} / 20 L }. Using Lemma E.3

T
‚àí1
X

l=0

^{(1} ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àíl} h‚àáf ^{(x} ^{l} ^{),} ^{Œ∏} l i

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l k ^{2} .

To handle the sums above, we introduce a new notation:
(
‚àö
‚àö
‚àáf ^{(x} ^{t} ^{),} ^{if} k‚àáf ^{(x} ^{t} ^{)k} ‚â§ ^{2} ^{L} ^{exp(‚àí} ^{Œ≥Œºt} ^{/} ^{2} ^{)} ^{‚àÜ,}
Œ∑ t =
0,
otherwise,

26

(92)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

for t = 0, 1, . . . , T ‚àí 1. These vectors are bounded almost surely:
‚àö
‚àö
kŒ∑ t k ‚â§ 2 L exp(‚àí ^{Œ≥Œºt} / 2 ) ‚àÜ

(93)

for all t = 0, 1, . . . , T ‚àí 1. In other words, E T ‚àí1 implies Œ∑ t = ‚àáf (x ^{t} ) for all t = 0, 1, . . . , T ‚àí 1, meaning that from
E T ‚àí1 it follows that

‚àÜ T

‚â§

exp(‚àíŒ≥ŒºT )‚àÜ ‚àí Œ≥(1 ‚àí LŒ≥)

+

LŒ≥
2

‚àí1
2 T
X

l=0

T
‚àí1
X

l=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l i

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l k ^{2} .

To handle the sums appeared on the right-hand side of the previous inequality we consider unbiased and biased parts of Œ∏ l :
h
h
i
i
e Œæ t (x t ) ‚àí ‚àáf (x t ).
e Œæ t (x t ) , Œ∏ b = E Œæ t ‚àáf
e Œæ t (x t ) ‚àí E Œæ t ‚àáf
(94)
Œ∏ t u = ‚àáf
t

for all l = 0, . . . , T ‚àí 1. By definition we have Œ∏ l = Œ∏ l ^{u} + Œ∏ l ^{b} for all l = 0, . . . , T ‚àí 1. Therefore, E T ‚àí1 implies

‚àÜ T

‚â§

^{exp(‚àíŒ≥ŒºT} ^{)‚àÜ} ‚àíŒ≥(1 ‚àí ^{LŒ≥)}

+ LŒ≥ 2

|

+ LŒ≥ 2

|

T
‚àí1
X

l=0

|

T
‚àí1
X

l=0

^{(1} ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àíl} hŒ∑ l ^{,} ^{Œ∏} l ^{u} i ‚àíŒ≥(1 ‚àí ^{LŒ≥)}

{z

} |

1

l=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l ^{b} i

{z

2

T
‚àí1
X




(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl E Œæ l kŒ∏ l ^{u} k ^{2} + LŒ≥ ^{2}
(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l ^{u} k ^{2} ‚àí E Œæ l kŒ∏ l ^{u} k ^{2}

{z

}

3

T
‚àí1
X

l=0

T
‚àí1
X

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l ^{b} k ^{2} .

{z

|

l=0

{z

4

}

}

(95)

}

5

where we also apply inequality ka + bk ^{2} ‚â§ 2kak ^{2} + 2kbk ^{2} holding for all a, b ‚àà R ^{d} to upper bound kŒ∏ l k ^{2} . It remains to
derive good enough high-probability upper-bounds for the terms 1, 2, 3, 4, 5, i.e., to finish our inductive proof we need
to show that 1 + 2 + 3 + 4 + 5 ‚â§ exp(‚àíŒ≥ŒºT )‚àÜ with high probability. In the subsequent parts of the proof, we will
need to use many times the bounds for the norm and second moments of Œ∏ t ^{u} and Œ∏ t ^{b} . First, by definition of clipping operator,
we have with probability 1 that
kŒ∏ l u k ‚â§ 2Œª l .
(96)

^{Moreover,} ^{since} ^{E} T ‚àí1 ^{implies} ^{that} k‚àáf ^{(x} ^{l} ^{)k} ^{2} ‚â§ ^{Œª} ^{l} ^{/} ^{2} ^{for} ^{all} ^{l} ^{=} ^{0,} ^{1,} ^{.} ^{.} ^{.} ^{,} ^{T} ‚àí ^{1} ^{(see} ^{(91)),} ^{from} ^{Lemma} ^{5.1} ^{we} ^{also} ^{have}
that E T ‚àí1 implies

Œ∏ l b ‚â§

for all l = 0, 1, . . . , T ‚àí 1.

Upper bound for 1.

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1
l

i
h
2
œÉ Œ± ,
E Œæ l kŒ∏ l ^{u} k ‚â§ 18Œª ^{2‚àíŒ±}
l

(97)

(98)

By definition of Œ∏ l ^{u} , we have E Œæ l [Œ∏ l ^{u} ] = 0 and


E Œæ l ‚àíŒ≥(1 ‚àí ^{LŒ≥)(1} ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àíl} hŒ∑ l ^{,} ^{Œ∏} l ^{u} i ^{=} ^{0.}

Next, sum 1 has bounded with probability 1 terms:

| ‚àí Œ≥(1 ‚àí LŒ≥)(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l ^{u} i|

(83)

‚â§

(93),(96)

‚â§

(83),(86)

‚â§

27

Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí l))kŒ∑ l k ¬∑ kŒ∏ l ^{u} k
‚àö
4 L‚àÜŒ≥ exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí ^{l} / 2 ))Œª l
exp(‚àíŒ≥ŒºT )‚àÜ def
= c.
5 ln 4(K+1)
Œ≤

(99)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance



def
The summands also have bounded conditional variances œÉ l ^{2} = E Œæ l Œ≥ ^{2} (1 ‚àí LŒ≥) ^{2} (1 ‚àí Œ≥Œº) ^{2T} ‚àí2‚àí2l hŒ∑ l , Œ∏ l ^{u} i ^{2} :

œÉ l 2



E Œæ l Œ≥ ^{2} (1 ‚àí LŒ≥) ^{2} exp(‚àíŒ≥Œº(2T ‚àí 2 ‚àí 2l))kŒ∑ l k ^{2} ¬∑ kŒ∏ l ^{u} k ^{2}


4Œ≥ ^{2} L‚àÜ exp(‚àíŒ≥Œº(2T ‚àí 2 ‚àí l))E Œæ l kŒ∏ l ^{u} k ^{2}


10Œ≥ ^{2} L‚àÜ exp(‚àíŒ≥Œº(2T ‚àí l))R ^{2} E Œæ l kŒ∏ l ^{u} k ^{2} .

‚â§

(93),(83)

‚â§

(83)

‚â§

(100)

‚àí1
^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {‚àíŒ≥(1 ‚àí ^{LŒ≥)(1} ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àíl} hŒ∑ l ^{,} ^{Œ∏} l ^{u} i} ^{T} l=0
is a bounded martingale difference sequence
2 T ‚àí1
^{with} ^{bounded} ^{conditional} ^{variances} {œÉ l } l=0 ^{.} ^{Next,} ^{we} ^{apply} ^{Bernstein‚Äôs} ^{inequality} ^{(Lemma} ^{B.2)} ^{with} ^{X} l ^{=} ‚àíŒ≥(1 ‚àí

2

)‚àÜ
LŒ≥)(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l ^{u} i, parameter c as in (99), b = 15 exp(‚àíŒ≥ŒºT )‚àÜ, G = ^{exp(‚àí2Œ≥ŒºT}
4(K+1) ^{:}

150 ln

(

T ‚àí1

X
1
exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}
^{P} |1| ^{>} ^{exp(‚àíŒ≥ŒºT} ^{)‚àÜ} ^{and}
œÉ l 2 ‚â§
5
150 ln ^{4(K+1)}

l=0

Œ≤

)


‚â§ 2 exp ‚àí

Œ≤

b 2
2G + 2cb / 3



=

Œ≤
.
2(K + 1)

Equivalently, we have

Œ≤
P{E 1 } ‚â• 1 ‚àí
,
2(K + 1)

for E 1 =

(

either

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}
œÉ l 2 >
150 ln ^{4(K+1)}
l=0
Œ≤

)
1
^{or} |1| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)‚àÜ} ^{.} ^{(101)}
5

In addition, E T ‚àí1 implies that

T
‚àí1
X

l=0

œÉ l 2

(100)

10Œ≥ ^{2} L‚àÜ exp(‚àí2Œ≥ŒºT )

‚â§

T
‚àí1
X

l=0

^{(98),T} ‚â§K+1

180Œ≥ ^{2} L‚àÜ exp(‚àí2Œ≥ŒºT )œÉ ^{Œ±}

‚â§



E Œæ l kŒ∏ l u k 2
exp(‚àíŒ≥Œºl)

K
X

l=0

Œª 2‚àíŒ±
l
exp(‚àíŒ≥Œºl)

‚àö Œ± ‚àö 4‚àíŒ±
K
exp(‚àí2Œ≥ŒºT )œÉ ^{Œ±} X
180Œ≥ L ‚àÜ

Œ±

(86)

=

180Œ≥

=

120 2‚àíŒ± ln ^{2‚àíŒ±} ^{4(K+1)}
Œ≤
‚àö Œ± ‚àö 4‚àíŒ±
Œ±

L

‚àÜ

exp(‚àí2Œ≥ŒºT )œÉ

l=0

K
Œ± X

1
¬∑ (exp(‚àíŒ≥Œº(1 + l / 2 ))) ^{2‚àíŒ±}
exp(‚àíŒ≥Œºl)

exp(Œ≥Œº(Œ± ‚àí 2)) ¬∑ exp

120 2‚àíŒ± ln ^{2‚àíŒ±} ^{4(K+1)}
l=0
Œ≤
‚àö Œ± ‚àö 4‚àíŒ±
Œ±
Œ±
180Œ≥ L ‚àÜ
exp(‚àí2Œ≥ŒºT )œÉ (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

‚â§



Œ≥ŒºŒ±l
2



120 2‚àíŒ± ln ^{2‚àíŒ±} ^{4(K+1)}
Œ≤

exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}

(83)

‚â§

150 ln ^{4(K+1)}
Œ≤

,

(102)

where we also show that E T ‚àí1 implies

‚àö Œ± ‚àö 4‚àíŒ±
(K + 1) exp( ^{Œ≥ŒºŒ±K}
Œ≥ Œ± L ‚àÜ
Œª 2‚àíŒ±
2 )
l
.
‚â§
Œ≥ L‚àÜ
4(K+1)
2‚àíŒ±
exp(‚àíŒ≥Œºl)
120 ^{2‚àíŒ±} ln
l=0
Œ≤

2

K
X

28

(103)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Upper bound for 2.

(83)

2

‚â§

(93),(97)

‚â§

(86)

=

From E T ‚àí1 it follows that

Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1))

T
‚àí1
X

l=0

kŒ∑ l k ¬∑ kŒ∏ l b k
exp(‚àíŒ≥Œºl)

T
‚àí1
X
‚àö
2 ^{1+Œ±} Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1)) ‚àÜœÉ ^{Œ±}

1
Œ±‚àí1
Œ≥Œºl / 2 )
Œª
exp(‚àí
l=0 ^{l}

T
‚àí1
X
‚àö 1‚àíŒ± ‚àö 2‚àíŒ±
2 ^{1+Œ±} ¬∑ 120 ^{Œ±‚àí1} L
‚àÜ
exp(‚àíŒ≥Œº(T ‚àí 1))Œ≥ ^{Œ±} œÉ ^{Œ±} ln ^{Œ±‚àí1} ^{4(K+1)}
Œ≤

exp( ^{Œ≥Œºl} / 2 )

exp (‚àíŒ≥Œº(1 + ^{l} / 2 ))


K
X
‚àö 1‚àíŒ± ‚àö 2‚àíŒ±
Œ≥ŒºŒ±l
Œ±‚àí1 4(K+1)
1+Œ±
Œ±‚àí1
Œ± Œ±
2
¬∑ 120
exp
L
‚àÜ
exp(‚àíŒ≥Œº(T ‚àí 1))Œ≥ œÉ ln
Œ≤
2
l=0


‚àö 1‚àíŒ± ‚àö 2‚àíŒ±
Œ≥ŒºŒ±K
Œ±‚àí1 4(K+1)
1+Œ±
Œ±‚àí1
Œ± Œ±
(K + 1) exp
2
¬∑ 120
L
‚àÜ
exp(‚àíŒ≥Œº(T ‚àí 1))Œ≥ œÉ ln
Œ≤
2
1
exp(‚àíŒ≥ŒºT )‚àÜ.
5

Œ±‚àí1

l=0

^{T} ‚â§K+1

‚â§

‚â§

(83)

‚â§

Upper bound for 3.

From E T ‚àí1 it follows that

3

=

(98)

2

LŒ≥ exp(‚àíŒ≥Œº(T ‚àí 1))

T
‚àí1
X

l=0



E Œæ l kŒ∏ l u k 2
exp(‚àíŒ≥Œºl)

T
‚àí1
X

Œª 2‚àíŒ±
l
exp(‚àíŒ≥Œºl)

‚â§

18LŒ≥ ^{2} exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{Œ±}

(103)

‚àö Œ± ‚àö 2‚àíŒ±
18Œ≥ L ‚àÜ
exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{Œ±} (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

‚â§

(83)

‚â§

Upper bound for 4.

(104)

Œ±

l=0

120 2‚àíŒ± ln ^{2‚àíŒ±} ^{4(K+1)}
Œ≤

1
exp(‚àíŒ≥ŒºT )‚àÜ.
5

(105)

First, we have

h

i
LŒ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl E Œæ l kŒ∏ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} = 0.

Next, sum 4 has bounded with probability 1 terms:



LŒ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l ^{u} k ^{2} ‚àí E Œæ l kŒ∏ l ^{u} k ^{2}

(96)

‚â§

(86)

=

‚â§

def

=

8LŒ≥ ^{2} exp(‚àíŒ≥ŒºT )Œª ^{2} l
exp(‚àíŒ≥Œº(1 + l))
exp(‚àíŒ≥Œº(T + 1))‚àÜ

1800 ln ^{2} ^{4(K+1)}
Œ≤

exp(‚àíŒ≥ŒºT )‚àÜ

5 ln 4(K+1)
Œ≤

c.

The summands also have conditional variances
h
 2 i

def
œÉ
b l ^{2} = E Œæ l L ^{2} Œ≥ ^{4} (1 ‚àí Œ≥Œº) ^{2T} ‚àí2‚àí2l kŒ∏ l ^{u} k ^{2} ‚àí E Œæ l kŒ∏ l ^{u} k ^{2}

29

(106)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

that are bounded

œÉ
b l 2

LŒ≥ ^{2} exp(‚àí2Œ≥ŒºT )‚àÜ

(106)

‚â§

‚â§

5 exp(‚àíŒ≥Œº(1 + l)) ln ^{4(K+1)}
Œ≤
2

2LŒ≥ exp(‚àí2Œ≥ŒºT )‚àÜ

5 exp(‚àíŒ≥Œº(1 + l)) ln ^{4(K+1)}
Œ≤

E Œæ l




 
kŒ∏ l u k 2 ‚àí E Œæ l kŒ∏ l u k 2



E Œæ l kŒ∏ l u k 2 .

(107)



 T ‚àí1
In other words, we showed that LŒ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l ^{u} k ^{2} ‚àí E Œæ l kŒ∏ l ^{u} k ^{2}
is a bounded martingale difference
l=0
2 T ‚àí1
sequence with bounded conditional variances {b
^{œÉ} l } l=0 ^{.} ^{Next,} ^{we} ^{apply} ^{Bernstein‚Äôs} ^{inequality} ^{(Lemma} ^{B.2)} ^{with} ^{X} l ^{=}
 u 2 
)‚àÜ 2
2
^{T} ‚àí1‚àíl
u 2
LŒ≥ (1 ‚àí Œ≥Œº)
kŒ∏ l k ‚àí E Œæ l kŒ∏ l k , parameter c as in (106), b = 5 ^{1} exp(‚àíŒ≥ŒºT )‚àÜ, G = ^{exp(‚àí2Œ≥ŒºT}
4(K+1) ^{:}

150 ln

(

T ‚àí1

X
exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}
1
œÉ
b l 2 ‚â§
^{P} |4| ^{>} ^{exp(‚àíŒ≥ŒºT} ^{)‚àÜ} ^{and}
5
150 ln ^{4(K+1)}

l=0

Œ≤

)


‚â§ 2 exp ‚àí

b 2
2G + 2cb / 3



=

Œ≤

Œ≤
.
2(K + 1)

Equivalently, we have

Œ≤
P{E 4 } ‚â• 1 ‚àí
,
2(K + 1)

for E 4 =

(

T
‚àí1
X

either

l=0

In addition, E T ‚àí1 implies that

T
‚àí1
X

l=0

œÉ
b l 2

(107)

‚â§

^{(98),T} ‚â§K+1

‚â§

(103)

‚â§

(53)

‚â§

Upper bound for 5.

5

=

LŒ≥ 2

‚â§

^{(86),T} ‚â§K+1

‚â§

‚â§

‚â§

(53)

‚â§

150 ln ^{4(K+1)}
Œ≤

)
1
^{or} |4| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)‚àÜ} ^{.} ^{(108)}
5



T ‚àí1
2LŒ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí 1))‚àÜ X E Œæ l kŒ∏ l ^{u} k ^{2}
exp(‚àíŒ≥Œºl)
5 ln 4(K+1)

l=0

Œ≤

K

36LŒ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí 1))‚àÜœÉ ^{Œ±} X

5 ln 4(K+1)
Œ≤

l=0

Œª 2‚àíŒ±
l
exp(‚àíŒ≥Œºl)

‚àö Œ±
‚àö 4‚àíŒ± Œ±
36 L Œ≥ ^{Œ±} exp(‚àíŒ≥Œº(2T ‚àí 1)) ‚àÜ
œÉ (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}

150 ln ^{4(K+1)}
Œ≤

5 ¬∑ 120 2‚àíŒ± ln ^{3‚àíŒ±} ^{4(K+1)}
Œ≤

.

(109)

From E T ‚àí1 it follows that

T
‚àí1
X

l=0

(97)

œÉ
b l 2 >

exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}

exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí l))kŒ∏ l ^{b} k ^{2}

2 ^{2Œ±} LŒ≥ ^{2} exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{2Œ±}

T
‚àí1
X

1

Œª ^{2Œ±‚àí2} exp(‚àíŒ≥Œºl)
l=0 ^{l}

‚àö 2Œ±



K
X
2 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} L exp(‚àíŒ≥ŒºT )œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{4(K+1)}
l
Œ≤
exp Œ≥Œº(2Œ± ‚àí 2) 1 +
exp(Œ≥Œºl)
‚àö 2Œ±‚àí2
2
‚àÜ
l=0
‚àö 2Œ±
K
X
4 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} L exp(‚àíŒ≥ŒºT )œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{4(K+1)}
Œ≤
exp(Œ≥ŒºŒ±l)
‚àö 2Œ±‚àí2
‚àÜ
l=0
‚àö 2Œ±
(K + 1) exp(Œ≥ŒºŒ±K)
4 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} L exp(‚àíŒ≥ŒºT )œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{4(K+1)}
Œ≤
‚àö 2Œ±‚àí2
‚àÜ
1
exp(‚àíŒ≥ŒºT )‚àÜ.
(110)
5

30

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Now, we have the upper bounds for 1, 2, 3, 4, 5. In particular, probability event E T ‚àí1 implies

(95)

(104) 1

2 ‚â§

‚àÜ T ‚â§ exp(‚àíŒ≥ŒºT )‚àÜ + 1 + 2 + 3 + 4 + 5,

(105) 1

exp(‚àíŒ≥ŒºT )‚àÜ,

5
T
‚àí1
X

l=0

3 ‚â§

5

exp(‚àíŒ≥ŒºT )‚àÜ,

^{(102)} exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}

œÉ l 2 ‚â§

150 ln ^{4(K+1)}
Œ≤

T
‚àí1
X

,

l=0

(110) 1

5 ‚â§

5

exp(‚àíŒ≥ŒºT )‚àÜ,

^{(109)} exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}

œÉ
b l 2 ‚â§

150 ln ^{4(K+1)}
Œ≤

.

Moreover, we also have (see (101), (108) and our induction assumption)

P{E T ‚àí1 } ‚â• 1 ‚àí

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
2(K + 1)

(T ‚àí 1)Œ≤
,
K +1

P{E 4 } ‚â• 1 ‚àí

Œ≤
,
2(K + 1)

where

E 1

E 4

=

=

(

either

T
‚àí1
X

œÉ l 2 >

l=0

(

either

T
‚àí1
X

l=0

œÉ
b l 2 >

exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}

150 ln ^{4(K+1)}
Œ≤

exp(‚àí2Œ≥ŒºT )‚àÜ ^{2}

150 ln ^{4(K+1)}
Œ≤

)
1
^{or} |1| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)‚àÜ} ^{,}
5
)
1
^{or} |4| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)‚àÜ} ^{.}
5

Thus, probability event E T ‚àí1 ‚à© E 1 ‚à© E 4 implies

‚àÜ T

(95)

‚â§
‚â§

exp(‚àíŒ≥ŒºT )‚àÜ + 1 + 2 + 3 + 4 + 5
2 exp(‚àíŒ≥ŒºT )‚àÜ,

which is equivalent to (90) for t = T , and

P{E T } ‚â• P{E T ‚àí1 ‚à© E 1 ‚à© E 4 } = 1 ‚àí P{E T ‚àí1 ‚à™ E 1 ‚à™ E 4 } ‚â• 1 ‚àí

TŒ≤
.
K +1

This finishes the inductive part of our proof, i.e., for all k = 0, 1, . . . , K + 1 we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) . In particular,
for k = K + 1 we have that with probability at least 1 ‚àí Œ≤

f (x ^{K+1} ) ‚àí f ‚àó ‚â§ 2 exp(‚àíŒ≥Œº(K + 1))‚àÜ.

Finally, if

Œ≥

=

B K

=

=

)
ln(B K )
,
,
min
Œº(K + 1)
250L ln ^{4(K+1)}
Œ≤
Ô£±
Ô£º
2(Œ±‚àí1)
Ô£≤
Ô£Ω
(K + 1) Œ± Œº 2 ‚àÜ


max 2,
6(K+1)
Ô£≥ 264600 Œ± ^{2} LœÉ 2 ln ^{2(Œ±‚àí1)}
Œ±
ln 2 (B K ) Ô£æ
Œ≤
Ô£º Ô£∂
Ô£±
Ô£´
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¨
2(Œ±‚àí1)
Ô£Ω Ô£∑
Ô£≤
2
Ô£∑
Ô£¨
Œ±
Œº
‚àÜ
K
Ô£∑
)!
(
2,
O Ô£¨
max
Ô£∑
Ô£¨
 
2(Œ±‚àí1)
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£∏
2 ‚àÜ
Ô£≠
Œ±
2
K
Œº
K
Ô£¥
Ô£¥
2 ln Œ±
Ô£¥
Ô£¥
LœÉ
max
2,
ln
2(Œ±‚àí1)
Ô£æ
Ô£≥
Œ≤
K
LœÉ 2 ln Œ±
( Œ≤ )

(

1

31

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

then with probability at least 1 ‚àí Œ≤

f (x K+1 ) ‚àí f ‚àó

‚â§

=

=

2 exp(‚àíŒ≥Œº(K + 1))‚àÜ
)
!
(
1
Œº(K + 1)
,
2‚àÜ max exp ‚àí
B K
250L ln ^{4(K+1)}
Œ≤
(
)! Ô£º Ô£∂
Ô£±
Ô£´
 
2(Œ±‚àí1)
2(Œ±‚àí1)
2
Ô£¥
Ô£¥
Œ±
K
2
Œº
‚àÜ
Ô£¥
Ô£¥
K
2
Œ±
Ô£¥
Ô£¥
! LœÉ ln
max 2,
2(Œ±‚àí1)
Ô£¨
Ô£¥
Ô£¥
Œ≤ ln
Ô£≤
Ô£Ω Ô£∑
K
2
Ô£¨
Ô£∑
LœÉ ln Œ± ( Œ≤ )
ŒºK
Ô£¨
Ô£∑ .
,
O Ô£¨ max ‚àÜ exp ‚àí
2(Œ±‚àí1)
Ô£∑
K
Ô£¥
Ô£¥
2
L
ln
Œ±
Ô£¥
Ô£¥
K
Œº
Ô£≠
Ô£∏
Œ≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£≥
Ô£æ

To get kx ^{K+1} ‚àí x ‚àó k ^{2} ‚â§ Œµ with probability at least 1 ‚àí Œ≤ it is sufficient to choose K such that both terms in the maximum
^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}
!
Œ±
  ^{2}  2(Œ±‚àí1)

 Œ± !
  
Œ±
L
LœÉ
‚àÜ
1 LœÉ ^{2} 2(Œ±‚àí1)
‚àÜ
L
ln
,
ln Œ±‚àí1 (B Œµ ) ,
ln
ln
ln
K = O
Œº
Œµ
ŒºŒ≤
Œµ
Œº 2 Œµ
Œ≤ Œº 2 Œµ

where

B Œµ = max

This concludes the proof.

Ô£±
Ô£¥
Ô£¥
Ô£≤

Ô£¥
Ô£¥
Ô£≥

2,

Œµ ln

 

1
Œ≤

‚àÜ

LœÉ 2
Œº 2 Œµ

Ô£º
Ô£¥
Ô£¥
Ô£Ω

 .
Œ±
 2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£æ

E.3. Convex Functions

Now, we focus on the case of convex functions. We start with the following lemma.

Lemma E.5. Let Assumptions 1.3 and 1.6 with Œº = 0 hold on Q = B 2R (x ‚àó ), where R ‚â• kx ^{0} ‚àí x ‚àó k, and let stepsize Œ≥
satisfy Œ≥ ‚â§ L ^{1} . If x ^{k} ‚àà Q for all k = 0, 1, . . . , K + 1, K ‚â• 0, then after K iterations of clipped-SGD we have


Œ≥ f (x K ) ‚àí f (x ‚àó ) ‚â§

kx 0 ‚àí x ‚àó k 2 ‚àí kx K+1 ‚àí x ‚àó k 2
K +1
K
K
2Œ≥ X k
Œ≥ 2 X
‚àí
hx ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{k} ), Œ∏ k i +
kŒ∏ k k 2 ,
K +1
K +1

k=0

(111)

k=0

K

x K

=

1 X k
x ,
K +1

(112)

k=0

where Œ∏ k is defined in (52).

e Œæ k (x ^{k} ), we derive for all k = 0, 1, . . . , K that
Proof. Using x ^{k+1} = x ^{k} ‚àí Œ≥ ‚àáf

kx k+1 ‚àí x ‚àó k 2

=

=

(10),Œº=0

‚â§

(7)

‚â§

Œ≥‚â§ 1 / L

‚â§

e Œæ k (x k )i + Œ≥ 2 k ‚àáf
e Œæ k (x k )k 2
kx ^{k} ‚àí x ‚àó k ^{2} ‚àí 2Œ≥hx ^{k} ‚àí x ‚àó , ‚àáf

kx ^{k} ‚àí ^{x} ^{‚àó} k ^{2} ‚àí ^{2Œ≥hx} ^{k} ‚àí ^{x} ^{‚àó} ^{,} ‚àáf ^{(x} ^{k} ^{)i} ‚àí ^{2Œ≥hx} ^{k} ‚àí ^{x} ^{‚àó} ^{,} ^{Œ∏} k i ^{+} ^{Œ≥} ^{2} k‚àáf ^{(x} ^{k} ^{)} ^{+} ^{Œ∏} k k ^{2}

kx ^{k} ‚àí x ‚àó k ^{2} ‚àí 2Œ≥ f (x ^{k} ) ‚àí f (x ‚àó ) ‚àí 2Œ≥hx ^{k} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{k} ), Œ∏ k i

^{+Œ≥} ^{2} k‚àáf ^{(x} ^{k} ^{)k} ^{2} ^{+} ^{Œ≥} ^{2} kŒ∏ k k ^{2}


kx ^{k} ‚àí x ‚àó k ^{2} ‚àí 2Œ≥ (1 ‚àí Œ≥L) f (x ^{k} ) ‚àí f (x ‚àó ) ‚àí 2Œ≥hx ^{k} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{k} ), Œ∏ k i

+Œ≥ 2 kŒ∏ k k 2


kx ^{k} ‚àí x ‚àó k ^{2} ‚àí Œ≥ f (x ^{k} ) ‚àí f (x ‚àó ) ‚àí 2Œ≥hx ^{k} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{k} ), Œ∏ k i + Œ≥ ^{2} kŒ∏ k k ^{2} .

32

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Summing up the above inequalities for k = 0, 1, . . . , K and rearranging the terms, we get

K

Œ≥ X
f (x k ) ‚àí f (x ‚àó )
K +1

k=0

K

‚â§

k=0

+

Œ≥ 2
K +1

k=0

K
X

k=0

kŒ∏ k k 2

K

kx ‚àí x k ‚àí kx K+1 ‚àí x ‚àó k 2
2Œ≥ X k
‚àí
hx ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{k} ), Œ∏ k i
K +1
K +1

0

=

K


1 X
2Œ≥ X k
kx k ‚àí x ‚àó k 2 ‚àí kx k+1 ‚àí x ‚àó k 2 ‚àí
hx ‚àí x ^{‚àó} ‚àí Œ≥‚àáf (x ^{k} ), Œ∏ k i
K +1
K +1

‚àó 2

k=0

2

+

Œ≥
K +1

K
X

k=0

kŒ∏ k k 2 .

Finally, we use the definition of x ^{K} and Jensen‚Äôs inequality and get the result.

Using this lemma we prove the main convergence result for clipped-SGD.

Theorem E.6 (Case 3 from Theorem 3.1). Let Assumptions 1.1, 1.3 and 1.6 with Œº = 0 hold on Q = B 2R (x ‚àó ), where
R ‚â• kx ^{0} ‚àí x ‚àó k, and
Ô£º
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£Ω
Ô£≤
R
1
,
,
(113)
Œ≥ ‚â§ min
 Œ±‚àí1

4(K+1)
Ô£¥
Œ± Ô£¥
1
1
Ô£¥
Ô£¥
4(K+1)
Ô£æ
Ô£≥ 80L ln Œ≤
108 Œ± ¬∑ 20œÉK Œ± ln Œ≤

Œª k ‚â° Œª =

R

40Œ≥ ln ^{4(K+1)}
Œ≤

,

(114)

for some K > 0 and Œ≤ ‚àà (0, 1] such that ln ^{4K}
Œ≤ ‚â• 1. Then, after K iterations of clipped-SGD the iterates with probability
at least 1 ‚àí Œ≤ satisfy
2R 2
‚àó
‚àö
and {x ^{k} } ^{K}
(115)
f (x K ) ‚àí f (x ‚àó ) ‚â§
k=0 ‚äÜ ^{B} 2R ^{(x} ^{).}
Œ≥(K + 1)

In particular, when Œ≥ equals the minimum from (53), then the iterates produced by clipped-SGD after K iterations with
probability at least 1 ‚àí Œ≤ satisfy
Ô£º Ô£∂
Ô£±
Ô£´
K Ô£Ω
Ô£≤ LR 2 ln K œÉR ln Œ±‚àí1
Œ±
Œ≤
Œ≤
Ô£∏ ,
(116)
f (x ^{K} ) ‚àí f (x ‚àó ) = O Ô£≠ max
,
Œ±‚àí1
Ô£æ
Ô£≥
K
Œ±
K

meaning that to achieve f (x ^{K} ) ‚àí f (x ‚àó ) ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SGD requires
(
 Œ±

 ^{Œ±} !)!

LR ^{2} œÉR Œ±‚àí1
1 œÉR Œ±‚àí1
,
K = O max
iterations/oracle calls.
ln
Œµ
Œµ
Œ≤
Œµ

(117)

Proof. Let R k = kx ^{k} ‚àí x ‚àó k for all k ‚â• 0. Next, our goal is to show by induction that R l ‚â§ 2R with high probability,
which allows to apply the result of Lemma E.5 and then use Bernstein‚Äôs inequality to estimate the stochastic part of the
upper-bound. More precisely, for each k = 0, . . . , K + 1 we consider probability event E k defined as follows: inequalities

‚àí2Œ≥

t‚àí1
X

l=0

hx ^{l} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{l} ), Œ∏ l i + Œ≥ ^{2}

t‚àí1
X

l=0

kŒ∏ l k 2

‚â§

R t

‚â§

R 2 ,

(118)

‚àö
2R

(119)

hold for all t = 0, 1, . . . , k simultaneously. We want to prove via induction that P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) for all k =
0, 1, . . . , K + 1. For k = 0 the statement is trivial. Assume that the statement is true for some k = T ‚àí 1 ‚â§ K:

33

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

P{E T ‚àí1 } ‚â• 1 ‚àí ^{(T} ‚àí1)Œ≤ / (K+1) . One needs to prove that P{E T } ‚â• 1 ‚àí ^{T} ^{Œ≤} / (K+1) . First, we notice that probability event
E T ‚àí1 implies that x t ‚àà B ‚àö 2R (x ‚àó ) for all t = 0, 1, . . . , T ‚àí 1. Moreover, E T ‚àí1 implies

e Œæ T ‚àí1 (x T ‚àí1 )k ‚â§ kx T ‚àí1 ‚àí x ‚àó k + Œ≥k ‚àáf
e Œæ T ‚àí1 (x T ‚àí1 )k ‚â§
kx T ‚àí x ‚àó k = kx T ‚àí1 ‚àí x ‚àó ‚àí Œ≥ ‚àáf

(114)
‚àö
2R + Œ≥Œª ‚â§ 2R,

i.e., x ^{0} , x ^{1} , . . . , x ^{T} ‚àà B 2R (x ‚àó ). Therefore, E T ‚àí1 implies {x ^{k} } Tk=0 ‚äÜ Q, meaning that the assumptions of Lemma E.5 are
satisfied and we have


Œ≥ f (x t‚àí1 ) ‚àí f (x ‚àó ) ‚â§

kx 0 ‚àí x ‚àó k 2 ‚àí kx t ‚àí x ‚àó k 2
t
t‚àí1
t‚àí1
X
2Œ≥
Œ≥ 2 X
‚àí
hx ^{l} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{l} ), Œ∏ l i +
kŒ∏ l k 2
t
t

l=0

(120)

l=0

for all t = 1, . . . , T simultaneously and for all t = 1, . . . , T ‚àí 1 this probability event also implies that
!
t‚àí1
t‚àí1
X
X
(118) 2R ^{2}
1
t‚àí1
‚àó
2
l
‚àó
l
2
2
f (x ) ‚àí f (x ) ‚â§
hx ‚àí x ‚àí Œ≥‚àáf (x ), Œ∏ l i + Œ≥
R ‚àí 2Œ≥
kŒ∏ l k
.
‚â§
Œ≥t
Œ≥t

l=0

(121)

l=0

Taking into account that f (x ^{T} ‚àí1 ) ‚àí f (x ‚àó ) ‚â• 0, we also derive from (120) that E T ‚àí1 implies

R T 2 ‚â§ R 2 ‚àí 2Œ≥

t‚àí1
X

l=0

hx ^{l} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{l} ), Œ∏ l i + Œ≥ ^{2}

t‚àí1
X

l=0

kŒ∏ l k 2 .

(122)

Next, we define random vectors

Œ∑ t =

(

x ^{t} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{t} ),
0,

if kx ^{t} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{t} )k ^{2} ‚â§ 2R,
otherwise,

for all t = 0, 1, . . . , T ‚àí 1. By definition, these random vectors are bounded with probability 1

kŒ∑ t k ‚â§ 2R.

(123)

Moreover, for t = 0, . . . , T ‚àí 1 event E T ‚àí1 implies

(6)

k‚àáf ^{(x} ^{t} ^{)k}

kx ^{t} ‚àí x ‚àó ‚àí Œ≥‚àáf (x ^{t} )k

(119) ‚àö

(113),(114) Œª

‚â§

Lkx ^{t} ‚àí x ^{‚àó} k ‚â§

‚â§

kx ^{t} ‚àí x ‚àó k + Œ≥k‚àáf (x ^{t} )k ‚â§

2LR

‚â§

(124) ‚àö

2

,

(124)

(113)

2R(1 + LŒ≥) ‚â§ 2R.

Next, we define the unbiased part and the bias of Œ∏ t as Œ∏ t ^{u} and Œ∏ t ^{b} , respectively:
i
i
h
h
e Œæ t (x t ) ‚àí E Œæ t ‚àáf
e Œæ t (x t ) , Œ∏ t b = E Œæ t ‚àáf
e Œæ t (x t ) ‚àí ‚àáf (x t ).
Œ∏ t u = ‚àáf

(125)

We notice that Œ∏ t = Œ∏ t ^{u} + Œ∏ t ^{b} . Using new notation, we get that E T ‚àí1 implies

R T 2

‚â§

^{R} ^{2} ‚àí2Œ≥

|

T
‚àí1
X

t=0

hŒ∏ t ^{u} ^{,} ^{Œ∑} t i ‚àí2Œ≥

{z

} |

1

+ 2Œ≥ 2

|

T
‚àí1
X

t=0

T
‚àí1
X

t=0

hŒ∏ t b , Œ∑ t i + 2Œ≥ 2

{z

}

|

t=0

2

|

T
‚àí1 
X

t=0

4

}

{z

5

{z

3

T
‚àí1
i
h
X
2
2
E Œæ t kŒ∏ t u k + 2Œ≥ 2
Œ∏ t b .

{z

h
i
2
2
kŒ∏ t u k ‚àí E Œæ t kŒ∏ t u k

}

}

(126)

It remains to derive good enough high-probability upper-bounds for the terms 1, 2, 3, 4, 5, i.e., to finish our inductive
proof we need to show that 1 + 2 + 3 + 4 + 5 ‚â§ R ^{2} with high probability. In the subsequent parts of the proof, we

34

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

will need to use many times the bounds for the norm and second moments of Œ∏ t ^{u} and Œ∏ t ^{b} . First, by definition of clipping
operator, we have with probability 1 that
kŒ∏ t ^{u} k ‚â§ 2Œª.
(127)

^{Moreover,} ^{since} ^{E} T ‚àí1 ^{implies} ^{that} k‚àáf ^{(x} ^{t} ^{)k} ‚â§ ^{Œª} ^{/} ^{2} ^{for} ^{t} ^{=} ^{0,} ^{1,} ^{.} ^{.} ^{.} ^{,} ^{T} ‚àí ^{1} ^{(see} ^{(124)),} ^{then,} ^{in} ^{view} ^{of} ^{Lemma} ^{5.1,} ^{we}
have that E T ‚àí1 implies

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1
18Œª ^{2‚àíŒ±} œÉ ^{Œ±} .

kŒ∏ t b k ‚â§


E Œæ t kŒ∏ t u k 2 ‚â§

(128)

(129)

Upper bound for 1. By definition of Œ∏ t ^{u} , we have E Œæ t [Œ∏ t ^{u} ] = 0 and

E Œæ t [‚àí2Œ≥hŒ∏ t ^{u} , Œ∑ t i] = 0.

Next, sum 1 has bounded with probability 1 terms:

(123),(127)

|2Œ≥ hŒ∏ t ^{u} ^{,} ^{Œ∑} t i | ‚â§ ^{2Œ≥kŒ∏} t ^{u} k ¬∑ kŒ∑ t k

‚â§

R 2

(114)

8Œ≥ŒªR =

def

5 ln 4(K+1)
Œ≤

= c.

(130)

def

The summands also have bounded conditional variances œÉ t ^{2} = E Œæ t [4Œ≥ ^{2} hŒ∏ t ^{u} , Œ∑ t i ^{2} ]:



 (123)

œÉ t ^{2} ‚â§ E Œæ t 4Œ≥ ^{2} kŒ∏ t ^{u} k ^{2} ¬∑ kŒ∑ t k ^{2} ‚â§ 16Œ≥ ^{2} R ^{2} E Œæ t kŒ∏ t ^{u} k ^{2} .

(131)

‚àí1
^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {‚àí2Œ≥ hŒ∏ t ^{u} ^{,} ^{Œ∑} t i} ^{T} t=0
is a bounded martingale difference sequence with bounded conditional
2 T ‚àí1
^{variances} {œÉ t } t=0 ^{.} ^{Next,} ^{we} ^{apply} ^{Bernstein‚Äôs} ^{inequality} ^{(Lemma} ^{B.2)} ^{with} ^{X} t ^{=} ‚àí2Œ≥ hŒ∏ t ^{u} ^{,} ^{Œ∑} t i, ^{parameter} ^{c} ^{as} ^{in} ^{(130),}
2
R 4
b = R 5 , G =
4(K+1) ^{:}

150 ln

Œ≤

(

R 2
^{P} |1| ^{>}
5

T
‚àí1
X

and

t=0

œÉ t 2 ‚â§

R 4

150 ln ^{4(K+1)}
Œ≤

)


‚â§ 2 exp ‚àí

b 2
2G + 2cb / 3



=

Œ≤
.
2(K + 1)

Equivalently, we have

Œ≤
,
P {E 1 } ‚â• 1 ‚àí
2(K + 1)

for E 1 =

(

either

T
‚àí1
X

œÉ t 2 >

t=0

R 4

150 ln ^{4(K+1)}
Œ≤

R 2
^{or} |1| ‚â§
5

)

.

(132)

In addition, E T ‚àí1 implies that

T
‚àí1
X

(131)

œÉ t 2

‚â§

t=0

16Œ≥ ^{2} R ^{2}

T
‚àí1
X

t=0


 (129)
E Œæ t kŒ∏ t ^{u} k ^{2} ‚â§ 288Œ≥ ^{2} R ^{2} œÉ ^{Œ±} T Œª ^{2‚àíŒ±}

9 ¬∑ 40 Œ± R 4‚àíŒ± œÉ Œ± T Œ≥ Œ± (113)
R 4
.
‚â§
2‚àíŒ± 4(K+1)
50 ln
150 ln ^{4(K+1)}
Œ≤
Œ≤

(114)

=

(133)

Upper bound for 2. From E T ‚àí1 it follows that

2

=

(114)

=

‚àí2Œ≥

T
‚àí1
X

hŒ∏ t b , Œ∑ t i ‚â§ 2Œ≥

t=0
Œ±
Œ±

T
‚àí1
X

t=0

kŒ∏ t b k ¬∑ kŒ∑ t k

(123),(128) 4 ¬∑ 2 ^{Œ±} Œ≥œÉ ^{Œ±} T R

‚â§

80
œÉ T R 2‚àíŒ± Œ≥ Œ± (113) R 2
^{¬∑} 1‚àíŒ± 4(K+1) ^{‚â§}
.
10 ln
5

Œ≤

Upper bound for 3. First, we have

ii
h

h
= 0.
E Œæ t 2Œ≥ 2 kŒ∏ t u k 2 ‚àí E Œæ t kŒ∏ t u k 2

35

Œª Œ±‚àí1

(134)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Next, sum 3 has bounded with probability 1 terms:
i
h

2
2
‚â§
2Œ≥ 2 kŒ∏ t u k ‚àí E Œæ t kŒ∏ t u k

i
h

2
2Œ≥ 2 kŒ∏ t u k 2 + E Œæ t kŒ∏ t u k

R 2
def
‚â§
= c.
4(K+1)
2 4(K+1)
100 ln
5 ln Œ≤
Œ≤

i 2 
h

def
2
2
:
The summands also have bounded conditional variances œÉ
e t 2 = E Œæ t 4Œ≥ 4 kŒ∏ t u k ‚àí E Œæ t kŒ∏ t u k

(127)

h
h
i i
2
u 2
u 2
t
t kŒ∏ k
E
2Œ≥
kŒ∏
k
‚àí
E
‚â§
Œæ
Œæ
t
t
4(K+1)

R 2

(135)

œÉ
e t 2

R 2

(114)

16Œ≥ ^{2} Œª ^{2} =

‚â§

‚â§

5 ln

4Œ≥ 2 R 2

5 ln 4(K+1)
Œ≤

Œ≤

(135)



E Œæ t kŒ∏ t u k 2 ,

(136)

io ^{T} ^{‚àí1}
h

n
u 2
u 2
2
t
since ln ^{4(K+1)}
kŒ∏
k
is a bounded martingale dif-
kŒ∏
k
‚àí
E
‚â•
1.
In
other
words,
we
showed
that
2Œ≥
Œæ
t
t
Œ≤

t=0

‚àí1
ference sequence
with bounded
conditional
variances {e
^{œÉ} t ^{2} } ^{T} t=0
. Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with
i
h


X t = 2Œ≥ 2 kŒ∏ t u k 2 ‚àí E Œæ t kŒ∏ t u k 2

(

R 2
^{P} |3| ^{>}
5

2

, parameter c as in (135), b = ^{R} 5 , G =

T
‚àí1
X

and

t=0

Equivalently, we have

Œ≤
,
P {E 3 } ‚â• 1 ‚àí
2(K + 1)

œÉ
e t 2 ‚â§

R 4

150 ln ^{4(K+1)}
Œ≤

for E 3 =

(

)


‚â§ 2 exp ‚àí

T
‚àí1
X

either

t=0

In addition, E T ‚àí1 implies that

T
‚àí1
X

t=0

œÉ
e t 2

R 4
:
150 ln ^{4(K+1)}
Œ≤

œÉ
e t 2 >

b 2
2G + 2cb / 3

R 4



=

Œ≤
.
2(K + 1)

R 2
^{or} |3| ‚â§
5

150 ln ^{4(K+1)}
Œ≤

)

.

(137)

T
‚àí1
X

4Œ≥ 2 R 2

 u 2  (129) 72Œ≥ ^{2} R ^{2} Œª ^{2‚àíŒ±} œÉ ^{Œ±} T
t kŒ∏ k
‚â§
E
Œæ
t
5 ln 4(K+1)
5 ln 4(K+1)
t=0
Œ≤
Œ≤

(136)

‚â§

R 4
9 ¬∑ 40 Œ± œÉ Œ± T R 4‚àíŒ± Œ≥ Œ± (113)
.
^{¬∑} 3‚àíŒ± 4(K+1) ^{‚â§}
1000 ln
150 ln ^{4(K+1)}

(114)

=

Œ≤

(138)

Œ≤

Upper bound for 4. From E T ‚àí1 it follows that

= 2Œ≥ 2

4

h
i (129)
Œ±
Œ≥ Œ± œÉ Œ± T R 2‚àíŒ± (113) R 2
(114) ^{9} ¬∑ ^{40}
2
E Œæ t kŒ∏ t u k
^{¬∑} 2‚àíŒ± 4(K+1) ^{‚â§}
.
‚â§ 36Œ≥ ^{2} Œª ^{2‚àíŒ±} œÉ ^{Œ±} T =
400
5
ln
t=0

T
‚àí1
X

(139)

Œ≤

Upper bound for 5. From E T ‚àí1 it follows that

5

= 2Œ≥ 2

T
‚àí1
X

Œ∏ t b

2 (128) 2 ¬∑ 4

‚â§

t=0

Œ± 2Œ±

œÉ

T Œ≥ ^{2} (114) 6400 ^{Œ±} œÉ ^{2Œ±} T Œ≥ ^{2Œ±} R ^{2‚àí2Œ±} (113) R ^{2}
=
‚â§
¬∑
.
800 ln ^{2(1‚àíŒ±)} ^{4(K+1)}
5

Œª 2(Œ±‚àí1)

Œ≤

Now, we have the upper bounds for 1, 2, 3, 4, 5. In particular, probability event E T ‚àí1 implies

(126)

R T 2 ‚â§ R 2 + 1 + 2 + 3 + 4 + 5,

(134) R ^{2}

2 ‚â§

T
‚àí1
X

t=0

(133)

œÉ t 2 ‚â§

5

,

(139) R ^{2}

4 ‚â§

R 4

150 ln

,
4(K+1)

,

5
T
‚àí1
X

t=0

Œ≤

36

(140) R ^{2}

5 ‚â§

(138)

œÉ
e t 2 ‚â§

5

,

R 4

150 ln ^{4(K+1)}
Œ≤

.

(140)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Moreover, we also have (see (132), (137) and our induction assumption)

(T ‚àí 1)Œ≤
,
K +1

P{E T ‚àí1 } ‚â• 1 ‚àí

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
2(K + 1)

P{E 3 } ‚â• 1 ‚àí

Œ≤
,
2(K + 1)

where

E 1

E 3

=

=

(

(

either

T
‚àí1
X

t=0

either

T
‚àí1
X

t=0

Thus, probability event E T ‚àí1 ‚à© E 1 ‚à© E 3 implies

R T 2

œÉ t 2 >

‚â§

R 2 +

œÉ
e t 2 >

R 4

150 ln ^{4(K+1)}
Œ≤

R 4

150 ln ^{4(K+1)}
Œ≤

R 2
^{or} |1| ‚â§
5

R 2
^{or} |3| ‚â§
5

)

)

,

.

R 2
R 2
R 2
R 2
R 2
+
+
+
+
= 2R 2 ,
5
5
5
5
5

which is equivalent to (118) and (119) for t = T , and


TŒ≤
.
P{E T } ‚â• P {E T ‚àí1 ‚à© E 1 ‚à© E 3 } = 1 ‚àí P E T ‚àí1 ‚à™ E 1 ‚à™ E 3 ‚â• 1 ‚àí P{E T ‚àí1 } ‚àí P{E 1 } ‚àí P{E 3 } ‚â• 1 ‚àí
K +1

This finishes the inductive part of our proof, i.e., for all k = 0, 1, . . . , K + 1 we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) . In particular,
for k = K + 1 we have that with probability at least 1 ‚àí Œ≤

(121)

f (x K ) ‚àí f (x ‚àó ) ‚â§

2R 2
Œ≥(K + 1)

and {x ^{k} } ^{K}
k=0 ‚äÜ ^{Q,} ^{which} ^{follows} ^{from} ^{(119).}

Finally, if

Œ≥ ‚â§ min

then with probability at least 1 ‚àí Œ≤

f (x K ) ‚àí f (x ‚àó ) ‚â§

=

=

Ô£±
Ô£¥
Ô£¥
Ô£≤

Ô£º
Ô£¥
Ô£¥
Ô£Ω

R
1
,
,

 Œ±‚àí1
4(K+1)
Ô£¥
Œ± Ô£¥
1
1
Ô£¥
Ô£¥
4(K+1)
Ô£≥ 80L ln Œ≤
Ô£æ
108 Œ± ¬∑ 20œÉK Œ± ln

Œ≤

2R 2
Œ≥(K + 1)
Ô£±
Ô£º
 Œ±‚àí1

Ô£¥
Œ± Ô£¥
1
1
4(K+1)
Ô£¥
Ô£¥
4(K+1)
Ô£≤ 160LR ^{2} ln
Ô£Ω
40 ¬∑ 108 Œ± œÉRK Œ± ln Œ≤
Œ≤
max
,
Ô£¥
Ô£¥
K +1
K +1
Ô£¥
Ô£¥
Ô£≥
Ô£æ
Ô£º
Ô£±
Ô£∂
Ô£´
K Ô£Ω
Ô£≤ LR 2 ln K œÉR ln Œ±‚àí1
Œ±
Œ≤
Œ≤
Ô£∏ .
O Ô£≠ max
,
Œ±‚àí1
Ô£æ
Ô£≥
K
K Œ±

To get f (x ^{K} ) ‚àí f (x ‚àó ) ‚â§ Œµ with probability at least 1 ‚àí Œ≤ it is sufficient to choose K such that both terms in the maximum
^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}
(
 Œ±

 ^{Œ±} !)!

LR ^{2} LR ^{2} œÉR Œ±‚àí1
1 œÉR Œ±‚àí1
ln
,
K = O max
,
ln
Œµ
ŒµŒ≤
Œµ
Œ≤
Œµ

which concludes the proof.

37

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

E.4. Quasi-Strongly Convex Functions

Finally, we consider clipped-SGD under smoothness and quasi-strong convexity assumptions. As the next lemma shows,
the gradient of such function is quasi-strongly monotone and star-cocoercive operator.

Lemma E.7. Consider differentiable function f : R ^{d} ‚Üí R. If f satisfies Assumption 1.5 on some set Q with parameter
Œº, then operator F (x) = ‚àáf (x) satisfies Assumption 1.9 on Q with parameter ^{Œº} / 2 . If f satisfies Assumptions 1.3 and 1.5
with Œº = 0 on some set Q, then operator F (x) = ‚àáf (x) satisfies Assumption 1.10 on Q with l = 2L.

Proof. We start with the first part. Assumption 1.5 on set Q means that for any x ‚àà Q

^{f} ^{(x} ^{‚àó} ^{)} ‚â• ^{f} ^{(x)} ^{+} h‚àáf ^{(x),} ^{x} ^{‚àó} ‚àí ^{xi} ^{+}

Œº
kx ‚àí x ‚àó k 2 .
2

For F (x) = ‚àáf (x) it implies that for all x ‚àà Q

hF (x), x ‚àí x ‚àó i ‚â• f (x) ‚àí f (x ‚àó ) +

Œº
Œº
kx ‚àí x ‚àó k 2 ‚â• kx ‚àí x ‚àó k 2 ,
2
2

i.e., Assumption 1.9 holds on Q with parameter ^{Œº} / 2 for operator F (x).

Next, we prove the second part. Assume that f satisfies Assumptions 1.3 and 1.5 with Œº = 0 on some set Q. Our goal
is to show that F (x) = ‚àáf (x) satisfies Assumption 1.10 on Q. In view of (Gorbunov et al., 2022b, Lemma C.6), this is
^{equivalent} ^{to} ^{showing} ^{that} ^{operator} ^{Id} ‚àí L ^{1} ^{F} ^{is} ^{non-expansive} ^{around} ^{x} ^{‚àó} ^{,} ^{i.e.,} ^{we} ^{need} ^{to} ^{show} ^{that} k(Id ‚àí L ^{1} ^{F} ^{)(x)} ‚àí
(Id ‚àí L ^{1} F )(x ‚àó )k ‚â§ kx ‚àí x ‚àó k for any x ‚àà Q. We have



Id ‚àí

1
F
L





2
1
(x) ‚àí Id ‚àí F (x ‚àó )
L

2

1
F (x)
L
1
2
kx ‚àí x ‚àó k ^{2} ‚àí hx ‚àí x ‚àó , F (x)i + 2 kF (x)k ^{2}
L
L
1
2
‚àó
‚àó 2
kx ‚àí ^{x} k ‚àí hx ‚àí ^{x} ^{,} ‚àáf ^{(x)i} ^{+} 2 k‚àáf ^{(x)k} ^{2}
L
L
2
2
‚àó 2
‚àó
kx ‚àí x k ‚àí (f (x) ‚àí f (x )) + (f (x) ‚àí f (x ‚àó ))
L
L
kx ‚àí x ‚àó k 2 .

x ‚àí x ‚àó ‚àí

=

=

=

(9),(7)

‚â§

=

This finishes the proof.

Therefore, using the result of Theorem H.6 with l := 2L and Œº := ^{Œº} / 2 , we get the convergence result for clipped-SGD
under smoothness and quasi-strong convexity assumptions.

Theorem E.8 (Case 4 in Theorem 3.1). Let Assumptions 1.1, 1.3, 1.5, hold for Q = B 2R (x ‚àó ) = {x ‚àà R ^{d} | kx ‚àí x ‚àó k ‚â§
2R}, where R ‚â• kx ^{0} ‚àí x ‚àó k, and
(
)
2 ln(B K )
1
,
0 < Œ≥ ‚â§ min
,
(141)
Œº(K + 1)
800L ln ^{4(K+1)}
Œ≤
Ô£º
Ô£±
2(Œ±‚àí1)
Ô£Ω
Ô£≤
(K + 1) Œ± Œº 2 R 2


(142)
B K = max 2,
4(K+1)
Ô£≥ 4 ¬∑ 5400 Œ± ^{2} œÉ 2 ln ^{2(Œ±‚àí1)}
Œ±
ln 2 (B K ) Ô£æ
Œ≤
Ô£±
Ô£º Ô£º
Ô£´
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¨
Ô£¥
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£≤
Ô£Ω
Ô£Ω
2 2
Ô£¨
Œ±
K
Œº
R
(
)!
= O Ô£¨
max
2,
,
(143)
Ô£¨
 
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
Ô£≠
2 R 2
Œ±
K
2
Œº
Ô£¥
K
Ô£¥
Ô£¥
2
Ô£¥
Ô£¥
max 2,
2(Œ±‚àí1)
Ô£≥ œÉ ln Œ±
Ô£æ Ô£¥
Ô£æ
Œ≤ ln
œÉ 2 ln Œ± ( K
Œ≤ )

Œª k

=

exp(‚àíŒ≥( ^{Œº} / 2 )(1 + ^{k} / 2 ))R

120Œ≥ ln ^{4(K+1)}
Œ≤

,

(144)

38

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

for some K ‚â• 0 and Œ≤ ‚àà (0, 1] such that ln ^{4(K+1)}
‚â• 1. Then, after K iterations the iterates produced by clipped-SGD
Œ≤
with probability at least 1 ‚àí Œ≤ satisfy

kx ^{K+1} ‚àí x ‚àó k ^{2} ‚â§ 2 exp(‚àíŒ≥( Œº / 2 )(K + 1))R ^{2} .

(145)

In particular, when Œ≥ equals the minimum from (141), then the iterates produced by clipped-SGD after K iterations with
probability at least 1 ‚àí Œ≤ satisfy
)! Ô£º Ô£∂
(
Ô£±
Ô£´
 
2(Œ±‚àí1)
2(Œ±‚àí1)
2 2
Ô£¥
Ô£¥
Œ±
Œº
R
K
2
Ô£¥
Ô£¥
K
2
Ô£¥
Ô£¥
! œÉ ln Œ±
max 2,
2(Œ±‚àí1)
Ô£¨
Ô£¥
Ô£¥
Œ≤ ln
Ô£Ω Ô£∑
Ô£≤
K
2 ln
Œ±
Ô£¨
Ô£∑
œÉ
( Œ≤ )
ŒºK
K
‚àó 2
2
Ô£∑ ,
kx ‚àí x k = O Ô£¨
,
max
R
exp
‚àí
(146)
2(Œ±‚àí1)
Ô£¨
Ô£∑
K
Ô£¥
Ô£¥
2
l
ln
Œ±
Ô£¥
Ô£¥
Œº
K
Ô£≠
Ô£∏
Œ≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
Ô£≥

meaning that to achieve kx ^{K} ‚àí x ‚àó k ^{2} ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SGD requires
!
Œ±
  ^{2}  2(Œ±‚àí1)

 Œ± !
 2  
Œ±
L
L
œÉ
R 2
R
1 œÉ ^{2} 2(Œ±‚àí1)
Œ±‚àí1
K = O
ln
,
ln
ln
ln
(B Œµ )
ln
Œº
Œµ
ŒºŒ≤
Œµ
Œº 2 Œµ
Œ≤ Œº 2 Œµ

iterations/oracle calls, where

B Œµ = max

Ô£±
Ô£¥
Ô£¥
Ô£≤

Ô£¥
Ô£¥
Ô£≥

2,

Œµ ln

 

1
Œ≤

39

R 2

4œÉ 2
Œº 2 Œµ

Ô£º
Ô£¥
Ô£¥
Ô£Ω

 .
Œ±
 2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£æ

(147)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

F. Missing Proofs for clipped-SSTM and R-clipped-SSTM

In this section, we provide the complete formulation of the main results for clipped-SSTM and R-clipped-SSTM
and the

e Œæ k (x ^{k+1} ) = clip ‚àáf Œæ k (x ^{k+1} ), Œª k .
missing proofs. For brevity, we will use the following notation: ‚àáf

Algorithm 2 Clipped Stochastic Similar Triangles Method (clipped-SSTM) (Gorbunov et al., 2020)

Input: starting point x ^{0} , number of iterations K, stepsize parameter a > 0, clipping levels {Œª k } ^{K‚àí1}
k=0 ^{,} ^{smoothness} ^{constant}
L.
1: Set A 0 = Œ± 0 = 0, y ^{0} = z ^{0} = x ^{0}
2: for k = 0, . . . , K ‚àí 1 do
3:
Set Œ± k+1 = ^{k+2}
2aL ^{,} ^{A} ^{k+1} ^{=} ^{A} ^{k} ^{+} ^{Œ±} ^{k+1}

k

4:

5:

6:

k

k+1 ^{z}
x k+1 = ^{A} k ^{y} A ^{+Œ±}
k+1

e Œæ k (x ^{k+1} ) = clip ‚àáf Œæ k (x ^{k+1} ), Œª k using a fresh sample Œæ ^{k} ‚àº D k
Compute ‚àáf
e Œæ k (x k+1 )
z ^{k+1} = z ^{k} ‚àí Œ± k+1 ‚àáf

7:
y k+1 =
8: end for

A k y ^{k} +Œ± k+1 z ^{k+1}
^{A} k+1

Output: y ^{K}

F.1. Convex Functions

We start with the following lemma, which is a special case of Lemma 6 from (Gorbunov et al., 2021). This result can be
seen the ‚Äúoptimization‚Äù part of the analysis of clipped-SSTM: the proof follows the same steps as the analysis of determin-
istic Similar Triangles Method (Gasnikov & Nesterov, 2016; Dvurechenskii et al., 2018) and separates stochasticity from
the deterministic part of the method.

Lemma F.1 (Special case of Lemma 4.1 from (Gorbunov et al., 2021)). Let Assumptions 1.3 and 1.6 with Œº = 0 hold
on Q = B 3R (x ‚àó ), where R ‚â• kx ^{0} ‚àí x ‚àó k, and let stepsize parameter a satisfy a ‚â• 1. If x ^{k} , y ^{k} , z ^{k} ‚àà B 3R (x ‚àó ) for all
k = 0, 1, . . . , N , N ‚â• 0, then after N iterations of clipped-SSTM for all z ‚àà B 3R (x ‚àó ) we have


A N f (y ^{N} ) ‚àí f (z)

‚â§

N ‚àí1

X
1 0
1
kz ‚àí zk 2 ‚àí kz N ‚àí zk 2 +
Œ± k+1 Œ∏ k+1 , z ‚àí z ^{k} + Œ± k+1 ‚àáf (x ^{k+1} )
2
2

k=0

+

N
‚àí1
X

k=0

^{Œ∏} k+1

def

=

^{Œ±} ^{2} k+1 kŒ∏ k+1 k ^{2} ^{,}

e Œæ k (x k+1 ) ‚àí ‚àáf (x k+1 ).
‚àáf

(148)

(149)

e Œæ k (x ^{k+1} ) we get that for all z ‚àà B 3R (x ‚àó )
Proof. For completeness, we provide the full proof. Using z ^{k+1} = z ^{k} ‚àí Œ± k+1 ‚àáf
and k = 0, 1, . . . , N ‚àí 1
D
E
E
D
D
E
e Œæ k (x k+1 ), z k+1 ‚àí z
e Œæ k (x ^{k+1} ), z ^{k} ‚àí z ^{k+1} + Œ± k+1 ‚àáf
e Œæ k (x k+1 ), z k ‚àí z
^{=} ^{Œ±} k+1 ‚àáf
^{Œ±} k+1 ‚àáf
E
D
e Œæ k (x k+1 ), z k ‚àí z k+1 + z k+1 ‚àí z k , z ‚àí z k+1
^{=} ^{Œ±} k+1 ‚àáf
E
D
e Œæ k (x k+1 ), z k ‚àí z k+1 ‚àí ^{1} kz k ‚àí z k+1 k 2
^{=} ^{Œ±} k+1 ‚àáf
2
1
1 k
(150)
+ kz ‚àí zk 2 ‚àí kz k+1 ‚àí zk 2 ,
2
2

^{where} ^{in} ^{the} ^{last} ^{step} ^{we} ^{apply} ^{2ha,} ^{bi} ^{=} ka ^{+} ^{bk} ^{2} ‚àí kak ^{2} ‚àí kbk ^{2} ^{with} ^{a} ^{=} ^{z} ^{k+1} ‚àí ^{z} ^{k} ^{and} ^{b} ^{=} ^{z} ‚àí ^{z} ^{k+1} ^{.} ^{The} ^{update} ^{rules}
(22) and (20) give the following formula:

y k+1

=



^{A} k ^{y} ^{k} ^{+} ^{Œ±} k+1 ^{z} ^{k}
^{Œ±} k+1 k+1
^{Œ±} k+1 k+1
A k y ^{k} + Œ± k+1 z ^{k+1}
=
+
z
‚àí z k = x k+1 +
z
‚àí z ^{k} . (151)
^{A} k+1
^{A} k+1
^{A} k+1
^{A} k+1

40

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

It implies

D
E
e Œæ k (x k+1 ), z k ‚àí z
^{Œ±} k+1 ‚àáf

(149),(150)

‚â§

(151)

=

(36)

‚â§

(151)

=

1
Œ± k+1 ‚àáf (x ^{k+1} ), z ^{k} ‚àí z ^{k+1} ‚àí kz ^{k} ‚àí z ^{k+1} k ^{2}
2
1
1
+Œ± k+1 Œ∏ k+1 , z ^{k} ‚àí z ^{k+1} + kz ^{k} ‚àí zk ^{2} ‚àí kz ^{k+1} ‚àí zk ^{2}
2
2
1
A k+1 ‚àáf (x ^{k+1} ), x ^{k+1} ‚àí y ^{k+1} ‚àí kz ^{k} ‚àí z ^{k+1} k ^{2}
2
1
1
+Œ± k+1 Œ∏ k+1 , z ^{k} ‚àí z ^{k+1} + kz ^{k} ‚àí zk ^{2} ‚àí kz ^{k+1} ‚àí zk ^{2}
2
2
 ^{A} k+1 ^{L} k+1
1
A k+1 f (x ^{k+1} ) ‚àí f (y ^{k+1} ) +
kx
‚àí y k+1 k 2 ‚àí kz k ‚àí z k+1 k 2
2
2
1 k
^{1} k+1
2
k
k+1
+ kz ‚àí zk ‚àí kz
‚àí zk 2
^{+Œ±} k+1 ^{Œ∏} k+1 ^{,} ^{z} ‚àí ^{z}
2
2


 ^{1} ^{Œ±} ^{2} k+1 ^{L}
A k+1 f (x ^{k+1} ) ‚àí f (y ^{k+1} ) +
‚àí 1 kz k ‚àí z k+1 k 2
^{2} ^{A} k+1
1
1
+Œ± k+1 Œ∏ k+1 , z ^{k} ‚àí z ^{k+1} + kz ^{k} ‚àí zk ^{2} ‚àí kz ^{k+1} ‚àí zk ^{2} ,
2
2

where in the third inequality we use x ^{k+1} , y ^{k+1} ‚àà B 3R (x ‚àó ). Since A k+1 ‚â• aL k+1 Œ± ^{2} k+1 (Lemma B.1) and a ‚â• 1 we can
continue our derivation as follows:
E
D

e Œæ k (x k+1 ), z k ‚àí z
‚â§ A k+1 f (x ^{k+1} ) ‚àí f (y ^{k+1} ) + Œ± k+1 Œ∏ k+1 , z ^{k} ‚àí z ^{k+1}
^{Œ±} k+1 ‚àáf

1
1
+ kz k ‚àí zk 2 ‚àí kz k+1 ‚àí zk 2 .
2
2

(152)

Convexity of f gives

D

e Œæ k (x k+1 ), y k ‚àí x k+1
‚àáf

The definition of x ^{k+1} (20) implies

E

(149)

=

‚àáf (x ^{k+1} ), y ^{k} ‚àí x ^{k+1} + Œ∏ k+1 , y ^{k} ‚àí x ^{k+1}

‚â§

f (y ^{k} ) ‚àí f (x ^{k+1} ) + Œ∏ k+1 , y ^{k} ‚àí x ^{k+1} .



Œ± k+1 x ^{k+1} ‚àí z ^{k} = A k y ^{k} ‚àí x ^{k+1}

since A k+1 = A k + Œ± k+1 . Putting all inequalities together, we derive that

D
E
e Œæ k (x k+1 ), x k+1 ‚àí z
^{Œ±} k+1 ‚àáf

=

(154)

=

(153),(152)

‚â§

(154)

=

=

D
E
D
E
e Œæ k (x k+1 ), z k ‚àí z
e Œæ k (x ^{k+1} ), x ^{k+1} ‚àí z ^{k} + Œ± k+1 ‚àáf
^{Œ±} k+1 ‚àáf
D
E
D
E
e Œæ k (x k+1 ), z k ‚àí z
e Œæ k (x ^{k+1} ), y ^{k} ‚àí x ^{k+1} + Œ± k+1 ‚àáf
A k ‚àáf


A k f (y ^{k} ) ‚àí f (x ^{k+1} ) + A k Œ∏ k+1 , y ^{k} ‚àí x ^{k+1}

+A k+1 f (x ^{k+1} ) ‚àí f (y ^{k+1} ) + Œ± k+1 Œ∏ k+1 , z ^{k} ‚àí z ^{k+1}
1
1
+ kz k ‚àí zk 2 ‚àí kz k+1 ‚àí zk 2
2
2

A k f (y ^{k} ) ‚àí A k+1 f (y ^{k+1} ) + Œ± k+1 Œ∏ k+1 , x ^{k+1} ‚àí z ^{k}

+Œ± k+1 f (x ^{k+1} ) + Œ± k+1 Œ∏ k+1 , z ^{k} ‚àí z ^{k+1}
1
1
+ kz k ‚àí zk 2 ‚àí kz k+1 ‚àí zk 2
2
2
A k f (y ^{k} ) ‚àí A k+1 f (y ^{k+1} ) + Œ± k+1 f (x ^{k+1} )
1
1
+Œ± k+1 Œ∏ k+1 , x ^{k+1} ‚àí z ^{k+1} + kz ^{k} ‚àí zk ^{2} ‚àí kz ^{k+1} ‚àí zk ^{2} .
2
2

41

(153)

(154)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Rearranging the terms, we get

E

D
e Œæ k (x k+1 ), z ‚àí x k+1 + ^{1} kz k ‚àí zk 2
Œ± k+1 f (x ^{k+1} ) + ‚àáf
2
^{1} k+1
k+1
k+1
2
‚àí z
‚àí ^{zk} ^{+} ^{Œ±} k+1 ^{Œ∏} k+1 ^{,} ^{x}
‚àí kz
2

(149)
= Œ± k+1 f (x ^{k+1} ) + ‚àáf (x ^{k+1} ), z ‚àí x ^{k+1}
1
1
+Œ± k+1 Œ∏ k+1 , z ‚àí x ^{k+1} + kz ^{k} ‚àí zk ^{2} ‚àí kz ^{k+1} ‚àí zk ^{2}
2
2
+Œ± k+1 Œ∏ k+1 , x ^{k+1} ‚àí z ^{k+1}
1
1
‚â§ Œ± k+1 f (z) + kz ^{k} ‚àí zk ^{2} ‚àí kz ^{k+1} ‚àí zk ^{2} + Œ± k+1 Œ∏ k+1 , z ‚àí z ^{k+1} ,
2
2
P ‚àí1
where in the last inequality we use the convexity of f . Taking into account A 0 = Œ± 0 = 0 and A N = ^{N}
k=0 ^{Œ±} k+1 ^{we} ^{sum}
up these inequalities for k = 0, . . . , N ‚àí 1 and get

A k+1 f (y ^{k+1} ) ‚àí A k f (y ^{k} )

A N f (y N )

‚â§

=

(149)

=

‚â§

N ‚àí1

X
1
1
Œ± k+1 Œ∏ k+1 , z ‚àí z ^{k+1}
A N f (z) + kz ^{0} ‚àí zk ^{2} ‚àí kz ^{N} ‚àí zk ^{2} +
2
2

k=0

1
1
A N f (z) + kz ^{0} ‚àí zk ^{2} ‚àí kz ^{N} ‚àí zk ^{2} +
2
2

1
1
A N f (z) + kz ^{0} ‚àí zk ^{2} ‚àí kz ^{N} ‚àí zk ^{2} +
2
2

+

N
‚àí1
X

k=0

N
‚àí1
X

k=0

N
‚àí1
X

k=0

E
D
e Œæ k (x k+1 )
^{Œ±} k+1 ^{Œ∏} k+1 ^{,} ^{z} ‚àí ^{z} ^{k} ^{+} ^{Œ±} k+1 ‚àáf

Œ± k+1 Œ∏ k+1 , z ‚àí z ^{k} + Œ± k+1 ‚àáf Œæ k (x ^{k+1} )

2

^{Œ±} ^{2} k+1 kŒ∏ k+1 k ^{,}

which concludes the proof.

Using this lemma we prove the main convergence result for clipped-SSTM.
Theorem F.2 (Full version of Theorem 3.2). Let Assumptions 1.1, 1.3 and 1.6 with Œº = 0 hold on Q = B 3R (x ‚àó ), where
R ‚â• kx ^{0} ‚àí x ‚àó k, and
Ô£±
Ô£º
Œ±‚àí1
1
4K Ô£Ω
Ô£≤
Œ± ln Œ±
900œÉ(K
+
1)K
4K
Œ≤
,
a ‚â• max 48600 ln ^{2}
,
(155)
Ô£≥
Ô£æ
Œ≤
LR

Œª k =

R

30Œ± k+1 ln ^{4K}
Œ≤

,

(156)

for some K > 0 and Œ≤ ‚àà (0, 1] such that ln ^{4K}
Œ≤ ‚â• 1. Then, after K iterations of clipped-SSTM the iterates with
probability at least 1 ‚àí Œ≤ satisfy

f (y K ) ‚àí f (x ‚àó ) ‚â§

6aLR ^{2}
K(K + 3)

k K
k K
‚àó
and {x ^{k} } ^{K+1}
k=0 ^{,} {z } k=0 ^{,} {y } k=0 ‚äÜ ^{B} ^{2R} ^{(x} ^{).}

(157)

In particular, when parameter a equals the maximum from (155), then the iterates produced by clipped-SSTM after K
iterations with probability at least 1 ‚àí Œ≤ satisfy
Ô£±
Ô£º Ô£∂
Ô£´
K Ô£Ω
Ô£≤ LR 2 ln 2 K œÉR ln Œ±‚àí1
Œ±
Œ≤
Œ≤
Ô£∏ ,
f (y ^{K} ) ‚àí f (x ‚àó ) = O Ô£≠ max
,
(158)
Œ±‚àí1
Ô£≥
Ô£æ
K 2
K Œ±

meaning that to achieve f (y ^{K} ) ‚àí f (x ‚àó ) ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SSTM requires
r
 Œ±

 Œ± !!

LR ^{2} LR ^{2} œÉR Œ±‚àí1
1 œÉR Œ±‚àí1
iterations/oracle calls.
ln
ln
,
K = O
Œµ
ŒµŒ≤
Œµ
Œ≤
Œµ

42

(159)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

e 0 = R 0 ,
Proof. The proof starts similarly to the proof of Theorem 4.1 from (Gorbunov et al., 2021). Let R k = kz ^{k} ‚àíx ‚àó k, R
k+1 k k
e
e
R k+1 = max{ R k , R k+1 } for all k ‚â• 0. We first show by induction that for all k ‚â• 0 the iterates x
, z , y lie
A 0 y 0 +Œ± 1 z 0
0
‚àó
0
0 e
1
= z . Next, assume that
in B R e k (x ). The induction base is trivial since y = z , R 0 = R 0 , and x =
A 1
e l we have that z ^{l} ‚àà B R (x ‚àó ) ‚äÜ B e (x ‚àó ). Since y ^{l}
x ^{l} , z ^{l‚àí1} , y ^{l‚àí1} ‚àà B e (x ‚àó ) for some l ‚â• 1. By definitions of R l and R

^{R} l‚àí1

R l

l

is a convex combination of y ^{l‚àí1} ‚àà B R e l‚àí1 (x ‚àó ) ‚äÜ B R e l (x ‚àó ), z ^{l} ‚àà B R e l (x ‚àó ) and B R e l (x ‚àó ) is a convex set we conclude that
y ^{l} ‚àà B R e l (x ‚àó ). Finally, since x ^{l+1} is a convex combination of y ^{l} and z ^{l} we have that x ^{l+1} lies in B R e l (x ‚àó ) as well.

e l ‚â§ 3R with high probability, which allows us to apply the result of Lemma F.1
Next, our goal is to show by induction that R
and then use Bernstein‚Äôs inequality to estimate the stochastic part of the upper-bound. More precisely, for each k =
0, . . . , K we consider probability event E k defined as follows: inequalities

t‚àí1
X

l=0

Œ± l+1 Œ∏ l+1 , x ‚àó ‚àí z ^{l} + Œ± l+1 ‚àáf Œæ l (x ^{l+1} ) +

R t ‚â§ 2R

t‚àí1
X

l=0

2

^{Œ±} ^{2} l+1 kŒ∏ l+1 k ‚â§ ^{R} ^{2} ^{,}

(160)

(161)

hold for all t = 0, 1, . . . , k simultaneously. We want to prove via induction that P{E k } ‚â• 1 ‚àí ^{kŒ≤} / K for all k = 0, 1, . . . , K.
For k = 0 the statement is trivial: the left-hand side of (160) equals zero and R ‚â• R 0 by definition. Assume that the
statement is true for some k = T ‚àí 1 ‚â§ K ‚àí 1: P{E T ‚àí1 } ‚â• 1 ‚àí ^{(T} ‚àí1)Œ≤ / K . One needs to prove that P{E T } ‚â• 1 ‚àí ^{T} ^{Œ≤} / K .
e t ‚â§ 2R for all t = 0, 1, . . . , T ‚àí 1. Moreover, it implies that
First, we notice that probability event E T ‚àí1 implies that R

(21)

(156)

e Œæ T ‚àí1 (x ^{T} )k ‚â§ 2R + Œ± T Œª T ‚àí1 ‚â§ 3R.
kz T ‚àí x ‚àó k ‚â§ kz T ‚àí x ‚àó k + Œ± T k ‚àáf

Therefore, E T ‚àí1 implies {x ^{k} } Tk=0 , {z ^{k} } Tk=0 , {y ^{k} } Tk=0 ‚äÜ B 3R (x ‚àó ), meaning that the assumptions of Lemma F.1 are satis-
fied and we have


A t f (y t ) ‚àí f (x ‚àó )

‚â§

t‚àí1

t‚àí1

l=0

l=0

X
1 2 1 2 X
^{Œ±} ^{2} l+1 kŒ∏ l+1 k ^{2} (162)
Œ± l+1 Œ∏ l+1 , x ‚àó ‚àí z ^{l} + Œ± l+1 ‚àáf (x ^{l+1} ) +
R 0 ‚àí R t +
2
2

for all t = 0, 1, . . . , T simultaneously and for all t = 1, . . . , T ‚àí 1 this probability event also implies that

f (y t ) ‚àí f (x ‚àó )

(160),(162) ^{1} R ^{2} ‚àí ^{1} R ^{2} + R ^{2}
2 0
2 t

‚â§

A t

‚â§

3R 2
6aLR ^{2}
=
.
2A t
t(t + 3)

(163)

Taking into account that f (y ^{T} ) ‚àí f (x ‚àó ) ‚â• 0, we also derive that E T ‚àí1 implies

R T 2

‚â§ R 0 2 + 2

|

T
‚àí1
X

t=0

Œ± t+1 Œ∏ t+1 , x ‚àó ‚àí z ^{t} + Œ± t+1 ‚àáf (x ^{t+1} ) + 2

{z

T
‚àí1
X

t=0

^{Œ±} ^{2} t+1 kŒ∏ t+1 k

}

2B T

‚â§ R 2 + 2B T .

2

(164)

Before we estimate B T , we need to derive a few useful inequalities. We start with showing that E T ‚àí1 implies
k‚àáf ^{(x} ^{t+1} ^{)k} ‚â§ ^{Œª} ^{t} ^{/} ^{2} ^{for} ^{all} ^{t} ^{=} ^{0,} ^{1,} ^{.} ^{.} ^{.} ^{,} ^{T} ‚àí ^{1.} ^{For} ^{t} ^{=} ^{0} ^{we} ^{have} ^{x} ^{1} ^{=} ^{x} ^{0} ^{and}

4K

(6)

k‚àáf ^{(x} ^{1} ^{)k} ^{=} k‚àáf ^{(x} ^{0} ^{)k} ‚â§ ^{Lkx} ^{0} ‚àí ^{x} ^{‚àó} k ‚â§

43

R
Œª 0 60 ln Œ≤
=
¬∑
aŒ± 1
2
a

(155) Œª 0

‚â§

2

.

(165)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Next, for t = 1, . . . , T ‚àí 1 event E T ‚àí1 implies

k‚àáf ^{(x} ^{t+1} ^{)k}

‚â§

(6),(7)

‚â§

(154),(163)

‚â§

‚â§

=

(38),(156)

‚â§

=

‚â§

k‚àáf ^{(x} ^{t+1} ^{)} ‚àí ‚àáf ^{(y} ^{t} ^{)k} ^{+} k‚àáf ^{(y} ^{t} ^{)k}
p
Lkx ^{t+1} ‚àí y ^{t} k + 2L (f (y ^{t} ) ‚àí f (x ‚àó ))
s
^{LŒ±} t+1 t+1
12aL ^{2} R ^{2}
t
kx
‚àí z k +
A t
t(t + 3)
s
4LRŒ± t+1
12aL ^{2} R ^{2}
+
A t
t(t + 3)
Ô£´
Ô£∂
s
2 4K
2 Œ± 2
240LŒ± ^{2} t+1 ln ^{4K}
12aL
ln
R
t+1
Œ≤
Œ≤ Ô£∏
Ô£≠
+ 60
A
t(t
+
3)
60Œ± t+1 ln ^{4K}
t
Œ≤
v
Ô£´
Ô£∂
u


t+2 ^{2} 2 4K
t+2 ^{2}
4K
u
2
Œª t Ô£¨ ^{240L} 2aL ^{ln} Œ≤
t ^{12aL} 2aL ^{ln} Œ≤ Ô£∑
+ 60
Ô£≠
Ô£∏
t(t+3)
2
t(t + 3)

4aL

Ô£´

Ô£∂
s
4K
2
3(t + 2) ^{2} ln ^{4K}
Œª t Ô£≠ 240(t + 2) ln Œ≤
Œ≤ Ô£∏
+ 60
2
t(t + 3)a
t(t + 3)a
!
‚àö
4K
(155) Œª t
90 3 ln 4K
Œª t 540 ln Œ≤
Œ≤
‚àö
+
,
‚â§
2
a
a
2

(166)

2

9
where in the last row we use ^{(t+2)}
t(t+3) ‚â§ 4 ^{for} ^{all} ^{t} ‚â• ^{1.} ^{Therefore,} ^{probability} ^{event} ^{E} T ‚àí1 ^{implies} ^{that}

kx ^{‚àó} ‚àí ^{z} ^{t} ^{+} ^{Œ±} t+1 ‚àáf ^{(x} ^{t+1} ^{)k} ‚â§ kx ^{‚àó} ‚àí ^{z} ^{t} k ^{+} ^{Œ±} t+1 k‚àáf ^{(x} ^{t+1} ^{)k}

(161),(165),(166)

‚â§

2R +

R
‚â§ 3R
60 ln 4K
Œ≤

(167)

for all t = 0, 1, . . . , T ‚àí 1. Next, we define random vectors

Œ∑ t =

(

x ‚àó ‚àí z ^{t} + Œ± t+1 ‚àáf (x ^{t+1} ), if kx ‚àó ‚àí z ^{t} + Œ± t+1 ‚àáf (x ^{t+1} )k ‚â§ 3R,
0,
otherwise,

for all t = 0, 1, . . . , T ‚àí 1. By definition these random vectors are bounded with probability 1

kŒ∑ t k ‚â§ 3R

(168)

and probability event E T ‚àí1 implies that Œ∑ t = x ‚àó ‚àí z ^{t} + Œ± t+1 ‚àáf (x ^{t+1} ) for all t = 0, 1, . . . , T ‚àí 1. Then, form E T ‚àí1 it
follows that

B T

=

T
‚àí1
X

t=0

^{Œ±} t+1 hŒ∏ t+1 ^{,} ^{Œ∑} t i ^{+}

T
‚àí1
X

t=0

2

^{Œ±} ^{2} t+1 kŒ∏ t+1 k ^{.}

u
b
Next, we define the unbiased part and the bias of ^{Œ∏} t+1 as ^{Œ∏} t+1
and ^{Œ∏} t+1
respectively:

h
i
u
e Œæ t (x t+1 ) ,
e Œæ t (x t+1 ) ‚àí E Œæ t ‚àáf
^{Œ∏} t+1
= ‚àáf

44

h
i
b
e Œæ t (x t+1 ) ‚àí ‚àáf (x t+1 ).
^{Œ∏} t+1
= E Œæ t ‚àáf

(169)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

u
b
We notice that ^{Œ∏} t+1 ^{=} ^{Œ∏} t+1
^{+} ^{Œ∏} t+1
. Using new notation, we get that E T ‚àí1 implies

B T

T
‚àí1
X

=

u
b
^{Œ±} t+1 ^{Œ∏} t+1
^{+} ^{Œ∏} t+1
, Œ∑ t +

t=0

T
‚àí1
X

‚â§

u
b
^{Œ±} ^{2} t+1 ^{Œ∏} t+1
^{+} ^{Œ∏} t+1

2

t=0

u
, Œ∑ t +
^{Œ±} t+1 ^{Œ∏} t+1

t=0

|

T
‚àí1
X

{z

}

1

+2

|

T
‚àí1
X

^{Œ±} ^{2} t+1 ^{E} ^{Œæ} ^{t}

t=0

{z

T
‚àí1
X

b
, Œ∑ t + 2
^{Œ±} t+1 ^{Œ∏} t+1

t=0

h

|

{z

u
^{Œ∏} t+1

2

i

+2

}

4

|

T
‚àí1
X

|

}

2

T
‚àí1
X

^{Œ±} ^{2} t+1

t=0

u
^{Œ∏} t+1

2

{z

‚àí E Œæ t

h

2

u
^{Œ∏} t+1

2

{z

.

i

}

3

b
^{Œ±} ^{2} t+1 ^{Œ∏} t+1

t=0



(170)

}

5

It remains to derive good enough high-probability upper-bounds for the terms 1, 2, 3, 4, 5, i.e., to finish our inductive
proof we need to show that 1 + 2 + 3 + 4 + 5 ‚â§ R ^{2} with high probability. In the subsequent parts of the proof, we
u
b
will need to use many times the bounds for the norm and second moments of ^{Œ∏} t+1
and ^{Œ∏} t+1
. First, by definition of clipping
operator, we have with probability 1 that
u
k ‚â§ 2Œª t .
(171)
kŒ∏ t+1

^{Moreover,} ^{since} ^{E} T ‚àí1 ^{implies} ^{that} k‚àáf ^{(x} ^{t+1} ^{)k} ‚â§ ^{Œª} ^{t} ^{/} ^{2} ^{for} ^{t} ^{=} ^{0,} ^{1,} ^{.} ^{.} ^{.} ^{,} ^{T} ‚àí ^{1} ^{(see} ^{(165)} ^{and} ^{(166)),} ^{then,} ^{in} ^{view} ^{of}
Lemma 5.1, we have that E T ‚àí1 implies

b
kŒ∏ t+1
k
 u 2 
^{E} ^{Œæ} ^{t} kŒ∏ t+1 k

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1
t

(172)

‚â§ 18Œª ^{2‚àíŒ±}
œÉ Œ± .
t

(173)

‚â§

u
u
Upper bound for ^{1.} By definition of Œ∏ t+1
, we have E Œæ t ^{[Œ∏} t+1
] = 0 and



u
, Œ∑ t = 0.
^{E} Œæ ^{t} ^{Œ±} t+1 ^{Œ∏} t+1

Next, sum 1 has bounded with probability 1 terms:

u
u
k ¬∑ kŒ∑ t k
^{,} ^{Œ∑} t | ‚â§ ^{Œ±} t+1 kŒ∏ t+1
|Œ± t+1 ^{Œ∏} t+1

(168),(171)

‚â§

(156)

^{6Œ±} t+1 ^{Œª} t ^{R} ^{=}

^{R} ^{2} def
= c.
5 ln 4K
Œ≤

(174)

2

def

u
, Œ∑ t ]:
The summands also have bounded conditional variances ^{œÉ} t ^{2} ^{=} E Œæ t ^{[Œ±} ^{2} t+1 ^{Œ∏} t+1

 (168)

 u 2 
u
^{œÉ} t ^{2} ‚â§ ^{E} ^{Œæ} ^{t} ^{Œ±} ^{2} t+1 kŒ∏ t+1
k ^{2} ¬∑ kŒ∑ ^{t} k ^{2} ‚â§ ^{9Œ±} ^{2} t+1 ^{R} ^{2} ^{E} ^{Œæ} ^{t} kŒ∏ t+1
k .

(175)

‚àí1
u
^{,} ^{Œ∑} ^{t} } ^{T} t=0
is a bounded martingale difference sequence with bounded condi-
In other words, we showed that {Œ± t+1 ^{Œ∏} t+1
2 T ‚àí1
u
tional variances {œÉ t } t=0 . Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with ^{X} t ^{=} ^{Œ±} t+1 ^{Œ∏} t+1
, Œ∑ t , parameter c as
R 2
R 4
^{in} ^{(174),} ^{b} ^{=} 5 ^{,} ^{G} ^{=} 150 ln 4K ^{:}

Œ≤

(

R 2
^{P} |1| ^{>}
5

and

T
‚àí1
X

t=0

œÉ t 2 ‚â§

R 4
150 ln ^{4K}
Œ≤

)


‚â§ 2 exp ‚àí

b 2
2G + 2cb / 3



=

Œ≤
.
2K

Equivalently, we have

Œ≤
,
P {E 1 } ‚â• 1 ‚àí
2K

for E 1 =

(

either

T
‚àí1
X

t=0

45

œÉ t 2 >

R 4
150 ln ^{4K}
Œ≤

R 2
^{or} |1| ‚â§
5

)

.

(176)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

In addition, E T ‚àí1 implies that

T
‚àí1
X

(175)

œÉ t 2

‚â§

t=0

(156)

‚â§

‚â§

9R

2

T
‚àí1
X

^{Œ±} ^{2} t+1 ^{E} ^{Œæ} ^{t}

t=0



u
kŒ∏ t+1
k 2

T
‚àí1
X

T
‚àí1
X
 (173)
Œ± 2
‚â§ 162œÉ R
Œ± ^{2} t+1 Œª ^{2‚àíŒ±}
t

t=0

T
‚àí1
X
162œÉ ^{Œ±} R ^{4‚àíŒ±}
162œÉ ^{Œ±} R ^{4‚àíŒ±}
Œ±
Œ±
=
(t + 2) Œ±
t+1
2‚àíŒ± ¬∑ 2 Œ± a Œ± L Œ± ln 2‚àíŒ± 4K
30 2‚àíŒ± ln 2‚àíŒ± 4K
30
Œ≤ t=0
Œ≤ t=0

1 162œÉ ^{Œ±} R ^{4‚àíŒ±} T (T + 1) ^{Œ±} (155)
R 4
¬∑
.
‚â§
a Œ±
150 ln ^{4K}
60L Œ± ln ^{2‚àíŒ±} ^{4K}
Œ≤
Œ≤

(177)

Upper bound for 2. From E T ‚àí1 it follows that

2

T
‚àí1
X

‚â§

t=0

b
^{Œ±} t+1 kŒ∏ t+1
k ¬∑ kŒ∑ t k

(168),(172)

‚â§

3R ¬∑ 2 Œ± œÉ Œ±

T
‚àí1
X

30
Œ± t+1 ^{(156)}
Œ±
Œ±‚àí1 ‚â§ ^{12RœÉ} ¬∑
Œª
t=0 ^{t}

Œ±‚àí1

T
‚àí1
X
ln Œ±‚àí1 4K
Œ≤

R Œ±‚àí1

t=0

T
‚àí1
Œ± 2‚àíŒ±
2
X
T (T + 1) Œ± ln Œ±‚àí1 4K
360œÉ ^{Œ±} R ^{2‚àíŒ±} ln ^{Œ±‚àí1} ^{4K}
1 180œÉ R
Œ≤
Œ≤ (155) R
Œ±
‚â§
¬∑
.
(t
+
2)
‚â§
2 Œ± a Œ± L Œ±
a Œ±
L Œ±
5
t=0

‚â§

Œ± Œ±
t+1

(178)

Upper bound for 3. First, we have


h
ii
h
2
2
u
u
= 0.
‚àí ^{E} ^{Œæ} ^{t} ^{Œ∏} t+1
^{E} ^{Œæ} ^{t} ^{2Œ±} ^{2} t+1 ^{Œ∏} t+1

Next, sum 3 has bounded with probability 1 terms:

h
i
2
2
u
u
^{2Œ±} ^{2} t+1 ^{Œ∏} t+1
‚àí ^{E} ^{Œæ} ^{t} ^{Œ∏} t+1

‚â§

(171)

‚â§

h
i

2
u
u
^{2Œ±} ^{2} t+1 kŒ∏ t+1
k ^{2} ^{+} ^{E} ^{Œæ} ^{t} ^{Œ∏} t+1

(156)

16Œ± ^{2} t+1 Œª ^{2} t ‚â§

^{R} ^{2} def
= c.
5 ln 4K
Œ≤

(179)



h
i 2 
2
2
def
u
u
:
‚àí ^{E} ^{Œæ} ^{t} ^{Œ∏} t+1
The summands also have bounded conditional variances œÉ
e t ^{2} ^{=} ^{E} ^{Œæ} ^{t} ^{4Œ±} ^{4} t+1 ^{Œ∏} t+1

h
R 2
2
t 2Œ±
E
Œæ
t+1
5 ln 4K
Œ≤

(179)

œÉ
e t 2

‚â§

u
^{Œ∏} t+1

2

‚àí E Œæ k

h

u
^{Œ∏} t+1

2

i i

 u 2 
‚â§ ^{Œ±} ^{2} t+1 ^{R} ^{2} ^{E} ^{Œæ} ^{t} kŒ∏ t+1
k ,

(180)

n

h
io ^{T} ^{‚àí1}
2
2
2
u
u
t
since ln ^{4K}
‚â•
1.
In
other
words,
we
showed
that
2Œ±
Œ∏
Œ∏
‚àí
E
is a bounded martingale
Œæ
t+1
t+1
t+1
Œ≤

t=0

‚àí1
difference sequence
^{œÉ} t ^{2} } ^{T} t=0
. Next, we apply Bernstein‚Äôs inequality (Lemma B.2)
 with bounded h conditional
i variances {e

with ^{X} t ^{=} ^{2Œ±} ^{2} t+1

u
^{Œ∏} t+1

2

u
^{Œ∏} t+1

‚àí E Œæ t

(

^{P} |3| ^{>}

R 2
5

and

2

Œ≤

T
‚àí1
X

t=0

Equivalently, we have

Œ≤
P {E 3 } ‚â• 1 ‚àí
,
2K

œÉ
e t 2 ‚â§

for E 3 =

(

R 4
150 ln ^{4K}
Œ≤

either

t=0

œÉ
e t 2

(180)

‚â§

R 2

T
‚àí1
X

t=0

)

T
‚àí1
X

t=0

In addition, E T ‚àí1 implies that

T
‚àí1
X

4

2

^{,} ^{parameter} ^{c} ^{as} ^{in} ^{(179),} ^{b} ^{=} ^{R} 5 ^{,} ^{G} ^{=} 150 ^{R} ln 4K ^{:}


‚â§ 2 exp ‚àí

œÉ
e t 2 >

b 2
2G + 2cb / 3

R 4
150 ln ^{4K}
Œ≤



=

R 2
^{or} |3| ‚â§
5

T
‚àí1
X
 u 2 
 u 2  (177)
‚â§
^{Œ±} ^{2} t+1 ^{E} ^{Œæ} ^{t} kŒ∏ t+1
k ‚â§ 9R 2
^{Œ±} ^{2} t+1 ^{E} ^{Œæ} ^{t} kŒ∏ t+1
k

t=0

46

Œ≤
.
2K

)

.

R 4
.
150 ln ^{4K}
Œ≤

(181)

(182)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Upper bound for 4. From E T ‚àí1 it follows that

4

= 2

T
‚àí1
X

h

^{Œ±} ^{2} t+1 ^{E} ^{Œæ} ^{t}

t=0

u
^{Œ∏} t+1

2

i

h
i
X
R 2
R 2
1
2 (177)
u
2
2
t
Œ∏
¬∑
9R
Œ±
E
.
‚â§
‚â§
Œæ
t+1
t+1
R 2
5
150 ln ^{4K}
Œ≤
t=0

T ‚àí1

‚â§

(183)

Upper bound for 5. From E T ‚àí1 it follows that

5 =

2

T
‚àí1
X

b
^{Œ±} ^{2} t+1 ^{Œ∏} t+1

t=0

=

2

2Œ±+1

2

‚â§ 2 2Œ±+1 œÉ 2Œ±

T
‚àí1
X

^{Œ±} ^{2} t+1 (156) ^{2}
=
Œª 2Œ±‚àí2
t=0 ^{t}

T
‚àí1
X
¬∑ 30 2Œ±‚àí2 œÉ 2Œ± ln 2Œ±‚àí2 4K
Œ≤

2Œ±+1

R 2Œ±‚àí2

Œ± 2Œ±
t+1

t=0
2Œ±‚àí2 4K T ‚àí1
2Œ±‚àí2 4K
2Œ±
2Œ±
2
X
¬∑ 30
œÉ ln
1 1800œÉ T (T + 1) ln
Œ≤
Œ≤ (155) R
2Œ±
¬∑
‚â§
.
(t
+
2)
‚â§
2 2Œ± a 2Œ± L 2Œ± R 2Œ±‚àí2
a 2Œ±
L 2Œ± R 2Œ±‚àí2
5
t=0

2Œ±‚àí2 2Œ±

(184)

Now, we have the upper bounds for 1, 2, 3, 4, 5. In particular, probability event E T ‚àí1 implies

(170)

B T ‚â§ R 2 + 1 + 2 + 3 + 4 + 5,

(178) R ^{2}

2 ‚â§

T
‚àí1
X

t=0

(177)

œÉ t 2 ‚â§

5

,

(183) R ^{2}

4 ‚â§

,

5
T
‚àí1
X

R 4
,
150 ln ^{4K}
Œ≤

t=0

(184) R ^{2}

5 ‚â§

(182)

œÉ
e t 2 ‚â§

5

,

R 4
.
150 ln ^{4K}
Œ≤

Moreover, we also have (see (176), (181) and our induction assumption)

P{E T ‚àí1 } ‚â• 1 ‚àí

(T ‚àí 1)Œ≤
,
K

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
2K

P{E 3 } ‚â• 1 ‚àí

Œ≤
,
2K

where

E 1

E 3

=

=

(

(

either

T
‚àí1
X

t=0

either

T
‚àí1
X

t=0

Thus, probability event E T ‚àí1 ‚à© E 1 ‚à© E 3 implies

B T

R T 2

‚â§

(164)

‚â§

œÉ t 2 >

R 2 +

œÉ
e t 2 >

R 4
150 ln ^{4K}
Œ≤

R 2
^{or} |1| ‚â§
5

R 4
150 ln ^{4K}
Œ≤

R 2
^{or} |3| ‚â§
5

)

)

,

.

R 2
R 2
R 2
R 2
R 2
+
+
+
+
= 2R 2 ,
5
5
5
5
5

R ^{2} + 2R ^{2} ‚â§ (2R) ^{2} ,

which is equivalent to (160) and (161) for t = T , and


TŒ≤
.
P{E T } ‚â• P {E T ‚àí1 ‚à© E 1 ‚à© E 3 } = 1 ‚àí P E T ‚àí1 ‚à™ E 1 ‚à™ E 3 ‚â• 1 ‚àí P{E T ‚àí1 } ‚àí P{E 1 } ‚àí P{E 3 } ‚â• 1 ‚àí
K

This finishes the inductive part of our proof, i.e., for all k = 0, 1, . . . , K we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / K . In particular, for
k = K we have that with probability at least 1 ‚àí Œ≤

(163)

f (y K ) ‚àí f (x ‚àó ) ‚â§

6aLR ^{2}
K(K + 3)

k K
k K
‚àó
and {x ^{k} } ^{K+1}
k=0 ^{,} {z } k=0 ^{,} {y } k=0 ‚äÜ ^{B} 2R ^{(x} ^{),} ^{which} ^{follows} ^{from} ^{(161).}

47

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Finally, if

Ô£±
Ô£≤

1

Œ±‚àí1

4K 900œÉ(K + 1)K Œ± ln Œ±
,
a = max 48600 ln ^{2}
Ô£≥
Œ≤
LR

then with probability at least 1 ‚àí Œ≤

f (y K ) ‚àí f (x ‚àó ) ‚â§

=

Ô£º

4K Ô£Ω
Œ≤

Ô£æ

,

Ô£º
Ô£±
Œ±‚àí1
1
4K Ô£Ω
2 2 4K
Ô£≤
Œ± ln Œ±
5400œÉR(K
+
1)K
291600LR
ln
6aLR
Œ≤
Œ≤
= max
,
Ô£æ
Ô£≥
K(K + 3)
K(K + 3)
K(K + 3)
Ô£º
Ô£±
Ô£∂
Ô£´
K Ô£Ω
Ô£≤ LR 2 ln 2 K œÉR ln Œ±‚àí1
Œ±
Œ≤
Œ≤
Ô£∏ .
,
O Ô£≠ max
Œ±‚àí1
Ô£æ
Ô£≥
K 2
Œ±
K

2

To get f (y ^{K} ) ‚àí f (x ‚àó ) ‚â§ Œµ with probability at least 1 ‚àí Œ≤ it is sufficient to choose K such that both terms in the maximum
^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}
r
 Œ±

 Œ± !!

LR ^{2} LR ^{2} œÉR Œ±‚àí1
1 œÉR Œ±‚àí1
ln
K = O
ln
,
Œµ
ŒµŒ≤
Œµ
Œ≤
Œµ

that concludes the proof.

F.2. Strongly Convex Functions

In the strongly convex case, we consider the restarted version of clipped-SSTM (R-clipped-SSTM). The main result is
summarized below.

Algorithm 3 Restarted clipped-SSTM (R-clipped-SSTM) (Gorbunov et al., 2020)

Input: starting point x ^{0} , number of restarts œÑ , number of steps of clipped-SSTM between restarts {K t } œÑt=1 , stepsize
K 1 ‚àí1
K 2 ‚àí1
K œÑ ‚àí1
parameters {a t } œÑt=1 , clipping levels {Œª ^{1} k } k=0
^{,} {Œª ^{2} k } k=0
^{,} ^{.} ^{.} ^{.} ^{,} {Œª ^{œÑk} } k=0
, smoothness constant L.
0
0
1: xÃÇ = x
2: for t = 1, . . . , œÑ do
K t ‚àí1
, and
3:
Run ^{clipped-SSTM} (Algorithm 2) for K t iterations with stepsize parameter a t , clipping levels {Œª tk } k=0
t‚àí1
t
starting point xÃÇ . Define the output of clipped-SSTM by xÃÇ .
4: end for
Output: xÃÇ ^{œÑ}

Theorem F.3 (Full version of Theorem 3.3). Let Assumptions 1.1, 1.3, 1.6 with Œº > 0 hold for Q = B 3R (x ‚àó ), where
R ‚â• kx ^{0} ‚àí x ‚àó k ^{2} and R-clipped-SSTM runs clipped-SSTM œÑ times. Let
q
Ô£±
Ô£º Ô£π
Ô£Æ
s
Œ±
Œ± !


 Œ±‚àí1
 Œ±‚àí1
2 œÑ
Ô£≤
Ô£Ω
2
LR
2160
t‚àí1
^{LR} t‚àí1
5400œÉR t‚àí1
4œÑ 5400œÉR t‚àí1
Ô£∫ ,
, 2
K t = Ô£Ø
ln
(185)
ln
‚àö
Ô£Ø max Ô£≥ 1080
Ô£æ Ô£∫
Œµ t
Œµ t Œ≤
Œµ t
Œ≤
Œµ t
Ô£Ø
Ô£∫


2
^{ŒºR} t‚àí1
ŒºR 2
4K t œÑ
R
Œµ t =
, ln
, R t‚àí1 = (t‚àí1) / 2 , œÑ = log 2
‚â• 1,
(186)
4
2Œµ
Œ≤
2
Ô£º
Ô£±
1
Œ±‚àí1
4K t œÑ Ô£Ω
Œ±
Ô£≤
Œ±
900œÉ(K
+
1)K
ln
t
4K
œÑ
t
Œ≤
t
,
(187)
,
a t = max 48600 ln ^{2}
Ô£æ
Ô£≥
Œ≤
LR t

Œª tk =

R t
t
30Œ± k+1 ln ^{4K} Œ≤ ^{t} ^{œÑ}

for t = 1, . . . , œÑ . Then to guarantee f (xÃÇ ^{œÑ} ) ‚àí f (x ‚àó ) ‚â§ Œµ with probability ‚â• 1 ‚àí Œ≤ R-clipped-SSTM requires
(s
‚àö
Œ±
  Œ±
 ^{2} !  ^{2}  2(Œ±‚àí1)
 2 
 ^{2} !)!
L
L
œÉ
ŒºR
ŒºR
1 œÉ ^{2} 2(Œ±‚àí1)
ŒºR
O max
ln ‚àö
,
ln
ln
ln
ln
ŒºŒ≤
Œº
Œµ
Œµ
ŒºŒµ
Œ≤ ŒºŒµ
Œµ

48

(188)

(189)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

iterations/oracle calls. Moreover, with probability ‚â• 1 ‚àí Œ≤ the iterates of R-clipped-SSTM at stage t stay in the ball
^{B} ^{2R} t‚àí1 ^{(x} ^{‚àó} ^{).}

Proof. We show by induction that for any t = 1, . . . , œÑ with probability at least 1 ‚àí ^{tŒ≤} / œÑ inequalities

f (xÃÇ ^{l} ) ‚àí f (x ^{‚àó} ) ‚â§ Œµ l ,

kxÃÇ ^{l} ‚àí ^{x} ^{‚àó} k ^{2} ‚â§ ^{R} l ^{2} ^{=}

R 2
2 l

(190)

hold for l = 1, . . . , t simultaneously. First, we prove the base of the induction. Theorem F.2 implies that with probability
at least 1 ‚àí ^{Œ≤} / œÑ
Ô£±
Ô£º
1
4K 1 œÑ Ô£Ω
Ô£≤ 291600LR 2 ln ^{2} ^{4K} 1 ^{œÑ} 5400œÉR(K 1 + 1)K Œ± ln ^{Œ±‚àí1}
Œ±
2
6a
LR
1
(187)
1
Œ≤
Œ≤
f (xÃÇ ^{1} ) ‚àí f (x ^{‚àó} ) ‚â§
= max
,
Ô£≥
Ô£æ
K 1 (K 1 + 3)
K 1 (K 1 + 3)
K 1 (K 1 + 3)
Ô£º
Ô£±
4K 1 œÑ Ô£Ω
Ô£≤ 291600LR 2 ln ^{2} ^{4K} 1 ^{œÑ} 5400œÉR ln ^{Œ±‚àí1}
Œ±
Œ≤
Œ≤
,
‚â§ max
Œ±‚àí1
Ô£æ
Ô£≥
K 1 2
Œ±
K 1

(185)

‚â§

Œµ 1 =

ŒºR 2
4

and, due to the strong convexity,

kxÃÇ ^{1} ‚àí ^{x} ^{‚àó} k ^{2} ‚â§

2(f (xÃÇ ^{1} ) ‚àí f (x ‚àó ))
R 2
‚â§
= R 1 2 .
Œº
2

The base of the induction is proven. Now, assume that the statement holds for some t = T < œÑ , i.e., with probability at
least 1 ‚àí ^{T} ^{Œ≤} / œÑ inequalities
R 2
(191)
^{f} ^{(xÃÇ} ^{l} ^{)} ‚àí ^{f} ^{(x} ^{‚àó} ^{)} ‚â§ ^{Œµ} l ^{,} kxÃÇ ^{l} ‚àí ^{x} ^{‚àó} k ^{2} ‚â§ ^{R} l ^{2} ^{=} l
2
^{hold} ^{for} ^{l} ^{=} ^{1,} ^{.} ^{.} ^{.} ^{,} ^{T} ^{simultaneously.} ^{In} ^{particular,} ^{with} ^{probability} ^{at} ^{least} ^{1} ‚àí ^{T} ^{Œ≤} ^{/} ^{œÑ} ^{we} ^{have} kxÃÇ ^{T} ‚àí ^{x} ^{‚àó} k ^{2} ‚â§ ^{R} T ^{2} ^{.} ^{Applying}
Theorem F.2 and using union bound for probability events, we get that with probability at least 1 ‚àí ^{(T} ^{+1)Œ≤} / œÑ

f (xÃÇ ^{T} ^{+1} ) ‚àí f (x ‚àó )

6a T +1 LR T 2
K T +1 (K T +1 + 3)
Ô£º
Ô£±
1
4K T +1 œÑ Ô£Ω
Ô£≤ 291600LR 2 ln ^{2} ^{4K} T +1 ^{œÑ} 5400œÉR T (K T +1 + 1)K Œ± ln ^{Œ±‚àí1}
Œ±
T
T +1
Œ≤
Œ≤
,
max
Ô£æ
Ô£≥ K T +1 (K T +1 + 3)
K T +1 (K T +1 + 3)
Ô£º
Ô£±
4K T +1 œÑ Ô£Ω
Ô£≤ 291600LR 2 ln ^{2} ^{4K} T +1 ^{œÑ} 5400œÉR T ln ^{Œ±‚àí1}
Œ±
T
Œ≤
Œ≤
max
,
Œ±‚àí1
Ô£æ
Ô£≥
K T 2 +1
Œ±
K T +1

‚â§

(187)

=

‚â§

(185)

‚â§

Œµ T +1 =

ŒºR T 2
4

and, due to the strong convexity,

kxÃÇ ^{T} ^{+1} ‚àí ^{x} ^{‚àó} k ^{2} ‚â§

R 2
2(f (xÃÇ ^{T} ^{+1} ) ‚àí f (x ‚àó ))
‚â§ T = R T 2 +1 .
Œº
2

Thus, we finished the inductive part of the proof. In particular, with probability at least 1 ‚àí Œ≤ inequalities

f (xÃÇ ^{l} ) ‚àí f (x ^{‚àó} ) ‚â§ Œµ l ,

kxÃÇ ^{l} ‚àí ^{x} ^{‚àó} k ^{2} ‚â§ ^{R} l ^{2} ^{=}

R 2
2 l

hold for l = 1, . . . , œÑ simultaneously, which gives for l = œÑ that with probability at least 1 ‚àí Œ≤

f (xÃÇ ^{œÑ} ) ‚àí f (x ‚àó ) ‚â§ Œµ œÑ =

ŒºR œÑ 2 ‚àí1
ŒºR ^{2} (186)
= œÑ +1 ‚â§ Œµ.
4
2

49

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

It remains to calculate the overall number of oracle calls during all runs of clipped-SSTM. We have
Ô£º Ô£∂
Ô£± s
Ô£∂
Ô£´ q
Ô£´
Œ±
Œ± !


 Œ±‚àí1
 Œ±‚àí1
2 œÑ
œÑ
œÑ
Ô£Ω
Ô£≤ LR 2
^{LR} t‚àí1
X
X
^{œÑ} ^{œÉR} t‚àí1
t‚àí1
Ô£∏ , ^{œÉR} t‚àí1
Ô£∏
ln Ô£≠ ‚àö
ln
K t = O Ô£≠
max
Ô£æ
Ô£≥
Œµ t
Œµ t Œ≤
Œµ t
Œ≤
Œµ t
t=1
t=1
(s
‚àö ! 
Œ±
^{Œ±} !)!

 Œ±‚àí1
 Œ±‚àí1
œÑ
X
L
LœÑ
œÑ
œÉ
œÉ
ln
,
ln ‚àö
= O
max
Œº
ŒºŒ≤
^{ŒºR} t‚àí1
^{Œ≤} ^{ŒºR} t‚àí1
t=1
( s
Œ±
‚àö ! œÑ 


 ^{Œ±} !)!
t
X œÉ ¬∑ 2 ^{t} / 2 Œ±‚àí1
œÑ œÉ ¬∑ 2 / 2 Œ±‚àí1
L
LœÑ
ln
= O max œÑ
,
ln ‚àö
Œº
ŒºŒ≤
ŒºR
Œ≤
ŒºR
t=1
(s
)!
‚àö
Œ±
 Œ±‚àí1

 Œ± ! œÑ
 2 
 2 ! 
œÑ
Œ±t
œÉ
ŒºR
L
L
^{œÑ} ^{œÉ} ¬∑ ^{2} ^{/} ^{2} ^{Œ±‚àí1} X 2(Œ±‚àí1)
ŒºR
= O max
ln ‚àö
,
ln
ln
2
ln
Œº
Œµ
ŒºŒ≤
Œµ
ŒºR
Œ≤
ŒºR
t=1
!
)!
(s
‚àö
 Œ±

 Œ±
 2 
 2 ! 
Œ±
Œ±œÑ
L
L
œÑ
œÉ Œ±‚àí1
œÉ Œ±‚àí1
ŒºR
ŒºR
ln
¬∑ 2 2(Œ±‚àí1) 2 2(Œ±‚àí1)
= O max
ln ‚àö
,
ln
ln
Œº
Œµ
ŒºŒ≤
Œµ
ŒºR
Œ≤ ŒºR
(s
‚àö
Œ±
  Œ±
 2 
 ^{2} !  ^{2}  2(Œ±‚àí1)
 ^{2} !)!
œÉ
ŒºR
ŒºR
1 œÉ ^{2} 2(Œ±‚àí1)
ŒºR
L
L
ln ‚àö
,
,
ln
ln
ln
ln
= O max
Œº
Œµ
ŒºŒ≤
Œµ
ŒºŒµ
Œ≤ ŒºŒµ
Œµ

which concludes the proof.

50

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

G. Missing Proofs for clipped-SEG

In this section, we provide the complete formulation of the main results for clipped-SSTM
and

 and R-clipped-SSTM
x k ) =
the missing proofs. For brevity, we will use the following notation: F e Œæ 1 k (x ^{k} ) = clip F Œæ 1 k (x ^{k} ), Œª k and F e Œæ 2 k (e


clip F Œæ 2 k (e
x k ), Œª k .

Algorithm 4 Clipped Stochastic Extragradient (clipped-SEG) (Gorbunov et al., 2022a)

Input: starting point x ^{0} , number of iterations K, stepsize Œ≥ > 0, clipping levels {Œª k } ^{K‚àí1}
k=0 ^{.}
1: for k = 0, . . . , K do


2:
Compute F e Œæ 1 k (x ^{k} ) = clip F Œæ 1 k (x ^{k} ), Œª k using a fresh sample Œæ 1 ^{k} ‚àº D k
3:
x
e k = x k ‚àí Œ≥ F e Œæ 1 k (x k )


4:
Compute F e Œæ 2 k (e
x ^{k} ) = clip F Œæ 2 k (e
x ^{k} ), Œª k using a fresh sample Œæ 2 ^{k} ‚àº D k
5:
x k+1 = x k ‚àí Œ≥ F e Œæ 2 k (e
x k )
6: end for
K
P
1
x
e K
Output: x ^{K+1} or x
e K
avg ^{=} K+1

k=0

G.1. Monotone Problems

We start with the following lemma derived by Gorbunov et al. (2022b). Since this lemma handles only deterministic part
of the algorithm, the proof is the same as in the original work.
Lemma G.1 (Lemma C.1 from (Gorbunov et al., 2022b)). Let Assumptions 1.7 and 1.8 hold for Q = B 4R (x ‚àó ), where
‚àö
e ^{k} lie in B 4R (x ‚àó ) for all k = 0, 1, . . . , K for some K ‚â• 0, then for all
R ‚â• kx ^{0} ‚àí x ‚àó k and 0 < Œ≥ ‚â§ ^{1} / 2L . If x ^{k} and x
‚àó
u ‚àà B 4R (x ) the iterates produced by clipped-SEG satisfy

K

hF (u), x
e K
avg ‚àí ^{ui}

x
e K
avg

Œ∏ k

œâ k

‚â§

X

kx 0 ‚àí uk 2 ‚àí kx K+1 ‚àí uk 2
Œ≥
+
kŒ∏ k k ^{2} + 2kœâ k k ^{2}
2Œ≥(K + 1)
2(K + 1)

k=0

+

def

=

1
K +1

K

K
X

hx k ‚àí u ‚àí Œ≥F (e
x k ), Œ∏ k i,

(192)

k=0

1 X k
x
e ,
K +1

(193)

k=0

def

=

def

=

F (e
x k ) ‚àí F e Œæ 2 k (e
x k ),

(194)

F (x k ) ‚àí F e Œæ 1 k (x k ).

(195)

Using this lemma we prove the main convergence result for clipped-SEG in the monotone case.
Theorem G.2 (Case 1 in Theorem 4.1). Let Assumptions 1.1, 1.7, 1.8 hold for Q = B 4R (x ‚àó ), where R ‚â• kx ^{0} ‚àí x ‚àó k, and
Ô£±
Ô£º
2‚àíŒ±
Ô£≤
Ô£Ω
1
20 Œ± R
0 < Œ≥ ‚â§ min
,
,
(196)
Œ±‚àí1
Ô£≥ 160L ln ^{6(K+1)} 10800 Œ± ^{1} (K + 1) Œ± ^{1} œÉ ln Œ± ^{6(K+1)} Ô£æ

Œ≤

Œª k ‚â° Œª =

R

20Œ≥ ln ^{6(K+1)}
Œ≤

Œ≤

,

(197)

for some K ‚â• 0 and Œ≤ ‚àà (0, 1] such that ln ^{6(K+1)}
‚â• 1. Then, after K iterations the iterates produced by clipped-SEG
Œ≤
with probability at least 1 ‚àí Œ≤ satisfy

Gap R (e
x K
avg ^{)} ‚â§

9R 2
2Œ≥(K + 1)

‚àó
‚àó
and {x ^{k} } ^{K+1}
x k } K+1
k=0 ‚äÜ ^{B} ^{3R} ^{(x} ^{),} {e
k=0 ‚äÜ ^{B} ^{4R} ^{(x} ^{),}

51

(198)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

where x
e K
avg is defined in (193). In particular, when Œ≥ equals the minimum from (196), then the iterates produced by
clipped-SEG after K iterations with probability at least 1 ‚àí Œ≤ satisfy
Ô£º Ô£∂
Ô£±
Ô£´
K Ô£Ω
Ô£≤ LR 2 ln K œÉR ln Œ±‚àí1
Œ±
Œ≤
Œ≤
Ô£∏ ,
Ô£≠ max
,
(199)
Gap R (e
x K
Œ±‚àí1
avg ^{)} ^{=} O
Ô£æ
Ô£≥
K
Œ±
K

meaning that to achieve Gap R (e
x K
avg ) ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SEG requires
!
 Œ±

LR ^{2} LR ^{2} œÉR Œ±‚àí1 œÉR
ln
K = O
iterations/oracle calls.
ln
,
Œµ
ŒµŒ≤
Œµ
ŒµŒ≤

(200)

Proof. The proof follows similar steps as the proof of Theorem C.1 from (Gorbunov et al., 2022a). The key difference is
related to the application of Bernstein inequality and estimating biases and variances of stochastic terms.

Let R k = kx ^{k} ‚àíx ‚àó k for all k ‚â• 0. As in the previous results, the proof is based on the induction argument and showing that
the iterates do not leave some ball around the solution with high probability. More precisely, for each k = 0, 1, . . . , K + 1
we consider probability event E k as follows: inequalities
(
)
t‚àí1
t‚àí1
X
X

0
2
l
l
2
2
2
hx ‚àí u ‚àí Œ≥F (e
x ), Œ∏ l i + Œ≥
kŒ∏ l k + 2kœâ l k
max ‚àó kx ‚àí uk + 2Œ≥
‚â§ 9R 2 ,
(201)

u‚ààB R (x )

l=0

|

{z

l=0

}

A t

Œ≥

t‚àí1
X

l=0

Œ∏ l ‚â§ R

(202)

hold for t = 0, 1, . . . , k simultaneously. We want to prove P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) for all k = 0, 1, . . . , K + 1 by induction.
The base of the induction is trivial: for k = 0 we have kx ^{0} ‚àí uk ^{2} ‚â§ 2kx ^{0} ‚àí x ‚àó k ^{2} + 2kx ‚àó ‚àí uk ^{2} ‚â§ 4R ^{2} ‚â§ 9R ^{2} and
P k‚àí1
kŒ≥ l=0 ^{Œ∏} l k ^{=} ^{0} ^{for} ^{any} ^{u} ‚àà ^{B} R ^{(x} ^{‚àó} ^{).} ^{Next,} ^{assume} ^{that} ^{for} ^{k} ^{=} ^{T} ‚àí ^{1} ‚â§ ^{K} ^{the} ^{statement} ^{holds:} P{E T ‚àí1 } ‚â•
1 ‚àí ^{(T} ‚àí1)Œ≤ / (K+1) . Given this, we need to prove P{E T } ‚â• 1 ‚àí ^{T} ^{Œ≤} / (K+1) . We start with showing that E T ‚àí1 implies
R t ‚â§ 3R for all t = 0, 1, . . . , T (also by induction). For t = 0 this is already shown. Now, assume that R t ‚â§ 3R for all
t = 0, 1, . . . , t ‚Ä≤ for some t ‚Ä≤ < T . Then for t = 0, 1, . . . , t ‚Ä≤

ke
x t ‚àí x ‚àó k

=

‚â§

kx t ‚àí x ‚àó ‚àí Œ≥ F e Œæ 1 t (x t )k ‚â§ kx t ‚àí x ‚àó k + Œ≥k F e Œæ 1 t (x t )k

(197)

kx t ‚àí x ‚àó k + Œ≥Œª ‚â§ 3R +

R

20 ln 6(K+1)
Œ≤

‚â§ 4R.

(203)

Therefore, the conditions of Lemma G.1 are satisfied and we have that E T ‚àí1 implies
n
o
‚Ä≤
‚Ä≤
max ‚àó 2Œ≥(t ^{‚Ä≤} + 1)hF (u), x
e ^{t} avg ‚àí ^{ui} ^{+} kx ^{t} ^{+1} ‚àí ^{uk} ^{2}
u‚ààB R (x )
)
(
t ‚Ä≤
P
l
l
0
2
x ), Œ∏ l i
‚â§ max ‚àó kx ‚àí uk + 2Œ≥ hx ‚àí u ‚àí Œ≥F (e

u‚ààB R (x )

l=0

+Œ≥ 2

t ‚Ä≤
P

l=0

(201)

‚â§ 9R 2 ,

kŒ∏ l k ^{2} + 2kœâ l k ^{2}



meaning that

‚Ä≤

kx t +1 ‚àí x ‚àó k 2 ‚â§

max ‚àó

u‚ààB R (x )

o
n
‚Ä≤
‚Ä≤
2Œ≥(t ‚Ä≤ + 1)hF (u), x
e ^{t} avg ‚àí ^{ui} ^{+} kx ^{t} ^{+1} ‚àí ^{uk} ^{2} ‚â§ ^{9R} ^{2} ^{,}

i.e., R t ‚Ä≤ +1 ‚â§ 3R. In other words, we derived that probability event E T ‚àí1 implies R t ‚â§ 3R and

max ‚àó 2Œ≥(t + 1)hF (u), x
e ^{t} avg ‚àí ui + kx ^{t+1} ‚àí uk ^{2} ‚â§ 9R ^{2}

u‚ààB R (x )

52

(204)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

for all t = 0, 1, . . . , T . In addition, due to (203) E T ‚àí1 also implies that ke
x ^{t} ‚àí x ‚àó k ‚â§ 4R for all t = 0, 1, . . . , T . Thus,
E T ‚àí1 implies

kx t ‚àí x ‚àó ‚àí Œ≥F (e
x t )k

(11)

‚â§

x t ‚àí x ‚àó k
kx ^{t} ‚àí x ‚àó k + Œ≥kF (e
x ^{t} )k ‚â§ 3R + Œ≥Lke

‚â§

3R + 4RŒ≥L ‚â§ 5R,

(196)

(203)

(205)

for all t = 0, 1, . . . , T . Next, we introduce random vectors
(
x t ‚àí x ‚àó ‚àí Œ≥F (e
x t ), if kx t ‚àí x ‚àó ‚àí Œ≥F (e
x ^{t} )k ‚â§ 5R,
Œ∑ t =
0,
otherwise,

for all t = 0, 1, . . . , T . These vectors are bounded almost surely:

kŒ∑ t k ‚â§ 5R

(206)

for all t = 0, 1, . . . , T . Moreover, due to (205), probability event E T ‚àí1 implies Œ∑ t = x ^{t} ‚àí x ‚àó ‚àí Œ≥F (e
x ^{t} ) for all t =
0, 1, . . . , T and
(
)
T
‚àí1
T
‚àí1
T
‚àí1
X
X
X

0
2
‚àó
hx ‚àí u, Œ∏ l i + 2Œ≥
hx l ‚àí x ‚àó ‚àí Œ≥F (e
x l ), Œ∏ l i + Œ≥ 2
A T =
max ‚àó kx ‚àí uk + 2Œ≥
kŒ∏ l k ^{2} + 2kœâ l k ^{2}

u‚ààB R (x )

‚â§ 4R 2 + 2Œ≥

max

u‚ààB R (x ‚àó )

= 4R ^{2} + 2Œ≥R

T
‚àí1
X

(*

l=0

x ‚àó ‚àí u,

Œ∏ l + 2Œ≥

l=0

T
‚àí1
X

l=0

T
‚àí1
X

l=0

Œ∏ l

+)

l=0

+ 2Œ≥

hŒ∑ l , Œ∏ l i + Œ≥ 2

T
‚àí1
X

l=0

T
‚àí1
X

l=0

l=0

hŒ∑ l , Œ∏ l i + Œ≥ 2

T
‚àí1
X

kŒ∏ l k ^{2} + 2kœâ l k ^{2}

l=0


kŒ∏ l k ^{2} + 2kœâ l k ^{2} ,



where A T is defined in (201).

To handle the sums appeared in the right-hand side of the previous inequality we consider unbiased and biased parts of
Œ∏ l , œâ l :
h
h
i
i
def
def
Œ∏ l u = E Œæ 2 l F e Œæ 2 l (e
x l ) ‚àí F e Œæ 2 l (e
x l ), Œ∏ l b = F (e
x l ) ‚àí E Œæ 2 l F e Œæ 2 l (e
x l ) ,
(207)
i
i
h
h
def
def
œâ l u = E Œæ 1 l F e Œæ 1 l (x l ) ‚àí F e Œæ 1 l (x l ), œâ l b = F (x l ) ‚àí E Œæ 1 l F e Œæ 1 l (x l ) ,
(208)

for all l = 0, . . . , T ‚àí 1. By definition we have Œ∏ l = Œ∏ l ^{u} + Œ∏ l ^{b} , œâ l = œâ l ^{u} + œâ l ^{b} for all l = 0, . . . , T ‚àí 1. Therefore, E T ‚àí1
implies

A T

‚â§

4R ^{2} + 2Œ≥R

T
‚àí1
X

Œ∏ l + 2Œ≥

l=0

+ 2Œ≥ 2

|

+ 2Œ≥ 2

|

+ 2Œ≥ 2

|

T
‚àí1 
X

l=0

|

T
‚àí1
X

l=0

hŒ∑ l , Œ∏ l u i + 2Œ≥

{z

}

1

l=0

{z

l=0

hŒ∑ l , Œ∏ l b i

{z

2

}

}





kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 2E Œæ 1 l kœâ l ^{u} k ^{2}

{z

4

T
‚àí1
X

l=0





E Œæ 2 l kŒ∏ l u k 2 + 2E Œæ 1 l kœâ l u k 2

3

T
‚àí1 
X

|

T
‚àí1
X


kŒ∏ l ^{b} k ^{2} + 2kœâ l ^{b} k ^{2} ,

{z

5

53

}

}

(209)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

where we also apply inequality ka + bk ^{2} ‚â§ 2kak ^{2} + 2kbk ^{2} holding for all a, b ‚àà R ^{d} to upper bound kŒ∏ l k ^{2} and kœâ l k ^{2} . It
P T ‚àí1
remains to derive good enough high-probability upper-bounds for the terms 2Œ≥R
l=0 ^{Œ∏} l ^{,} ^{1,} ^{2,} ^{3,} ^{4,} ^{5,} ^{i.e.,} ^{to} ^{finish}
P T ‚àí1
2
our inductive proof we need to show that 2Œ≥R
l=0 ^{Œ∏} l ^{+} ^{1} ^{+} ^{2} ^{+} ^{3} ^{+} ^{4} ^{+} ^{5} ‚â§ ^{5R} ^{with} ^{high} ^{probability.} ^{In} ^{the}

u
b
subsequent parts of the proof, we will need use many times the bounds for the norm and second moments of ^{Œ∏} t+1
and ^{Œ∏} t+1
.
First, by definition of clipping operator we have with probability 1 that

kŒ∏ l ^{u} k ‚â§ 2Œª,

kœâ l ^{u} k ‚â§ 2Œª.

(210)

Moreover, since E T ‚àí1 implies that

(203)

(11)

(197) ^{Œª}

R

(196)

(11)

kF (x ^{l} )k ‚â§ Lkx ^{l} ‚àí x ^{‚àó} k ‚â§ 3LR ‚â§

40Œ≥ ln ^{6(K+1)}
Œ≤

R

(196)

x ^{l} ‚àí x ^{‚àó} k ‚â§ 4LR ‚â§
kF (e
x ^{l} )k ‚â§ Lke

=

2

,

(197) ^{Œª}

40Œ≥ ln ^{6(K+1)}
Œ≤

=

2

for t = 0, 1, . . . , T ‚àí 1. Then, in view of Lemma 5.1, we have that E T ‚àí1 implies

Œ∏ l b ‚â§

for all l = 0, 1, . . . , T ‚àí 1.

Upper bound for 1.

2 Œ± œÉ Œ±
^{œâ} ^{l} ^{b} ^{‚â§} Œ±‚àí1 ^{,}
Œª i
h
E Œæ 1 l kœâ l k ^{2} ‚â§ 18Œª ^{2‚àíŒ±} œÉ ^{Œ±} ,
i
h
2
E Œæ 1 l kœâ l ^{u} k ‚â§ 18Œª ^{2‚àíŒ±} œÉ ^{Œ±} ,

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1

i
h
E Œæ 2 l kŒ∏ l k ^{2} ‚â§ 18Œª ^{2‚àíŒ±} œÉ ^{Œ±} ,
i
h
2
E Œæ 2 l kŒ∏ l ^{u} k ‚â§ 18Œª ^{2‚àíŒ±} œÉ ^{Œ±} ,

(211)

(212)

(213)

By definition of Œ∏ l ^{u} , we have E Œæ 2 l [Œ∏ l ^{u} ] = 0 and

E Œæ 2 l [2Œ≥hŒ∑ l , Œ∏ l ^{u} i] = 0.

Next, sum 1 has bounded with probability 1 terms:

|2Œ≥hŒ∑ l ^{,} ^{Œ∏} l ^{u} i| ‚â§ ^{2Œ≥kŒ∑} l k ¬∑ kŒ∏ l ^{u} k

(206),(210)

‚â§

(197)

20Œ≥RŒª =

R 2

def

= c.

ln 6(K+1)
Œ≤

(214)



def
The summands also have bounded conditional variances œÉ l ^{2} = E Œæ 2 l 4Œ≥ ^{2} hŒ∑ l , Œ∏ l ^{u} i ^{2} :

 (206)



œÉ l ^{2} ‚â§ E Œæ 2 l 4Œ≥ ^{2} kŒ∑ l k ^{2} ¬∑ kŒ∏ l ^{u} k ^{2} ‚â§ 100Œ≥ ^{2} R ^{2} E Œæ 2 l kŒ∏ l ^{u} k ^{2} .

(215)

‚àí1
^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {2Œ≥hŒ∑ l ^{,} ^{Œ∏} l ^{u} i} ^{T} l=0
is a bounded martingale difference sequence with bounded conditional
T
‚àí1
2
variances {œÉ l } l=0 . Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with X l = 2Œ≥hŒ∑ l , Œ∏ l ^{u} i, parameter c as in (214),
R 4
b = R 2 , G =
6(K+1) ^{:}

6 ln

Œ≤

(

2

^{P} |1| ^{>} ^{R} ^{and}

T
‚àí1
X

l=0

œÉ l 2 ‚â§

R 4

6 ln 6(K+1)
Œ≤

)



b 2
‚â§ 2 exp ‚àí
2G + 2cb / 3



=

Œ≤
.
3(K + 1)

Equivalently, we have

Œ≤
,
P{E 1 } ‚â• 1 ‚àí
3(K + 1)

for E 1 =

(

either

T
‚àí1
X

l=0

54

œÉ l 2 >

R 4

6 ln 6(K+1)
Œ≤

or

|1| ‚â§ ^{R}

2

)

.

(216)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

In addition, E T ‚àí1 implies that

T
‚àí1
X

œÉ l 2

l=0

(215)

T
‚àí1
X

 ^{(213),T} ‚â§K+1

E Œæ 2 l kŒ∏ l u k 2
‚â§
1800(K + 1)Œ≥ ^{2} R ^{2} Œª ^{2‚àíŒ±} œÉ ^{Œ±}

‚â§

100Œ≥ ^{2} R ^{2}

(197)

1800(K + 1)Œ≥ ^{Œ±} œÉ ^{Œ±} R ^{4‚àíŒ±} (196)
R 4
‚â§
.
20 2‚àíŒ± ln 2‚àíŒ± ^{6(K+1)}
6 ln 6(K+1)
Œ≤
Œ≤

l=0

‚â§

(217)

Upper bound for 2. From E T ‚àí1 it follows that

2

‚â§

2Œ≥

T
‚àí1
X

l=0

kŒ∑ l k ¬∑ kŒ∏ l b k

^{(206),(211),T} ‚â§K+1 10 ¬∑ 2 ^{Œ±} (K + 1)Œ≥RœÉ ^{Œ±}

‚â§

Œª Œ±‚àí1

(196)
10 ¬∑ 2 ^{Œ±} ¬∑ 20 ^{Œ±‚àí1} (K + 1)Œ≥ ^{Œ±} œÉ ^{Œ±} ln ^{Œ±‚àí1} ^{6(K+1)}
Œ≤
‚â§ R 2 .
Œ±‚àí2
R

(197)

=

(218)

Upper bound for 3. From E T ‚àí1 it follows that

2Œ≥

2

T
‚àí1
X

l=0

4Œ≥

2

T
‚àí1
X

l=0

^{(212),T} ‚â§K+1

E Œæ 2 l [kŒ∏ l ^{u} k ^{2} ]

‚â§

^{(212),T} ‚â§K+1

E Œæ 1 l [kœâ l ^{u} k ^{2} ]

‚â§

(219),(220)

‚â§

3

Upper bound for 4.

(197)

36Œ≥ ^{2} (K + 1)Œª ^{2‚àíŒ±} œÉ ^{Œ±} =

(197)

72Œ≥ ^{2} (K + 1)Œª ^{2‚àíŒ±} œÉ ^{Œ±} =

36Œ≥ ^{Œ±} (K + 1)œÉ ^{Œ±}

20 2‚àíŒ± ln 2‚àíŒ± ^{6(K+1)}
Œ≤

72Œ≥ ^{Œ±} (K + 1)œÉ ^{Œ±}

20 2‚àíŒ± ln 2‚àíŒ± ^{6(K+1)}
Œ≤

(196)

‚â§

(196)

‚â§

1 2
R ,
12

(219)

1 2
R ,
12

(220)

1 2
R .
6

(221)

By the construction we have
h
i



2Œ≥ ^{2} E Œæ 1 l ,Œæ 2 l kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 2E Œæ 1 l kœâ l ^{u} k ^{2} = 0.

Next, sum 1 has bounded with probability 1 terms:





2Œ≥ ^{2} kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 2E Œæ 1 l kœâ l ^{u} k ^{2}

‚â§

(210)

‚â§

(197)

‚â§



2Œ≥ 2 kŒ∏ l u k 2 + 2Œ≥ 2 E Œæ 2 l kŒ∏ l u k 2


+4Œ≥ ^{2} kœâ l ^{u} k ^{2} + 4Œ≥ ^{2} E Œæ 1 l kœâ l ^{u} k ^{2}

48Œ≥ ^{2} Œª ^{2}
R 2

6 ln 6(K+1)
Œ≤

def

= c.

(222)

The summands also
 have bounded conditional variances



 2

2 def
4
:
œÉ
e l = 4Œ≥ E Œæ 1 l ,Œæ 2 l kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 2E Œæ 1 l kœâ l ^{u} k ^{2}

œÉ
e l 2

(222)

‚â§

Œ≥ 2 R 2

3 ln

E l l
6(K+1) ^{Œæ} 1 ^{,Œæ} 2

2

‚â§

Œ≤
2

2Œ≥ R

3 ln 6(K+1)
Œ≤

h

 i



kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 2E Œæ 1 l kœâ l ^{u} k ^{2}



E Œæ 1 l ,Œæ 2 l kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2} .

(223)


n



o ^{T} ^{‚àí1}
In other words, we showed that 2Œ≥ ^{2} kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 2E Œæ 1 l kœâ l ^{u} k ^{2}
is a bounded martingale

l=0

‚àí1
^{difference} ^{sequence} ^{with} ^{bounded} ^{conditional} ^{variances} {œÉ l ^{2} } ^{T} l=0
. Next, we apply Bernstein‚Äôs inequality (Lemma B.2)

55

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance






with X l = 2Œ≥ ^{2} kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 2E Œæ 1 l kœâ l ^{u} k ^{2} , parameter c as in (222), b =

1 2
6 R ,

G =

.

(224)

4

216 ln

R
6(K+1) ^{:}
Œ≤

(

T ‚àí1

X
1
R 4
^{P} |4| ^{>} ^{R} ^{2} ^{and}
œÉ
e l 2 ‚â§
6
216 ln ^{6(K+1)}

l=0

Œ≤

)


‚â§ 2 exp ‚àí

b 2
2G + 2cb / 3



=

Œ≤
.
3(K + 1)

Equivalently, we have

Œ≤
,
P{E 4 } ‚â• 1 ‚àí
3(K + 1)

for E 4 =

(

T
‚àí1
X

either

l=0

In addition, E T ‚àí1 implies that

T
‚àí1
X

l=0

T
‚àí1
X

2Œ≥ 2 R 2

(223)

œÉ
e l 2

‚â§

^{(213),T} ‚â§K+1

‚â§

œÉ
e l 2 >

R 4

216 ln ^{6(K+1)}
Œ≤

1
^{or} |4| ‚â§ ^{R} ^{2}
6





E Œæ 1 l ,Œæ 2 l kŒ∏ l ^{u} k ^{2} + 2kœâ l ^{u} k ^{2}
3 ln 6(K+1)
l=0
Œ≤
2 2 2‚àíŒ± Œ±

36(K + 1)Œ≥ R Œª

œÉ

ln 6(K+1)
Œ≤
36(K + 1)Œ≥ ^{Œ±} R ^{4‚àíŒ±} œÉ ^{Œ±} (196)

(197)

‚â§

‚â§

20 2‚àíŒ± ln 3‚àíŒ± ^{6(K+1)}
Œ≤

R 4

216 ln ^{6(K+1)}
Œ≤

.

)

(225)

Upper bound for 5. From E T ‚àí1 it follows that

5

2Œ≥ 2

=

T
‚àí1
X

l=0

(197)

=

Upper bound for 2Œ≥R

P T ‚àí1

l=0

6 ¬∑ 2

Œ∏ l .

2Œ±

kŒ∏ l ^{b} k ^{2} + 2kœâ l ^{b} k ^{2}

 ^{(211),T} ‚â§K+1 6 ¬∑ 2 ^{2Œ±} Œ≥ ^{2} œÉ ^{2Œ±} (K + 1)
‚â§
Œª 2Œ±‚àí2

¬∑ 20 2Œ±‚àí2 Œ≥ 2Œ± œÉ 2Œ± (K + 1) ln 2Œ±‚àí2 6(K+1)
(196) 1
Œ≤
‚â§ R 2 .
2Œ±‚àí2
R
6

(226)

To upper-bound this sum, we introduce new random vectors:

Ô£± l‚àí1
P
Ô£≤ Œ≥ P Œ∏ , if Œ≥ l‚àí1
Œ∏ r ‚â§ R,
r
Œ∂ l =
r=0
r=0
Ô£≥
0,
otherwise

for l = 1, 2, . . . , T ‚àí 1. These vectors are bounded with probability 1:

kŒ∂ l k ‚â§ R.

(227)

Therefore, taking into account (202), we derive that E T ‚àí1 implies
v
u
2
T
‚àí1
T
‚àí1
u
X
X
t
2
Œ∏ l
2Œ≥R
= 2R Œ≥
Œ∏ l

l=0

l=0

=

=

(207)

‚â§

v
+
* l‚àí1
u T ‚àí1
T
‚àí1
u X
X
X
t
2
2
kŒ∏ l k + 2Œ≥
Œ≥
Œ∏ r , Œ∏ l
2R Œ≥

l=0

l=0

l=0

l=0

r=0

v
u T ‚àí1
T
‚àí1
u X
X
2R t Œ≥ 2
kŒ∏ l k 2 + 2Œ≥
hŒ∂ l , Œ∏ l i

v
u
T
‚àí1
T
‚àí1
X
X
u
u
3
+
4
+
5
+
2Œ≥
hŒ∂
,
Œ∏
i
2R u
+
2Œ≥
hŒ∂ l , Œ∏ l b i.
l
l
u
t
l=0
l=0
|
{z
} |
{z
}

6

56

7

(228)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Similarly to the previous parts of the proof, we bound 6 and 7.

By definition of Œ∏ l ^{u} , we have E Œæ 2 l [Œ∏ l ^{u} ] = 0 and

Upper bound for 6.

E Œæ 2 l [2Œ≥hŒ∂ l , Œ∏ l ^{u} i] = 0.

Next, sum 6 has bounded with probability 1 terms:

|2Œ≥hŒ∂ l ^{,} ^{Œ∏} l ^{u} i| ‚â§ ^{2Œ≥kŒ∑} l k ¬∑ kŒ∏ l ^{u} k

(227),(210)

‚â§

R 2

(197)

4Œ≥RŒª ‚â§

def

= c.

4 ln 6(K+1)
Œ≤

(229)



def
The summands also have bounded conditional variances œÉÃÇ l ^{2} = E Œæ 2 l 4Œ≥ ^{2} hŒ∂ l , Œ∏ l ^{u} i ^{2} :

 (227)



œÉÃÇ l 2 ‚â§ E Œæ 2 l 4Œ≥ 2 kŒ∂ l k 2 ¬∑ kŒ∏ l u k 2 ‚â§ 4Œ≥ 2 R 2 E Œæ 2 l kŒ∏ l u k 2 .

(230)

‚àí1
^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {2Œ≥hŒ∂ l ^{,} ^{Œ∏} l ^{u} i} ^{T} l=0
is a bounded martingale difference sequence with bounded conditional
2 T ‚àí1
^{variances} {œÉÃÇ l } l=0 ^{.} ^{Next,} ^{we} ^{apply} ^{Bernstein‚Äôs} ^{inequality} ^{(Lemma} ^{B.2)} ^{with} ^{X} l ^{=} ^{2Œ≥hŒ∂} l ^{,} ^{Œ∏} l ^{u} i, ^{parameter} ^{c} ^{as} ^{in} ^{(229),}
2
R 4
b = R 4 , G =
6(K+1) ^{:}

96 ln

Œ≤

(

T ‚àí1

X
R 4
1
œÉÃÇ l 2 ‚â§
^{P} |5| ^{>} ^{R} ^{2} ^{and}
4
96 ln 4(K+1)

l=0

Equivalently, we have

E 6 =

(

Œ≤

T
‚àí1
X

either

l=0

In addition, E T ‚àí1 implies that

T
‚àí1
X

œÉÃÇ l 2

l=0

(230)

2

‚â§

œÉÃÇ l 2 >

4Œ≥ R

2

T
‚àí1
X

l=0

)


‚â§ 2 exp ‚àí

R 4

96 ln 6(K+1)
Œ≤

b 2
2G + 2cb / 3



1
^{or} |5| ‚â§ ^{R} ^{2}
4

=

)

Œ≤
.
3(K + 1)

.

(231)

 ^{(213),T} ‚â§K+1

‚â§
72(K + 1)Œ≥ ^{2} R ^{2} Œª ^{2‚àíŒ±} œÉ ^{Œ±}
E Œæ 2 l kŒ∏ l u k 2

72(K + 1)Œ≥ ^{Œ±} R ^{4‚àíŒ±} œÉ ^{Œ±} (196)
R 4
.
‚â§
6(K+1)
20 2‚àíŒ± ln 2‚àíŒ± ^{6(K+1)}
96
ln
Œ≤
Œ≤

(197)

=

(232)

Upper bound for 7. From E T ‚àí1 it follows that

7

‚â§

2Œ≥

T
‚àí1
X

l=0

kŒ∂ l k ¬∑ kŒ∏ l b k

=

Now, we have the upper bounds for 2Œ≥R

(209)

P T ‚àí1

l=0

2Œ≥R

T
‚àí1
X

T
‚àí1
X

l=0

(221) 1

3 ‚â§

R 4

T
‚àí1
X

6 ln

Œ≤

l=0

Œ∏ l + 1 + 2 + 3 + 4 + 5,

l=0

‚àö
‚â§ 2R 3 + 4 + 5 + 6 + 7,

2 ‚â§ R 2 ,

,
6(K+1)

(233)

(228)

Œ∏ l

l=0

(218)

(217)

Œª Œ±‚àí1

Œ∏ l , 1, 2, 3, 4, 5. In particular, probability event E T ‚àí1 implies

A T ‚â§ 4R ^{2} + 2Œ≥R

œÉ l 2 ‚â§

‚â§

2 ^{Œ±+1} ¬∑ 20 ^{Œ±‚àí1} (K + 1)Œ≥ ^{Œ±} œÉ ^{Œ±} ln ^{Œ±‚àí1} ^{6(K+1)}
(196) 1
Œ≤
‚â§ R 2 .
Œ±‚àí2
R
4

(197)

T
‚àí1
X

^{(227),(211),T} ‚â§K+1 2 ^{Œ±+1} (K + 1)Œ≥RœÉ ^{Œ±}

(226) 1

R 2 ,

5 ‚â§

(225)

R 4

6

œÉ
e l 2 ‚â§

216 ln

57

6

R 2 ,

,
6(K+1)

Œ≤

(233) 1

7 ‚â§

T
‚àí1
X

l=0

4

R 2 ,

(232)

œÉÃÇ l 2 ‚â§

R 4

96 ln 6(K+1)
Œ≤

.

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Moreover, we also have (see (216), (224), (231) and our induction assumption)

(T ‚àí 1)Œ≤
,
K +1
Œ≤
Œ≤
P{E 4 } ‚â• 1 ‚àí
, P{E 6 } ‚â• 1 ‚àí
,
3(K + 1)
3(K + 1)

P{E T ‚àí1 } ‚â• 1 ‚àí

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
3(K + 1)

where

E 1

E 4

E 6

(

=

(

=

(

=

either

T
‚àí1
X

œÉ l 2 >

l=0

either

T
‚àí1
X

l=0

either

T
‚àí1
X

œÉ
e l 2 >

œÉÃÇ l 2 >

l=0

R 4

6 ln 6(K+1)
Œ≤

R 4

216 ln ^{6(K+1)}
Œ≤

R 4

96 ln 6(K+1)
Œ≤

or

|1| ‚â§ ^{R}

2

)

,

)
1 2
^{or} |4| ‚â§ ^{R} ^{,}
6
)
1 2
^{or} |6| ‚â§ ^{R} ^{.}
4

Thus, probability event E T ‚àí1 ‚à© E 1 ‚à© E 4 ‚à© E 6 implies
r
T
‚àí1
X
1 2 1 2 1 2 1 2 1 2
R + R + R + R + R = R,
‚â§
Œ≥
Œ∏ l
6
6
6
4
4
l=0
r
1 2 1 2 1 2 1 2 1 2
R + R + R + R + R
A T ‚â§ 4R 2 + 2R
6
6
6
4
4
1 2 1 2 1 2
2
2
+R + R + R + R + R
6
6
6
‚â§ 9R 2 ,

(234)

(235)

which is equivalent to (201) and (202) for t = T , and

P{E T } ‚â• P{E T ‚àí1 ‚à© E 1 ‚à© E 4 ‚à© E 6 } = 1 ‚àí P{E T ‚àí1 ‚à™ E 1 ‚à™ E 4 ‚à™ E 6 } ‚â• 1 ‚àí

TŒ≤
.
K +1

This finishes the inductive part of our proof, i.e., for all k = 0, 1, . . . , K + 1 we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) . In particular,
for k = K + 1 we have that with probability at least 1 ‚àí Œ≤

Gap R (e
x K
=
max ‚àó hF (u), x
e K
avg ^{)}
avg ‚àí ^{ui}

u‚ààB R (x )

‚â§

(204)

‚â§


1
e ^{t} avg ‚àí ui + kx ^{K+1} ‚àí uk ^{2}
max ‚àó 2Œ≥(K + 1)hF (u), x
2Œ≥(K + 1) u‚ààB R (x )
9R 2
.
2Œ≥(K + 1)

Finally, if

Œ≥ = min

then with probability at least 1 ‚àí Œ≤

Gap R (e
x K
avg ^{)}

Ô£±
Ô£≤

2‚àíŒ±
Œ±

Ô£º
Ô£Ω

1
20
R
,
6(K+1) Ô£æ
Ô£≥ 160L ln ^{6(K+1)} 10800 Œ± ^{1} (K + 1) Œ± ^{1} œÉ ln ^{Œ±‚àí1}
Œ±

Œ≤

Œ≤

Ô£º
Ô£±
6(K+1) Ô£Ω
Ô£≤ 720LR 2 ln ^{6(K+1)} 9œÉR ln ^{Œ±‚àí1}
Œ±
9R 2
Œ≤
Œ≤
‚â§
= max
,
Œ±‚àí1
2‚àíŒ±
Ô£≥
2Œ≥(K + 1)
K +1
2 ¬∑ 20 Œ± (K + 1) Œ± Ô£æ
Ô£º Ô£∂
Ô£±
Ô£´
K Ô£Ω
Ô£≤ LR 2 ln K œÉR ln Œ±‚àí1
Œ±
Œ≤
Œ≤
Ô£∏ .
= O Ô£≠ max
,
Œ±‚àí1
Ô£æ
Ô£≥
K
K Œ±

58

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

To get Gap R (e
x K
avg ) ‚â§ Œµ with probability at least 1 ‚àí Œ≤ it is sufficient to choose K such that both terms in the maximum
^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}
!
 Œ±

LR ^{2} LR ^{2} œÉR Œ±‚àí1 œÉR
ln
,
ln
K = O
Œµ
ŒµŒ≤
Œµ
ŒµŒ≤

that concludes the proof.

G.2. Quasi-Strongly Monotone Problems

As in the monotone case, we use another lemma from (Gorbunov et al., 2022a) that handles the deterministic part of
clipped-SEG in the quasi-strongly monotone case.
Lemma G.3 (Lemma C.3 from (Gorbunov et al., 2022a)). Let Assumptions 1.7, 1.9 hold for Q = B 3R (x ‚àó ) = {x ‚àà R ^{d} |
kx ‚àí x ‚àó k ‚â§ 3R}, where R ‚â• kx ^{0} ‚àí x ‚àó k, and 0 < Œ≥ ‚â§ ^{1} / 2(L+2Œº) . If x ^{k} and x
e ^{k} lie in B 3R (x ‚àó ) for all k = 0, 1, . . . , K for
some K ‚â• 0, then the iterates produced by clipped-SEG satisfy

kx K+1 ‚àí x ‚àó k 2

‚â§ (1 ‚àí Œ≥Œº) ^{K+1} kx ^{0} ‚àí x ‚àó k ^{2} ‚àí 4Œ≥ ^{3} Œº

+2Œ≥

K
X

K
X

k=0

(1 ‚àí Œ≥Œº) ^{K‚àík} hF (x ^{k} ), œâ k i

(1 ‚àí Œ≥Œº) ^{K‚àík} hx ^{k} ‚àí x ‚àó ‚àí Œ≥F (e
x k ), Œ∏ k i

k=0

+Œ≥ 2

K
X

k=0

where Œ∏ k , œâ k are defined in (194), (195).


(1 ‚àí Œ≥Œº) ^{K‚àík} kŒ∏ k k ^{2} + 4kœâ k k ^{2} ,

(236)

Using this lemma we prove the main convergence result for clipped-SEG in the quasi-strongly monotone case.
Theorem G.4 (Case 2 in Theorem 4.1). Let Assumptions 1.1, 1.7, 1.9, hold for Q = B 3R (x ‚àó ) = {x ‚àà R ^{d} | kx ‚àí x ‚àó k ‚â§
3R}, where R ‚â• kx ^{0} ‚àí x ‚àó k, and
)
(
ln(B K )
1
,
,
(237)
0 < Œ≥ ‚â§ min
Œº(K + 1)
650L ln ^{6(K+1)}
Œ≤
Ô£º
Ô£±
2(Œ±‚àí1)
Ô£Ω
Ô£≤
(K + 1) Œ± Œº 2 R 2


(238)
B K = max 2,
2(Œ±‚àí1)
6(K+1)
Ô£≥ 264600 Œ± ^{2} œÉ ^{2} ln Œ±
ln 2 (B K ) Ô£æ
Œ≤
Ô£±
Ô£º Ô£º
Ô£´
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¨
Ô£¥
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£≤
Ô£Ω
Ô£Ω
2 2
Ô£¨
Œ±
K
Œº
R
)!
(
= O Ô£¨
max
2,
,
(239)
Ô£¨
 
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
Ô£≠
2 R 2
Œ±
Œº
K
2
Ô£¥
K
Ô£¥
Ô£¥
2
Ô£¥
Ô£¥
max 2,
2(Œ±‚àí1)
Ô£≥ œÉ ln Œ±
Ô£æ Ô£¥
Ô£æ
Œ≤ ln
œÉ 2 ln Œ± ( K
Œ≤ )

Œª k

=

exp(‚àíŒ≥Œº(1 + ^{k} / 2 ))R

120Œ≥ ln ^{6(K+1)}
Œ≤

,

(240)

‚â• 1. Then, after K iterations the iterates produced by clipped-SEG
for some K ‚â• 0 and Œ≤ ‚àà (0, 1] such that ln ^{6(K+1)}
Œ≤
with probability at least 1 ‚àí Œ≤ satisfy

kx ^{K+1} ‚àí x ‚àó k ^{2} ‚â§ 2 exp(‚àíŒ≥Œº(K + 1))R ^{2} .

(241)

In particular, when Œ≥ equals the minimum from (237), then the iterates produced by clipped-SEG after K iterations with
probability at least 1 ‚àí Œ≤ satisfy
(
)! Ô£º Ô£∂
Ô£±
Ô£´
 
2(Œ±‚àí1)
2(Œ±‚àí1)
2 2
Ô£¥
Ô£¥
Œ±
K
2
Œº
R
Ô£¥
Ô£¥
K
Ô£¥
Ô£¥
! œÉ 2 ln Œ±
max 2,
2(Œ±‚àí1)
Ô£¨
Ô£¥
Ô£¥
Œ≤ ln
Ô£Ω Ô£∑
Ô£≤
K
2
Œ±
Ô£¨
Ô£∑
œÉ ln
(
)
ŒºK
Œ≤
K
‚àó 2
2
Ô£¨
Ô£∑ ,
,
kx ‚àí x k = O Ô£¨ max R exp ‚àí
(242)
2(Œ±‚àí1)
Ô£∑
K
Ô£¥
Ô£¥
2
L
ln
Œ±
Ô£¥
Ô£¥
K
Œº
Ô£≠
Ô£∏
Œ≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
Ô£≥

59

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

meaning that to achieve kx ^{K} ‚àí x ‚àó k ^{2} ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SEG requires
!
Œ±
  ^{2}  2(Œ±‚àí1)

 Œ± !
 2  
Œ±
L
œÉ
R 2
R
1 œÉ ^{2} 2(Œ±‚àí1)
L
ln
,
ln
ln
ln Œ±‚àí1 (B Œµ )
ln
K = O
Œº
Œµ
ŒºŒ≤
Œµ
Œº 2 Œµ
Œ≤ Œº 2 Œµ

iterations/oracle calls, where

B Œµ = max

Ô£±
Ô£¥
Ô£¥
Ô£≤

Ô£¥
Ô£¥
Ô£≥

2,

Œµ ln

 

1
Œ≤

R 2

œÉ 2
Œº 2 Œµ

(243)

Ô£º
Ô£¥
Ô£¥
Ô£Ω

 .
Œ±
 2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£æ

Proof. Again, we will closely follow the proof of Theorem C.3 from (Gorbunov et al., 2022a) and the main difference will
be reflected in the application of Bernstein inequality and estimating biases and variances of stochastic terms.

Let R k = kx ^{k} ‚àíx ‚àó k for all k ‚â• 0. As in the previous results, the proof is based on the induction argument and showing that
the iterates do not leave some ball around the solution with high probability. More precisely, for each k = 0, 1, . . . , K + 1
we consider probability event E k as follows: inequalities

R t ^{2} ‚â§ 2 exp(‚àíŒ≥Œºt)R ^{2}

(244)

hold for t = 0, 1, . . . , k simultaneously. We want to prove P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) for all k = 0, 1, . . . , K + 1 by
induction. The base of the induction is trivial: for k = 0 we have R 0 ^{2} ‚â§ R ^{2} < 2R ^{2} by definition. Next, assume that for
k = T ‚àí 1 ‚â§ K the statement holds: P{E T ‚àí1 } ‚â• 1 ‚àí ^{(T} ‚àí1)Œ≤ / (K+1) . Given this, we need to prove P{E T } ‚â• 1 ‚àí ^{T} ^{Œ≤} / (K+1) .
Since R t ^{2} ‚â§ 2 exp(‚àíŒ≥Œºt)R ^{2} ‚â§ 9R ^{2} , we have x ^{t} ‚àà B 3R (x ‚àó ), where operator F is L-Lipschitz. Thus, E T ‚àí1 implies

kF (x t )k ‚â§

(244) ‚àö

Lkx ^{t} ‚àí x ^{‚àó} k ‚â§

2L exp(‚àí ^{Œ≥Œºt} / 2 )R

(237),(240) Œª
t

‚â§

2

(245)

and

kœâ t k 2

(245) 5

2k F e Œæ 1 (x ^{t} )k ^{2} + 2kF (x ^{t} )k ^{2} ‚â§

‚â§

2

^{(240)} exp(‚àíŒ≥Œºt)R ^{2}

Œª 2 t ‚â§

4Œ≥ 2

(246)

for all t = 0, 1, . . . , T ‚àí 1, where we use that ka + bk ^{2} ‚â§ 2kak ^{2} + 2kbk ^{2} holding for all a, b ‚àà R ^{d} .

Next, we need to prove that E T ‚àí1 implies ke
x ^{t} ‚àí x ‚àó k ‚â§ 3R and show several useful inequalities related to Œ∏ t . Lipschitzness
of F probability event E T ‚àí1 implies

ke
x t ‚àí x ‚àó k 2

kx ^{t} ‚àí x ‚àó ‚àí Œ≥ F e Œæ 1 (x ^{t} )k ^{2} ‚â§ 2kx ^{t} ‚àí x ‚àó k ^{2} + 2Œ≥ ^{2} k F e Œæ 1 (x ^{t} )k ^{2}

=

2R t 2 + 4Œ≥ 2 kF (x t )k 2 + 4Œ≥ 2 kœâ t k 2

‚â§

(11)

‚â§

2(1 + 2Œ≥ ^{2} L ^{2} )R t ^{2} + 4Œ≥ ^{2} kœâ t k ^{2}

‚â§

7 exp(‚àíŒ≥Œºt)R ^{2} ‚â§ 9R ^{2}

(237),(246)

(247)

and

kF (e
x t )k

‚â§

Lke
x t ‚àí x ‚àó k ‚â§

(237),(240) Œª
‚àö
t
7L exp(‚àí ^{Œ≥Œºt} / 2 )R
‚â§
2

(248)

for all t = 0, 1, . . . , T ‚àí 1. Therefore, E T ‚àí1 implies that x ^{t} , x
e ^{t} ‚àà B 3R (x ‚àó ) for all t = 0, 1, . . . , T ‚àí 1. Using Lemma G.3
T
and (1 ‚àí Œ≥Œº) ‚â§ exp(‚àíŒ≥ŒºT ), we obtain that E T ‚àí1 implies

R T 2

‚â§

exp(‚àíŒ≥ŒºT )R ^{2} ‚àí 4Œ≥ ^{3} Œº

+2Œ≥

T
‚àí1
X

l=0

+Œ≥ 2

l=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hF (x ^{l} ), œâ l i

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hx ^{l} ‚àí x ‚àó ‚àí Œ≥F (e
x l ), Œ∏ l i

T
‚àí1
X

l=0

T
‚àí1
X


(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l k ^{2} + 4kœâ l k ^{2} .

60

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

To handle the sums above, we introduce a new notation:
(
‚àö
F (x ^{t} ), if kF (x ^{t} )k ‚â§ 2L exp(‚àí ^{Œ≥Œºt} / 2 )R,
Œ∂ t =
0,
otherwise,
(
‚àö
x t ‚àí x ‚àó ‚àí Œ≥F (e
x t ), if kx t ‚àí x ‚àó ‚àí Œ≥F (e
x ^{t} )k ‚â§ 7(1 + Œ≥L) exp(‚àí ^{Œ≥Œºt} / 2 )R,
Œ∑ t =
0,
otherwise,

(249)

(250)

for t = 0, 1, . . . , T ‚àí 1. These vectors are bounded almost surely:
‚àö
‚àö
kŒ∂ t k ‚â§ 2L exp(‚àí ^{Œ≥Œºt} / 2 )R, kŒ∑ t k ‚â§ 7(1 + Œ≥L) exp(‚àí ^{Œ≥Œºt} / 2 )R
‚àö
for all t = 0, 1, . . . , T ‚àí 1. We also notice that E T ‚àí1 implies kF (x ^{t} )k ‚â§ 2L exp(‚àí ^{Œ≥Œºt} / 2 )R (due to (245)) and

kx t ‚àí x ‚àó ‚àí Œ≥F (e
x t )k

‚â§

(247),(248)

‚â§

(251)

kx ^{t} ‚àí x ‚àó k + Œ≥kF (e
x t )k
‚àö
7(1 + Œ≥L) exp(‚àí ^{Œ≥Œºt} / 2 )R

for t = 0, 1, . . . , T ‚àí 1. In other words, E T ‚àí1 implies Œ∂ t = F (x ^{t} ) and Œ∑ t = x ^{t} ‚àí x ‚àó ‚àí Œ≥F (e
x ^{t} ) for all t = 0, 1, . . . , T ‚àí 1,
meaning that from E T ‚àí1 it follows that

R T 2

‚â§

T
‚àí1
X

exp(‚àíŒ≥ŒºT )R ^{2} ‚àí 4Œ≥ ^{3} Œº

+2Œ≥

l=0

T
‚àí1
X

l=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∂ l , œâ l i

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l i + Œ≥ ^{2}

T
‚àí1
X

l=0


(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l k ^{2} + 4kœâ l k ^{2} .

To handle the sums appeared on the right-hand side of the previous inequality we consider unbiased and biased parts of
Œ∏ l , œâ l :
h
h
i
i
def
def
x l ) ‚àí E Œæ 2 l F e Œæ 2 l (e
Œ∏ l u = E Œæ 2 l F e Œæ 2 l (e
x l ) ‚àí F e Œæ 2 l (e
x l ), Œ∏ l b = F (e
x l ) ,
(252)
h
h
i
i
def
def
(253)
œâ l u = E Œæ 1 l F e Œæ 1 l (x l ) ‚àí F e Œæ 1 l (x l ), œâ l b = F (x l ) ‚àí E Œæ 1 l F e Œæ 1 l (x l ) ,

for all l = 0, . . . , T ‚àí 1. By definition we have Œ∏ l = Œ∏ l ^{u} + Œ∏ l ^{b} , œâ l = œâ l ^{u} + œâ l ^{b} for all l = 0, . . . , T ‚àí 1. Therefore, E T ‚àí1
implies

R T 2

‚â§

2

3

^{exp(‚àíŒ≥ŒºT} ^{)R} ‚àí4Œ≥ ^{Œº}

+ 2Œ≥

|

T
‚àí1
X

l=0

+ 2Œ≥ 2

|

+ 2Œ≥ 2

|

+ 2Œ≥ 2

|

|

T
‚àí1
X

l=0

(1 ‚àí Œ≥Œº)

hŒ∂ l ^{,} ^{œâ} l ^{u} i ‚àí4Œ≥ ^{3} ^{Œº}

{z

} |

^{T} ‚àí1‚àíl

1

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l ^{u} i + 2Œ≥

{z

}

3

l=0

{z

}

2

{z

4

{z

}

}






(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l ^{u} k ^{2} + 4kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 4E Œæ 1 l kœâ l ^{u} k ^{2}

T
‚àí1
X

{z

6

T
‚àí1
X

l=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∂ l , œâ l ^{b} i

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l ^{b} i

5

l=0

l=0






(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl E Œæ 2 l kŒ∏ l ^{u} k ^{2} + 4E Œæ 1 l kœâ l ^{u} k ^{2}

T
‚àí1
X

l=0

|

T
‚àí1
X

T
‚àí1
X


(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l ^{b} k ^{2} + 4kœâ l ^{b} k ^{2} .

{z

7

61

}

}

(254)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

where we also apply inequality ka + bk ^{2} ‚â§ 2kak ^{2} + 2kbk ^{2} holding for all a, b ‚àà R ^{d} to upper bound kŒ∏ l k ^{2} and kœâ l k ^{2} . It
remains to derive good enough high-probability upper-bounds for the terms 1, 2, 3, 4, 5, 6, 7, i.e., to finish our inductive
proof we need to show that 1 + 2 + 3 + 4 + 5 + 6 + 7 ‚â§ exp(‚àíŒ≥ŒºT )R ^{2} with high probability. In the subsequent
u
b
parts of the proof, we will need to use many times the bounds for the norm and second moments of ^{Œ∏} t+1
and ^{Œ∏} t+1
. First,
by definition of clipping operator, we have with probability 1 that

kŒ∏ l u k ‚â§ 2Œª l ,

kœâ l u k ‚â§ 2Œª l .

(255)

Moreover, since E T ‚àí1 implies that kF (x ^{l} )k ‚â§ ^{Œª} l / 2 and kF (e
x ^{l} )k ‚â§ ^{Œª} l / 2 for all l = 0, 1, . . . , T ‚àí 1 (see (245) and (248)),
from Lemma 5.1 we also have that E T ‚àí1 implies

Œ∏ l b ‚â§

œâ l b ‚â§

i
h
2
œÉ Œ± ,
E Œæ 2 l kŒ∏ l k ‚â§ 18Œª ^{2‚àíŒ±}
l
i
h
œÉ Œ± ,
E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚â§ 18Œª ^{2‚àíŒ±}
l

for all l = 0, 1, . . . , T ‚àí 1.

Upper bound for 1.

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1
l
i
h
2
œÉ Œ± ,
E Œæ 1 l kœâ l k ‚â§ 18Œª ^{2‚àíŒ±}
l
i
h
œÉ Œ± ,
E Œæ 1 l kœâ l ^{u} k ^{2} ‚â§ 18Œª ^{2‚àíŒ±}
l

2 Œ± œÉ Œ±
,
Œª Œ±‚àí1
l

(256)

(257)

(258)

By definition of œâ l ^{u} , we have E Œæ 1 l [œâ l ^{u} ] = 0 and



E Œæ 1 l ‚àí4Œ≥ ^{3} ^{Œº(1} ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àíl} hŒ∂ l ^{,} ^{œâ} l ^{u} i ^{=} ^{0.}

Next, sum 1 has bounded with probability 1 terms:

| ‚àí 4Œ≥ ^{3} Œº(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∂ l , œâ l ^{u} i|

‚â§

(251),(255)

‚â§

(237),(240)

‚â§

4Œ≥ ^{3} Œº exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí l))kŒ∂ l k ¬∑ kœâ l ^{u} k
‚àö
8 2Œ≥ ^{3} ŒºL exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí ^{l} / 2 ))RŒª l
exp(‚àíŒ≥ŒºT )R ^{2} def
= c.
7 ln 6(K+1)
Œ≤

(259)



def
The summands also have bounded conditional variances œÉ l ^{2} = E Œæ 1 l 16Œ≥ ^{6} Œº ^{2} (1 ‚àí Œ≥Œº) ^{2T} ‚àí2‚àí2l hŒ∂ l , œâ l ^{u} i ^{2} :

œÉ l 2

‚â§

(251)

‚â§

(237)

‚â§



E Œæ 1 l 16Œ≥ ^{6} Œº ^{2} exp(‚àíŒ≥Œº(2T ‚àí 2 ‚àí 2l))kŒ∂ l k ^{2} ¬∑ kœâ l ^{u} k ^{2}



36Œ≥ ^{6} Œº ^{2} L ^{2} exp(‚àíŒ≥Œº(2T ‚àí 2 ‚àí l))R ^{2} E Œæ 1 l kœâ l ^{u} k ^{2}

4Œ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí l))R ^{2}

2809 ln ^{6(K+1)}
Œ≤



E Œæ 1 l kœâ l u k 2 .

(260)

‚àí1
^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {‚àí4Œ≥ ^{3} ^{Œº(1} ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àíl} hŒ∂ l ^{,} ^{œâ} l ^{u} i} ^{T} l=0
is a bounded martingale difference sequence with
2 T ‚àí1
^{bounded} ^{conditional} ^{variances} {œÉ l } l=0 ^{.} ^{Next,} ^{we} ^{apply} ^{Bernstein‚Äôs} ^{inequality} ^{(Lemma} ^{B.2)} ^{with} ^{X} l ^{=} ‚àí4Œ≥ ^{3} ^{Œº(1} ‚àí

4

)R
Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∂ l , œâ l ^{u} i, parameter c as in (259), b = 17 exp(‚àíŒ≥ŒºT )R ^{2} , G = ^{exp(‚àí2Œ≥ŒºT}
6(K+1) ^{:}

294 ln

(

T ‚àí1

X
1
exp(‚àí2Œ≥ŒºT )R ^{4}
^{P} |1| ^{>} ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2} ^{and}
œÉ l 2 ‚â§
7
294 ln ^{6(K+1)}

l=0

Œ≤

)

Œ≤



b 2
‚â§ 2 exp ‚àí
2G + 2cb / 3



=

Œ≤
.
3(K + 1)

Equivalently, we have

Œ≤
,
P{E 1 } ‚â• 1 ‚àí
3(K + 1)

for E 1 =

(

either

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ l 2 >
294 ln ^{6(K+1)}
l=0
Œ≤

62

4

1
^{or} |1| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
7

)

. (261)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

In addition, E T ‚àí1 implies that

T
‚àí1
X

 u 2 
T ‚àí1
4Œ≥ ^{2} exp(‚àí2Œ≥ŒºT )R ^{2} X E Œæ 1 l kœâ l k
exp(‚àíŒ≥Œºl)
2809 ln ^{6(K+1)}

(260)

œÉ l 2

‚â§

l=0

l=0

Œ≤

^{(258),T} ‚â§K+1

‚â§

2

K
2 Œ± X

72Œ≥ exp(‚àí2Œ≥ŒºT )R œÉ

2809 ln ^{6(K+1)}
Œ≤

Œ±

l=0

72Œ≥ exp(‚àí2Œ≥ŒºT )R

(240)

‚â§

Œª 2‚àíŒ±
l
exp(‚àíŒ≥Œºl)

K
4‚àíŒ± Œ± X

œÉ

1

exp(‚àíŒ≥Œºl)
2809 ¬∑ 120 2‚àíŒ± ln ^{3‚àíŒ±} ^{6(K+1)}
l=0
Œ≤

¬∑ (exp(‚àíŒ≥Œº(1 + l / 2 ))) ^{2‚àíŒ±}

K

72Œ≥ ^{Œ±} exp(‚àí2Œ≥ŒºT )R ^{4‚àíŒ±} œÉ ^{Œ±} X

‚â§

2809 ¬∑ 120 2‚àíŒ± ln ^{3‚àíŒ±} ^{6(K+1)}
l=0
Œ≤

exp(Œ≥Œº(Œ± ‚àí 2)) ¬∑ exp

72Œ≥ ^{Œ±} exp(‚àí2Œ≥ŒºT )R ^{4‚àíŒ±} œÉ ^{Œ±} (K + 1) exp

‚â§

2809 ¬∑ 120 2‚àíŒ± ln ^{3‚àíŒ±} ^{6(K+1)}
Œ≤

exp(‚àí2Œ≥ŒºT )R ^{4}

(237)

‚â§

294 ln ^{6(K+1)}
Œ≤



Œ≥ŒºŒ±K
2





Œ≥ŒºŒ±l
2



,

(262)

where we also show that E T ‚àí1 implies

Œ≥ 2 R 2

K
X

l=0

Œ≥ ^{Œ±} R ^{4‚àíŒ±} (K + 1) exp( ^{Œ≥ŒºŒ±K}
Œª 2‚àíŒ±
2 )
l
‚â§
.
6(K+1)
2‚àíŒ±
2‚àíŒ±
exp(‚àíŒ≥Œºl)
120
ln

(263)

Œ≤

Upper bound for 2. From E T ‚àí1 it follows that

2

‚â§

(251),(256)

‚â§

(240)

=

^{T} ‚â§K+1

‚â§

‚â§

(237)

‚â§

4Œ≥ 3 Œº

T
‚àí1
X

l=0

2 2+Œ± ¬∑

exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí l))kŒ∂ l k ¬∑ kœâ l ^{b} k

T
‚àí1
X
‚àö
2 exp(‚àíŒ≥Œº(T ‚àí 1))Œ≥ ^{3} ŒºLR

œÉ Œ±

Œª ^{Œ±‚àí1} exp(‚àí Œ≥Œºl / 2 )
l=0 ^{l}

‚àö
T
‚àí1
X
2 ^{2+Œ±} ¬∑ 120 ^{Œ±‚àí1} 2 exp(‚àíŒ≥Œº(T ‚àí 1))Œ≥ ^{2+Œ±} ŒºLœÉ ^{Œ±} ln ^{Œ±‚àí1} ^{6(K+1)}
Œ≤

R Œ±‚àí2

1

Œ±‚àí1

exp (‚àíŒ≥Œº(1 + ^{l} / 2 ))
‚àö


K
X
2 ^{3+Œ±} ¬∑ 120 ^{Œ±‚àí1} 2 exp(‚àíŒ≥Œº(T ‚àí 1))Œ≥ ^{2+Œ±} ŒºLœÉ ^{Œ±} ln ^{Œ±‚àí1} ^{6(K+1)}
Œ≥ŒºŒ±l
Œ≤
exp
R Œ±‚àí2
2
l=0


‚àö
Œ≥ŒºŒ±K
(K
+
1)
exp
2 ^{3+Œ±} ¬∑ 120 ^{Œ±‚àí1} 2 exp(‚àíŒ≥Œº(T ‚àí 1))Œ≥ ^{2+Œ±} ŒºLœÉ ^{Œ±} ln ^{Œ±‚àí1} ^{6(K+1)}
Œ≤
2

l=0

¬∑ exp(‚àí ^{Œ≥Œºl} / 2 )

R Œ±‚àí2

1
exp(‚àíŒ≥ŒºT )R ^{2} ,
7

(264)

where we also show that E T ‚àí1 implies

T
‚àí1
X

120
1
Œ≥R
‚â§
Œ±‚àí1
Œª
exp(‚àí ^{Œ≥Œºl} / 2 )
l=0 ^{l}

Upper bound for 3.

Œ±‚àí1 6(K+1)
Œ≥ (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 ) ln
Œ≤

Œ±‚àí1 Œ±

R Œ±‚àí2

By definition of Œ∏ l ^{u} , we have E Œæ 2 l [Œ∏ l ^{u} ] = 0 and



E Œæ 2 l 2Œ≥(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l ^{u} i = 0.

63

.

(265)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Next, sum 3 has bounded with probability 1 terms:

|2Œ≥(1 ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àíl} hŒ∑ l ^{,} ^{Œ∏} l ^{u} i|

‚â§

(251),(255)

‚â§

(237),(240)

‚â§

2Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí l))kŒ∑ l k ¬∑ kŒ∏ l ^{u} k
‚àö
4 7Œ≥(1 + Œ≥L) exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí ^{l} / 2 ))RŒª l
exp(‚àíŒ≥ŒºT )R ^{2} def
= c.
7 ln 6(K+1)
Œ≤

(266)



def
The summands also have bounded conditional variances œÉ
e l ^{2} = E Œæ 2 l 4Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{2T} ‚àí2‚àí2l hŒ∑ l , Œ∏ l ^{u} i ^{2} :


œÉ
e l ^{2} ‚â§ E Œæ 2 l 4Œ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí 2 ‚àí 2l))kŒ∑ l k ^{2} ¬∑ kŒ∏ l ^{u} k ^{2}

(251)

‚â§

(237)

‚â§



49Œ≥ ^{2} (1 + Œ≥L) ^{2} exp(‚àíŒ≥Œº(2T ‚àí 2 ‚àí l))R ^{2} E Œæ 2 l kŒ∏ l ^{u} k ^{2}


50Œ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí l))R ^{2} E Œæ 2 l kŒ∏ l ^{u} k ^{2} .

(267)

‚àí1
^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {2Œ≥(1 ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àíl} hŒ∑ l ^{,} ^{Œ∏} l ^{u} i} ^{T} l=0
is a bounded martingale difference sequence with bounded
2 T ‚àí1
conditional variances {e
œÉ l } l=0 . Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with X l = 2Œ≥(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl hŒ∑ l , Œ∏ l ^{u} i,

4

)R
parameter c as in (266), b = 17 exp(‚àíŒ≥ŒºT )R ^{2} , G = ^{exp(‚àí2Œ≥ŒºT}
6(K+1) ^{:}

294 ln

(

Œ≤

T ‚àí1

X
1
exp(‚àí2Œ≥ŒºT )R ^{4}
^{P} |3| ^{>} ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2} ^{and}
œÉ
e l 2 ‚â§
7
294 ln ^{6(K+1)}

l=0

Œ≤

)



b 2
‚â§ 2 exp ‚àí
2G + 2cb / 3



=

Œ≤
.
3(K + 1)

Equivalently, we have

Œ≤
,
P{E 3 } ‚â• 1 ‚àí
3(K + 1)

for E 3 =

(

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ
e l 2 >
294 ln ^{6(K+1)}
l=0
Œ≤

either

In addition, E T ‚àí1 implies that

T
‚àí1
X

l=0

œÉ
e l 2

(267)

‚â§

^{(258),T} ‚â§K+1

‚â§

(263)

‚â§

(237)

‚â§

2

50Œ≥ exp(‚àí2Œ≥ŒºT )R

900Œ≥ ^{2} exp(‚àí2Œ≥ŒºT )R ^{2} œÉ ^{Œ±}

900Œ≥ exp(‚àí2Œ≥ŒºT )R

exp(‚àí2Œ≥ŒºT )R ^{4}

294 ln ^{6(K+1)}
Œ≤

2Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1))

(265)

‚â§

(237)

‚â§

)

. (268)

exp(‚àíŒ≥Œºl)

K
X

Œª 2‚àíŒ±
l
exp(‚àíŒ≥Œºl)

œÉ (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

4‚àíŒ± Œ±

120 2‚àíŒ± ln ^{2‚àíŒ±} ^{6(K+1)}
Œ≤

‚â§

‚â§

1
^{or} |3| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
7



E Œæ 2 l kŒ∏ l u k 2

l=0

Œ±

From E T ‚àí1 it follows that

(251),(256)

T
‚àí1
X

l=0

Upper bound for 4.

4

2

4

T
‚àí1
X

l=0

.

(269)

kŒ∑ l k ¬∑ kŒ∏ l b k
exp(‚àíŒ≥Œºl)

T
‚àí1
X
‚àö
2 ^{1+Œ±} 7Œ≥(1 + Œ≥L) exp(‚àíŒ≥Œº(T ‚àí 1))RœÉ ^{Œ±}

1

Œª ^{Œ±‚àí1} exp(‚àí Œ≥Œºl / 2 )
l=0 ^{l}



‚àö
ln Œ±‚àí1 6(K+1)
2 ^{3+Œ±} ¬∑ 120 ^{Œ±‚àí1} 7Œ≥ ^{Œ±} (1 + Œ≥L) exp(‚àíŒ≥ŒºT )(K + 1) exp ^{Œ≥ŒºŒ±K}
2
Œ≤

R Œ±‚àí2

1
exp(‚àíŒ≥ŒºT )R ^{2} .
7

(270)

64

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

From E T ‚àí1 it follows that

Upper bound for 5.

5

=

(258)

2

2Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1))

l=0





E Œæ 2 l kŒ∏ l u k 2 + 4E Œæ 1 l kœâ l u k 2

exp(‚àíŒ≥Œºl)

T
‚àí1
X

Œª 2‚àíŒ±
l
exp(‚àíŒ≥Œºl)

‚â§

180Œ≥ ^{2} exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{Œ±}

(263)

180Œ≥ ^{Œ±} R ^{2‚àíŒ±} exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{Œ±} (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

‚â§

l=0

120 2‚àíŒ± ln ^{2‚àíŒ±} ^{6(K+1)}
Œ≤

1
exp(‚àíŒ≥ŒºT )R ^{2} .
7

(237)

‚â§

Upper bound for 6.

T
‚àí1
X

(271)

First, we have

h



i
2Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl E Œæ 1 l ,Œæ 2 l kŒ∏ l ^{u} k ^{2} + 4kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 4E Œæ 1 l kœâ l ^{u} k ^{2} = 0.

Next, sum 6 has bounded with probability 1 terms:





2Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l ^{u} k ^{2} + 4kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 4E Œæ 1 l kœâ l ^{u} k ^{2}

(255)

‚â§

(240)

‚â§

def

=

80Œ≥ ^{2} exp(‚àíŒ≥ŒºT )Œª ^{2} l
exp(‚àíŒ≥Œº(1 + l))
exp(‚àíŒ≥ŒºT )R ^{2}

7 ln 6(K+1)
Œ≤

c.

(272)

The summands also have conditional variances




 2

def
œÉ
b l ^{2} = E Œæ 1 l ,Œæ 2 l 4Œ≥ ^{4} (1 ‚àí Œ≥Œº) ^{2T} ‚àí2‚àí2l kŒ∏ l ^{u} k ^{2} + 4kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 4E Œæ 1 l kœâ l ^{u} k ^{2}

that are bounded

œÉ
b l 2

2Œ≥ ^{2} exp(‚àí2Œ≥ŒºT )R ^{2}

(272)

‚â§

‚â§

7 exp(‚àíŒ≥Œº(1 + l)) ln

E l l
6(K+1) ^{Œæ} 1 ^{,Œæ} 2

Œ≤

4Œ≥ ^{2} exp(‚àí2Œ≥ŒºT )R ^{2}

7 exp(‚àíŒ≥Œº(1 + l)) ln ^{6(K+1)}
Œ≤

In other words, we showed that

n

h



 i

kŒ∏ l ^{u} k ^{2} + 4kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 4E Œæ 1 l kœâ l ^{u} k ^{2}



E Œæ 1 l ,Œæ 2 l kŒ∏ l ^{u} k ^{2} + 4kœâ l ^{u} k ^{2} .

(273)





o ^{T} ^{‚àí1}
2Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àíl kŒ∏ l ^{u} k ^{2} + 4kœâ l ^{u} k ^{2} ‚àí E Œæ 2 l kŒ∏ l ^{u} k ^{2} ‚àí 4E Œæ 1 l kœâ l ^{u} k ^{2}
is a

l=0

‚àí1
bounded martingale difference sequence with bounded
^{œÉ} l ^{2} } ^{T} l=0
. Next, we apply
Bernstein‚Äôs in-
 conditional variances {b


 u 2 
2
^{T} ‚àí1‚àíl
u 2
u 2
u 2
equality (Lemma B.2) with X l = 2Œ≥ (1 ‚àí Œ≥Œº)
kŒ∏ l k + 4kœâ l k ‚àí E Œæ 2 l kŒ∏ l k ‚àí 4E Œæ 1 l kœâ l k , parameter c as

4

)R
in (272), b = 7 ^{1} exp(‚àíŒ≥ŒºT )R ^{2} , G = ^{exp(‚àí2Œ≥ŒºT}
6(K+1) ^{:}

294 ln

(

Œ≤

T ‚àí1

X
1
exp(‚àí2Œ≥ŒºT )R ^{4}
^{P} |6| ^{>} ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2} ^{and}
œÉ
b l 2 ‚â§
7
294 ln ^{6(K+1)}

l=0

Œ≤

)


‚â§ 2 exp ‚àí

b 2
2G + 2cb / 3



=

Œ≤
.
3(K + 1)

Equivalently, we have

Œ≤
,
P{E 6 } ‚â• 1 ‚àí
3(K + 1)

for E 6 =

(

either

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ
b l 2 >
294 ln ^{6(K+1)}
l=0
Œ≤

65

4

1
^{or} |6| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
7

)

. (274)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

In addition, E T ‚àí1 implies that

T
‚àí1
X

l=0

œÉ
b l 2

(273)

‚â§

^{(258),T} ‚â§K+1

‚â§

‚â§

‚â§

^{(240),T} ‚â§K+1

‚â§

‚â§

‚â§

(237)

‚â§

360Œ≥ exp(‚àíŒ≥Œº(2T ‚àí 1))R

œÉ (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

4‚àíŒ± Œ±

exp(‚àí2Œ≥ŒºT )R ^{4}

294 ln ^{6(K+1)}
Œ≤

.

(275)

From E T ‚àí1 it follows that

2Œ≥ 2

T
‚àí1
X

l=0

(256)

l=0

Œ±

Œª 2‚àíŒ±
l
exp(‚àíŒ≥Œºl)

7 ¬∑ 120 2‚àíŒ± ln ^{3‚àíŒ±} ^{6(K+1)}
Œ≤

(237)

=

K

360Œ≥ exp(‚àíŒ≥Œº(2T ‚àí 1))R ^{2} œÉ ^{Œ±} X

2

7 ln 6(K+1)
Œ≤

‚â§

7

l=0

Œ≤

(263)

Upper bound for 7.

 u 2

T ‚àí1
u 2
4Œ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí 1))R ^{2} X E Œæ 1 l ,Œæ 2 l kŒ∏ l k + 4kœâ l k
exp(‚àíŒ≥Œºl)
7 ln 6(K+1)

exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí l)) kŒ∏ l ^{b} k ^{2} + 4kœâ l ^{b} k ^{2}

2Œ± 2

10 ¬∑ 2 Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1))œÉ

2Œ±

T
‚àí1
X



1
2Œ±‚àí2
Œª
exp(‚àíŒ≥Œºl)
l=0 ^{l}

K
X
20 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} exp(‚àíŒ≥ŒºT )œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{6(K+1)}
Œ≤

R 2Œ±‚àí2

l=0

K
X
40 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} exp(‚àíŒ≥ŒºT )œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{6(K+1)}
Œ≤

R 2Œ±‚àí2




l
exp(Œ≥Œºl)
exp Œ≥Œº(2Œ± ‚àí 2) 1 +
2

exp(Œ≥ŒºŒ±l)

l=0

(K + 1) exp(Œ≥ŒºŒ±K)
40 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} exp(‚àíŒ≥ŒºT )œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{6(K+1)}
Œ≤

R 2Œ±‚àí2

1
exp(‚àíŒ≥ŒºT )R ^{2} .
7

(276)

Now, we have the upper bounds for 1, 2, 3, 4, 5, 6, 7. In particular, probability event E T ‚àí1 implies

(254)

R T ^{2} ‚â§ exp(‚àíŒ≥ŒºT )R ^{2} + 1 + 2 + 3 + 4 + 5 + 6 + 7,

(264) 1

(270) 1

exp(‚àíŒ≥ŒºT )R ^{2} ,
7
7
(271) 1
(276) 1
5 ‚â§
exp(‚àíŒ≥ŒºT )R ^{2} , 7 ‚â§
exp(‚àíŒ≥ŒºT )R ^{2} ,
7
7
T
‚àí1
T
‚àí1
T
‚àí1
X
X
X
^{(269)} exp(‚àí2Œ≥ŒºT )R ^{4}
^{(275)} exp(‚àí2Œ≥ŒºT )R ^{4}
^{(262)} exp(‚àí2Œ≥ŒºT )R ^{4}
2
,
œÉ
e
,
œÉ
b l 2 ‚â§
.
‚â§
œÉ l 2 ‚â§
l
6(K+1)
6(K+1)
294 ln Œ≤
294 ln Œ≤
294 ln ^{6(K+1)}
l=0
l=0
l=0
Œ≤

2 ‚â§

exp(‚àíŒ≥ŒºT )R ^{2} ,

4 ‚â§

Moreover, we also have (see (261), (268), (274) and our induction assumption)

(T ‚àí 1)Œ≤
,
K +1
Œ≤
Œ≤
P{E 3 } ‚â• 1 ‚àí
, P{E 6 } ‚â• 1 ‚àí
,
3(K + 1)
3(K + 1)

P{E T ‚àí1 } ‚â• 1 ‚àí

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
3(K + 1)

66

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

where

E 1

=

E 3

=

E 6

=

(

(

(

either

either

either

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ l 2 >
294 ln ^{6(K+1)}
l=0
Œ≤

4

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ
e l 2 >
294 ln ^{6(K+1)}
l=0
Œ≤

4

exp(‚àí2Œ≥ŒºT )R
œÉ
b l 2 >
294 ln ^{6(K+1)}
l=0
Œ≤

4

T
‚àí1
X

1
^{or} |1| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
7

1
^{or} |3| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
7

1
^{or} |6| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
7

)

)

)

,

,

.

Thus, probability event E T ‚àí1 ‚à© E 1 ‚à© E 3 ‚à© E 6 implies

R T 2

(254)

‚â§

‚â§

exp(‚àíŒ≥ŒºT )R ^{2} + 1 + 2 + 3 + 4 + 5 + 6 + 7

2 exp(‚àíŒ≥ŒºT )R ^{2} ,

which is equivalent to (244) for t = T , and

P{E T } ‚â• P{E T ‚àí1 ‚à© E 1 ‚à© E 3 ‚à© E 6 } = 1 ‚àí P{E T ‚àí1 ‚à™ E 1 ‚à™ E 3 ‚à™ E 6 } ‚â• 1 ‚àí

TŒ≤
.
K +1

This finishes the inductive part of our proof, i.e., for all k = 0, 1, . . . , K + 1 we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) . In particular,
for k = K + 1 we have that with probability at least 1 ‚àí Œ≤

kx ^{K+1} ‚àí x ‚àó k ^{2} ‚â§ 2 exp(‚àíŒ≥Œº(K + 1))R ^{2} .

Finally, if

Œ≥

=

B K

=

=

)
ln(B K )
,
,
min
Œº(K + 1)
650L ln ^{6(K+1)}
Œ≤
Ô£±
Ô£º
2(Œ±‚àí1)
Ô£≤
Ô£Ω
(K + 1) Œ± Œº 2 R 2


max 2,
6(K+1)
Ô£≥ 264600 Œ± ^{2} œÉ 2 ln ^{2(Œ±‚àí1)}
Œ±
ln 2 (B K ) Ô£æ
Œ≤
Ô£º Ô£º
Ô£±
Ô£´
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¨
Ô£¥
2(Œ±‚àí1)
Ô£Ω
Ô£Ω
Ô£≤
2 2
Ô£¨
Œ±
Œº
R
K
)!
(
2,
O Ô£¨
max
Ô£¨
 
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
Ô£≠
2 2
Œ±
2
K
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
max 2, ^{K} 2(Œ±‚àí1) ^{Œº} ^{R}
Ô£æ
Ô£æ
Ô£≥ œÉ 2 ln Œ±
Œ≤ ln
K
2
Œ±
œÉ ln
( Œ≤ )

(

1

then with probability at least 1 ‚àí Œ≤

kx K+1 ‚àí x ‚àó k 2

‚â§

=

=

2 exp(‚àíŒ≥Œº(K + 1))R ^{2}
(
!
)
Œº(K + 1)
1
2
2R max exp ‚àí
,
B K
650L ln ^{6(K+1)}
Œ≤
)! Ô£º Ô£∂
(
Ô£±
Ô£´
 
2(Œ±‚àí1)
2(Œ±‚àí1)
2 2
Ô£¥
Ô£¥
Œ±
Œº
R
K
2
Ô£¥
Ô£¥
K
2
Ô£¥
Ô£¥
! œÉ ln Œ±
max 2,
2(Œ±‚àí1)
Ô£¥
Ô£¨
Ô£¥
Œ≤ ln
Ô£Ω Ô£∑
Ô£≤
K
2 ln
Œ±
Ô£∑
Ô£¨
œÉ
(
)
ŒºK
Œ≤
2
Ô£∑ .
,
O Ô£¨
max
R
exp
‚àí
2(Œ±‚àí1)
Ô£∑
Ô£¨
K
Ô£¥
Ô£¥
L ln Œ≤
Ô£¥
Ô£¥
K Œ± Œº 2
Ô£∏
Ô£≠
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
Ô£≥

To get kx ^{K+1} ‚àí x ‚àó k ^{2} ‚â§ Œµ with probability at least 1 ‚àí Œ≤ it is sufficient to choose K such that both terms in the maximum
^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}
!
Œ±
  ^{2}  2(Œ±‚àí1)

 Œ± !
 2  
Œ±
L
œÉ
R 2
R
1 œÉ ^{2} 2(Œ±‚àí1)
L
ln
,
ln Œ±‚àí1 (B Œµ ) ,
ln
ln
ln
K = O
Œº
Œµ
ŒºŒ≤
Œµ
Œº 2 Œµ
Œ≤ Œº 2 Œµ

67

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

where

B Œµ = max

This concludes the proof.

Ô£±
Ô£¥
Ô£¥
Ô£≤

Ô£¥
Ô£¥
Ô£≥

2,

Œµ ln

 

1
Œ≤

68

R 2

œÉ 2
Œº 2 Œµ

Ô£º
Ô£¥
Ô£¥
Ô£Ω

 .
Œ±
 2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£æ

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

H. Missing Proofs for clipped-SGDA

In this section, we provide the complete formulation of the main results for
 clipped-SGDA and the missing proofs. For
brevity, we will use the following notation: F e Œæ k (x ^{k} ) = clip F Œæ k (x ^{k} ), Œª k .

Algorithm 5 Clipped Stochastic Gradient Descent Ascent (clipped-SGDA) (Gorbunov et al., 2022a)

Input: starting point x ^{0} , number of iterations K, stepsize Œ≥ > 0, clipping levels {Œª k } ^{K‚àí1}
k=0 ^{.}
1: for k = 0, . . . , K do

2:
Compute F e Œæ k (x ^{k} ) = clip F Œæ k (x ^{k} ), Œª k using a fresh sample Œæ ^{k} ‚àº D k
3:
x k+1 = x k ‚àí Œ≥ F e Œæ k (x k )
4: end for
K
P
1
Output: x ^{K+1} or x ^{K}
x K
avg ^{=} K+1

k=0

H.1. Monotone Star-Cocoercive Problems

We start with the following lemma derived by Gorbunov et al. (2022b). Since this lemma handles only deterministic part
of the algorithm, the proof is the same as in the original work.

Lemma H.1 (Lemma D.1 from (Gorbunov et al., 2022b)). Let Assumptions 1.8 and 1.10 hold for Q = B 3R (x ‚àó ), where
R ‚â• kx ^{0} ‚àí x ‚àó k and 0 < Œ≥ ‚â§ ^{2} / l . If x ^{k} lies in B 3R (x ‚àó ) for all k = 0, 1, . . . , K for some K ‚â• 0, then for all u ‚àà B 3R (x ‚àó )
the iterates produced by clipped-SGDA satisfy

K

hF (u), x ^{K}
avg ‚àí ^{ui}

‚â§

X

Œ≥
kx 0 ‚àí uk 2 ‚àí kx K+1 ‚àí uk 2
+
kF (x k )k 2 + kœâ k k 2
2Œ≥(K + 1)
2(K + 1)

k=0

+

x K
avg

def

=

1
K +1

K

K
X

k=0

hx k ‚àí u ‚àí Œ≥F (x k ), œâ k i,

(277)

1 X k
x ,
K +1

(278)

k=0

œâ k

def

=

F (x k ) ‚àí F e Œæ k (x k ).

(279)

K
P

Also we need to use the following lemma to estimate the term

k=0

of the main theorem.

kF (x ^{k} )k ^{2} from the right hand side of (277) in the proof

def

Lemma H.2 (Lemma D.2 from (Gorbunov et al., 2022b)). Let Assumption 1.10 hold for Q = B 3R (x ‚àó ), where R ‚â• R 0 =
kx ^{0} ‚àí x ‚àó k and 0 < Œ≥ ‚â§ ^{2} / l . If x ^{k} lies in B 3R (x ‚àó ) for all k = 0, 1, . . . , K for some K ‚â• 0, then the iterates produced by
clipped-SGDA satisfy

Œ≥
K +1



 X
K
2
‚àí Œ≥
kF (x k )k 2
l

k=0

K

‚â§

2Œ≥ X k
kx 0 ‚àí x ‚àó k 2 ‚àí kx K+1 ‚àí x ‚àó k 2
+
hx ‚àí x ‚àó ‚àí Œ≥F (x k ), œâ k i
K +1
K +1

k=0

2

+

Œ≥
K +1

K
X

k=0

kœâ k k 2 ,

(280)

where œâ k is defined in (279).

Using those lemmas, we prove the main convergence result for clipped-SGDA in the monotone star-cocoercive case.

Theorem H.3 (Case 1 in Theorem 4.2). Let Assumptions 1.1, 1.8, 1.10 hold for Q = B 3R (x ‚àó ), where R ‚â• kx ^{0} ‚àí x ‚àó k,

69

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

and

0 <Œ≥

Œª k ‚â° Œª

‚â§ min

=

Ô£±
Ô£≤

1

Ô£≥ 170l ln ^{6(K+1)}

Œ≤

R

60Œ≥ ln ^{6(K+1)}
Œ≤

R

,

1

Œ±‚àí1

1

97200 Œ± (K + 1) Œ± œÉ ln Œ±

,

Ô£º
Ô£Ω

6(K+1) Ô£æ
Œ≤

,

(281)

(282)

‚â• 1. Then, after K iterations the iterates produced by clipped-SGDA
for some K ‚â• 0 and Œ≤ ‚àà (0, 1] such that ln ^{6(K+1)}
Œ≤
with probability at least 1 ‚àí Œ≤ satisfy

Gap R (x ^{K}
avg ^{)} ‚â§

5R 2
Œ≥(K + 1)

‚àó
and {x ^{k} } ^{K+1}
k=0 ‚äÜ ^{B} ^{3R} ^{(x} ^{),}

(283)

where x ^{K}
avg is defined in (278). In particular, when Œ≥ equals the minimum from (281), then the iterates produced by
clipped-SGDA after K iterations with probability at least 1 ‚àí Œ≤ satisfy
Ô£º Ô£∂
Ô£±
Ô£´
K Ô£Ω
Ô£≤ lR 2 ln K œÉR ln Œ±‚àí1
Œ±
Œ≤
Œ≤
Ô£≠ max
Ô£∏ ,
Gap R (e
x K
(284)
,
Œ±‚àí1
avg ^{)} ^{=} O
Ô£æ
Ô£≥
K
K Œ±

meaning that to achieve Gap R (e
x K
avg ) ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SGDA requires
 Œ±

 Œ± !!

lR ^{2} lR ^{2} œÉR Œ±‚àí1
1 œÉR Œ±‚àí1
ln
,
K = O
iterations/oracle calls.
ln
Œµ
ŒµŒ≤
Œµ
Œ≤
Œµ

(285)

Proof. The proof follows similar steps as the proof of Theorem D.1 from (Gorbunov et al., 2022a). The key difference is
related to the application of Bernstein inequality and estimating biases and variances of stochastic terms.

Let R k = kx ^{k} ‚àíx ‚àó k for all k ‚â• 0. As in the previous results, the proof is based on the induction argument and showing that
the iterates do not leave some ball around the solution with high probability. More precisely, for each k = 0, 1, . . . , K + 1
we consider probability event E k as follows: inequalities

kx t ‚àí x ‚àó k 2 ‚â§ 2R 2

and Œ≥

t‚àí1
X

l=0

œâ l ‚â§ R

(286)

hold for t = 0, 1, . . . , k simultaneously. We want to prove that P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) for all k = 0, 1, . . . , K + 1 by
P ‚àí1
^{induction.} ^{The} ^{base} ^{of} ^{the} ^{induction} ^{is} ^{trivial:} ^{for} ^{k} ^{=} ^{0} ^{we} ^{have} ^{R} 0 ^{2} ‚â§ ^{2R} ^{2} ^{by} ^{definition} ^{and} l=0 ^{œâ} l ^{=} ^{0.} ^{Next,}
assume that the statement holds for k = T ‚â§ K, i.e., we have P{E T } ‚â• 1 ‚àí ^{T} ^{Œ≤} / (K+1) . Given this, we need to prove
that P{E T +1 } ‚â• 1 ‚àí ^{(T} ^{+1)Œ≤} / (K+1) . Since probability event E T implies R t ^{2} ‚â§ 2R ^{2} , we have x ^{t} ‚àà B 2R (x ‚àó ) for all
t = 0, 1, . . . , T . According to this, the assumptions of Lemma H.2 hold and E T implies (Œ≥ < ^{1} / l )

T
X
Œ≥
kF (x t )k 2
^{l(T} ^{+} ^{1)} t=0

kx 0 ‚àí x ‚àó k 2 ‚àí kx T +1 ‚àí x ‚àó k 2
T +1

‚â§

+

T
T
Œ≥ 2 X
2Œ≥ X t
hx ‚àí x ‚àó ‚àí Œ≥F (x t ), œâ t i +
kœâ t k 2
^{T} ^{+} ^{1} t=0
^{T} ^{+} ^{1} t=0

(287)

and by l-star-cocoersivity we have

kF (x t )k

‚â§

(286) ‚àö

lkx ^{t} ‚àí x ‚àó k ‚â§

2lR

(281),(282) Œª

‚â§

2

T
X

kœâ t k 2 .

for all t = 0, 1, . . . , T . Using (287), we obtain

R T 2 +1 ‚â§ R 0 2 + 2Œ≥

T
X

t=0

hx t ‚àí x ‚àó ‚àí Œ≥F (x t ), œâ t i + Œ≥ 2

70

t=0

(288)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Due to (288), we have

kx t ‚àí x ‚àó ‚àí Œ≥F (x t )k

‚â§

kx ^{t} ‚àí x ‚àó k + Œ≥kF (x ^{t} )k

‚â§

2R + 2RŒ≥l ‚â§ 3R,

(286)

(14),(286)

‚â§

(281)

2R + Œ≥lkx ^{t} ‚àí x ‚àó k

(289)

for all t = 0, 1, . . . , T . To handle the sum above, we introduce a new vector
(
x ^{t} ‚àí x ‚àó ‚àí Œ≥F (x ^{t} ), if kx ^{t} ‚àí x ‚àó ‚àí Œ≥F (x ^{t} )k ‚â§ 3R,
Œ∑ t =
0,
otherwise,

for all t = 0, 1, . . . , T . This vector Œ∑ t is bounded with probability 1:

kŒ∑ t k ‚â§ 3R

(290)

for all t = 0, 1, . . . , T . We also notice that probability event E T implies Œ∑ t = x ^{t} ‚àí x ‚àó ‚àí Œ≥F (x ^{t} ) for all t = 0, 1, . . . , T
Thus, thanks to (289), E T implies

R T 2 +1 ‚â§ R 2 + 2Œ≥

T
X

t=0

hŒ∑ t , œâ t i + Œ≥ 2

T
X

t=0

kœâ t k 2 .

To handle the sums appeared on the right-hand side of the previous inequality we consider unbiased and biased parts of œâ t :
h
h
i
i
def
def
(291)
œâ t u = E Œæ t F e Œæ t (x t ) ‚àí F e Œæ t (x t ), œâ t b = F (x t ) ‚àí E Œæ t F e Œæ t (x t )

for all t = 0, . . . , T . Also, by definition we have œâ t = œâ t ^{u} + œâ t ^{b} for all t = 0, . . . , T . Therefore, E T implies

R T 2 +1

‚â§

R 2 + 2Œ≥

|

t=0

hŒ∑ t , œâ t u i + 2Œ≥

{z

T
X

t=0

|

}

1

+ 2Œ≥ 2

|

T
X

T
X

t=0

hŒ∑ t , œâ t b i + 2Œ≥ 2

{z

}

2

|

T
X

t=0



E Œæ t kœâ t u k 2

{z

3

T
X



kœâ t u k 2 ‚àí E Œæ t kœâ t u k 2 + 2Œ≥ 2
kœâ t b k 2 .

{z

4

We notice that the above inequality does not rely on monotonicity of F .

}

|

t=0

{z

5

}

(292)

}

According to the induction assumption, from probability event E T we have x ^{t} ‚àà B 2R (x ‚àó ) for all t = 0, 1, . . . , T . Thus,
the assumptions of Lemma H.1 hold and probability event E T implies
(
)
T
X
0
2
t
t
T
hx ‚àí u ‚àí Œ≥F (x ), œâ t i
2Œ≥(T + 1)Gap R (x avg ) ‚â§
max ‚àó kx ‚àí uk + 2Œ≥

u‚ààB R (x )

+Œ≥ 2

t=0

T
X

t=0

=

max

u‚ààB R (x ‚àó )

+2Œ≥

(

T
X

t=0

+Œ≥ 2

T
X

t=0


kF (x t )k 2 + kœâ t k 2 ,

0

2

kx ‚àí uk + 2Œ≥

T
X

t=0

‚àó

hx ‚àí u, œâ t i

hx t ‚àí x ‚àó ‚àí Œ≥F (x t ), œâ t i


kF (x t )k 2 + kœâ t k 2 .

71

)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

As we mentioned before, E T implies Œ∑ t = x ^{t} ‚àí x ‚àó ‚àí Œ≥F (x ^{t} ) for all t = 0, 1, . . . , T as well as (287) and Œ≥ < ^{1} / l . Due to
that, probability event E T implies
( T
)
X
 0
‚àó
2
T
hx ‚àí u, œâ t i
2Œ≥(T + 1)Gap R (x avg ) ‚â§
max ‚àó kx ‚àí uk + 2Œ≥ max ‚àó

u‚ààB R (x )

u‚ààB R (x )

‚â§

T

X
Œ≥
kF (x t )k 2 + Œ≥ 2
kœâ t k 2
+2Œ≥
hŒ∑ t , œâ t i +
^{l} t=0
t=0
t=0
+)
(*
T
X
‚àó
2
x ‚àí u,
œâ t
4R + 2Œ≥ max ‚àó

u‚ààB R (x )

+R 2 + 4Œ≥

T
X

t=0

‚â§

t=0

T
X

T
X

5R ^{2} + 2Œ≥R

t=0

hŒ∑ t , œâ t i + 2Œ≥ 2

T
X

t=0

T
X

t=0

kœâ t k 2

œâ t + 2 ¬∑ (1 + 2 + 3 + 4 + 5) ,

(293)

where we also aplly inequality ka + bk ^{2} ‚â§ 2kak ^{2} + 2kbk ^{2} holding for all a, b ‚àà R ^{d} to upper bound kœâ t k ^{2} .
P T
It remains to derive good enough high-probability upper-bounds for the terms 1, 2, 3, 4, 5 and 2Œ≥R
t=0 ^{œâ} t ^{,} ^{i.e.,}
P
T
2
to finish our inductive proof we need to show that 1 + 2 + 3 + 4 + 5 ‚â§ R ^{2} and 2Œ≥R
t=0 ^{œâ} t ‚â§ ^{2R} ^{with} ^{high}
probability.In the subsequent parts of the proof, we will need to use many times the bounds for the norm and second
moments of œâ t ^{u} , œâ t ^{b} . First, by Lemma C.1, we have with probability 1 that

kœâ t u k ‚â§ 2Œª

(294)

for all t = 0, 1, . . . , T . Moreover, due to Lemma C.1, we also have that E T implies

for all t = 0, 1, . . . , T .

2 Œ± œÉ Œ±
^{œâ} ^{t} ^{b} ^{‚â§} Œ±‚àí1 ^{,}
h
i Œª
b 2
t
‚â§ 18Œª ^{2‚àíŒ±} œÉ ^{Œ±} ,
œâ t
E Œæ
i
h
E Œæ t kœâ t ^{u} k ^{2} ‚â§ 18Œª ^{2‚àíŒ±} œÉ ^{Œ±}

(295)

(296)

(297)

By definition of œâ t ^{u} , we have E Œæ t [œâ t ^{u} ] = 0 and

Upper bound for 1.

E Œæ t [2Œ≥hŒ∑ t , œâ t ^{u} i] = 0.

Next, the sum 1 has bounded with probability 1 term:

|2Œ≥hŒ∑ t ^{,} ^{œâ} t ^{u} i| ‚â§ ^{2Œ≥kŒ∑} t k ¬∑ kœâ t ^{u} k

(290),(294)

‚â§

R 2

(282)

12Œ≥RŒª ‚â§

5 ln 6(K+1)
Œ≤

def

= c.

(298)



def
Moreover, these summands also have bounded conditional variances œÉ t ^{2} = E Œæ t 4Œ≥ ^{2} hŒ∑ t , œâ t ^{u} i ^{2} :



 (290)

œÉ t ^{2} ‚â§ E Œæ t 4Œ≥ ^{2} kŒ∑ t k ^{2} ¬∑ kœâ t ^{u} k ^{2} ‚â§ 36Œ≥ ^{2} R ^{2} E Œæ t kœâ t ^{u} k ^{2} .

(299)

^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {2Œ≥hŒ∑ t ^{,} ^{œâ} t ^{u} i} t‚â•0 ^{is} ^{a} ^{bounded} ^{martingale} ^{difference} ^{sequence} ^{with} ^{bounded} ^{conditional}
variances {œÉ t ^{2} } t‚â•0 . Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with X t = 2Œ≥hŒ∑ t , œâ t ^{u} i, parameter c as in (298),
2
R 4
b = R 5 , G =
6(K+1) ^{:}

150 ln

Œ≤

(

T
X
R 4
R 2
and
œÉ t 2 ‚â§
^{P} |1| ^{>}
5
150 ln ^{6(K+1)}
t=0

Œ≤

)

72


‚â§ 2 exp ‚àí

b 2
2G + 2cb / 3



=

Œ≤
.
3(K + 1)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Equivalently, we have

Œ≤
, for E 1 =
P{E 1 } ‚â• ^{1} ‚àí 3(K+1)

(

T
X

either

œÉ t 2 >

t=0

R 2
^{or} |1| ‚â§
5

R 4

150 ln ^{6(K+1)}
Œ≤

)

.

(300)

In addition, E T implies that

T
X

(299)

œÉ t 2

36Œ≥ ^{2} R ^{2}

‚â§

t=0

T
X

t=0

^{(297),T} ‚â§K+1

‚â§

648Œ≥ R œÉ (K + 1)Œª ^{2‚àíŒ±}

‚â§

648Œ≥ ^{Œ±} R ^{4‚àíŒ±} œÉ ^{Œ±} (K + 1) ln ^{Œ±‚àí2}

(282)

2



E Œæ t kœâ t u k 2

2 Œ±

R 4

(281)

‚â§

150 ln ^{6(K+1)}
Œ≤

6(K + 1)
Œ≤

.

(301)

From E T it follows that

Upper bound for 2.

T
X

kŒ∑ l k ¬∑ kœâ t b k

^{(290),(295),T} ‚â§K+1

œÉ Œ±
^{6} ¬∑ ^{2} ^{Œ±} ^{Œ≥R(K} ^{+} ^{1)} Œ±‚àí1
Œª

‚â§

2Œ≥

(282)

12 ¬∑ 120 ^{Œ±‚àí1} Œ≥ ^{Œ±} œÉ ^{Œ±} R ^{2‚àíŒ±} (K + 1) ln ^{Œ±‚àí1}

2

t=0

=

‚â§

6(K + 1) ^{(281)} R ^{2}
.
‚â§
Œ≤
5

(302)

From E T it follows that

Upper bound for 3.

3

=

2Œ≥

2

T
X

t=0

(282)

‚â§

Upper bound for 4.


 ^{(297),T} ‚â§K+1
E Œæ t kœâ t u k 2
‚â§
36Œ≥ ^{2} Œª ^{2‚àíŒ±} œÉ ^{Œ±} (K + 1)

36Œ≥ ^{Œ±} R ^{2‚àíŒ±} œÉ ^{Œ±} (K + 1) ln ^{Œ±‚àí2}

6(K + 1) ^{(281)} R ^{2}
‚â§
.
Œ≤
5

(303)

First, we have




2Œ≥ 2 E Œæ t kœâ t u k 2 ‚àí E Œæ t kœâ t u k 2 = 0.

Next, the sum 4 has bounded with probability 1 terms:



2Œ≥ 2 kœâ t u k 2 ‚àí E Œæ t kœâ t u k 2

 (294)

2Œ≥ 2 kœâ t u k 2 + E Œæ t kœâ t u k 2
‚â§ 16Œ≥ ^{2} Œª ^{2}

‚â§

R 2

R 2

(282)

‚â§
6(K+1)

def

= c.
225 ln Œ≤
h

 2 i
def
that are bounded
The summands also have conditional variances œÉ
e t 2 = 4Œ≥ 4 E Œæ t kœâ t u k 2 ‚àí E Œæ t kœâ t u k 2

(304)

œÉ
e t 2 ‚â§

2Œ≥ 2 R 2

225 ln

E t
6(K+1) ^{Œæ}

Œ≤



‚â§

 

kœâ t u k 2 ‚àí E Œæ t kœâ t u k 2 ‚â§

5 ln 6(K+1)
Œ≤

4Œ≥ 2 R 2

225 ln ^{6(K+1)}
Œ≤



E Œæ t kœâ t u k 2 .

(304)

(305)

^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {kœâ t ^{u} k ^{2} ‚àí ^{E} Œæ ^{t} ^{[kœâ} t ^{u} k ^{2} ^{]}} t‚â•0 ^{is} ^{a} ^{bounded} ^{martingale} ^{difference} ^{sequence} ^{with} ^{bounded}
conditional variances {e
œÉ t ^{2} } t‚â•0 .Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with X t = kœâ t ^{u} k ^{2} ‚àí E Œæ t [kœâ t ^{u} k ^{2} ],
2
R 4
parameter c as in (304), b = ^{R} 5 , G =
6(K+1) ^{:}

150 ln

(

Œ≤

T
X
R 4
R 2
and
œÉ
e t 2 ‚â§
^{P} |4| ^{>}
5
150 ln ^{6(K+1)}
t=0

Œ≤

)

73


‚â§ 2 exp ‚àí

b 2
2G + 2cb / 3



=

Œ≤
.
3(K + 1)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Equivalently, we have

Œ≤
, for E 4 =
P{E 4 } ‚â• ^{1} ‚àí 3(K+1)

(

T
X

either

t=0

In addition, E T implies that

T
X

t=0

‚â§

225 ln ^{6(K+1)}
t=0
Œ≤

‚â§

‚â§

)

.

(306)


 ^{(297),T} ‚â§K+1 8Œ≥ ^{2} R ^{2} (K + 1) 2‚àíŒ± Œ±
‚â§
Œª
œÉ
E Œæ t kœâ t u k 2
25 ln 6(K+1)
Œ≤

(307)

From E T it follows that

5

2Œ≥ 2

=

T
X

t=0

P T

kœâ t b k 2

^{(295),T} ‚â§K+1

‚â§

œÉ 2Œ±
2 ^{2Œ±+1} ¬∑ 60 ^{2Œ±‚àí2} Œ≥ ^{2} (K + 1) 2Œ±‚àí2
Œª

(282)

=

2 2Œ±+1 ¬∑ 60 2Œ±‚àí2 Œ≥ 2Œ± (K + 1)

(281)

R 2
.
5

‚â§

Upper bound for Œ≥

150 ln ^{6(K+1)}
Œ≤

^{8} Œ± 4‚àíŒ±
6(K + 1)
Œ≥ R
(K + 1)œÉ ^{Œ±} ln ^{Œ±‚àí3}
25
Œ≤
4
R
.
150 ln ^{6(K+1)}
Œ≤

(282)

(281)

Upper bound for 5.

T
X

4Œ≥ 2 R 2

(305)

œÉ
e t 2

œÉ
e t 2 >

R 2
^{or} |4| ‚â§
5

R 4

t=0 ^{œâ} ^{t}

œÉ 2Œ±
6(K + 1)
ln 2Œ±‚àí2
R 2Œ±‚àí2
Œ≤

(308)

. To estimate this term from above, we consider a new vector:

Ô£± l‚àí1
P
Ô£≤ Œ≥ P œâ , if Œ≥ l‚àí1
œâ r ‚â§ R,
r
Œ∂ l =
r=0
Ô£≥ r=0
0,
otherwise

for l = 1, 2, . . . , T ‚àí 1.This vector is bounded almost surely:

kŒ∂ l k ‚â§ R.

(309)

Thus, by (286), probability event E T implies

Œ≥

T
X

œâ l

=

v
u
2
T
u
t Œ≥ 2 X œâ
l

l=0

l=0

=

=

(292)

‚â§

v
+
* l‚àí1
u
T
T
u X
X
X
t Œ≥ 2
2
kœâ k + 2Œ≥
Œ≥
œâ ,œâ

l

r

l=0

l=0

l=0

l=0

l

r=0

v
u
T
T
u X
X
t Œ≥ 2
kœâ l k 2 + 2Œ≥
hŒ∂ l , œâ l i

v
u
T
T
X
X
u
u
u 3 + 4 + 5 + 2Œ≥
+
2Œ≥
hŒ∂ l , œâ l b i.
hŒ∂
,
œâ
i
l
l
u
t
l=0
l=0
{z
} |
{z
}
|

6

Following similar steps as before, we bound 6 and 7.

74

7

(310)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Upper bound for 6.

By definition of œâ u ^{t} , we have E Œæ t [œâ t ^{u} ] = 0 and

E Œæ t [2Œ≥hŒ∂ t , œâ t ^{u} i] = 0.

Next, sum 6 has bounded with probability 1 terms:

|2Œ≥hŒ∂ t ^{,} ^{œâ} t ^{u} i| ‚â§ ^{2Œ≥kŒ∂} t k ¬∑ kœâ t ^{u} k

(309),(294)

‚â§

R 2

(282)

4Œ≥RŒª ‚â§

def

= c.

5 ln 6(K+1)
Œ≤

(311)



def
The summands also have bounded conditional variances œÉ
b t 2 = E Œæ t 4Œ≥ 2 hŒ∂ t , œâ t u i 2 :

 (309)



œÉ
b t 2 ‚â§ E Œæ t 4Œ≥ 2 kŒ∂ t k 2 ¬∑ kœâ t u k 2 ‚â§ 4Œ≥ 2 R 2 E Œæ t kœâ t u k 2 .

(312)

^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {2Œ≥hŒ∂ t ^{,} ^{œâ} t ^{u} i} t‚â•0 ^{is} ^{a} ^{bounded} ^{martingale} ^{difference} ^{sequence} ^{with} ^{bounded} ^{conditional}
2
variances {b
œÉ t ^{2} } t‚â•0 . Applying Bernstein‚Äôs inequality (Lemma B.2) with X t = 2Œ≥hŒ∂ t , œâ t ^{u} i, parameter c as in (311), b = ^{R} 5 ,
R 4
G =
6(K+1) ^{:}

150 ln

Œ≤

(

T
X
R 2
R 4
^{P} |6| ^{>}
œÉ
b t 2 ‚â§
and
5
150 ln ^{6(K+1)}
t=0

Œ≤

)


‚â§ 2 exp ‚àí



=

or

R 2
|6| ‚â§
5

b 2
2G + 2cb / 3

Œ≤
.
3(K + 1)

Equivalently, we have

Œ≤
for E 6 =
P{E 6 } ‚â• ^{1} ‚àí 3(K+1)

(

T
X

either

R 4

t=0

œÉ
b t 2 >

T
X



E Œæ t kœâ t u k 2

150 ln ^{6(K+1)}
Œ≤

)

.

(313)

In addition, E T implies that

T
X

t=0

œÉ
b t 2

(312)

‚â§

^{(297),T} ‚â§K+1

4Œ≥ 2 R 2

t=0

‚â§

72Œ≥ ^{2} R ^{2} œÉ ^{Œ±} (K + 1)Œª ^{2‚àíŒ±}

‚â§

72Œ≥ ^{Œ±} R ^{4‚àíŒ±} œÉ ^{Œ±} (K + 1) ln ^{Œ±‚àí2}

(282)

R 4

(281)

‚â§

Upper bound for 7.

150 ln ^{6(K+1)}
Œ≤

6(K + 1)
Œ≤

.

(314)

From E T it follows that

7

T
X

kŒ∂ t k ¬∑ kœâ t b k

^{(309),(295),T} ‚â§K+1

œÉ Œ±
^{8} ¬∑ ^{2} ^{Œ±} ^{Œ≥R(K} ^{+} ^{1)} Œ±‚àí1
Œª

‚â§

2Œ≥

(282)

16 ¬∑ 120 ^{Œ±‚àí1} Œ≥ ^{Œ±} œÉ ^{Œ±} R ^{2‚àíŒ±} (K + 1) ln ^{Œ±‚àí1}

=

t=0

‚â§

75

6(K + 1) ^{(281)} R ^{2}
‚â§
.
Œ≤
5

(315)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Now, we have the upper bounds for 1, 2, 3, 4, 5, 6, 7. In particular, probability event E T ‚àí1 implies

(292)

R T 2 +1 ‚â§ R 2 + 1 + 2 + 3 + 4 + 5,

(293)

2Œ≥(T + 1)Gap R (x ^{T} avg ) ‚â§ 5R ^{2} + 2Œ≥R

Œ≥

(302) R

2 ‚â§

T
X

t=0

5

150 ln

Œ≤

‚â§

3 ‚â§

T
X

t=0

t=0

œâ t + 2 ¬∑ (1 + 2 + 3 + 4 + 5) ,

3 + 4 + 5 + 6 + 7,

(303) R ^{2}

,

,
6(K+1)

(310) ‚àö

œâ l

l=0
2

R 4

(301)

œÉ t 2 ‚â§

T
X

T
X

5

(308) R ^{2}

,

5 ‚â§

R 4

(307)

œÉ
e t 2 ‚â§

5

,

150 ln

,
6(K+1)

(315) R ^{2}

7 ‚â§

T
X

t=0

Œ≤

Moreover, we also have (see (300), (306), (315) and our induction assumption)

TŒ≤
,
K +1
Œ≤
P{E 4 } ‚â• 1 ‚àí
,
3(K + 1)

5

,

R 4

(314)

œÉ
b t 2 ‚â§

150 ln ^{6(K+1)}
Œ≤

.

P{E T } ‚â• 1 ‚àí

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
3(K + 1)

where

E 1

(

=

E 4

(

=

E 6

(

=

T
X

either

œÉ t 2 >

t=0

T
X

either

t=0

T
X

either

t=0

œÉ
e t 2 >

œÉ
b t 2 >

R 4

P{E 6 } ‚â• 1 ‚àí

R 2
^{or} |1| ‚â§
5

150 ln ^{6(K+1)}
Œ≤

R 2
^{or} |4| ‚â§
5

R 4

150 ln ^{6(K+1)}
Œ≤

R 4

R 2
^{or} |6| ‚â§
5

150 ln ^{6(K+1)}
Œ≤

Œ≤
,
3(K + 1)

)

)

)

,

,

.

Thus, probability event E T ‚à© E 1 ‚à© E 4 ‚à© E 6 implies

R T 2 +1

Œ≥

T
X

œâ l

l=0

2Œ≥(T + 1)Gap R (x ^{T} avg )

‚â§ R 2 + 1 + 2 + 3 + 4 + 5 ‚â§ 2R 2 ,

‚â§

‚àö
3 + 4 + 5 + 6 + 7 ‚â§ R,

‚â§ 6R ^{2} + 2Œ≥R

‚â§ 10R ^{2} ,

T
X

t=0

œâ t + 2 ¬∑ (1 + 2 + 3 + 4 + 5)

which gives (286) for t = T , and

P{E T +1 } ‚â• P{E T ‚à© E 1 ‚à© E 4 ‚à© E 6 } = 1 ‚àí P{E T ‚à™ E 1 ‚à™ E 4 ‚à™ E 6 } ‚â• 1 ‚àí

TŒ≤
.
K +1

This finishes the inductive part of our proof, i.e., for all k = 0, 1, . . . , K + 1 we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) . In particular,
for k = K + 1 we have that with probability at least 1 ‚àí Œ≤

Gap R (x ^{K}
avg ^{)} ‚â§

Finally, if

Œ≥ = min

Ô£±
Ô£≤

1
,
Ô£≥ 170l ln ^{6(K+1)}

Œ≤

5R 2
.
Œ≥(K + 1)

R

Ô£º
Ô£Ω

1
Œ±‚àí1
1
Ô£æ
97200 Œ± (K + 1) Œ± œÉ ln Œ± ^{6(K+1)}
Œ≤

76

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

then with probability at least 1 ‚àí Œ≤

Ô£±
Ô£º
1
Œ±‚àí1
6(K+1)
6(K+1) Ô£Ω
2
Ô£≤
Œ±
Œ±
5
¬∑
97200
800LR
ln
œÉR
ln
5R
Œ≤
Œ≤
= max
,
Œ±‚àí1
Ô£≥
Ô£æ
Œ≥(K + 1)
K +1
(K + 1) Œ±
Ô£º
Ô£±
Ô£∂
Ô£´
K Ô£Ω
Ô£≤ lR 2 ln K œÉR ln Œ±‚àí1
Œ±
Œ≤
Œ≤
Ô£∏ .
,
O Ô£≠ max
Œ±‚àí1
Ô£æ
Ô£≥
K
K Œ±

2

Gap R (e
x K
avg ^{)} ‚â§

=

To get Gap R (e
x K
avg ) ‚â§ Œµ with probability at least 1 ‚àí Œ≤ it is sufficient to choose K such that both terms in the maximum
^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}
 Œ±

 Œ± !!

lR ^{2} lR ^{2} œÉR Œ±‚àí1
1 œÉR Œ±‚àí1
K = O
ln
ln
,
Œµ
ŒµŒ≤
Œµ
Œ≤
Œµ

that concludes the proof.

H.2. Star-Cocoercive Problems

Theorem H.4 (Case 2 in Theorem 4.2). Let Assumptions 1.1, 1.10 hold for Q = B 2R (x ‚àó ), where R ‚â• kx ^{0} ‚àí x ‚àó k, and
Ô£±
Ô£º
Ô£≤
Ô£Ω
R
1
,
0 < Œ≥ ‚â§ min
,
(316)
1
Œ±‚àí1
1
Ô£≥ 170l ln ^{4(K+1)}
Ô£æ
97200 Œ± (K + 1) Œ± œÉ ln Œ± ^{4(K+1)}
Œ≤
Œ≤

Œª k ‚â° Œª

=

R

60Œ≥ ln ^{4(K+1)}
Œ≤

,

(317)

for some K ‚â• 0 and Œ≤ ‚àà (0, 1] such that ln ^{4(K+1)}
‚â• 1. Then, after K iterations the iterates produced by clipped-SGDA
Œ≤
with probability at least 1 ‚àí Œ≤ satisfy
K
2lR ^{2}
1 X
.
(318)
kF (x k )k 2 ‚â§
K +1
Œ≥(K + 1)

k=0

In particular, when Œ≥ equals the minimum from (316), then the iterates produced by clipped-SGDA after K iterations with
probability at least 1 ‚àí Œ≤ satisfy
Ô£º Ô£∂
Ô£±
Ô£´
K Ô£Ω
K
Ô£≤ l 2 R 2 ln K lœÉR ln ^{Œ±‚àí1}
Œ±
1 X
Œ≤
Œ≤
Ô£∏ ,
,
kF (x ^{k} )k ^{2} = O Ô£≠ max
(319)
Œ±‚àí1
Ô£æ
Ô£≥ K +1
K +1
K Œ±

k=0

1
^{meaning} ^{that} ^{to} ^{achieve} K+1

K = O

K
P

k=0

kF (x ^{k} )k ^{2} ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SGDA requires

l 2 R 2 l 2 R 2
ln
,
Œµ
ŒµŒ≤



lœÉR
Œµ

Œ±
 Œ±‚àí1

ln

1
Œ≤



lœÉR
Œµ

Œ± !!
 Œ±‚àí1

iterations/oracle calls.

(320)

Proof. Again, we will closely follow the proof of Theorem D.2 from (Gorbunov et al., 2022a) and the main difference will
be reflected in the application of Bernstein inequality and estimating biases and variances of stochastic terms.

Let R k = kx ^{k} ‚àí x ‚àó k for all k ‚â• 0. As the previous result, the proof is based on on the induction argument and showing
that the iterates do not leave some ball around the solution with high probability. More precisely, for each k = 0, . . . , K + 1
we define probability event E k as follows: inequalities

kx t ‚àí x ‚àó k 2 ‚â§ 2R 2 ,

77

(321)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

hold for t = 0, 1, . . . , k simultaneously. We want to prove that P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) for all k = 0, 1, . . . , K + 1 by
induction. One of the important things is that inequalities (287) and (292) are obtained without assuming monotonicity of
by ln 4(K+1)
),
F . Thus, if we do exactly the same steps as in the proof of Theorem H.3 (up to the replacement of ln ^{6(K+1)}
Œ≤
Œ≤
we gain that

(292)

R T 2 +1 ‚â§ R 2 + 1 + 2 + 3 + 4 + 5,

(302) R ^{2}

2 ‚â§

T
X

t=0

5

3 ‚â§

R 4

(301)

œÉ t 2 ‚â§

(303) R ^{2}

,

150 ln

,
4(K+1)

5
T
X

t=0

Œ≤

,

(308) R ^{2}

5 ‚â§

5

R 4

(307)

œÉ
e t 2 ‚â§

,

150 ln ^{4(K+1)}
Œ≤

.

Moreover, we also have (see (300), (306) and our induction assumption)

TŒ≤
,
K +1

P{E T } ‚â• 1 ‚àí

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
2(K + 1)

P{E 4 } ‚â• 1 ‚àí

Œ≤
,
2(K + 1)

where

E 1

(

=

E 4

(

=

T
X

either

œÉ t 2 >

t=0

T
X

either

t=0

Thus probability event E T ‚àí1 ‚à© E 1 ‚à© E 4 implies

œÉ
e t 2 >

R 2
^{or} |1| ‚â§
5

R 4

150 ln ^{4(K+1)}
Œ≤

R 4

R 2
^{or} |4| ‚â§
5

150 ln ^{4(K+1)}
Œ≤

)

)

,

.

R T 2 +1 ‚â§ R 2 + 1 + 2 + 3 + 4 + 5 ‚â§ 2R 2 ,

and

P{E T +1 } ‚â• P{E T ‚à© E 1 ‚à© E 4 } = 1 ‚àí P{E T ‚à™ E 1 ‚à™ E 4 } ‚â• 1 ‚àí

TŒ≤
.
K +1

(322)

This finishes the inductive part of our proof, i.e. for all k = 0, 1, . . . , K + 1 we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) . In particular,
for k = K + 1 we have that with probability at least 1 ‚àí Œ≤

K

1 X
kF (x k )k 2
K +1

k=0

Finally, if

Œ≥ = min

then with probability at least 1 ‚àí Œ≤

K

1 X
kF (x k )k 2
K +1

k=0

Ô£±
Ô£≤

‚â§

2
l(R ^{2} ‚àí R K+1
) l(1 + 2 + 3 + 4 + 5)
+
Œ≥(K + 1)
Œ≥(K + 1)

‚â§

2lR ^{2}
.
Œ≥(K + 1)

(287)

1

Ô£≥ 170l ln ^{4(K+1)}

Œ≤

R

,

1

1

Œ±‚àí1

97200 Œ± (K + 1) Œ± œÉ ln Œ±

Ô£º
Ô£Ω

4(K+1) Ô£æ
Œ≤

Ô£º
Ô£±
1
4(K+1) Ô£Ω
Ô£≤ 340l 2 R 2 ln ^{4(K+1)} 2 ¬∑ 97200 Œ± lœÉR ln ^{Œ±‚àí1}
Œ±
2lR ^{2}
Œ≤
Œ≤
= max
,
‚â§
Œ±‚àí1
Ô£æ
Ô£≥
Œ≥(K + 1)
K +1
(K + 1) Œ±
Ô£º Ô£∂
Ô£±
Ô£´
K Ô£Ω
Ô£≤ l 2 R 2 ln K lœÉR ln ^{Œ±‚àí1}
Œ±
Œ≤
Œ≤
Ô£∏ .
= O Ô£≠ max
,
Œ±‚àí1
Ô£æ
Ô£≥
K
K Œ±

78

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

1
^{To} ^{get} K+1

K
P

k=0

kF (x ^{k} )k ^{2} ‚â§ Œµ with probability at least 1 ‚àí Œ≤ it is sufficient to choose K such that both terms in the

^{maximum} ^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}

K = O

l 2 R 2 l 2 R 2
ln
,
Œµ
ŒµŒ≤



lœÉR
Œµ

Œ±
 Œ±‚àí1

ln

1
Œ≤



lœÉR
Œµ

Œ± !!
 Œ±‚àí1

that concludes the proof.

H.3. Quasi-Strongly Monotone Star-Cocoercive Problems

As in the monotone case, we use another lemma from (Gorbunov et al., 2022a) that handles the deterministic part of
clipped-SGDA in the quasi-strongly monotone case.
Lemma H.5 (Lemma D.3 from (Gorbunov et al., 2022a)). Let Assumptions 1.9, 1.10 hold for Q = B 2R (x ‚àó ) = {x ‚àà R ^{d} |
kx ‚àí x ‚àó k ‚â§ 2R}, where R ‚â• kx ^{0} ‚àí x ‚àó k, and 0 < Œ≥ ‚â§ ^{1} / l . If x ^{k} lie in B 2R (x ‚àó ) for all k = 0, 1, . . . , K for some K ‚â• 0,
then the iterates produced by clipped-SGDA satisfy

kx K+1 ‚àí x ‚àó k 2

‚â§

(1 ‚àí Œ≥Œº) ^{K+1} kx ^{0} ‚àí x ‚àó k ^{2} + 2Œ≥

+Œ≥ 2

K
X

k=0

where œâ k are defined in (279).

K
X

k=0

(1 ‚àí Œ≥Œº) ^{K‚àík} hx ^{k} ‚àí x ‚àó ‚àí Œ≥F (x ^{k} ), œâ k i

(1 ‚àí Œ≥Œº) ^{K‚àík} kœâ k k ^{2} ,

(323)

Using this lemma we prove the main convergence result for clipped-SGDA in the quasi-strongly monotone case.
Theorem H.6 (Case 2 in Theorem 4.2). Let Assumptions 1.1, 1.9, 1.10, hold for Q = B 2R (x ‚àó ) = {x ‚àà R ^{d} | kx ‚àí x ‚àó k ‚â§
2R}, where R ‚â• kx ^{0} ‚àí x ‚àó k, and
)
(
1
ln(B K )
,
(324)
0 < Œ≥ ‚â§ min
,
Œº(K + 1)
400l ln ^{4(K+1)}
Œ≤
Ô£±
Ô£º
2(Œ±‚àí1)
Ô£≤
Ô£Ω
(K + 1) Œ± Œº 2 R 2


B K = max 2,
(325)
4(K+1)
Ô£≥ 5400 Œ± ^{2} œÉ 2 ln ^{2(Œ±‚àí1)}
Œ±
ln 2 (B K ) Ô£æ
Œ≤
Ô£±
Ô£º Ô£º
Ô£´
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¨
Ô£¥
Ô£¥
Ô£¥ Ô£¥
2(Œ±‚àí1)
Ô£≤
Ô£Ω
Ô£Ω
2 2
Ô£¨
Œ±
K
Œº R
(
)!
= O Ô£¨
2,
max
,
(326)
Ô£¨
 
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
Ô£≠
2 R 2
Œ±
K
2
Œº
K
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
max 2,
2(Œ±‚àí1)
Ô£≥ œÉ 2 ln Œ±
Ô£æ
Ô£æ Ô£¥
Œ≤ ln
œÉ 2 ln Œ± ( K
Œ≤ )

Œª k

=

exp(‚àíŒ≥Œº(1 + ^{k} / 2 ))R

120Œ≥ ln ^{4(K+1)}
Œ≤

,

(327)

for some K ‚â• 0 and Œ≤ ‚àà (0, 1] such that ln ^{4(K+1)}
‚â• 1. Then, after K iterations the iterates produced by clipped-SGDA
Œ≤
with probability at least 1 ‚àí Œ≤ satisfy

kx ^{K+1} ‚àí x ‚àó k ^{2} ‚â§ 2 exp(‚àíŒ≥Œº(K + 1))R ^{2} .

(328)

In particular, when Œ≥ equals the minimum from (324), then the iterates produced by clipped-SGDA after K iterations with
probability at least 1 ‚àí Œ≤ satisfy
(
)! Ô£º Ô£∂
Ô£±
Ô£´
 
2(Œ±‚àí1)
2(Œ±‚àí1)
2 2
Ô£¥
Ô£¥
Œ±
K
2
Œº
R
Ô£¥
Ô£¥
K
Ô£¥
Ô£¥
! œÉ 2 ln Œ±
max 2,
2(Œ±‚àí1)
Ô£¥
Ô£¨
Ô£¥
Œ≤ ln
Ô£Ω Ô£∑
Ô£≤
K
2
Œ±
Ô£∑
Ô£¨
œÉ ln
(
)
ŒºK
Œ≤
K
‚àó 2
2
Ô£∑ ,
Ô£¨
,
(329)
kx ‚àí x k = O Ô£¨ max R exp ‚àí
2(Œ±‚àí1)
Ô£∑
K
Ô£¥
Ô£¥
2
l
ln
Œ±
Ô£¥
Ô£¥
K
Œº
Ô£∏
Ô£≠
Œ≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
Ô£≥

79

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

meaning that to achieve kx ^{K} ‚àí x ‚àó k ^{2} ‚â§ Œµ with probability at least 1 ‚àí Œ≤ clipped-SGDA requires
!
Œ±
  ^{2}  2(Œ±‚àí1)

 Œ± !
 2  
Œ±
1 œÉ ^{2} 2(Œ±‚àí1)
l
œÉ
R 2
R
l
ln Œ±‚àí1 (B Œµ )
ln
ln
,
ln
ln
K = O
Œº
Œµ
ŒºŒ≤
Œµ
Œº 2 Œµ
Œ≤ Œº 2 Œµ

iterations/oracle calls, where

B Œµ = max

Ô£±
Ô£¥
Ô£¥
Ô£≤

Ô£¥
Ô£¥
Ô£≥

2,

Œµ ln

 

1
Œ≤

R 2

œÉ 2
Œº 2 Œµ

(330)

Ô£º
Ô£¥
Ô£¥
Ô£Ω

 .
Œ±
 2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£æ

Proof. Again, we will closely follow the proof of Theorem D.3 from (Gorbunov et al., 2022a) and the main difference will
be reflected in the application of Bernstein inequality and estimating biases and variances of stochastic terms.

Let R k = kx ^{k} ‚àíx ‚àó k for all k ‚â• 0. As in the previous results, the proof is based on the induction argument and showing that
the iterates do not leave some ball around the solution with high probability. More precisely, for each k = 0, 1, . . . , K + 1
we consider probability event E k as follows: inequalities

R t ^{2} ‚â§ 2 exp(‚àíŒ≥Œºt)R ^{2}

(331)

hold for t = 0, 1, . . . , k simultaneously. We want to prove P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) for all k = 0, 1, . . . , K + 1 by
induction. The base of the induction is trivial: for k = 0 we have R 0 ^{2} ‚â§ R ^{2} < 2R ^{2} by definition. Next, assume that for
k = T ‚àí 1 ‚â§ K the statement holds: P{E T ‚àí1 } ‚â• 1 ‚àí ^{(T} ‚àí1)Œ≤ / (K+1) . Given this, we need to prove P{E T } ‚â• 1 ‚àí ^{T} ^{Œ≤} / (K+1) .
Since R t ^{2} ‚â§ 2 exp(‚àíŒ≥Œºt)R ^{2} ‚â§ 2R ^{2} , we have x ^{t} ‚àà B 2R (x ‚àó ), where operator F is l-star-cocoersive. Thus, E T ‚àí1 implies

kF (x t )k

‚â§

(331) ‚àö

lkx ^{t} ‚àí x ‚àó k ‚â§

2l exp(‚àí ^{Œ≥Œºt} / 2 )R

(324),(327) Œª t

‚â§

2

(332)

and

kœâ t k 2

‚â§

(332) 5

2k F e Œæ (x ^{t} )k ^{2} + 2kF (x ^{t} )k ^{2} ‚â§

2

^{(327)} exp(‚àíŒ≥Œºt)R ^{2}

Œª 2 t ‚â§

4Œ≥ 2

(333)

for all t = 0, 1, . . . , T ‚àí 1, where we use that ka + bk ^{2} ‚â§ 2kak ^{2} + 2kbk ^{2} holding for all a, b ‚àà R ^{d} .

Using Lemma H.5 and (1 ‚àí Œ≥Œº) ^{T} ‚â§ exp(‚àíŒ≥ŒºT ), we obtain that E T ‚àí1 implies

R T 2

‚â§

exp(‚àíŒ≥ŒºT )R ^{2} + 2Œ≥

T
‚àí1
X

t=0

+Œ≥ 2

T
‚àí1
X

t=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít hx ^{t} ‚àí x ‚àó ‚àí Œ≥F (x ^{t} ), œâ t i

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít kœâ t k ^{2} .

To handle the sums above, we introduce a new notation:
(
‚àö
x ^{t} ‚àí x ‚àó ‚àí Œ≥F (x ^{t} ), if kx ^{t} ‚àí x ‚àó ‚àí Œ≥F (x ^{t} )k ‚â§ 2(1 + Œ≥l) exp(‚àí ^{Œ≥Œºt} / 2 )R,
Œ∑ t =
0,
otherwise,

for t = 0, 1, . . . , T ‚àí 1. This vector is bounded almost surely:
‚àö
kŒ∑ t k ‚â§ 2(1 + Œ≥l) exp(‚àí ^{Œ≥Œºt} / 2 )R
‚àö
for all t = 0, 1, . . . , T ‚àí 1. We also notice that E T ‚àí1 implies kF (x ^{t} )k ‚â§ 2l exp(‚àí ^{Œ≥Œºt} / 2 )R (due to (332)) and

kx t ‚àí x ‚àó ‚àí Œ≥F (x t )k

‚â§

(332)

‚â§

80

kx ^{t} ‚àí x ^{‚àó} k + Œ≥kF (x ^{t} )k
‚àö
2(1 + Œ≥l) exp(‚àí ^{Œ≥Œºt} / 2 )R

(334)

(335)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

for t = 0, 1, . . . , T ‚àí 1. In other words, E T ‚àí1 implies Œ∑ t = x ^{t} ‚àí x ‚àó ‚àí Œ≥F (x ^{t} ) for all t = 0, 1, . . . , T ‚àí 1, meaning that
from E T ‚àí1 it follows that

R T 2

‚â§ exp(‚àíŒ≥ŒºT )R ^{2} + 2Œ≥

T
‚àí1
X

t=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít hŒ∑ t , œâ t i + Œ≥ ^{2}

T
‚àí1
X

t=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít kœâ t k ^{2} .

To handle the sums appeared on the right-hand side of the previous inequality we consider unbiased and biased parts of œâ t :



def
œâ t b = F (x t ) ‚àí E Œæ 1 t F Œæ t (x t ) ,



def
œâ t u = E Œæ t F Œæ t (x t ) ‚àí F e Œæ t (x t ),

(336)

for all t = 0, . . . , T ‚àí 1. By definition we have œâ t = œâ t ^{u} + œâ t ^{b} for all t = 0, . . . , T ‚àí 1. Therefore, E T ‚àí1 implies

R T 2

‚â§ exp(‚àíŒ≥ŒºT )R ^{2} + 2Œ≥

+ 2Œ≥

|

+ 2Œ≥

|

|

T
‚àí1
X

t=0

T
‚àí1
X

t=0

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít hŒ∑ t , œâ t ^{u} i

{z

1

(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít hŒ∑ t , œâ t ^{b} i + 2Œ≥ ^{2}

{z

}

2

2

T
‚àí1
X

t=0

^{T} ‚àí1‚àít

(1 ‚àí Œ≥Œº)

{z

|

T
‚àí1
X

kœâ t u k 2 ‚àí E Œæ

4

t=0

}



(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít E Œæ kœâ t ^{u} k ^{2}

{z

}

3

T
‚àí1
X
 u 2 
2
+
2Œ≥
(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít kœâ t ^{b} k ^{2} .
kœâ t k

}

|

t=0

{z

5

(337)

}

where we also apply inequality ka + bk ^{2} ‚â§ 2kak ^{2} + 2kbk ^{2} holding for all a, b ‚àà R ^{d} to upper bound kœâ t k ^{2} . It remains to
derive good enough high-probability upper-bounds for the terms 1, 2, 3, 4, 5, i.e., to finish our inductive proof we need
to show that 1 + 2 + 3 + 4 + 5 ‚â§ exp(‚àíŒ≥ŒºT )R ^{2} with high probability. In the subsequent parts of the proof, we will
need to use many times the bounds for the norm and second moments of œâ t ^{u} and œâ t ^{b} . First, by Lemma 5.1, we have with
probability 1 that
kœâ t u k ‚â§ 2Œª t .
(338)

Moreover, since E T ‚àí1 implies that kF (x ^{t} )k ‚â§ ^{Œª} t / 2 and kF (x ^{t} )k ‚â§ ^{Œª} t / 2 for all t = 0, 1, . . . , T ‚àí 1 (see (332)), from
Lemma 5.1 we also have that E T ‚àí1 implies

for all t = 0, 1, . . . , T ‚àí 1.

Upper bound for 1.

2 Œ± œÉ Œ±
^{œâ} ^{t} ^{b} ‚â§ Œ±‚àí1 ^{,}
Œª t
h
i
2
‚â§ 18Œª ^{2‚àíŒ±}
œÉ Œ± ,
E Œæ t œâ t b
t
i
h
2
œÉ Œ± ,
E Œæ t kœâ t ^{u} k ‚â§ 18Œª ^{2‚àíŒ±}
t

(339)

(340)

(341)

By definition of œâ t ^{u} , we have E Œæ t [œâ t ^{u} ] = 0 and


E Œæ t 2Œ≥(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít hŒ∑ t , œâ t ^{u} i = 0.

Next, sum 1 has bounded with probability 1 terms:

|2Œ≥(1 ‚àí ^{Œ≥Œº)} ^{T} ^{‚àí1‚àít} hŒ∑ t ^{,} ^{œâ} t ^{u} i|

‚â§

(335),(338)

‚â§

(324),(327)

‚â§

2Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí t))kŒ∑ t k ¬∑ kœâ t ^{u} k
‚àö
4 2Œ≥(1 + Œ≥l) exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí ^{t} / 2 ))RŒª t
exp(‚àíŒ≥ŒºT )R ^{2} def
= c.
5 ln 4(K+1)
Œ≤

81

(342)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance



def
The summands also have bounded conditional variances œÉ t ^{2} = E Œæ t 4Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{2T} ‚àí2‚àí2t hŒ∑ t , œâ t ^{u} i ^{2} :

œÉ t 2

‚â§

(335)

‚â§

(324)

‚â§



E Œæ t 4Œ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí 2 ‚àí 2t))kŒ∑ t k ^{2} ¬∑ kœâ t ^{u} k ^{2}


8Œ≥ ^{2} (1 + Œ≥l) ^{2} exp(‚àíŒ≥Œº(2T ‚àí 2 ‚àí t))R ^{2} E Œæ t kœâ t ^{u} k ^{2}


10Œ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí t))R ^{2} E Œæ t kœâ t ^{u} k ^{2} .

(343)

‚àí1
^{In} ^{other} ^{words,} ^{we} ^{showed} ^{that} {2Œ≥(1‚àíŒ≥Œº) ^{T} ^{‚àí1‚àít} hŒ∑ t ^{,} ^{œâ} t ^{u} i} ^{T} t=0
is a bounded martingale difference sequence with bounded
2 T ‚àí1
conditional variances {œÉ t } t=0 . Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with X t = 2Œ≥(1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít hŒ∑ t , œâ t ^{u} i,
)R 4
parameter c as in (342), b = 15 exp(‚àíŒ≥ŒºT )R ^{2} , F = ^{exp(‚àí2Œ≥ŒºT}
4(K+1) ^{:}

300 ln

Œ≤

(

T
‚àí1
X
1
exp(‚àí2Œ≥ŒºT )R ^{4}
^{P} |1| ^{>} ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2} ^{and}
œÉ t 2 ‚â§
5
300 ln ^{4(K+1)}
t=0

Œ≤

)


‚â§ 2 exp ‚àí

b 2
2F + 2cb / 3



=

Œ≤
.
2(K + 1)

Equivalently, we have

Œ≤
,
P{E 1 } ‚â• 1 ‚àí
2(K + 1)

for E 1 =

(

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ t 2 >
300 ln ^{4(K+1)}
t=0
Œ≤

either

4

1
^{or} |1| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
5

)

. (344)

In addition, E T ‚àí1 implies that

T
‚àí1
X

t=0

œÉ t 2

(343)

‚â§

^{(341),T} ‚â§K+1

‚â§

(327)

‚â§

(324)

‚â§



E Œæ t kœâ t u k 2
10Œ≥ exp(‚àí2Œ≥ŒºT )R
exp(‚àíŒ≥Œºt)
t=0

2

exp(‚àí2Œ≥ŒºT )R ^{4}

300 ln ^{4(K+1)}
Œ≤

2Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1))

(327)

‚â§

(324)

‚â§

Œª 2‚àíŒ±
t
exp(‚àíŒ≥Œºt)
t=0

120 2‚àíŒ± ln ^{2‚àíŒ±} ^{4(K+1)}
Œ≤

‚â§

‚â§

K
X

180Œ≥ ^{Œ±} exp(‚àí2Œ≥ŒºT )R ^{4‚àíŒ±} œÉ ^{Œ±} (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

From E T ‚àí1 it follows that

(335),(339)

T
‚àí1
X

180Œ≥ ^{2} exp(‚àí2Œ≥ŒºT )R ^{2} œÉ ^{Œ±}

Upper bound for 2.

2

2

.

(345)

T
‚àí1
X

kŒ∑ t k ¬∑ kœâ t b k
exp(‚àíŒ≥Œºt)
t=0

T
‚àí1
X
‚àö
2 ^{1+Œ±} 2Œ≥(1 + Œ≥l) exp(‚àíŒ≥Œº(T ‚àí 1))RœÉ ^{Œ±}

1

Œª ^{Œ±‚àí1} exp(‚àí Œ≥Œºt / 2 )
t=0 ^{t}



‚àö
ln Œ±‚àí1 4(K+1)
2 ^{3+Œ±} ¬∑ 120 ^{Œ±‚àí1} 2Œ≥ ^{Œ±} (1 + Œ≥l) exp(‚àíŒ≥ŒºT )œÉ ^{Œ±} (K + 1) exp ^{Œ≥ŒºŒ±T}
2
Œ≤

R Œ±‚àí2

1
exp(‚àíŒ≥ŒºT )R ^{2} .
5

(346)

82

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

Upper bound for 3.

From E T ‚àí1 it follows that



E Œæ t kœâ t u k 2
2Œ≥ exp(‚àíŒ≥Œº(T ‚àí 1))
exp(‚àíŒ≥Œºt)
t=0

=

3

(341)

T
‚àí1
X

Œª 2‚àíŒ±
t
exp(‚àíŒ≥Œºt)
t=0

‚â§

144Œ≥ ^{2} exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{Œ±}

(327)

144Œ≥ ^{Œ±} R ^{2‚àíŒ±} exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{Œ±} (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

‚â§

120 2‚àíŒ± ln ^{2‚àíŒ±} ^{4(K+1)}
Œ≤

1
exp(‚àíŒ≥ŒºT )R ^{2} .
5

(324)

‚â§

Upper bound for 4.

T
‚àí1
X

2

(347)

First, we have




2Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít E Œæ t kœâ t ^{u} k ^{2} ‚àí E Œæ t kœâ t ^{u} k ^{2} = 0.

Next, sum 4 has bounded with probability 1 terms:



2Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít kœâ t ^{u} k ^{2} ‚àí E Œæ t kœâ t ^{u} k ^{2}

(338)

‚â§

(327)

‚â§

def

=

16Œ≥ ^{2} exp(‚àíŒ≥ŒºT )Œª ^{2} l
exp(‚àíŒ≥Œº(1 + t))
exp(‚àíŒ≥ŒºT )R ^{2}

5 ln 4(K+1)
Œ≤

c.

(348)

The summands also have conditional variances
h
 2 i

def
œÉ
e t ^{2} = E Œæ t 4Œ≥ ^{4} (1 ‚àí Œ≥Œº) ^{2T} ‚àí2‚àí2t kœâ t ^{u} k ^{2} ‚àí E Œæ t kœâ t ^{u} k ^{2}

that are bounded

œÉ
e t 2

(348)

‚â§

‚â§

2Œ≥ ^{2} exp(‚àí2Œ≥ŒºT )R ^{2}

5 exp(‚àíŒ≥Œº(1 + t)) ln ^{4(K+1)}
Œ≤
2
2

4Œ≥ exp(‚àí2Œ≥ŒºT )R

5 exp(‚àíŒ≥Œº(1 + t)) ln ^{4(K+1)}
Œ≤

E Œæ t



 

kœâ t u k 2 ‚àí E Œæ t kœâ t u k 2



E Œæ t kœâ t u k 2 .

(349)



 T ‚àí1
In other words, we showed that 2Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít kœâ t ^{u} k ^{2} ‚àí E Œæ t kœâ t ^{u} k ^{2} t=0 is a bounded martingale difference
‚àí1
sequence with bounded conditional variances {e
^{œÉ} t ^{2} } ^{T} t=0
. Next, we apply Bernstein‚Äôs inequality (Lemma B.2) with X t =


)R 4
2Œ≥ ^{2} (1 ‚àí Œ≥Œº) ^{T} ‚àí1‚àít kœâ t ^{u} k ^{2} ‚àí E Œæ t kœâ t ^{u} k ^{2} , parameter c as in (348), b = 15 exp(‚àíŒ≥ŒºT )R ^{2} , G = ^{exp(‚àí2Œ≥ŒºT}
4(K+1) ^{:}

300 ln

(

T ‚àí1

X
exp(‚àí2Œ≥ŒºT )R ^{4}
1
œÉ
e t 2 ‚â§
^{P} |4| ^{>} ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2} ^{and}
5
294 ln ^{4(K+1)}

l=0

Œ≤

)



b 2
‚â§ 2 exp ‚àí
2G + 2cb / 3



=

Œ≤

Œ≤
.
2(K + 1)

Equivalently, we have

Œ≤
P{E 4 } ‚â• 1 ‚àí
,
2(K + 1)

for E 4 =

(

either

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ
e t 2 >
300 ln ^{4(K+1)}
t=0
Œ≤

83

4

1
^{or} |4| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
5

)

. (350)

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

In addition, E T ‚àí1 implies that

T
‚àí1
X

l=0

œÉ
e t 2



T ‚àí1
4Œ≥ ^{2} exp(‚àíŒ≥Œº(2T ‚àí 1))R ^{2} X E Œæ t kœâ l ^{u} k ^{2}
exp(‚àíŒ≥Œºt)
5 ln 4(K+1)
t=0

(349)

‚â§

Œ≤

^{(341),T} ‚â§K+1

‚â§

72Œ≥ ^{Œ±} exp(‚àíŒ≥Œº(2T ‚àí 1))R ^{4‚àíŒ±} œÉ ^{Œ±} (K + 1) exp( ^{Œ≥ŒºŒ±K}
2 )

‚â§

5 ¬∑ 120 2‚àíŒ± ln ^{3‚àíŒ±} ^{4(K+1)}
Œ≤

exp(‚àí2Œ≥ŒºT )R ^{4}

(324)

‚â§

5

=

2Œ≥ 2

‚â§

^{(327),T} ‚â§K+1

‚â§

‚â§

‚â§

(324)

‚â§

300 ln ^{4(K+1)}
Œ≤

.

(351)

From E T ‚àí1 it follows that

T
‚àí1
X

l=0

(339)

Œª 2‚àíŒ±
t
exp(‚àíŒ≥Œºt)
t=0

5 ln 4(K+1)
Œ≤

(327)

Upper bound for 5.

K

72Œ≥ exp(‚àíŒ≥Œº(2T ‚àí 1))R ^{2} œÉ ^{Œ±} X

2

exp(‚àíŒ≥Œº(T ‚àí 1 ‚àí t)) kœâ t ^{b} k ^{2}

2 ¬∑ 2 ^{2Œ±} Œ≥ ^{2} exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{2Œ±}

T
‚àí1
X



1

Œª ^{2Œ±‚àí2} exp(‚àíŒ≥Œºt)
t=0 ^{t}



t
exp(Œ≥Œºt)
exp Œ≥Œº(2Œ± ‚àí 2) 1 +
2
t=0



K
X
2 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} exp(‚àíŒ≥Œº(T ‚àí 1))œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{4(K+1)}
Œ≤

R 2Œ±‚àí2

K
X
4 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} exp(‚àíŒ≥Œº(T ‚àí 3))œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{4(K+1)}
Œ≤

R 2Œ±‚àí2

exp(Œ≥ŒºŒ±t)

t=0

(K + 1) exp(Œ≥ŒºŒ±K)
4 ¬∑ 2 ^{2Œ±} ¬∑ 120 ^{2Œ±‚àí2} Œ≥ ^{2Œ±} exp(‚àíŒ≥Œº(T ‚àí 3))œÉ ^{2Œ±} ln ^{2Œ±‚àí2} ^{4(K+1)}
Œ≤

R 2Œ±‚àí2

1
exp(‚àíŒ≥ŒºT )R ^{2} .
5

(352)

Now, we have the upper bounds for 1, 2, 3, 4, 5. In particular, probability event E T ‚àí1 implies

(337)

R T ^{2} ‚â§ exp(‚àíŒ≥ŒºT )R ^{2} + 1 + 2 + 3 + 4 + 5,

(346) 1

2 ‚â§

5

(352) 1

5 ‚â§

T
‚àí1
X

t=0

5

300 ln ^{4(K+1)}
Œ≤

3 ‚â§

5

exp(‚àíŒ≥ŒºT )R ^{2} ,

exp(‚àíŒ≥ŒºT )R ^{2}

^{(345)} exp(‚àí2Œ≥ŒºT )R ^{4}

œÉ t 2 ‚â§

(347) 1

exp(‚àíŒ≥ŒºT )R ^{2} ,

T
‚àí1
X

,

t=0

^{(351)} exp(‚àí2Œ≥ŒºT )R ^{4}

œÉ
e t 2 ‚â§

300 ln ^{4(K+1)}
Œ≤

Moreover, we also have (see (344), (350) and our induction assumption)

P{E T ‚àí1 } ‚â• 1 ‚àí

P{E 1 } ‚â• 1 ‚àí

Œ≤
,
2(K + 1)

(T ‚àí 1)Œ≤
,
K +1

P{E 4 } ‚â• 1 ‚àí

84

Œ≤
.
2(K + 1)

.

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

where

E 1

=

E 4

=

(

(

either

either

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ t 2 >
300 ln ^{4(K+1)}
t=0
Œ≤

4

T
‚àí1
X

exp(‚àí2Œ≥ŒºT )R
œÉ
e t 2 >
300 ln ^{4(K+1)}
t=0
Œ≤

4

1
^{or} |1| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
5

1
^{or} |4| ‚â§ ^{exp(‚àíŒ≥ŒºT} ^{)R} ^{2}
5

)

)

,

.

Thus, probability event E T ‚àí1 ‚à© E 1 ‚à© E 4 implies

R T 2

(337)

exp(‚àíŒ≥ŒºT )R ^{2} + 1 + 2 + 3 + 4 + 5
2 exp(‚àíŒ≥ŒºT )R ^{2} ,

‚â§
‚â§

which is equivalent to (331) for t = T , and

P{E T } ‚â• P{E T ‚àí1 ‚à© E 1 ‚à© E 4 } = 1 ‚àí P{E T ‚àí1 ‚à™ E 1 ‚à™ E 4 } ‚â• 1 ‚àí

TŒ≤
.
K +1

This finishes the inductive part of our proof, i.e., for all k = 0, 1, . . . , K + 1 we have P{E k } ‚â• 1 ‚àí ^{kŒ≤} / (K+1) . In particular,
for k = K + 1 we have that with probability at least 1 ‚àí Œ≤

kx ^{K+1} ‚àí x ‚àó k ^{2} ‚â§ 2 exp(‚àíŒ≥Œº(K + 1))R ^{2} .

Finally, if

Œ≥

=

B K

=

=

)
ln(B K )
,
,
min
Œº(K + 1)
400l ln ^{4(K+1)}
Œ≤
Ô£º
Ô£±
2(Œ±‚àí1)
Ô£Ω
Ô£≤
2 2
Œ±
Œº R
(K + 1)


max 2,
4(K+1)
Ô£≥ 5400 Œ± ^{2} œÉ 2 ln ^{2(Œ±‚àí1)}
Œ±
ln 2 (B K ) Ô£æ
Œ≤
Ô£±
Ô£º Ô£º
Ô£´
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£¨
Ô£¥
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£≤
Ô£Ω
Ô£Ω
2
2
Ô£¨
Œ±
K
Œº
R
Ô£¨
)!
(
O Ô£¨ max 2,
 
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£¥
Ô£≠
Œ±
Œº 2 R 2
K
2
Ô£¥
Ô£¥
Ô£¥
K
Ô£¥
Ô£¥
Ô£¥
max
2,
ln
2(Œ±‚àí1)
Ô£≥ œÉ 2 ln Œ±
Ô£æ
Ô£æ
Œ≤
K
2
œÉ ln Œ± ( Œ≤ )

(

1

then with probability at least 1 ‚àí Œ≤

kx K+1 ‚àí x ‚àó k 2

‚â§

=

=

2 exp(‚àíŒ≥Œº(K + 1))R ^{2}
(
)
!
Œº(K + 1)
1
2
2R max exp ‚àí
,
B K
400l ln ^{4(K+1)}
Œ≤
)! Ô£º Ô£∂
(
Ô£±
Ô£´
 
2(Œ±‚àí1)
2(Œ±‚àí1)
2 2
Ô£¥
Ô£¥
Œ±
2
Ô£¥
Ô£¥
K
Ô£¥
Ô£¥
! œÉ 2 ln Œ±
max 2, ^{K} 2(Œ±‚àí1) ^{Œº} ^{R}
Ô£¥
Ô£¨
Ô£¥
Œ≤ ln
Ô£Ω Ô£∑
Ô£≤
K
2
Ô£∑
Ô£¨
œÉ ln Œ± ( Œ≤ )
ŒºK
2
Ô£∑ .
Ô£¨
,
O Ô£¨ max R exp ‚àí
2(Œ±‚àí1)
Ô£∑
K
Ô£¥
Ô£¥
2
l
ln
Œ±
Ô£¥
Ô£¥
K
Œº
Ô£∏
Ô£≠
Œ≤
Ô£¥
Ô£¥
Ô£¥
Ô£¥
Ô£æ
Ô£≥

To get kx ^{K+1} ‚àí x ‚àó k ^{2} ‚â§ Œµ with probability at least 1 ‚àí Œ≤ it is sufficient to choose K such that both terms in the maximum
^{above} ^{are} O(Œµ). ^{This} ^{leads} ^{to}
!
Œ±
  ^{2}  2(Œ±‚àí1)

 Œ± !
 2  
Œ±
l
œÉ
R 2
R
1 œÉ ^{2} 2(Œ±‚àí1)
l
ln
,
ln Œ±‚àí1 (B Œµ ) ,
ln
ln
ln
K = O
Œº
Œµ
ŒºŒ≤
Œµ
Œº 2 Œµ
Œ≤ Œº 2 Œµ

85

-----
High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance

where

B Œµ = max

This concludes the proof.

Ô£±
Ô£¥
Ô£¥
Ô£≤

Ô£¥
Ô£¥
Ô£≥

2,

Œµ ln

 

1
Œ≤

86

R 2

œÉ 2
Œº 2 Œµ

Ô£º
Ô£¥
Ô£¥
Ô£Ω

 .
Œ±
 2(Œ±‚àí1)
Ô£¥
Ô£¥
Ô£æ

-----